virtually enhancing the perception of user actions . <eos> this paper proposes using virtual reality to enhance the perception of actions by distant users on a shared application . here , distance may refer either to space ( e.g. in a remote synchronous collaboration ) or time ( e.g. during playback of recorded actions ) . our approach consists in immersing the application in a virtual inhabited 3d space and mimicking user actions by animating avatars . we illustrate this approach with two applications , the one for remote collaboration on a shared application and the other to playback recorded sequences of user actions . we suggest this could be a low cost enhancement for telepresence .
dynamic range improvement of multistage multibit sigma delta modulator for low oversampling ratios . <eos> this paper presents an improved architecture of the multistage multibit sigma delta modulators ( eams ) for wide band applications . our approach is based on two resonator topologies , high q cascade of resonator with feedforward ( hqcrff ) and low q cascade of integrator with feedforward ( lqceff ) . because of in band zeros introduced by internal loop filters , the proposed architecture enhances the suppression of the in band quantization noise at a low osr . the hqcrff based modulator with single bit quantizer has two modes of operation , modulation and oscillation . when the hqcrff based modulator is operating in oscillation mode , the feedback path from the quantizer output to the input summing node is disabled and hence the modulator output is free of the quantization noise terms . although operating in oscillation mode is not allowed for single stage sigmadeltam , the oscillation of hqcrff based modulator can improve dynamic range ( dr ) of the multistage ( mash ) sigmadeltam . the key to improving dr is to use hqcrff based modulator in the first stage and have the first stage oscillated . when the first stage oscillates , the coarse quantization noise vanishes and hence circuit nonidealities , such as finite op amp gain and capacitor mismatching , do not cause leakage quantization noise problem . according to theoretical and numerical analysis , the proposed mash architecture can inherently have wide dr without using additional calibration techniques .
an ontology modelling perspective on business reporting . <eos> in this paper , we discuss the motivation and the fundamentals of an ontology representation of business reporting data and metadata structures as defined in the extensible business reporting language ( xbrl ) standard . the core motivation for an ontology representation is the enhanced potential for integrated analytic applications that build on quantitative reporting data combined with structured and unstructured data from additional sources . applications of this kind will enable significant enhancements in regulatory compliance management , as they enable business analytics combined with inference engines for statistical , but also for logical inferences . in order to define a suitable ontology representation of business reporting language structures , an analysis of the logical principles of the reporting metadata taxonomies and further classification systems is presented . based on this analysis , a representation of the generally accepted accounting principles taxonomies in xbrl by an ontology provided in the web ontology language ( owl ) is proposed . an additional advantage of this representation is its compliance with the recent ontology definition metamodel ( odm ) standard issued by omg .
the self organizing map . <eos> an overview of the self organizing map algorithm , on which the papers in this issue are based , is presented in this article .
the amygdala and development of the social brain . <eos> the amygdala comprises part of an extended network of neural circuits that are critically involved in the processing of socially salient stimuli . such stimuli may be explicitly social , such as facial expressions , or they may be only tangentially social , such as abstract shapes moving with apparent intention relative to one another . the coordinated interplay between neural activity in the amygdala and other brain regions , especially the medial prefrontal cortex , the occipitofrontal cortex , the fusiform gyrus , and the superior temporal sulcus , allows us to develop social responses and to engage in social behaviors appropriate to our species . the harmonious functioning of this integrated social cognitive network may be disrupted by congenital or acquired lesions , by genetic anomalies , and by exceptional early experiences . each form of disruption is associated with a slightly different outcome , dependent on the timing of the experience , the location of the lesion , or the nature of the genetic anomaly . studies in both humans and primates concur the dysregulation of basic emotions , especially the processing of fear and anger , is an almost invariable consequence of such disruption . these , in turn , have direct or indirect consequences for social behavior .
modeling and cost analysis of an improved movement based location update scheme in wireless communication networks . <eos> a movement based location update ( mblu ) scheme is an lu scheme , under which a user equipment ( ue ) performs an lu when the number of cells crossed by the ue reaches a movement threshold . the mblu scheme suffers from ping pong lu effect . the ping pong lu effect arises when the ue that moves repetitively between two adjacent cells performs lus in the same way as in the case of straight movement . to tackle the ping pong lu effect encountered by the original mblu ( omblu ) scheme , an improved mblu ( imblu ) scheme was proposed in the literature . under the imblu scheme , the ue performs an lu when the number of different cells , rather than the number of cells , visited by the ue reaches the movement threshold . in this paper we develop an embedded markov chain model to calculate the signaling cost of the imblu scheme . we derive analytical formulas for the signaling cost , whose accuracy is tested through simulation . it is observed from a numerical study based on these formulas that <digit> ) the signaling cost is a downward convex function of the movement threshold , implying the existence of an optimal movement threshold that can minimize the signaling cost , and that <digit> ) the reduction in the signaling cost achieved by the imblu scheme relative to the omblu scheme is more prominent in the case of stronger repetitiveness in the ue movement . the model developed and the formulas derived in this paper can guide the implementation of the imblu scheme in wireless communication networks .
a modified offset roll printing for thin film transistor applications . <eos> in order to realize a high resolution and high throughput printing method for thin film transistor application , a modified offset roll printing was studied . this roll printing chiefly consists of a blanket with low surface energy and a printing plate ( clich ) with high surface energy . in this study , a finite element analysis was done to predict the blanket deformation and to find the optimal angle of clichs sidewall . various etching methods were investigated to obtain a high resolution clich and the surface energy of the blanket and clich was analyzed for ink transfer . a high resolution clich with the sidewall angle of <digit> and the intaglio depth of <digit> m was fabricated by the deep reactive ion etching method . based on the surface energy analysis , we extracted the most favorable condition to transfer inks from a blanket to a clich , and thus thin films were deposited on a si clich to increase the surface energy . through controlling roll speed and pressure , two inks , etch resist and silver paste , were printed on a rigid substrate , and the fine patterns of <digit> m width and <digit> m line spacing were achieved . by using this printing process , the top gate amorphous indiumgalliumzinc oxide tfts with channel width length of <digit> <digit> m were successfully fabricated by printing etch resists .
on the complement graph and defensive k k alliances . <eos> in this paper , we obtain several tight bounds of the defensive k k alliance number in the complement graph from other parameters of the graph . in particular , we investigate the relationship between the alliance numbers of the complement graph and the minimum and maximum degree , the domination number and the isoperimetric number of the graph . moreover , we prove the np completeness of the decision problem underlying the defensive k k alliance number .
hyperspectral image segmentation through evolved cellular automata . <eos> efficient segmentation of hyperspectral images through the use of cellular automata . the rule set for the ca is automatically obtained using an evolutionary algorithm . synthetic images of much lower dimensionality are used to evolve the automata . the ca works with spectral information but do not project it onto a lower dimension . the ca based classification outperforms reference techniques .
analytical and empirical evaluation of the impact of gaussian noise on the modulations employed by bluetooth enhanced data rates . <eos> bluetooth ( bt ) is a leading technology for the deployment of wireless personal area networks and body area networks . versions 2.0 and 2.1 of the standard , which are massively implemented in commercial devices , improve the throughput of the bt technology by enabling the so called enhanced data rates ( edr ) . edrs are achieved by utilizing new modulation techniques ( <digit> dqpsk and <digit> dpsk ) , apart from the typical gaussian frequency shift keying modulation supported by previous versions of bt . this manuscript presents and validates a model to characterize the impact of white noise on the performance of these modulations . the validation is systematically accomplished in a testbed with actual bt interfaces and a calibrated white noise generator .
spectral analysis of irregularly sampled data paralleling the regularly sampled data approaches . <eos> the spectral analysis of regularly sampled ( rs ) data is a well established topic , and many useful methods are available for performing it under different sets of conditions . the same can not be said about the spectral analysis of irregularly sampled ( is ) data despite a plethora of published works on this topic , the choice of a spectral analysis method for is data is essentially limited , on either technical or computational grounds , to the periodogram and its variations . in our opinion this situation is far from satisfactory , given the importance of the spectral analysis of is data for a considerable number of applications in such diverse fields as engineering , biomedicine , economics , astronomy , seismology , and physics , to name a few . in this paper we introduce a number of is data approaches that parallel the methods most commonly used for spectral analysis of rs data the periodogram ( per ) , the capon method ( cap ) , the multiple signal characterization method ( music ) , and the estimation of signal parameters via rotational invariance technique ( esprit ) . the proposed is methods are as simple as their rs counterparts , both conceptually and computationally . in particular , the fast algorithms derived for the implementation of the rs data methods can be used mutatis mutandis to implement the proposed parallel is methods . moreover , the expected performance based ranking of the is methods is the same as that of the parallel rs methods all of them perform similarly on data consisting of well separated sinusoids in noise , music and esprit outperform the other methods in the case of closely spaced sinusoids in white noise , and cap outperforms per for data whose spectrum has a small to medium dynamic range ( music and esprit should not be used in the latter case ) .
time series data mining . <eos> in almost every scientific field , measurements are performed over time . these observations lead to a collection of organized data called time series . the purpose of time series data mining is to try to extract all meaningful knowledge from the shape of data . even if humans have a natural capacity to perform these tasks , it remains a complex problem for computers . in this article we intend to provide a survey of the techniques applied for time series data mining . the first part is devoted to an overview of the tasks that have captured most of the interest of researchers . considering that in most cases , time series task relies on the same components for implementation , we divide the literature depending on these common aspects , namely representation techniques , distance measures , and indexing methods . the study of the relevant literature has been categorized for each individual aspects . four types of robustness could then be formalized and any kind of distance could then be classified . finally , the study submits various research trends and avenues that can be explored in the near future . we hope that this article can provide a broad and deep understanding of the time series data mining research field .
a small hybrid jit for embedded systems . <eos> just in time ( jit ) compilation is a technology used to improve speed of virtual machines that support dynamic loading of applications . it is better known as a technique that accelerates java programs . current jit compilers are either huge in size or compile complete methods of the application requiring large amounts of memory for their functioning . this has made java virtual machines for embedded systems devoid of jit compilers . this paper explains a simple technique of combining interpretation with compilation to get a hybrid interpretation strategy . it also describes a new code generation technique that works using its self code . the combination gives a jit compiler that is very small ( 10k ) and suitable for java virtual machines for embedded systems .
rationality of induced ordered weighted operators based on the reliability of the source of information in group decision making . <eos> the aggregation of preference relations in group decision making ( gdm ) problems can be carried out based on either the reliability of the preference values to be aggregated , as is the case with ordered weighted averaging operators , or on the reliability of the source of information that provided the preferences , as is the case with weighted mean operators . in this paper , we address the problem of aggregation based on the reliability of the source of information , with a double aim a ) to provide a general framework for induced ordered weighted operators based upon the source of information , and b ) to provide a study of their rationality . we study the conditions which need to be verified by an aggregation operator in order to maintain the rationality assumptions on the individual preferences in the aggregation phase of the selection process of alternatives . in particular , we show that any aggregation operator based on the reliability of the source of information does verify these conditions .
digital preservation of knowledge in the public sector a pre ingest tool . <eos> this paper describes the need for coordinating pre ingest activities in digital preservation of archival records . as a result of the wide use of electronic records management systems ( erms ) in agencies , the focus is on several issues relating to the interaction of the agencys erms and public repositories . this paper indicates the importance of using digital recordkeeping metadata to meet more precisely and at the same time semi automatically the criteria set by memory institutions . the paper provides an overview of one prospective solution and describes the estonian national archives universal archiving module ( uam ) . a case study reports the use of the uam in preserving the digital records of the estonian minister for population and ethnic affairs . in this project , the preparation and transfer of archival records was divided into ten phases , starting from the description of the archival creator and ending with controlled transfer . the case study raises questions about how much recordkeeping metadata can be used in archival description and how the interaction of the agencys erms and ingest by the archives could be more automated . the main issues ( e.g. classification , metadata elements variations , mapping , and computer files conversions ) encountered during that project are discussed . findings show that the open archival information system functional models ingest part should be reconceptualised to take into account preparatory work . adding detailed metadata about the structure , context and relationships in the right place at the right time could get one step closer to digital codified knowledge archiving by creating synergies with various other digital repositories .
the regulation of serca type pumps by phospholamban and sarcolipin . <eos> both sarcolipin ( sln ) and phospholamban ( pln ) lower the apparent affinity of either serca1a or serca2a for ca2 . since sln and pln are coexpressed in the heart , interactions among these three proteins were investigated . when serca1a or serca2a were coexpressed in hek <digit> cells with both sln and pln , superinhibition resulted . the ability of sln to elevate the content of pln monomers accounts , at least in part , for the superinhibitory effects of sln in the presence of pln . to evaluate the role of sln in skeletal muscle , sln cdna was injected directly into rat soleus muscle and force characteristics were analyzed . overexpression of sln resulted in significant reductions in both twitch and tetanic peak force amplitude and maximal rates of contraction and relaxation and increased fatigability with repeated electrical stimulation . ca2 uptake in muscle homogenates was impaired , suggesting that overexpression of sln may reduce the sarcoplasmic reticulum ca2 store . sln and pln appear to bind to the same regulatory site in serca . however , in a ternary complex , pln occupies the regulatory site and sln binds to the exposed side of pln and to serca .
bootstrap confidence intervals for principal response curves . <eos> the principal response curve ( prc ) model is of use to analyse multivariate data resulting from experiments involving repeated sampling in time . the time dependent treatment effects are represented by prcs , which are functional in nature . the sample prcs can be estimated using a raw approach , or the newly proposed smooth approach . the generalisability of the sample prcs can be judged using confidence bands . the quality of various bootstrap strategies to estimate such confidence bands for prcs is evaluated . the best coverage was obtained with bca bc a intervals using a non parametric bootstrap . the coverage appeared to be generally good , except for the case of exactly zero population prcs for all conditions . then , the behaviour is irregular , which is caused by the sign indeterminacy of the prcs . the insights obtained into the optimal bootstrap strategy are useful to apply in the prc model , and more generally for estimating confidence intervals in singular value decomposition based methods .
a concurrent specification of brzozowski 's dfa construction algorithm . <eos> in this paper two concurrent versions of brzozowski 's deterministic finite automaton ( dfa ) construction algorithm are developed from first principles , the one being a slight refinement of the other . we rely on hoare 's csp as our notation . the specifications that are proposed of the brzozowski algorithm are in terms of the concurrent composition of a number of top level processes , each participating process itself composed of several other concurrent processes . after considering a number of alternatives , this particular overall architectural structure seemed like a natural and elegant mapping from the sequential algorithm 's structure . while we have carefully argued the reasons for constructing the concurrent versions as proposed in the paper , there are of course , a large range of alternative design choices that could be made . there might also be scope for a more fine grained approach to updating sets or checking for similarity of regular expressions . at this stage , we have chosen to abstract away from these considerations , and leave their exploration for a subsequent step in our research .
collaborative multimedia learning environments . <eos> i use the term collaborative , to identify a way that enables conversation to occur in , about , and around the digital medium , therefore making the digital artifacts contributed by all individuals a key element of a conversation as opposed to consecutive , linear presentations used by most faculty at the design school.installations of collaborative multimedia in classrooms at the harvard university graduate school of design show an enhancement of the learning process via shared access to media resources and enhanced spatial conditions within which these resources are engaged . through observation and controlled experiments i am investigating how the use of shared , collaborative interfaces for interaction with multiple displays in a co local environment enhances the learning process . the multiple spatial configurations and formats of learning mandate that with more effective interfaces and spaces for sharing digital media with fellow participants , the classroom can be used much more effectively and thus , learning and interaction with multimedia can be improved .
clustering multi way data via adaptive subspace iteration . <eos> clustering multi way data is a very important research topic due to the intrinsic rich structures in real world datasets . in this paper , we propose the subspace clustering algorithm on multi way data , called asi t ( adaptive subspace iteration on tensor ) . asi t is a special version of high order svd ( hosvd ) , and it simultaneously performs subspace identification using 2dsvd and data clustering using k means . the experimental results on synthetic data and real world data demonstrate the effectiveness of asi t.
sat based model checking for security protocols analysis . <eos> we present a model checking technique for security protocols based on a reduction to propositional logic . at the core of our approach is a procedure that , given a description of the protocol in a multi set rewriting formalism and a positive integer k , builds a propositional formula whose models ( if any ) correspond to attacks on the protocol . thus , finding attacks on protocols boils down to checking a propositional formula for satisfiability , problem that is usually solved very efficiently by modern sat solvers . experimental results indicate that the approach scales up to industrial strength security protocols with performance comparable with ( and in some cases superior to ) that of other state of the art protocol analysers .
existence of positive solutions for 2n <digit> n th order singular superlinear boundary value problems . <eos> this paper investigates the existence of positive solutions for 2n <digit> n th order ( n > <digit> n > <digit> ) singular superlinear boundary value problems . a necessary and sufficient condition for the existence of c2n <digit> 0,1 c <digit> n <digit> <digit> , <digit> as well as c2n <digit> 0,1 c <digit> n <digit> <digit> , <digit> positive solutions is given by constructing a special cone and with the e e norm .
embedding the internet in the lives of college students online and offline behavior . <eos> the internet is increasingly becoming embedded in the lives of most american citizens . college students constitute a group who have made particularly heavy use of the technology for everything from downloading music to distance education to instant messaging . researchers know a lot about the uses made of the internet by this group of people but less about the relationship between their offline activities and online behavior . this study reports the results of a web survey of a group of university undergraduates exploring the nature of both online and offline in five areas the use of news and information , the discussion of politics , the seeking of health information , the use of blogs , and the downloading of media and software .
a capacitive tactile sensor array for surface texture discrimination . <eos> this paper presents a silicon mems based capacitive sensing array , which has the ability to resolve forces in the sub mn range , provides directional response to applied loading and has the ability to differentiate between surface textures . texture recognition is achieved by scanning surfaces over the sensing array and assessing the frequency spectrum of the sensor outputs .
segmenting , modeling , and matching video clips containing multiple moving objects . <eos> this paper presents a novel representation for dynamic scenes composed of multiple rigid objects that may undergo different motions and are observed by a moving camera . multiview constraints associated with groups of affine covariant scene patches and a normalized description of their appearance are used to segment a scene into its rigid components , construct three dimensional models of these components , and match instances of models recovered from different image sequences . the proposed approach has been applied to the detection and matching of moving objects in video sequences and to shot matching , i.e. , the identification of shots that depict the same scene in a video clip .
weighted fuzzy interpolative reasoning systems based on interval type <digit> fuzzy sets . <eos> in this paper , we present a weighted fuzzy interpolative reasoning method for sparse fuzzy rule based systems based on interval type <digit> fuzzy sets . we also apply the proposed weighted fuzzy interpolative reasoning method to deal with the truck backer upper control problem . the proposed method satisfies the seven evaluation indices for fuzzy interpolative reasoning . the experimental results show that the proposed method outperforms the existing methods . it provides us with a useful way for dealing with fuzzy interpolative reasoning in sparse fuzzy rule based systems .
vestibular prehab . <eos> a sudden unilateral loss or impairment of vestibular function causes vertigo , dizziness , and impaired postural function . in most occasions , everyday activities supported or not by vestibular rehabilitation programs will promote a compensation and the symptoms subside . as the compensatory process requires sensory input , matching performed motor activity , both motor learning of exercises and matching to sensory input are required . if there is a simultaneous cerebellar lesion caused by the tumor or the surgery of the posterior cranial fossa , there may be a risk of a combined vestibulocerebellar lesion , with reduced compensatory abilities and with prolonged or sometimes permanent disability . on the other hand , a slow gradual loss of unilateral function occurring as the subject continues well learned everyday activities may go without any prominent symptoms . a pretreatment plan was therefore implemented before planned vestibular lesions , that is , prehab . this was first done in subjects undergoing gentamicin treatment for morbus mnire . subjects would perform vestibular exercises for <digit> days before the first gentamicin installation , and then continue doing so until free of symptoms . most subjects would only experience slight dizziness while losing vestibular function . the approachwhich is reported herewas then expanded to patients with pontine angle tumors requiring surgery , but with remaining vestibular function to ease postoperative symptoms and reduce risk of combined cerebellovestibular lesions . twelve patients were treated with prehab and had gentamicin installations transtympanically . in all cases there was a caloric loss , loss of vor in head impulse tests , and impaired subjective vertical and horizontal . spontaneous , positional nystagmus , subjective symptoms , and postural function were normalized before surgery and postoperative recovery was swift . pretreatment training with vestibular exercises continued during the successive loss of vestibular function during gentamicin treatment , and pre op gentamicin ablation of vestibular function offers a possibility to reduce malaise and speed up recovery .
sw1pers sliding windows and <digit> persistence scoring discovering periodicity in gene expression time series data . <eos> identifying periodically expressed genes across different processes ( e.g. the cell and metabolic cycles , circadian rhythms , etc ) is a central problem in computational biology . biological time series may contain ( multiple ) unknown signal shapes of systemic relevance , imperfections like noise , damping , and trending , or limited sampling density . while there exist methods for detecting periodicity , their design biases ( e.g. toward a specific signal shape ) can limit their applicability in one or more of these situations .
parallel generation of unstructured surface grids . <eos> in this paper , a new grid generation system is presented for the parallel generation of unstructured triangular surface grids . the object oriented design and implementation of the system , the internal components and the parallel meshing process itself are described . initially in a rasterisation stage , the geometry to be meshed is analysed and a smooth distribution of local element sizes in <digit> d space is set up automatically and stored in a cartesian mesh . this background mesh is used by the advancing front surface mesher as spacing definition for the triangle generation . both the rasterisation and the meshing are mpi parallelised . the underlying principles and strategies will be outlined together with the advantages and limitations of the approach . the paper will be concluded with examples demonstrating the capabilities of the presented approach .
h structured model reduction algorithms for linear discrete systems via lmi based optimisation . <eos> in this article , h structured model reduction is addressed for linear discrete systems . two important classes of systems are considered for structured model reduction , i.e. markov jump systems and uncertain systems . the problem we deal with is the development of algorithms with the flexibility to allow any structure in the reduced order system design , such as the structure of an original system , decentralisation of a networked system , pole assignment of the reduced system , etc. the algorithms are derived such that an associated model reduction error guarantees to satisfy a prescribed h norm bound constraint . a new condition for the existence of desired reduced order models preserving a certain structure is presented in a set of linear matrix inequalities ( lmi ) and non convex equality constraints . effective computational algorithms involving lmi are suggested to solve the matrix inequalities characterising a solution of the structured model reduction problem . numerical examples demonstrate the advantages of the proposed model reduction method .
a power aware code compression design for risc vliw architecture . <eos> we studied the architecture of embedded computing systems from the viewpoint of power consumption in memory systems and used a selective code compression ( scc ) approach to realize our design . based on the lzw ( lempel ziv welch ) compression algorithm , we propose a novel cost effective compression and decompression method . the goal of our study was to develop a new scc approach with an extended decision policy based on the prediction of power consumption . our decompression method had to be easily implemented in hardware and to collaborate with the embedded processor . the hardware implementation of our decompression engine uses the tsmc 0.18 mu m 2p6m model and its cell based libraries . to calculate power consumption more accurately , we used a static analysis method to estimate the power overhead of the decompression engine . we also used variable sized branch blocks and considered several features of very long instruction word ( vliw ) processors for our compression , including the instruction level parallelism ( ilp ) technique and the scheduling of instructions . our code compression methods are not limited to vliw machines , and can be applied to other kinds of reduced instruction set computer ( risc ) architecture .
globallocal negotiations for implementing configurable packages the power of initial organizational decisions . <eos> the purpose of this paper is to draw attention to the critical influence that initial organizational decisions regarding power and knowledge balance between internal members and external consultants have on the globallocal negotiation that characterizes configurable packages implementation . to do this , we conducted an intensive research study of a configurable information technology ( it ) implementation project in a canadian firm .
communication in random geometric radio networks with positively correlated random faults . <eos> we study the feasibility and time of coin communication in random geometric radio networks , where nodes fail randomly with positive correlation . we consider a set of radio stations with the same communication range , distributed in a random uniform way on a unit square region . in order to capture fault dependencies , we introduce the ranged spot model in which damaging events , called spots , occur randomly and independently on the region , causing faults in all nodes located within distance s from them . node faults within distance 2s become dependent in this model and are positively correlated . we investigate the impact of the spot arrival rate on the feasibility and the time of communication in the fault free part of the network . we provide an algorithm which broadcasts correctly with probability l epsilon in faulty random geometric radio networks of diameter d in time o ( d log l epsilon ) .
consumer complaint behaviour in telecommunications the case of mobile phone users in spain . <eos> consumer complaint behaviour theory is used to analyze spanish telecommunications data . the main determinants are the type of problem , socio demographic factors and the user s type of contract . proper complaint handling leads to satisfied , loyal and profitable consumers .
combining owl ontologies using epsilon connections . <eos> the standardization of the web ontology language ( owl ) leaves ( at least ) two crucial issues for web based ontologies unsatisfactorily resolved , namely how to represent and reason with multiple distinct , but linked ontologies , and how to enable effective knowledge reuse and sharing on the semantic web . in this paper , we present a solution for these fundamental problems based on e connections . we aim to use e connections to provide modelers with suitable means for developing web ontologies in a modular way and to provide an alternative to the owl imports construct . with such motivation , we present in this paper a syntactic and semantic extension of the web ontology language that covers e connections of owl dl ontologies . we show how to use such an extension as an alternative to the owl imports construct in many modeling situations . we investigate different combinations of the logics shin ( d ) , shon ( d ) and shio ( d ) for which it is possible to design and implement reasoning algorithms , well suited for optimization . finally , we provide support for e connections in both an ontology editor , swoop , and an owl reasoner , pellet . ( c ) <digit> elsevier b. v. all rights reserved .
model with artificial neural network to predict the relationship between the soil resistivity and dry density of compacted soil . <eos> this paper presents a technique to obtain the outcomes of soil dry density and optimum moisture contents with artificial neural network ( ann ) for compacted soil monitoring through soil resistivity measurement in geotechnical engineering . the compacted soil monitoring through soil electrical resistivity shows the important role in the construction of highway embankments , earth dams and many other engineering structure . generally , soil compaction is estimated through the determination of maximum dry density at optimum moisture contents in laboratory test . to estimate the soil compaction in conventional soil monitoring technique is time consuming and costly for the laboratory testing with a lot of samples of compacted soil . in this work , an ann model is developed for predicting the relationship between dry density of compacted soil and soil electrical resistivity based on experimental data in soil profile . the regression analysis between the output and target values shows that the r <digit> values are 0.99 and 0.93 for the training and testing sets respectively for the implementation of ann in soil profile . the significance of our research is to obtain an intelligent model for getting faster , cost effective and consistent outcomes in soil compaction monitoring through electrical resistivity for a wide range of applications in geotechnical investigation .
a fuzzy bi criteria transportation problem . <eos> in this paper , a fuzzy bi criteria transportation problem is studied . here , the model concentrates on two criteria total delivery time and total profit of transportation . the delivery times on links are fuzzy intervals with increasing linear membership functions , whereas the total delivery time on the network is a fuzzy interval with a decreasing linear membership function . on the other hand , the transporting profits on links are fuzzy intervals with decreasing linear membership functions and the total profit of transportation is a fuzzy number with an increasing linear membership function . supplies and demands are deterministic numbers . a nonlinear programming model considers the problem using the max min criterion suggested by bellman and zadeh . we show that the problem can be simplified into two bi level programming problems , which are solved very conveniently . a proposed efficient algorithm based on parametric linear programming solves the bi level problems . to explain the algorithm two illustrative examples are provided , systematically . ( c ) <digit> elsevier ltd. all rights reserved .
capacity gain of mixed multicast unicast transport schemes in a tv distribution network . <eos> this paper presents three approaches to estimate the required resources in an infrastructure where digital tv channels can be delivered in unicast or multicast ( broadcast ) mode . such situations arise for example in cable tv , iptv distribution networks or in ( future ) hybrid mobile tv networks . the three approaches presented are an exact calculation , a gaussian approximation and a simulation tool . we investigate two scenarios that allow saving bandwidth resources . in a static scenario , the most popular channels are multicast and the less popular channels rely on unicast . in a dynamic scenario , the list of multicast channels is dynamic and governed by the users ' behavior . we prove that the dynamic scenario always outperforms the static scenario . we demonstrate the robustness , versatility and the limits of our three approaches . the exact calculation application is limited because it is computationally expensive for cases with large numbers of users and channels , while the gaussian approximation is good exactly for such systems . the simulation tool takes long to yield results for small blocking probabilities . we explore the capacity gain regions under varying model parameters . finally , we illustrate our methods by discussing some realistic network scenarios using channel popularities based on measurement data as much as possible .
quantitatively evaluating the influence of online social interactions in the community assisted digital library . <eos> online social interactions are useful in information seeking from digital libraries , but how to measure their influence on the user 's information access actions has not yet been revealed . studies on this problem give us interesting insights into the workings of human dynamics in the context of information access from digital libraries . on the basis , we wish to improve the technological supports to provide more intelligent services in the ongoing china america million books digital library so that it can reach its potential in serving human needs . our research aims at developing a common framework to model online social interaction process in community assisted digital libraries . the underlying philosophy of our work is that the online social interaction can be viewed as a dynamic process , and the next state of each participant in this process ( e.g. , personal information access competency ) depends on the value of the previous states of all participants involving interactions in the period . hence , considering the dynamics of interaction process , we model each participant with a hidden markov model ( hmm ) chain and then employ the influence model , which was developed by c. asavathiratham as a dynamic bayes net ( dbn ) of representing the influences a number of markov chains have on each other , to analyze the effects of participants influencing each other . therefore , one can think of the entire interaction process as a dbn framework having two levels of structure the local level and the network level . each participant i has a local hmm chain ggr ( a ) which characterizes the transition of his internal states in the interaction process with state transition probability sum over j d ij p ( s i t s j t <digit> ) ( here states are his personal information access competence in different periods , while observations are his information access actions ) . meanwhile , the network level , which is described by a network graph ggr ( d t ) where d d ij is the influence factor matrix , represents the interacting relations between participants . the strength of each connection , d ij , describes the influence factor of the participant j at its begin on the one i at its end . hence , this model describes the dynamic inter influence process of the internal states of all participants involving online interactions . to automatically build the model , we need firstly to extract observed features from the data of online social interactions and information access actions . obviously , the effects of interactions are stronger if messages are exchanged more frequently , or the participants access more information in the online digital libraries during the period of time . based on this consideration , we select the interaction measure im i , j t and the amount of information ia j t as the estimation features of x i t . the interaction measure ia i t and the amount of information parameterize the features calculated automatically from the data of online social interactions between the participants i and j , and the features calculated from the data of information access actions respectively . secondly , we need to develop a mechanism for learning the parameters d ij and p ( s i t s j t <digit> . given sequences of observations x i t for each chain i , we may easily utilize the expectation maximization algorithm or the gradient based learning algorithm to get their estimation equations . we ran our experiments in the online digital library of w3c consortium ( www.w3c.org ) , which contains a mass of news , electronic papers or other materials related to web technologies . users may access and download any information and materials in this digital library , and also may free discuss on any related technological problems by means of its mailing lists . six users were selected in our experiments to collaboratively perform paper gathering tasks related to four given topics . any user might call for help from the others through the mailing lists when had difficulties in this process . all participants were required to record subjective evaluations of the effects that the others influenced his tasks . each experiment was scheduled by ten phases . and in each phase , we sampled im i , j t and ia i t for each participant and then fed them into the learning algorithms to automatically build the influence model . by comparing with the subjective influence graphs , the experimental results show that the influence model can estimate approximately the influences of online social interactions .
the ceo cio relationship revisited an empirical assessment of satisfaction with is . <eos> the necessity of integrating information systems ( is ) into corporate strategy has received widespread attention in recent years . strategic planning has moved is from serving primarily as a support function to a point where it may influence corporate strategy . the strength of this influence , however , usually is determined by the nature of the relationship between the chief information officer ( cio ) and the ceo . generally the more satisfied ceos are with cios , the greater the influence is has on top level decisions . results of a nationwide survey of motor carrier ceos and cios indicate that ceos are generally satisfied with their cios ' activities , and that cios perceive ceos as placing a high priority on strategic is plans . however , is does not appear to be truly a part of corporate strategy formulation .
simulations of photosynthesis by a k subset transforming system with membrane . <eos> by considering the inner regions of living cells ' membranes , p systems with inner regions are introduced . then , a new type of membrane computing systems are considered , called k subset transforming systems with membranes , which can treat nonintegral multiplicities of objects . as an application , a k subset transforming system is proposed in order to model the light reactions of the photosynthesis . the behaviour of such systems is simulated on a computer ,
technologische innovation und die auswirkung auf geschftsmodell , organisation und unternehmenskultur die transformation der ibm zum global integrierten , dienstleistungsorientierten unternehmen . <eos> im vorliegenden beitrag wird der einfluss von innovationen der informations und kommunikationstechnologie ( ikt ) auf die transformation von unternehmen untersucht . zunchst werden die allgemeinen ikt getriebenen entwicklungslinien der globalisierung und der dienstleistungsorientierung beschrieben . die nachfolgende analyse der transformation der ibm corporation ber die letzten 50jahre zu einem global integrierten , dienstleistungsorientierten unternehmen macht deutlich , dass ikt innovationen mit gleichzeitigen anpassungen des geschftsmodells , der organisation und der unternehmenskultur begegnet werden muss . die fhigkeit zu derartiger adaption gewinnt eine zunehmend zentrale bedeutung fr unternehmen .
modular robotics as a tool for education and entertainment . <eos> we developed i blocks , a modular electronic building block system and here we show how this system has proven useful , especially as an educational tool that allows hands on learning in an easy manner . through user studies we find limitations of the first i blocks system , and we show how the system can be improved by introducing a graphical user interface for authoring the contents of the individual i block . this is done by developing a new cubic block shape with new physical and electrical connectors , and by including new embedded electronics . we developed and evaluated the i blocks as a manipulative technology through studies in both schools and hospitals , and in diverse cultures such as in denmark , finland , italy and tanzania .
the selective use of redundancy for video streaming over vehicular ad hoc networks . <eos> video streaming over vehicular ad hoc networks ( vanets ) offers the opportunity to deploy many interesting services . these services , however , are strongly prone to packet loss due to the highly dynamic topology and shared wireless medium inherent in the vanets . a possible strategy to enhance the delivery rate is to use redundancy for handling packet loss . this is a suitable technique for vanets as it does not require any interaction between the source and receivers . in this work , we discuss novel approaches for the use of redundancy based on the particularities of video streaming over vanets . a thorough study on the use of redundancy through erasure coding and network coding in both video unicast and video broadcast in vanets is provided . we investigate each strategy , design novel solutions and compare their performance . we evaluated the proposed solutions from the perspective not only of cost as bandwidth utilization , but also the offered receiving rate of unique video content at the application layer . this perspective is fundamental to understanding how redundancy can be used without limiting the video quality that can be displayed to end users . furthermore , we propose the selective use of redundancy solely on data that is more relevant to the video quality . this approach offers increases in overall video quality without leading to an excessive overhead nor to a substantial decrease in the receiving rate of unique video content .
syntactic recognition of ecg signals by attributed finite automata . <eos> a syntactic pattern recognition method of electrocardiograms ( ecg ) is described in which attributed automata are used to execute the analysis of ecg signals . an ecg signal is first encoded into a string of primitives and then attributed automata are used to analyse the string . we have found that we can perform fast and reliable analysis of ecg signals by attributed automata .
lane mark extraction for automobiles under complex conditions . <eos> we proposed a vision based lane mark extraction method . we used multi adaptive thresholds for different blocks . based on the results , our method was robust for complex conditions . the proposed system could operate in real time .
cultural differences explaining the differences in results in gss implications for the next decade . <eos> for the next decade , the support that comes from group support systems ( gss ) will be increasingly directed towards culturally diversified groups . while there have been many gss studies concerning culture and cultural differences , no dedicated review of gss researches exists for the identification of current gaps and opportunities of doing cross cultural gss research . for this purpose , this paper provides a comprehensive review utilizing a taxonomy of six categories research type , gss technology used , independent variables , dependent variables , use of culture , and findings . additionally , this study also aims to illustrate how differences in experimental results arising from comparable studies , but from a different cultural setting , can be explained consistently using hofstede 's dimensions . to do so , we presented a comparative study on the use of gss in australia and singapore and explain the differences in results using hofstede 's g. hofstede , culture 's consequencesinternational differences in work related values , sage , beverly hills , ca ( <digit> ) . cultural dimensions . last , but not least , we present the implications of the impact of culture on gss research for the next decade from the viewpoint of the three gss stakeholders the facilitators , gss software designers , and the gss researchers . with the above , this paper seeks ( i ) to prepare a comprehensive map of gss research involving culture , and ( ii ) to prepare a picture of what all these mean and where we should be heading in the next decade .
building an ip based community wireless mesh network assessment of pacman as an ip address autoconfiguration protocol . <eos> wireless mesh networks are experiencing rapid progress and inspiring numerous applications in different scenarios . due to features such as autoconfiguration , self healing , connectivity coverage extension and support for dynamic topologies these particular characteristics make wireless mesh networks an appropriate architectural basis for the design of easy to deploy community or neighbourhood networks one of the main challenges in building a community network using mesh networks is the minimisation of user intervention in the ip address configuration of the network nodes in this paper we first consider the process of building an ip based mesh network using typical residential routers , exploring the options for the configuration of their wireless interfaces . then we focus on ip address autoconfiguration , identifying the specific requirements for community mesh networks and analysing the applicability of existing solutions . as a result of that analysis , we select pacman , an efficient distributed address autoconfiguration mechanism originally designed for ad hoc networks . and we perform an experimental study using off the shelf routers and assuming worst case scenarios analysing its behaviour as an ip address autoconfiguration mechanism for community wireless mesh networks the results of the conducted assessment show that pacman meets all the identified requirements of the community scenario ( c ) <digit> elsevier b v all rights reserved .
approximating partition functions of the two state spin system . <eos> two state spin system is a classical topic in statistical physics . we consider the problem of computing the partition function of the system on a bounded degree graph . based on the self avoiding tree , we prove the system exhibits strong correlation decay under the condition that the absolute value of inverse temperature is small . due to strong correlation decay property , an fptas for the partition function is presented and uniqueness of gibbs measure of the two state spin system on a bounded degree infinite graph is proved , under the same condition . this condition is sharp for ising model . ( c ) <digit> elsevier b.v. all rights reserved .
efficient memory utilization for high speed fpga based hardware emulators with sdrams . <eos> fpga based hardware . emulators are often used for the verification of lsi functions . they generally have dedicated external memories , such as sdrams , to compensate for the lack of memory capacity in fpgas . in such a case , access between the fpgas and the dedicated external memory may represent a major bottleneck with respect to emulation speed since the dedicated external memory may have to emulate a large number of memory blocks . in this paper , we propose three methods , dynamic clock control ( dcc ) , memory mapping optimization ( mmo ) , and efficient access scheduling ( eas ) , to avoid this bottleneck . dcc controls an emulation clock dynamically in accord with the number of memory accesses within one emulation clock cycle . eas optimizes the ordering of memory access to the dedicated external memory , and mmo optimizes the arrangement of the dedicated external memory addresses to which respective memories will be emulated . with them , emulation speed can be made 29.0 times faster , as evaluated in actual lsi emulations .
minimum stress optimal design with the level set method . <eos> this paper is devoted to minimum stress design in structural optimization . we propose a simple and efficient numerical algorithm for shape and topology optimization based on the level set method coupled with the topological derivative . we compute a shape derivative , as well as a topological derivative , for a stress based objective function . using an adjoint equation we implement a gradient algorithm for the minimization of the objective function . several numerical examples in <digit> d and <digit> d are discussed .
determination of wire recovery length in steel cables and its practical applications . <eos> in the presence of relatively significant states of radial pressures between the helical wires of a steel cable ( spiral strand and or wire rope ) , and significant levels of interwire friction , the individual broken wires tend to take up their appropriate share of the axial load within a certain length from the fractured end , which is called the recovery ( or development ) length . the paper presents full details of the formulations for determining the magnitude of recovery length in any layer of an axially loaded multi layered spiral strand with any construction details . the formulations are developed for cases of fully bedded in ( old ) spiral strands within which the pattern of interlayer contact forces and associated significant values of line contact normal forces between adjacent wires in any layer , are fully stabilised , and also for cases when ( in the presence of gaps between adjacent wires ) hoop line contact forces do not exist and only radial forces are present . based on a previously reported extensive series of theoretical parametric studies using a wide range of spiral strand constructions with widely different wire ( and cable ) diameters and lay angles , a very simple method ( aimed at practising engineers ) for determining the magnitude of recovery length in any layer of an axially loaded spiral strand with any type of construction details is prestented . using the final outcome of theoretical parametric studies , the minimum length of test specimens for axial fatigue tests whose test data may safely be used for estimating the axial fatigue lives of the much longer cables under service conditions may now be determined in a straightforward fashion . moreover , the control length over which one should count the number of broken wires for cable discard purposes is suggested to be equal to one recovery length whose upper bound value for both spiral strands and or wire ropes with any construction details is theoretically shown to be equal to 2.5 lay lengths .
generalized pcm coding of images . <eos> pulse code modulation ( pcm ) with embedded quantization allows the rate of the pcm bitstream to be reduced by simply removing a fixed number of least significant bits from each codeword . although this source coding technique is extremely simple , it has poor coding efficiency . in this paper , we present a generalized pcm ( gpcm ) algorithm for images that simply removes bits from each codeword . in contrast to pcm , however , the number and the specific bits that a gpcm encoder removes in each codeword depends on its position in the bitstream and the statistics of the image . since gpcm allows the encoding to be performed with different degrees of computational complexity , it can adapt to the computational resources that are available in each application . experimental results show that gpcm outperforms pcm with a gain that depends on the rate , the computational complexity of the encoding , and the degree of inter pixel correlation of the image .
influence of motor and converter non linearities on dynamic properties of dc drive with field weakening range . <eos> improvement of the dynamic properties of dc drive in the field weakening range was the aim of investigation . the non linear model of the drive system was applied . in the paper results of the comparative analysis of two emf control structures are presented . the classic emf control structure with subordinated excitation current control loop was compared with this one consisting of a non linear compensation block . for both control structures different kinds of the parameter designing for the emf and excitation controllers are considered . verification of the theoretical assumptions and synthesis methods of the investigated control structures are made by simulation tests using the pspice language .
rental software valuation in it investment decisions . <eos> the growth of application service providers ( asps ) is very rapid , leading to a number of options to organizations interested in developing new information technology services . the advantages of an asp include spreading out payments over a contract period and flexibility in terms of responding to changes in technology . likewise , newer risks are associated with asps , including pricing variability . some of the more common capital budgeting models may not be appropriate in this volatile marketplace . however , option models allow for many of the quirks to be considered . modification of the option pricing model and an analytical solution method incorporated into a spreadsheet for decision support are described and illustrated . the analytical tool allows for better decisions compared to traditional value analysis methods which do not fully account for the entry and exit options of the market .
an averaging scheme for macroscopic numerical simulation of nonconvex minimization problems . <eos> averaging or gradient recovery techniques , which are a popular tool for improved convergence or superconvergence of finite element methods in elliptic partial differential equations , have not been recommended for nonconvex minimization problems as the energy minimization process enforces finer and finer oscillations and hence at the first glance , a smoothing step appears even counterproductive . for macroscopic quantities such as the stress field , however , this counterargument is no longer true . in fact , this paper advertises an averaging technique for a surprisingly improved convergence behavior for nonconvex minimization problems . similar to a finite volume scheme , numerical experiments on a double well benchmark example provide empirical evidence of superconvergence phenomena in macroscopic numerical simulations of oscillating microstructures .
which app a recommender system of applications in markets implementation of the service for monitoring users ' interaction . <eos> users face the information overload problem when downloading applications in markets . this is mainly due to ( i ) the increasing unmanageable number of applications and ( ii ) the lack of an accurate and fine grained categorization of the applications in the markets . to address this issue , we present an integrated solution which recommends to the users applications by considering a big amount of information that is , according to their previously consumed applications , use pattern , tags used to annotate resources and history of ratings . we focus this paper on the service for monitoring users ' interaction . ( c ) <digit> elsevier ltd. all rights reserved .
new statistical features for the design of fiber optic statistical mode sensors . <eos> novel statistical features are proposed for the design of statistical mode sensors . proposed statistical features are first and second order moments . features are compared in terms of precision error , non linearity , and hysteresis .
an efficient indexing method for content based image retrieval . <eos> in this paper , we propose an efficient indexing method for content based image retrieval . the proposed method introduces the ordered quantization to increase the distinction among the quantized feature descriptors . thus , the feature point correspondences can be determined by the quantized feature descriptors , and they are used to measure the similarity between query image and database image . to implement the above scheme efficiently , a multi dimensional inverted index is proposed to compute the number of feature point correspondences , and then approximate ransac is investigated to estimate the spatial correspondences of feature points between query image and candidate images returned from the multi dimensional inverted index . the experimental results demonstrate that our indexing method improves the retrieval efficiency while ensuring the retrieval accuracy in the content based image retrieval .
prediction intervals in linear regression taking into account errors on both axes . <eos> this study reports the expressions for the variances in the prediction of the response and predictor variables calculated with the bivariate least squares ( bls ) regression technique . this technique takes into account the errors on both axes . our results are compared with those of a simulation process based on six different real data sets . the mean error in the results from the new expressions is between <digit> % and <digit> % . with weighted least squares , ordinary least squares , the constant variance ratio approach and orthogonal regression , on the other hand , mean errors can be as high as <digit> % , <digit> % , <digit> % and <digit> % respectively . an important property of the prediction intervals calculated with bls is that the results are not affected when the axes are switched . copyright ( c ) <digit> john wiley sons , ltd .
the piam approach to modular integrated assessment modelling . <eos> the next generation of integrated assessment modelling is envisaged as being organised as a modular process , in which modules encapsulating knowledge from different scientific disciplines are independently developed at distributed institutions and coupled afterwards in accordance with the question raised by the decision maker . such a modular approach needs to respect several stages of the model development process , approaching modularisation and integration on a conceptual , numerical , and technical level . the paper discusses the challenges at each level and presents partial solutions developed by the piam ( potsdam integrated assessment modules ) project at the potsdam institute for climate impact research ( pik ) . the challenges at each level differ greatly in character and in the work done addressing them . at the conceptual level , the notion of conceptual consistency of modular integrated models is discussed . at the numerical level , it is shown how an adequate modularisation of a problem from climateeconomy leads to a modular configuration into which independently developed climate and economic modules can be plugged . at the technical level , a software tool is presented which provides a simple consistent interface for data transfer between modules running on distributed and heterogeneous computer platforms .
finding multivariate outliers in fmri time series data . <eos> multivariate outlier detection methods are applicable to fmri time series data . removing outliers increases spatial specificity without hurting classification . simulation shows pcout is more sensitivity to small outliers than hd bacon .
cardinal consistency of reciprocal preference relations a characterization of multiplicative transitivity . <eos> consistency of preferences is related to rationality , which is associated with the transitivity property . many properties suggested to model transitivity of preferences are inappropriate for reciprocal preference relations . in this paper , a functional equation is put forward to model the cardinal consistency in the strength of preferences of reciprocal preference relations . we show that under the assumptions of continuity and monotonicity properties , the set of representable uninorm operators is characterized as the solution to this functional equation . cardinal consistency with the conjunctive representable cross ratio uninorm is equivalent to tanino 's multiplicative transitivity property . because any two representable uninorms are order isomorphic , we conclude that multiplicative transitivity is the most appropriate property for modeling cardinal consistency of reciprocal preference relations . results toward the characterization of this uninorm consistency property based on a restricted set of ( n <digit> ) preference values , which can be used in practical cases to construct perfect consistent preference relations , are also presented .
infomarker a new internet information service system . <eos> as the web grows , the massive increase in information is placing severe burdens on information retrieval and sharing . automated search engines and directories with small editorial staff are unable to keep up with the increasing submission of web sites . to address the problem , this paper presents infomarker an internet information service system based on open directory and zero keyword inquiry . the open directory sets up a net community in which the increasing net citizens can each organize a small portion of the web and present it to the others . by means of zero keyword inquiry , user can get the information he is interested in without inputting any keyword that is often required by search engines . in infomarker , user can record the web address he likes and can put forward an information request based on his web records . the information matching engine checks the information in the open directory to find what fits user 's needs and adds it to user 's web address records . the key to the matching process is layered keyword mapping . infomarker provides people with a whole new approach to getting information and shows a wide prospect .
grain flow measurements with x ray techniques . <eos> the use of low energy x rays , up to <digit> kev , densitometry is demonstrated for grain flow rate measurements through laboratory experiments . mass flow rates for corn were related to measured x ray intensity in gray scale units with a 0.99 correlation coefficient for flow rates ranging from <digit> to <digit> kg s. larger flow rate values can be measured by using higher energy or a higher tube current . measurements were done in real time at a <digit> hz sampling rate . flow rate measurements are relatively independent of grain moisture due to a negligible change in the x ray attenuation coefficients at typical moisture content values from <digit> to <digit> % . grain flow profile changes did not affect measurement accuracy . x rays easily capture variations in the corn thickness profile . due to the low energy of the x ray photons , biological shielding can be accomplished with <digit> mm thick lead foil or <digit> mm of steel .
dynamic performance enhancement of microgrids by advanced sliding mode controller . <eos> dynamics are the most important problems in the microgrid operation . in the islanded microgrid , the mismatch of parallel operations of inverters during dynamics can result in the instability . this paper considers severe dynamics which can occur in the microgrid . microgrid can have different configurations with different load and generation dynamics which are facing voltage disturbances . as a result , microgrid has many uncertainties and is placed in the distribution network where is full of voltage disturbances . moreover , characteristics of the distribution network and distributed energy resources in the islanded mode make microgrid vulnerable and easily lead to instability . the main aim of this paper is to discuss the suitable mathematical modeling based on microgrid characteristics and to design properly inner controllers to enhance the dynamics of microgrid with uncertain and changing parameters . this paper provides a method for inner controllers of inverter based distributed energy resources to have a suitable response for different dynamics . parallel inverters in distribution networks were considered to be controlled by nonlinear robust voltage and current controllers . theoretical prove beyond simulation results , reveal evidently the effectiveness of the proposed controller .
new identification procedure for continuous time radio frequency power amplifier model . <eos> in this paper , we present a new method for characterization of radio frequency power amplifier ( pa ) in the presence of nonlinear distortions which affect the modulated signal in radiocommunication transmission system . the proposed procedure uses a gray box model where pa dynamics are modeled with a mimo continuous filter and the nonlinear characteristics are described as general polynomial functions , approximated by means of taylor series . using the baseband input and output data , model parameters are obtained by an iterative identification algorithm based on output error method . initialization and excitation problems are resolved by an association of a new technique using initial values extraction with a multi level binary sequence input exciting all pa dynamics . finally , the proposed estimation method is tested and validated on experimental data .
manufacturing lead time rules customer retention versus tardiness costs . <eos> inaccurate production backlog information is a major cause of late deliveries , which can result in penalty fees and loss of reputation . we identify conditions when it is particularly worthwhile to improve an information system to provide good lead time information . we first analyze a sequential decision process model of lead time decisions at a firm which manufactures standard products to order , and has complete backlog information . there are poisson arrivals , stochastic processing times , customers may balk in response to quoted delivery dates , and revenues are offset by tardiness penalties . we characterize an optimal policy and show how to accelerate computations . the second part of the paper is a computational comparison of this optimum ( with full backlog information ) with a lead time quotation rule that is optimal with statistical shop status information . this reveals when the partial information method does well and when it is worth implementing measures to improve information transfer between operations and sales .
on the wiberg algorithm for matrix factorization in the presence of missing components . <eos> this paper considers the problem of factorizing a matrix with missing components into a product of two smaller matrices , also known as principal component analysis with missing data ( pcamd ) . the wiberg algorithm is a numerical algorithm developed for the problem in the community of applied mathematics . we argue that the algorithm has not been correctly understood in the computer vision community . although there are many studies in our community , almost every one of which refers to the wiberg study , as far as we know , there is no literature in which the performance of the wiberg algorithm is investigated or the detail of the algorithm is presented . in this paper , we present derivation of the algorithm along with a problem in its implementation that needs to be carefully considered , and then examine its performance . the experimental results demonstrate that the wiberg algorithm shows a considerably good performance , which should contradict the conventional view in our community , namely that minimization based algorithms tend to fail to converge to a global minimum relatively frequently . the performance of the wiberg algorithm is such that even starting with random initial values , it converges in most cases to a correct solution , even when the matrix has many missing components and the data are contaminated with very strong noise . our conclusion is that the wiberg algorithm can also be used as a standard algorithm for the problems of computer vision .
semi supervised local fisher discriminant analysis for dimensionality reduction . <eos> when only a small number of labeled samples are available , supervised dimensionality reduction methods tend to perform poorly because of overfitting . in such cases , unlabeled samples could be useful in improving the performance . in this paper , we propose a semi supervised dimensionality reduction method which preserves the global structure of unlabeled samples in addition to separating labeled samples in different classes from each other . the proposed method , which we call semi supervised local fisher discriminant analysis ( self ) , has an analytic form of the globally optimal solution and it can be computed based on eigen decomposition . we show the usefulness of self through experiments with benchmark and real world document classification datasets .
a stable fluidstructure interaction solver for low density rigid bodies using the immersed boundary projection method . <eos> dispersion of low density rigid particles with complex geometries is ubiquitous in both natural and industrial environments . we show that while explicit methods for coupling the incompressible navierstokes equations and newton 's equations of motion are often sufficient to solve for the motion of cylindrical particles with low density ratios , for more complex particles such as a body with a protrusion they become unstable . we present an implicit formulation of the coupling between rigid body dynamics and fluid dynamics within the framework of the immersed boundary projection method . similarly to previous work on this method , the resulting matrix equation in the present approach is solved using a block lu decomposition . each step of the block lu decomposition is modified to incorporate the rigid body dynamics . we show that our method achieves second order accuracy in space and first order in time ( third order for practical settings ) , only with a small additional computational cost to the original method . our implicit coupling yields stable solution for density ratios as low as <digit> <digit> . we also consider the influence of fictitious fluid located inside the rigid bodies on the accuracy and stability of our method .
latent word context model for information retrieval . <eos> the application of word sense disambiguation ( wsd ) techniques to information retrieval ( ir ) has yet to provide convincing retrieval results . major obstacles to effective wsd in ir include coverage and granularity problems of word sense inventories , sparsity of document context , and limited information provided by short queries . in this paper , to alleviate these issues , we propose the construction of latent context models for terms using latent dirichletallocation . we propose building one latent context per word , using a well principled representation of local context based on word features . in particular , context words are weighted using a decaying function according to their distance to the target word , which is learnt from data in an unsupervised manner . the resulting latent features are used to discriminate word contexts , so as to constrict querys semantic scope . consistent and substantial improvements , including on difficult queries , are observed on trec test collections , and the techniques combines well with blind relevance feedback . compared to traditional topic modeling , wsd and positional indexing techniques , the proposed retrieval model is more effective and scales well on large scale collections .
clusterization , frustration and collectivity in random networks . <eos> we consider the random erdos renyi network with enhanced clusterization and ising spins s <digit> at the network nodes . mutually linked spins interact with energy j. magnetic properties of the system that are dependent on the clustering coefficient c are investigated with the monte carlo heat bath algorithm . for j > <digit> the curie temperature t ( c ) increases from 3.9 to 5.5 when c increases from almost zero to 0.18 . these results deviate only slightly from the mean field theory . for j < <digit> the spin glass phase appears below t ( sg ) this temperature decreases with c , on the contrary to the mean field calculations . the results are interpreted in terms of social systems .
gps ins integration utilizing dynamic neural networks for vehicular navigation . <eos> recently , methods based on artificial intelligence ( ai ) have been suggested to provide reliable positioning information for different land vehicle navigation applications integrating the global positioning system ( gps ) with the inertial navigation system ( ins ) . all existing ai based methods are based on relating the ins error to the corresponding ins output at certain time instants and do not consider the dependence of the error on the past values of ins . this study , therefore , suggests the use of input delayed neural networks ( idnn ) to model both the ins position and velocity errors based on current and some past samples of ins position and velocity , respectively . this results in a more reliable positioning solution during long gps outages . the proposed method is evaluated using road test data of different trajectories while both navigational and tactical grade ins are mounted inside land vehicles and integrated with gps receivers . the performance of the idnn based model is also compared to both conventional ( based mainly on kalman filtering ) and recently published al based techniques . the results showed significant improvement in positioning accuracy especially for cases of tactical grade ins and long gps outages . ( c ) <digit> elsevier b.v. all rights reserved .
a unified probabilistic framework for automatic 3d facial expression analysis based on a bayesian belief inference and statistical feature models . <eos> textured 3d face models capture precise facial surfaces along with the associated textures , making it possible for an accurate description of facial activities . in this paper , we present a unified probabilistic framework based on a novel bayesian belief network ( bbn ) for 3d facial expression and action unit ( au ) recognition . the proposed bbn performs bayesian inference based on statistical feature models ( sfm ) and gibbs boltzmann distribution and feature a hybrid approach in fusing both geometric and appearance features along with morphological ones . when combined with our previously developed morphable partial face model ( sfam ) , the proposed bbn has the capacity of conducting fully automatic facial expression analysis . we conducted extensive experiments on the two public databases , namely the bu 3dfe dataset and the bosphorus dataset . when using manually labeled landmarks , the proposed framework achieved an average recognition rate of 94.2 % and 85.6 % for the <digit> and <digit> au on face data from the bosphorus dataset respectively , and 89.2 % for the six universal expressions on the bu 3dfe dataset . using the landmarks automatically located by sfam , the proposed bbn still achieved an average recognition rate of 84.9 % for the six prototypical facial expressions . these experimental results demonstrate the effectiveness of the proposed approach and its robustness in landmark localization errors . published by elsevier b.v.
mpml3d scripting agents for the 3d internet . <eos> the aim of this paper is two fold . first , it describes a scripting language for specifying communicative behavior and interaction of computer controlled agents ( bots ) in the popular three dimensional ( 3d ) multiuser online world of second life and the emerging opensimulator project . while tools for designing avatars and in world objects in second life exist , technology for nonprogrammer content creators of scenarios involving scripted agents is currently missing . therefore , we have implemented new client software that controls bots based on the multimodal presentation markup language 3d ( mpml3d ) , a highly expressive xml based scripting language for controlling the verbal and nonverbal behavior of interacting animated agents . second , the paper compares second life and opensimulator platforms and discusses the merits and limitations of each from the perspective of agent control . here , we also conducted a small study that compares the network performance of both platforms .
tlb and snoop energy reduction using virtual caches in low power chip multiprocessors . <eos> in our quest to bring down the power consumption in low power chip multiprocessors , we have found that tlb and snoop accesses account for about <digit> % of the energy wasted by all l1 data cache accesses . we have investigated the prospects of using virtual caches to bring down the number of tlb accesses . a key observation is that while the energy wasted in the tlbs are cut , the energy associated with snoop accesses becomes higher . we then contribute with two techniques to reduce the number of snoop accesses and their energy cost . virtual caches together with the proposed techniques are shown to reduce the energy wasted in the l1 caches and the tlbs by about <digit> % .
extracting tennis statistics from wireless sensing environments . <eos> creating statistics from sporting events is now widespread with most efforts to automate this process using various sensor devices . the problem with many of these statistical applications is that they require proprietary applications to process the sensed data and there is rarely an option to express a wide range of query types . instead , applications tend to contain built in queries with predefined outputs . in the research presented in this paper , data from a wireless network is converted to a structured and highly interoperable format to facilitate user queries by expressing high level queries in a standard database language and automatically generating the results required by coaches .
critical success factors of inter organizational information systems a case study of cisco and xiao tong in china . <eos> this paper reports a case study of an inter organizational information system ( ios ) of cisco and xiao tong in china . we interviewed their senior managers , heads of departments and employees who have been directly affected in their work . other sources of information are company documents and publicly available background information . the study examines the benefits of the ios for both corporations . the research also reveals seven critical success factors for the ios , namely intensive stimulation , shared vision , cross organizational implementation team , high integration with internal information systems , inter organizational business process re engineering , advanced legacy information system and infrastructure and shared industry standard . ( c ) <digit> elsevier b.v. all rights reserved .
maize grain shape approaches for dem modelling . <eos> the shape of a grain of maize was approached using the multi sphere method . models with single spherical particles and with rolling friction were also used . results from two dem software codes were compared . recommendations on the shape approach for dem modelling were provided .
multi sector antenna performance in dense wireless networks . <eos> sectorized antennas provide an attractive solution to increase wireless network capacity through higher spatial reuse . despite their increasing popularity , the real world performance characteristics of such antennas in dense wireless mesh networks are not well understood . in this demo , we demonstrate our multi sector antenna prototypes and their performance through video streaming over an indoor wireless network in the presence of interfering nodes . we use our graphical tool to vary the sender , receiver , and interferer antenna configurations and the resulting performance is directly visible in the video quality displayed at the receiver .
improvement of 3p and 6r mechanical robots reliability and quality applying fmea and qfd approaches . <eos> in the past few years , extending usage of robotic systems has increased the importance of robot reliability and quality . to improve the robot reliability and quality by applying standard approaches such as failure mode and effect analysis ( fmea ) and quality function deployment ( qfd ) during the design of robot is necessary . fmea is a qualitative method which determines the critical failure modes in robot design . in this method risk priority number is used to sort failures with respect to critical situation . two examples of mechanical robots are analyzed by using this method and critical failure modes are determined for each robot . corrective actions are proposed for critical items to modify robots reliability and reduce their risks . finally by using qfd , quality of these robots is improved according to the customers requirements . in this method by making four matrixes , optimum values for all technical parameters are determined and the final product has the desired quality .
informatics methodologies for evaluation research in the practice setting . <eos> a continuing challenge in health informatics and health evaluation is to enable access to the practice of health care so that the determinants of successful care and good health outcomes can be measured , evaluated and analysed . furthermore the results of the analysis should be available to the health care practitioner or to the patient as might be appropriate , so that he or she can use this information for continual improvement of practice and optimisation of outcomes . in this paper we review two experiences , one in primary care , the famus project , and the other in hospital care , the autocontrol project . each project demonstrates an informatics approach for evaluation research in the clinical setting and indicates ways in which useful information can be obtained which with appropriate feed back and education can be used towards the achievement of better health . emphasis is given to data collection methods compatible with practice and to high quality information feedback , particularly in the team context , to enable the formulation of strategies for practice improvement .
an improved som algorithm and its application to color feature extraction . <eos> reducing the redundancy of dominant color features in an image and meanwhile preserving the diversity and quality of extracted colors is of importance in many applications such as image analysis and compression . this paper presents an improved self organization map ( som ) algorithm namely mfd som and its application to color feature extraction from images . different from the winner take all competitive principle held by conventional som algorithms , mfd som prevents , to a certain degree , features of non principal components in the training data from being weakened or lost in the learning process , which is conductive to preserving the diversity of extracted features . besides , mfd som adopts a new way to update weight vectors of neurons , which helps to reduce the redundancy in features extracted from the principal components . in addition , we apply a linear neighborhood function in the proposed algorithm aiming to improve its performance on color feature extraction . experimental results of feature extraction on artificial datasets and benchmark image datasets demonstrate the characteristics of the mfd som algorithm .
a motion planning system for mobile robots . <eos> in this paper , a motion planning system for a mobile robot is proposed . path planning tries to find a feasible path for mobile robots to move from a starting node to a target node in an environment with obstacles . a genetic algorithm is used to generate an optimal path by taking the advantage of its strong optimization ability . mobile robot , obstacle and target localizations are realized by means of camera and image processing . a graphical user interface ( gui ) is designed for the motion planning system that allows the user to interact with the robot system and to observe the robot environment . all the software components of the system are written in matlab that provides to use non predefined accessories rather than the robot firmware has , to avoid confusing in c libraries of robot 's proprietary software , to control the robot in detail and not to re compile the programs frequently in real time dynamic operations .
a unified strategy for search and result representation for an online bibliographical catalogue . <eos> purpose one of the biggest concerns of modem information retrieval systems is reducing the user effort required for manual traversal and filtering of long matching document lists . thus , the first goal of this research is to propose an improved scheme for representation of search results . further , it aims to explore the impact of various user information needs on the searching process with the aim of finding a unified searching approach well suited for different query types and retrieval tasks . design methodology approach the bow online bibliographical catalogue is based on a hierarchical concept index to which entries are linked . the key idea is that searching in the hierarchical catalogue should take advantage of the catalogue structure and return matching topics from the hierarchy , rather than just a long list of entries . likewise , when new entries are inserted , a search for relevant topics to which they should be linked is required . therefore , a similar hierarchical scheme for query topic matching can be applied for both tasks . findings the experiments show that different query types used for the above tasks are best treated by different topic ranking functions . to further examine this phenomenon a user study was conducted , where various statistical weighting factors were incorporated and their impact on the performance for different query types was measured . finally , it is found that the mixed strategy that applies the most suitable ranking function to each query type yielded a significant increase in precision relative to the baseline and to employing any examined strategy in isolation on the entire set of user queries . originality value the main contributions of this paper are the alternative approach for compact and concise representation of search results , which were implemented in the bow online bibliographical catalogue and the unified or mixed strategy for search and result representation applying the most suitable ranking function to each query type , which produced superior results compared to different single strategy based approaches .
robust multiple phase switched capacitor dc dc converter with digital interleaving regulation scheme . <eos> an integrated switched capacitor ( sc ) dc dc converter with a digital interleaving regulation scheme is presented . by interleaving the newly structured charge pump ( cp ) cells in multiple phases , the input current ripple and output voltage ripple are reduced significantly . the converter exhibits excellent robustness , even when one of the cp cells fails to operate . a fully digital controller is employed with a hysteretic control algorithm . it features dead beat system stability and fast transient response . hspice post layout simulation shows that , with a 1.5 v input power supply , the sc converter accurately provides an adjustable regulated power output in a range of 1.6 to 2.7 v. the maximum output ripple is <digit> mv when a full load of 0.54 w is supplied . transient response of 1.8 ms is observed when the load current switches from half to full load ( from <digit> to <digit> ma ) .
teeth recognition based on multiple attempts in mobile device . <eos> most traditional biometric approaches generally utilize a single image for personal identification . however , these approaches sometimes failed to recognize users in practical environment due to false detected or undetected subject . therefore , this paper proposes a novel recognition approach based on multiple frame images that are implemented in mobile devices . the aim of this paper is to improve the recognition accuracy and to reduce computational complexity through multiple attempts . here , multiple attempts denote that multiple frame images are used in time of recognition procedure . among sequential frame images , an adequate subject , i.e. , teeth image , is chosen by subject selection module which is operated based on differential image entropy . the selected subject is then utilized as a biometric trait of traditional recognition algorithms including pca , lda , and ehmm . the performance evaluation of proposed method is performed using two teeth databases constructed by a mobile device . through experimental results , we confirm that the proposed method exhibits improved recognition accuracy of about 3.64.8 % , and offers the advantage of lower computational complexity than traditional biometric approaches .
a conceptual approach for the die structure design . <eos> a large number of decisions are made during the conceptual design stage which is characterized by a lack of complete geometric information . while existing cad systems supporting the geometric aspects of design have had little impact at the conceptual design stage . to support the conceptual die design and the top down design process , a new concept called conceptual assembly modeling framework ( camf ) is presented in this paper . firstly , the framework employs the zigzag function symbol mapping to implement the function design of the die . from the easily understood analytical results of the function symbol mapping matrix , the designer can evaluate the quality of a proposed die concept . secondly , a new method logic assembly modeling is proposed using logic components in this framework to satisfy the characteristic of the conceptual die design . representing shapes and spatial relations in logic can provide a natural , intuitive method of developing complete computer systems for reasoning about die construction design at the conceptual stage . the logic assembly which consists of logic components is an innovative representation that provides a natural link between the function design of the die and the detailed geometric design .
approximation algorithm for coloring of dotted interval graphs . <eos> dotted interval graphs were introduced by aumann et al. y. aumann , m. lewenstein , o. melamud , r. pinter , z. yakhini , dotted interval graphs and high throughput genotyping , in acm siam symposium on discrete algorithms . soda <digit> , pp. <digit> <digit> as a generalization of interval graphs . the problem of coloring these graphs found application in high throughput genotyping . jiang m. jiang , approximating minimum coloring and maximum independent set in dotted interval graphs , information processing letters <digit> ( <digit> ) <digit> <digit> improves the approximation ratio of aumann et al. y. aumann , m. lewenstein , o. melamud , r. pinter , z. yakhini , dotted interval graphs and high throughput genotyping , in acm siam symposium on discrete algorithms , soda <digit> , pp. <digit> <digit> . in this work we improve the approximation ratio of jiang m. jiang , approximating minimum coloring and maximum independent set in dotted interval graphs , information processing letters <digit> ( <digit> ) <digit> <digit> and aumarm et al. y. aumann , m. lewenstein , o. melamud , r. pinter , z. yakhini , dotted interval graphs and high throughput genotyping , in acm siam symposium on discrete algorithms , soda <digit> , pp. <digit> <digit> . in the exposition we develop a generalization of the problem of finding the maximum number of non attacking queens on a triangle . ( c ) <digit> elsevier b.v. all rights reserved .
scalable visibility color map construction in spatial databases . <eos> recent advances in 3d modeling provide us with real 3d datasets to answer queries , such as what is the best position for a new billboard and which hotel room has the best view in the presence of obstacles . these applications require measuring and differentiating the visibility of an object ( target ) from different viewpoints in a dataspace , e.g. , a billboard may be seen from many points but is readable only from a few points closer to it . in this paper , we formulate the above problem of quantifying the visibility of ( from ) a target object from ( of ) the surrounding area with a visibility color map ( vcm ) . a vcm is essentially defined as a surface color map of the space , where each viewpoint of the space is assigned a color value that denotes the visibility measure of the target from that viewpoint . measuring the visibility of a target even from a single viewpoint is an expensive operation , as we need to consider factors such as distance , angle , and obstacles between the viewpoint and the target . hence , a straightforward approach to construct the vcm that requires visibility computation for every viewpoint of the surrounding space of the target is prohibitively expensive in terms of both i os and computation , especially for a real dataset comprising thousands of obstacles . we propose an efficient approach to compute the vcm based on a key property of the human vision that eliminates the necessity for computing the visibility for a large number of viewpoints of the space . to further reduce the computational overhead , we propose two approximations namely , minimum bounding rectangle and tangential approaches with guaranteed error bounds . our extensive experiments demonstrate the effectiveness and efficiency of our solutions to construct the vcm for real 2d and 3d datasets .
toward a neurogenetic theory of neuroticism . <eos> recent advances in neuroscience and molecular biology have begun to identify neural and genetic correlates of complex traits . future theories of personality need to integrate these data across the behavioral , neural , and genetic level of analysis and further explain the underlying epigenetic processes by which genes and environmental variables interact to shape the structure and function of neural circuitry . in this chapter , i will review some of the work that has been conducted at the cognitive , neural , and molecular genetic level with respect to one specific personality traitneuroticism . i will focus particularly on individual differences with respect to memory , self reference , perception , and attention during processing of emotional stimuli and the significance of gene by environment interactions . this chapter is intended to serve as a tutorial bridge for psychologists who may be intrigued by molecular genetics and for molecular biologists who may be curious about how to apply their research to the study of personality .
technological means of communication and collaboration in archives and records management . <eos> this study explores the international collaboration efforts of archivists and records managers starting with the hypothesis that internet technologies have had a significant impact on both national and international communication for this previously conservative group . the use and importance of mailing lists for this purpose is studied in detail . a quantitative analysis looks globally at the numbers of lists in these fields and the numbers of subscribers . a qualitative analysis of list content is also described . the study finds that archivists and records managers have now created more than <digit> mailing lists related to their profession and have been contributing to these lists actively . it also ' estimates ' that about half of the profession follows a list relating to their work and that archivists seem to like lists more than records managers do . the study concludes that mailing lists can be seen as a virtual college binding these groups together to develop the field .
privacy preserving decision tree learning using unrealized data sets . <eos> privacy preservation is important for machine learning and data mining , but measures designed to protect private information often result in a trade off reduced utility of the training samples . this paper introduces a privacy preserving approach that can be applied to decision tree learning , without concomitant loss of accuracy . it describes an approach to the preservation of the privacy of collected data samples in cases where information from the sample database has been partially lost . this approach converts the original sample data sets into a group of unreal data sets , from which the original samples can not be reconstructed without the entire group of unreal data sets . meanwhile , an accurate decision tree can be built directly from those unreal data sets . this novel approach can be applied directly to the data storage as soon as the first sample is collected . the approach is compatible with other privacy preserving approaches , such as cryptography , for extra protection .
selected topics on assignment problems . <eos> we survey recent developments in the fields of bipartite matchings , linear sum assignment and bottleneck assignment problems and applications , multidimensional assignment problems , quadratic assignment problems , in particular lower bounds , special cases and asymptotic results , biquadratic and communication assignment problems .
fast parameter free region growing segmentation with application to surgical planning . <eos> in this paper , we propose a self assessed adaptive region growing segmentation algorithm . in the context of an experimental virtual reality surgical planning software platform , our method successfully delineates main tissues relevant for reconstructive surgery , such as fat , muscle , and bone . we rely on a self tuning approach to deal with a great variety of imaging conditions requiring limited user intervention ( one seed ) . the detection of the optimal parameters is managed internally using a measure of the varying contrast of the growing region , and the stopping criterion is adapted to the noise level in the dataset thanks to the sampling strategy used for the assessment function . sampling is referred to the statistics of a neighborhood around the seed ( s ) , so that the sampling period becomes greater when images are noisier , resulting in the acquisition of a lower frequency version of the contrast function . validation is provided for synthetic images , as well as real ct datasets . for the ct test images , validation is referred to manual delineations for <digit> cases and to subjective assessment for another <digit> . high values of sensitivity and specificity , as well as dice 's coefficient and jaccard 's index on one hand , and satisfactory subjective evaluation on the other hand , prove the robustness of our contrast based measure , even suggesting suitability for calibration of other region based segmentation algorithms .
accuracy and efficiency in computing electrostatic potential for an ion channel model in layered dielectric electrolyte media . <eos> this paper will investigate the numerical accuracy and efficiency in computing the electrostatic potential for a finite height cylinder , used in an explicit implicit hybrid solvation model for ion channel and embedded in a layered dielectric electrolyte medium representing a biological membrane and ionic solvents . a charge locating inside the cylinder cavity , where ion channel proteins and ions are given explicit atomistic representations , will be influenced by the polarization field of the surrounding implicit dielectric electrolyte medium . two numerical techniques , a specially designed boundary integral equation method and an image charge method , will be investigated and compared in terms of accuracy and efficiency for computing the electrostatic potential . the boundary integral equation method based on the three dimensional layered green s functions provides a highly accurate solution suitable for producing a benchmark reference solution , while the image charge method is found to give reasonable accuracy and highly efficient and viable to use the fast multipole method for interactions of a large number of charges in the atomistic region of the hybrid solvation model .
the social sharing of emotion ( sse ) in online social networks a case study in live journal . <eos> using content analysis , we gauge the occurrence of social sharing of emotion ( sse ) in live journal . we present a theoretical model of a three cycle process for online sse . a large part of emotional blog posts showed full initiation of social sharing . affective feedback provided empathy , emotional support and admiration . this study is the first one to empirically assess the occurrence and structure of online sse .
non testing approaches under reach help or hindrance perspectives from a practitioner within industry . <eos> legislation such as reach strongly advocates the use of alternative approaches including invitro , ( q ) sars , and chemical categories as a means to satisfy the information requirements for risk assessment . one of the most promising alternative approaches is that of chemical categories , where the underlying hypothesis is that the compounds within the category are similar and therefore should have similar biological activities . the challenge lies in characterizing the chemicals , understanding the mode mechanism of action for the activity of interest and deriving a way of relating these together to form inferences about the likely activity outcomes . ( q ) sars are underpinned by the same hypothesis but are packaged in a more formalized manner . since the publication of the white paper for reach , there have been a number of efforts aimed at developing tools , approaches and techniques for ( q ) sars and read across for regulatory purposes . while technical guidance is available , there still remains little practical guidance about how these approaches can or should be applied in either the evaluation of existing ( q ) sars or in the formation of robust categories . here we provide a perspective of how some of these approaches have been utilized to address our in house reach requirements .
realtime performance analysis of different combinations of fuzzypid and bias controllers for a two degree of freedom electrohydraulic parallel manipulator . <eos> development of a <digit> dof electrohydraulic motion simulator as a parallel manipulator . control of heave , pitch and combined heave and pitch motion of the parallel manipulator . design of pid , fuzzypid , self tuning fuzzypid and self tuning fuzzypid with bias controllers . use of different combinations of fuzzypid and bias controllers for study of real time control performance . best control response found for the self tuning fuzzypid with bias controller .
on the depth distribution of linear codes . <eos> the depth distribution of a linear code was recently introduced by etzion . in this correspondence , a number of basic and interesting properties for the depth of finite words and the depth distribution of linear codes are obtained . in addition , we study the enumeration problem of counting the number of linear subcodes with the prescribed depth constraints , and derive some explicit and interesting enumeration formulas . furthermore , we determine the depth distribution of reed muller code rm ( m , r ) . finally , we show that there are exactly nine depth equivalence classes for the ternary <digit> , <digit> , <digit> golay codes .
are we there yet . <eos> statistical approaches to artificial intelligence are behind most success stories of the field in the past decade . the idea of generating non trivial behaviour by analysing vast amounts of data has enabled recommendation systems , search engines , spam filters , optical character recognition , machine translation and speech recognition , among other things . as we celebrate the spectacular achievements of this line of research , we need to assess its full potential and its limitations . what are the next steps to take towards machine intelligence
the complexity of the matroid greedoid partition problem . <eos> we show that the maximum matroid greedoid partition problem is np hard to approximate to within <digit> <digit> epsilon for any epsilon > <digit> , which matches the trivial factor <digit> <digit> approximation algorithm . the main tool in our hardness of approximation result is an extractor code with polynomial rate , alphabet size and list size , together with an efficient algorithm for list decoding . we show that the recent extractor construction of guruswami , umans and vadhan v. guruswami . c. umans , s.p. vadhan , unbalanced expanders and randomness extractors from parvaresh vardy codes , in ieee conference on computational complexity , ieee computer society , <digit> , pp. <digit> <digit> can be used to obtain a code with these properties . we also show that the parameterized matroid greedoid partition problem is fixed parameter tractable . ( c ) <digit> elsevier b.v. all rights reserved .
exploring the ncrnancrna patterns based on bridging rules . <eos> ncrnas play an important role in the regulation of gene expression . however , many of their functions have not yet been fully discovered . there are complicated relationships between ncrnas in different categories . finding these relationships can contribute to identify ncrnas functions and properties . we extend the association rule to represent the relationship between two ncrnas . based on this rule , we can speculate the ncrnas function when it interacts with other ncrnas . we propose two measures to explore the relationships between ncrnas in different categories . entropy theory is to calculate how close two ncrnas are . association rule is to represent the interactions between ncrnas . we use three datasets from mirbase and rnadb . two from mirbase are designed for finding relationships between mirnas the other from rnadb is designed for relationships among mirna , snorna and pirna . we evaluate our measures from both biological significance and performance perspectives . all the cross species patterns regarding mirna that we found are proven correct using mirnamap 2.0 . in addition , we find novel cross genomes patterns such as ( hsa mir 190b hsa mir <digit> <digit> ) . according to the patterns we find , we can ( <digit> ) explore one ncrnas function from another with known function and ( <digit> ) speculate the functions of both of them based on the relationship even we do no understand either of them . our methods merits also include ( <digit> ) they are suitable for any ncrna datasets and ( <digit> ) they are not sensitive to the parameters .
gaussian mixture modelling to detect random walks in capital markets . <eos> in this paper , gaussian mixture modelling is used to detect random walks in capital markets with the kolmogorov smirnov test . the main idea is to use gaussian mixture modelling to fit asset return distributions and then use the kolmogorov smirnov test to determine the number of components . several quantities are used to characterize gaussian mixture models and ascertain whether random walks exist in capital markets . empirical studies on china securities markets and forex markets are used to demonstrate the proposed procedure . ( c ) <digit> elsevier ltd. all rights reserved .
scientific design rationale . <eos> design rationale should be regarded both as a tool for the practice of design , and as a method to enable the science of design . design rationale answers questions about why a given design takes the form that it does . answers to these why questions represent a significant portion of the knowledge generated from design research . this knowledge , along with that from empirical studies of designs in use , contributes to what simon called the sciences of the artificial . most research on the nature and use of design rationale has been analytic or theoretical . in this article , we describe an empirical study of the roles that design rationale can play in the conduct of design research . we report results from an interview study with <digit> design researchers investigating how they construe and carry out design as research . the results include an integrated framework of the affordances design rationale can contribute to design research . the framework and supporting qualitative data provide insight into how design rationale might be more effectively leveraged as a first class methodology for research into the creation and use of artifacts .
high flowability monomer resists for thermal nanoimprint lithography . <eos> in this paper , we have been using polymer and thermally curable monomer resists in a full 8in . wafer thermal nanoimprint lithography process . using exactly the same imprinting conditions , we observed that a monomer solution provides a much larger resist redistribution than a polymer resist . imprinting fresnel zone plates , composed of micro and nano meter features , was possible only with the monomer resist . in order to reduce the shrinkage ratio of the monomer resists , acrylatesilsesquioxane materials were synthesised . with a simple diffusion like model , we could extract a mean free path of 1.1 mm for the monomer resist , while a polymer flows only on distances below <digit> m in the same conditions .
binarized support vector machines . <eos> the widely used support vector machine ( svm ) method has shown to yield very good results in supervised classification problems . other methods such as classification trees have become more popular among practitioners than svm thanks to their interpretability , which is an important issue in data mining . in this work , we propose an svm based method that automatically detects the most important predictor variables and the role they play in the classifier . in particular , the proposed method is able to detect those values and intervals that are critical for the classification . the method involves the optimization of a linear programming problem in the spirit of the lasso method with a large number of decision variables . the numerical experience reported shows that a rather direct use of the standard column generation strategy leads to a classification method that , in terms of classification ability , is competitive against the standard linear svm and classification trees . moreover , the proposed method is robust i.e. , it is stable in the presence of outliers and invariant to change of scale or measurement units of the predictor variables . when the complexity of the classifier is an important issue , a wrapper feature selection method is applied , yielding simpler but still competitive classifiers .
ambrosio tortorelli segmentation of stochastic images model extensions , theoretical investigations and numerical methods . <eos> we discuss an extension of the ambrosio tortorelli approximation of the mumford shah functional for the segmentation of images with uncertain gray values resulting from measurement errors and noise . our approach yields a reliable precision estimate for the segmentation result , and it allows us to quantify the robustness of edges in noisy images and under gray value uncertainty . we develop an ansatz space for such images by identifying gray values with random variables . the use of these stochastic images in the minimization of energies of ambrosio tortorelli type leads to stochastic partial differential equations for a stochastic smoothed version of the original image and a stochastic phase field for the edge set . for the discretization of these equations we utilize the generalized polynomial chaos expansion and the generalized spectral decomposition ( gsd ) method . in contrast to the simple classical sampling technique , this approach allows for an efficient determination of the stochastic properties of the output image and edge set by computations on an optimally small set of random variables . also , we use an adaptive grid approach for the spatial dimensions to further improve the performance , and we extend an edge linking method for the classical ambrosio tortorelli model for use with our stochastic model . the performance of the method is demonstrated on artificial data and a data set from a digital camera as well as real medical ultrasound data . a comparison of the intrusive gsd discretization with a stochastic collocation and a monte carlo sampling is shown .
a provably convergent heuristic for stochastic bicriteria integer programming . <eos> we propose a general purpose algorithm aps ( adaptive pareto sampling ) for determining the set of pareto optimal solutions of bicriteria combinatorial optimization ( co ) problems under uncertainty , where the objective functions are expectations of random variables depending on a decision from a finite feasible set . aps is iterative and population based and combines random sampling with the solution of corresponding deterministic bicriteria co problem instances . special attention is given to the case where the corresponding deterministic bicriteria co problem can be formulated as a bicriteria integer linear program ( ilp ) . in this case , well known solution techniques such as the algorithm by chalmet et al. can be applied for solving the deterministic subproblem . if the execution of aps is terminated after a given number of iterations , only an approximate solution is obtained in general , such that aps must be considered a metaheuristic . nevertheless , a strict mathematical result is shown that ensures , under rather mild conditions , convergence of the current solution set to the set of pareto optimal solutions . a modification replacing or supporting the bicriteria ilp solver by some metaheuristic for multicriteria co problems is discussed . as an illustration , we outline the application of the method to stochastic bicriteria knapsack problems by specializing the general framework to this particular case and by providing computational examples .
the impact of a simulation game on operations management education . <eos> this study presents a new simulation game and analyzes its impact on operations management education . the proposed simulation was empirically tested by comparing the number of mistakes during the first and second halves of the game . data were gathered from <digit> teams of four or five undergraduate students in business administration , taking their first course in operations management . to assess learning , instead of relying solely on an overall performance measurement , as is usually done in the skill based learning literature , we analyzed the evolution of different types of mistakes that were made by students in successive rounds of play . our results show that although simple decision making skills can be acquired with traditional teaching methods , simulation games are more effective when students have to develop decision making abilities for managing complex and dynamic situations . ( c ) <digit> elsevier ltd. all rights reserved .
covering a set of points in a plane using two parallel rectangles . <eos> in this paper we consider the problem of finding two parallel rectangles in arbitrary orientation for covering a given set of n points in a plane , such that the area of the larger rectangle is minimized . we propose an algorithm that solves the problem in o ( n ( <digit> ) ) time using o ( n ( <digit> ) ) space . without altering the complexity , our approach can be used to solve another optimization problem namely , minimize the sum of the areas of two arbitrarily oriented parallel rectangles covering a given set of points in a plane . ( c ) <digit> elsevier b.v. all rights reserved .
investigating the extreme programming system an empirical study . <eos> in this paper we discuss our empirical study about the advantages and difficulties <digit> greek software companies experienced applying extreme programming ( xp ) as a holistic system in software development . based on a generic xp system including feedback influences and using a cause effect model including social technical affecting factors , as our research tool , the study statistically evaluates the application of xp practices in the software companies being studied . data were collected from <digit> managers and developers , using the sample survey technique with questionnaires and interviews , in a time period of six months . practices were analysed individually , using descriptive statistics ( ds ) , and as a whole by building up different models using stepwise discriminant analysis ( da ) . the results have shown that companies , facing various problems with common code ownership , on site customer , <digit> hour week and metaphor , prefer to develop their own tailored xp method and way of working practices that met their requirements . pair programming and test driven development were found to be the most significant success factors . interactions and hidden dependencies for the majority of the practices as well as communication and synergy between skilled personnel were found to be other significant success factors . the contribution of this preliminary research work is to provide some evidence that may assist companies in evaluating whether the xp system as a holistic framework would suit their current situation .
an optimal gts scheduling algorithm for time sensitive transactions in ieee 802.15.4 networks . <eos> ieee 802.15.4 is a new enabling standard for low rate wireless personal area networks and has been widely accepted as a de facto standard for wireless sensor networking . while primary motivations behind 802.15.4 are low power and low cost wireless communications , the standard also supports time and rate sensitive applications because of its ability to operate in tdma access modes . the tdma mode of operation is supported via the guaranteed time slot ( gts ) feature of the standard . in a beacon enabled network topology , the personal area network ( pan ) coordinator reserves and assigns the gts to applications on a first come first served ( fcfs ) basis in response to requests from wireless sensor nodes . this fixed fcfs scheduling service offered by the standard may not satisfy the time constraints of time sensitive transactions with delay deadlines . such operating scenarios often arise in wireless video surveillance and target detection applications running on sensor networks . in this paper , we design an optimal work conserving scheduling algorithm for meeting the delay constraints of time sensitive transactions and show that the proposed algorithm outperforms the existing scheduling model specified in ieee 802.15.4 .
controlled dense coding with cluster state . <eos> two schemes for controlled dense coding with a one dimensional four particle cluster state are investigated . in this protocol , the supervisor ( cliff ) can control the channel and the average amount of information transmitted from the sender ( alice ) to the receiver ( bob ) by adjusting the local measurement angle theta . it is shown that the results for the average amounts of information are unique from the different two schemes .
slope stability analysis using the limit equilibrium method and two finite element methods . <eos> in this paper , the factors of safety and critical slip surfaces obtained by the limit equilibrium method ( lem ) and two finite element methods ( the enhanced limit strength method ( elsm ) and strength reduction method ( srm ) ) are compared . several representative two dimensional slope examples are analysed . using the associated flow rule , the results showed that the two finite element methods were generally in good agreement and that the lem yielded a slightly lower factor of safety than the two finite element methods did . moreover , a key condition regarding the stress field is shown to be necessary for elsm analysis .
deformation invariant attribute vector for deformable registration of longitudinal brain mr images . <eos> this paper presents a novel approach to define deformation invariant attribute vector ( diav ) for each voxel in 3d brain image for the purpose of anatomic correspondence detection . the diav method is validated by using synthesized deformation in 3d brain mri images . both theoretic analysis and experimental studies demonstrate that the proposed diav is invariant to general nonlinear deformation . moreover , our experimental results show that the diav is able to capture rich anatomic information around the voxels and exhibit strong discriminative ability . the diav has been integrated into a deformable registration algorithm for longitudinal brain mr images , and the results on both simulated and real brain images are provided to demonstrate the good performance of the proposed registration algorithm based on matching of diavs .
carbapenem resistant enterobacteriaceae biology , epidemiology , and management . <eos> introduced in the 1980s , carbapenem antibiotics have served as the last line of defense against multidrug resistant gram negative organisms . over the last decade , carbapenem resistant enterobacteriaceae ( cre ) have emerged as a significant public health threat . this review summarizes the molecular genetics , natural history , and epidemiology of cre and discusses approaches to prevention and treatment .
hypergraph based inductive learning for generating implicit key phrases . <eos> this paper presents a novel approach to generate implicit key phrases which are ignored in previous researches . recent researches prefer to extract key phrases with semi supervised transductive learning methods , which avoid the problem of training data . in this paper , based on a transductive learning method , we formulate the phrases in the document as a hypergraph and expand the hypergraph to include implicit phrases , which are ranked by an inductive learning approach . the highest ranked phrases are seen as implicit key phrases , and experimental results demonstrate the satisfactory performance of this approach .
strategic commitment to price to stimulate downstream innovation in a supply chain . <eos> it is generally in a firms interest for its supply chain partners to invest in innovations . to the extent that these innovations either reduce the partners variable costs or stimulate demand for the end product , they will tend to lead to higher levels of output for all of the firms in the chain . however , in response to the innovations of its partners , a firm may have an incentive to opportunistically increase its own prices . the possibility of such opportunistic behavior creates a hold up problem that leads supply chain partners to underinvest in innovation . clearly , this hold up problem could be eliminated by a pre commitment to price . however , by making an advance commitment to price , a firm sacrifices an important means of responding to demand uncertainty . in this paper we examine the trade off that is faced when a firms channel partner has opportunities to invest in either cost reduction or quality improvement , i.e. demand enhancement . should it commit to a price in order to encourage innovation , or should it remain flexible in order to respond to demand uncertainty . we discuss several simple wholesale pricing mechanisms with respect to this trade off .
mutation based software testing using program schemata . <eos> mutation analysis is a powerful technique for assessing the quality of test data used in unit testing software . unfortunately , current automated mutation analysis systems suffer from severe performance problems . in this paper the principles of mutation analysis are reviewed , current automation approaches are described , and a new method of performing mutation analysis is outlined . performance improvements of over <digit> % are reported and other advantages of this new method are highlighted .
bamboo a data centric , object oriented approach to many core software . <eos> traditional data oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming . in these languages , a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low level , thread oriented concurrency primitives . this simplification comes at a cost traditional data oriented approaches restrict the mutation of state and , in practice , the types of data structures a program can effectively use . bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data oriented programming models to support mutation of arbitrary data structures . we have implemented a compiler for bamboo which generates code for the tilepro64 many core processor . we have evaluated this implementation on six benchmarks tracking , a feature tracking algorithm from computer vision kmeans , a k means clustering algorithm montecarlo , a monte carlo simulation filterbank , a multi channel filter bank fractal , a mandelbrot set computation and series , a fourier series computation . we found that our compiler generated implementations that obtained speedups ranging from 26.2 x to 61.6 x when executed on <digit> cores .
performance optimization problem in speculative prefetching . <eos> speculative prefetching has been proposed to improve the response time of network access . previous studies in speculative prefetching focus on building and evaluating access models for the purpose of access prediction . this paper investigates a complementary area which has been largely ignored , that of performance modeling . we analyze the performance of a prefetcher that has uncertain knowledge about future accesses . our performance metric is the improvement in access time , for which we derive a formula in terms of resource parameters ( time available and time required for prefetching ) and speculative parameters ( probabilities for next access ) . we develop a prefetch algorithm to maximize the improvement in access time . the algorithm is based on finding the best solution to a stretch knapsack problem , using theoretically proven apparatus to reduce the search space . an integration between speculative prefetching and caching is also investigated .
inspiring collaboration through the use of videoconferencing technology . <eos> at the beginning of <digit> the university of washington opened the odegaard videoconference studio which allowed groups on campus to communicate with colleagues that were physically in different locations . the opening of this facility inspired all sorts of collaborating on a more frequent basis as traveling , and more importantly the time and expense involved with traveling , was now not as necessary in order to have a meeting . many boundaries for collaboration were removed through the use of different types of technology that allowed for video and audio conferencing , and , data and application sharing . this provided for a way to share ideas in more detail , make decisions , and receive feedback quicker , making the overall process more efficient , personal , and overall more effective .
expanders , sorting in rounds and superconcentrators of limited depth . <eos> expanding graphs and superconcentrators are relevant to theoretical computer science in several ways . here we use finite geometries to construct explicitly highly expanding graphs with essentially the smallest possible number of edges . our graphs enable us to improve significantly previous results on a parallel sorting problem , by describing an explicit algorithm to sort n elements in k time units using ogr ( n agr k ) processors , where , e.g. , agr <digit> <digit> <digit> . using our graphs we can also construct efficient n superconcentrators of limited depth . for example , we construct an n superconcentrator of depth <digit> with ogr ( n <digit> <digit> ) edges better than the previous known results .
synchrony and frequency regulation by synaptic delay in networks of self inhibiting neurons . <eos> we show that a pair of mutually coupled self inhibitory neurons can display stable synchronous oscillations provided only that the delay to the onset of inhibition is sufficiently long . the frequency of these oscillations is determined either entirely by the length of the synaptic delay , or by the synaptic delay and intrinsic time constants . we also show how cells can exhibit transient synchronous oscillations where the length of the transients is determined by the synaptic delay , but where the frequency is largely independent of the delay .
minimizing power dissipation during write operation to register files . <eos> this paper presents a power reduction mechanism for the write operation in register files ( regfiles ) , which adds a conditional charge sharing structure to the pair of complementary bit lines in each column of the regfile . because the read and write ports for the regfile are separately implemented , it is possible to avoid pre charging the bit line pair for consecutive writes . more precisely , when writing same values to some cells in the same column of the regfile , it is possible to eliminate energy consumption due to precharging of the bit line pair . at the same time , when writing opposite values to some cells in the same column of the regfile , it is possible to reduce energy consumed in charging the bit line pair thanks to charge sharing . motivated by these observations , we modify the bit line structure of the write ports in the regfile such that i ) we remove per cycle bitline pre charging and ii ) we employ conditional data dependent charge sharing . experimental results on a set of spec2000int mediabench benchmarks show an average of 61.5 % energy savings with 5.1 % area overhead and 16.2 % increase in write access delay .
a decision support framework for metrics selection in goal based measurement programs gqm dsfms . <eos> complex gqm based measurement programs lead to the need for decision support in metric selection . we provide an decision support framework in choosing an optimal set of metrics to maximize measurement goal achievement for a given budget . the framework was evaluated by comparison with expert opinion in a cmmi level <digit> company . extent of addressing information needs under a fixed budged was higher when selecting metrics using the framework .
energy area delay trade offs in the physical design of on chip segmented bus architecture . <eos> the increasing gap between design productivity and chip complexity , and the emerging systems on chip ( soc ) architectural template have led to the wide utilization of reusable intellectual property ( ip ) cores . the physical design implementation of the macro cells ( ip blocks or pre designed blocks ) in general needs to find a well balanced solution among chip area , on chip interconnect energy and critical path delay . we are especially interested in the entire trade off curve among these three criteria at the floorplanning stage . we show this concept for a real communication scheme based on segmented bus , rather than just an extreme solution . a fast exploration design flow from the memory organization to the final layout is introduced to explore the design space .
system integration of a miniature rotorcraft for aerial tele operation research . <eos> this paper describes the development and integration of the systems required for research into human interaction with a tele operated miniature rotorcraft . because of the focus on vehicles capable of operating indoors , the size of the vehicle was limited to <digit> cm , and therefore the hardware had to be carefully chosen to meet the ensuing size and weight constraints , while providing sufficient flight endurance . the components described in this work include the flight hardware , electronics , sensors , and software necessary to conduct tele operation experiments . the integration tasks fall into three main areas . first , the paper discusses the choice of rotorcraft platform best suited for indoor operation addressing the issues of size , payload capabilities , and power consumption . the second task was to determine what electronics and sensing could be integrated into a rotorcraft with significant payload limitations . finally , the third task involved characterizing the various components both individually and as a complete system . the paper concludes with an overview of ongoing tele operation research performed with the embedded rotorcraft platform . ( c ) <digit> elsevier ltd. all rights reserved .
rank inclusion in criteria hierarchies . <eos> this paper presents a method called rank inclusion in criteria hierarchies ( rich ) for the analysis of incomplete preference information in hierarchical weighting models . in rich , the decision maker is allowed to specify subsets of attributes which contain the most important attribute or , more generally , to associate a set of rankings with a given set of attributes . such preference statements lead to possibly non convex sets of feasible attribute weights , allowing decision recommendations to be obtained through the computation of dominance relations and decision rules . an illustrative example on the selection of a subcontractor is presented , and the computational properties of rich are considered .
automatic relative orientation of large scale imagery over urban areas using modified iterated hough transform . <eos> the automation of relative orientation ( ro ) has been the major focus of the photogrammetric research community in the last decade . despite the reported progress , there is no reliable ( robust ) approach that can perform automatic relative orientation ( aro ) using large scale imagery over urban areas . a reliable and general method for solving matching problems in various photogrammetric activities has been developed at the ohio state university . this approach has been used to solve single photo resection using free form linear features , surface matching and relative orientation . the approach estimates the parameters of a mathematical model relating the entities of two datasets when the correspondence of the involved entities is unknown . when applied to relative orientation , the coplanarity model is used to relate extracted edge pixels and or feature points from a stereo pair . in its execution , the relative orientation parameters are solved sequentially , using the coplanarity model to evaluate all possible pairings of the input primitives and choosing the most probable solution . as a result of this technique , the matched entities that correspond to the parameter solution are implicitly determined . experiments using real data conclude that this is a robust method for relative orientation for both urban and rural scenes .
emergency railway wagon scheduling by hybrid biogeography based optimization . <eos> railway transportation plays an important role in many disaster relief and other emergency supply chains . based on the analysis of several recent disaster rescue operations in china , the paper proposes a mathematical model for emergency railway wagon scheduling , which considers multiple target stations requiring relief supplies , source stations for providing supplies , and central stations for allocating railway wagons . under the emergency environment , the aim of the problem is to minimize the weighted time for delivering all the required supplies to the targets . for efficiently solving the problem , we develop a new hybrid biogeography based optimization ( bbo ) algorithm , which uses a local ring topology of population to avoid premature convergence , includes the differential evolution ( de ) mutation operator to perform effective exploration , and takes some problem specific mechanisms for fine tuning the search process and handling the constraints . computational experiments show that our algorithm is robust and scalable , and outperforms some state of the art heuristic algorithms on a set of problem instances .
biasvariance analysis in estimating true query model for information retrieval . <eos> we study the retrieval effectiveness stability tradeoff in query model estimation . this tradeoff is investigated through a novel angle , i.e. , biasvariance tradeoff . we formulate the performance biasvariance and estimation biasvariance . we investigate various query estimation methods using biasvariance analysis . experiments have been conducted to verify hypotheses on biasvariance analysis .
qualitative constraint satisfaction problems an extended framework with landmarks . <eos> dealing with spatial and temporal knowledge is an indispensable part of almost all aspects of human activity . the qualitative approach to spatial and temporal reasoning , known as qualitative spatial and temporal reasoning ( qstr ) , typically represents spatial temporal knowledge in terms of qualitative relations ( e.g. , to the east of , after ) , and reasons with spatial temporal knowledge by solving qualitative constraints . when formulating qualitative constraint satisfaction problems ( csps ) , it is usually assumed that each variable could be here , there and everywhere . ( <digit> ) practical applications such as urban planning , however , often require a variable to take its value from a certain finite domain , i.e. it is required to be ' here or there , but not everywhere ' . entities in such a finite domain often act as reference objects and are called landmarks in this paper . the paper extends the classical framework of qualitative csps by allowing variables to take values from finite domains . the computational complexity of the consistency problem in this extended framework is examined for the five most important qualitative calculi , viz. point algebra , interval algebra , cardinal relation algebra , rcc5 , and rcc8 . we show that all these consistency problems remain in np and provide , under practical assumptions , efficient algorithms for solving basic constraints involving landmarks for all these calculi . ( c ) <digit> elsevier b.v. all rights reserved .
the interaction of software prefetching with ilp processors in shared memory systems . <eos> current microprocessors aggressively exploit instruction level parallelism ( ilp ) through techniques such as multiple issue , dynamic scheduling , and non blocking reads . recent work has shown that memory latency remains a significant performance bottleneck for shared memory multiprocessor systems built of such processors.this paper provides the first study of the effectiveness of software controlled non binding prefetching in shared memory multiprocessors built of state of the art ilp based processors . we find that software prefetching results in significant reductions in execution time ( <digit> % to <digit> % ) for three out of five applications on an ilp system . however , compared to previous generation system , software prefetching is significantly less effective in reducing the memory stall component of execution time on an ilp system . consequently , even after adding software prefetching , memory stall time accounts for over <digit> % of the total execution time in four out of five applications on our ilp system.this paper also investigates the interaction of software prefetching with memory consistency models on ilp based multiprocessors . in particular , we seek to determine whether software prefetching can equalize the performance of sequential consistency ( sc ) and release consistency ( rc ) . we find that even with software prefetching , for three out of five applications , rc provides a significant reduction in execution time ( <digit> % to <digit> % ) compared to sc .
a scalable and extensible framework for query answering over rdf . <eos> the semantic web is gaining increasing interest to fulfill the need of sharing , retrieving , and reusing information . in this context , the resource description framework ( rdf ) has been conceived to provide an easy way to represent any kind of data and metadata , according to a lightweight model and syntaxes for serialization ( rdf xml , n3 , etc. ) . despite rdf has the advantage of being general and simple , it can not be used as a storage model as it is , since it can be easily shown that even simple management operations involve serious performance limitations . in this paper we present a framework which provides a flexible and persistent layer relying on a novel storage model that guarantees good scalability and performance of query evaluation . the approach is based on the notion of construct , that represents a concept of the domain of interest . this makes the approach easily extensible and independent from the specific knowledge representation language . based on this representation , reasoning capabilities are supported by a rule based engine . finally we present experimental results over real world scenarios to demonstrate the feasibility of the approach .
using interactive <digit> d visualization for public consultation . <eos> <digit> d models are often developed to aid the design and development of indoor and outdoor environments . this study explores the use of interactive <digit> d visualization for public consultation for outdoor environments . two visualization techniques ( interactive <digit> d visualization and static visualization ) were compared using the method of individual testing . visualization technique had no effect on the perception of the represented outdoor environment , but there was a preference for using interactive <digit> d. previously established mechanisms for a preference for interactive <digit> d visualization in other domains were confirmed in the perceived strengths and weaknesses of visualization techniques . in focus group discussion , major preferences included provision of more information through interactive <digit> d visualization and wider access to information for public consultation . from a users ' perspective , the findings confirm the strong potential of interactive <digit> d visualization for public consultation . ( c ) <digit> elsevier b.v. all rights reserved .
polymorphic nodal elements and their application in discontinuous galerkin methods . <eos> in this work , we discuss two different but related aspects of the development of efficient discontinuous galerkin methods on hybrid element grids for the computational modeling of gas dynamics in complex geometries or with adapted grids . in the first part , a recursive construction of different nodal sets for hp finite elements is presented . they share the property that the nodes along the sides of the two dimensional elements and along the edges of the three dimensional elements are the legendregausslobatto points . the different nodal elements are evaluated by computing the lebesgue constants of the corresponding vandermonde matrix . in the second part , these nodal elements are applied within the modal discontinuous galerkin framework . we still use a modal based formulation , but introduce a nodal based integration technique to reduce computational cost in the spirit of pseudospectral methods . we illustrate the performance of the scheme on several large scale applications and discuss its use in a recently developed space time expansion discontinuous galerkin scheme .
deployment based solution for prolonging lifetime in sensor networks with multiple mobile sinks . <eos> enhancing sensor network lifetime is an important research topic for wireless sensor networks . solutions based on linear programming , clustering , controlled non uniform node distributions and mobility are presented separately in the literature . even thought , the problem is still open and not fully solved . drawbacks exist for all the above solutions when considered separately . perhaps a solution that is able to provide composite benefits of some of them could better solve the problem . in this paper , we introduce a solution for prolonging the lifetime of sensor networks . the proposed solution is based on a deployment strategy of multiple mobile sinks . in our proposal , data traffic is directed away from the network center toward the network peripheral where sinks would be initially deployed . sinks stay stationary while collecting the data reports that travel over the network perimeter toward them . eventually perimeter nodes would be exposed to a peeling phenomenon which results in partitioning one or more sinks from their one hop neighbors . the partitioned sinks move discrete steps following the direction of the progressive peeling towards the network center . the mechanism maintains the network connectivity and delays the occurrence of partition . moreover , it balances the load among nodes and reduces the energy consumption . the performance of the proposed protocol is evaluated using intensive simulations . the results show the efficiency ( in terms of both reliability and connectivity ) of our deployment strategy with the associated data collection protocol .
design and applications of an algorithm benchmark system in a computational problem solving environment . <eos> benchmark tests are often used to evaluate the quality of products by a set of common criteria . in this paper we describe a computational problem solving environment based on open source codes and an algorithm benchmark system , which is embedded in the environment as a plug in system . the algorithm benchmark system can be used to compare the performance of various algorithms or to evaluate the behavior of an algorithm with different input instances . the current implementation allows users to compare or evaluate algorithms written in c c . some examples of the algorithm benchmark system that evaluates the memory utilization , time complexity and the output of algorithms are also presented . algorithm benchmark impresses the learning effect students can not only comprehend the performance of respective algorithms but also write their own programs to challenge the best known results .
automated performance tuning . <eos> this tutorial presents automated techniques for implementing and optimizing numeric and symbolic libraries on modern computing platforms including sse , multicore , and gpu . obtaining high performance requires effective use of the memory hierarchy , short vector instructions , and multiple cores . highly tuned implementations are difficult to obtain and are platform dependent . for example , intel core i7 <digit> xe has a peak floating point performance of over <digit> gflops and the nvidia tesla c870 has a peak floating point performance of over <digit> gflops , however , achieving close to peak performance on such platforms is extremely difficult . consequently , automated techniques are now being used to tune and adapt high performance libraries such as atlas ( math atlas.sourceforge.net ) , plasma ( icl.cs.utk.edu plasma ) and magma ( icl.cs.utk.edu magma ) for dense linear algebra , oski ( bebop.cs.berkeley.edu oski ) for sparse linear algebra , fftw ( www.fftw.org ) for the fast fourier transform ( fft ) , and spiral ( www.spiral.net ) for wide class of digital signal processing ( dsp ) algorithms . intel currently uses spiral to generate parts of their mkl and ipp libraries .
explicit solutions for a class indirect pharmacodynamic response models . <eos> explicit solutions for four , ordinary differential equation ( ode ) based , types of indirect response models are presented . these response models were introduced by dayneka et al in <digit> j. pharmacokinet . biopharm . <digit> ( <digit> ) <digit> to describe pharmacodynamic responses utilizing inhibitory or stimulatory em , x type functions . the explicit solutions are expressed in terms of hypergeometric f <digit> ( <digit> ) functions and their analytical continuations . a practical application is demonstrated for modeling the kinetics of drug action for ibandronate , a potent bisphosphonate that suppresses bone turnover resulting in a reduction in the markers of bone turnover . ten times shorter model evaluation times , with the explicit solution compared with the differential equation implementation , may enhance situations where a large number of model evaluations are needed , such as clinical trial simulations and parameter estimation . ( c ) <digit> elsevier ireland ltd. all rights reserved .
a web based consumer oriented intelligent decision support system for personalized e services . <eos> due to the rapid advancement of electronic commerce and web technologies in recent years , the concepts and applications of decision support systems have been significantly extended . one quickly emerging research topic is the consumer oriented decision support system that provides functional supports to consumers for efficiently and effectively making personalized decisions . in this paper we present an integrated framework for developing web based consumer oriented intelligent decision support systems to facilitate all phases of consumer decision making process in business to consumer e services applications . major application functional modules comprised in the system framework include consumer and personalized management , navigation and search , evaluation and selection , planning and design , community and collaboration management , auction and negotiation , transactions and payments , quality and feedback control , as well as communications and information distributions . system design and implementation methods will be illustrated using an example . also explored are various potential e services application domains including e tourism and e investment .
efficient segment based video transcoding proxy for mobile multimedia services . <eos> to support various bandwidth requirements for mobile multimedia services for future heterogeneous mobile environments , such as portable notebooks , personal digital assistants ( pdas ) , and 3g cellular phones , a transcoding video proxy is usually necessary to provide mobile clients with adapted video streams by not only transcoding videos to meet different needs on demand , but also caching them for later use . traditional proxy technology is not applicable to a video proxy because it is less cost effective to cache the complete videos to fit all kinds of clients in the proxy . since transcoded video objects have inheritance dependency between different bit rate versions , we can use this property to amortize the retransmission overhead from transcoding other objects cached in the proxy . in this paper , we propose the object relation graph ( org ) to manage the static relationships between video versions and an efficient replacement algorithm to dynamically manage video segments cached in the proxy . specifically , we formulate a transcoding time constrained profit function to evaluate the profit from caching each version of an object . the profit function considers not only the sum of the costs of caching individual versions of an object , but also the transcoding relationship among these versions . in addition , an effective data structure , cached object relation tree ( cort ) , is designed to facilitate the management of multiple versions of different objects cached in the transcoding proxy . experimental results show that the proposed algorithm outperforms companion schemes in terms of the byte hit ratios and the startup latency .
automated process planning method to machine a b spline free form feature on a mill turn center . <eos> in this paper , we present a methodology for automating the process planning and nc code generation for a widely encountered class of free form features that can be machined on a <digit> axis mill turn center . the free form feature family that is considered is that of extruded protrusions whose cross section is a closed , periodic b spline curve . in this methodology , for machining a part with b spline protrusion located at the free end , the part is first rough turned to the maximum profile diameter of the b spline , followed by rough profile cutting and finish profiling with axially mounted end mill tools . the identification and sequencing of machining volumes is completely automated , as is the generation of actual nc code . the approach supports both convex and non convex profiles . in the case of non convex profiles , the process planning algorithm ensures that there is no gouging of the work piece by the tool . the algorithm also identifies when sections of the tool path lie outside the work piece and utilizes rapid traverses in these regions to reduce cutting time . this methodology presents an integrated turn mill process planning where by making the process fully automated from design with no user intervention making the overall process planning efficient . the algorithm was tested on several examples and test parts using the unmodified nc code obtained from the implementation were run on a moriseiki mill turn center . the parts that were produced met the dimensional specifications of the desired part . ( c ) <digit> elsevier ltd. all rights reserved .
stability results for two classes of linear time delay and hybrid systems . <eos> the stability of linear time delay systems with point internal delays is difficult to deal with in practice because of the fact that their characteristic equation is usually of transcendent type rather than of polynomial type . this feature causes usually the system to possess an infinite number of poles . in this paper , stability tests for this class of systems are obtained either based on extensions of classical tests applicable to delay free systems or on approaches within the framework of two dimensional digital filters . some of those two dimensional stability tests are also proved to be useful for stability testing of a common class of linear hybrid systems which involve coupled continuous and digital substates after a slight ad hoc adaptation of the tests for that situation .
a pseudo nearest neighbor approach for missing data recovery on gaussian random data sets . <eos> missing data handling is an important preparation step for most data discrimination or mining tasks . inappropriate treatment of missing data may cause large errors or false results . in this paper , we study the effect of a missing data recovery method , namely the pseudo nearest neighbor substitution approach , on gaussian distributed data sets that represent typical cases in data discrimination and data mining applications . the error rate of the proposed recovery method is evaluated by comparing the clustering results of the recovered data sets to the clustering results obtained on the originally complete data sets . the results are also compared with that obtained by applying two other missing data handling methods , the constant default value substitution and the missing data ignorance ( non substitution ) methods . the experiment results provided a valuable insight to the improvement of the accuracy for data discrimination and knowledge discovery on large data sets containing missing values .
a lagrangian relaxation approach to the edge weighted clique problem . <eos> the b clique polytope cpnb is the convex hull of the node and edge incidence vectors of all subcliques of size at most b of a complete graph on n nodes . including the boolean quadric polytope qpn cpnn as a special case and being closely related to the quadratic knapsack polytope , it has received considerable attention in the literature . in particular , the max cut problem is equivalent with optimizing a linear function over cpnn . the problem of optimizing linear functions over cpnb has so far been approached via heuristic combinatorial algorithms and cutting plane methods . we study the structure of cpnb in further detail and present a new computational approach to the linear optimization problem based on the idea of integrating cutting planes into a lagrangian relaxation of an integer programming problem that balas and christofides had suggested for the traveling salesman problem . in particular , we show that the separation problem for tree inequalities becomes polynomial in our lagrangian framework . finally , computational results are presented .
resource aware programming in the pixie os . <eos> this paper presents pixie , a new sensor node operating system designed to support the needs of data intensive applications . these applications , which include high resolution monitoring of acoustic , seismic , acceleration , and other signals , involve high data rates and extensive in network processing . given the fundamentally resource limited nature of sensor networks , a pressing concern for such applications is their ability to receive feedback on , and adapt their behavior to , fluctuations in both resource availability and load . the pixie os is based on a dataflow programming model based on the concept of resource tickets , a core abstraction for representing resource availability and reservations . by giving the system visibility and fine grained control over resource management , a broad range of policies can be implemented . to shield application programmers from the burden of managing these details , pixie provides a suite of resource brokers , which mediate between low level physical resources and higher level application demands . pixie is implemented in nesc and supports limited backwards compatibility with tinyos . we describe pixie in the context of two applications limb motion analysis for patients undergoing treatment for motion disorders , and acoustic target detection using a network of microphones . we present a range of experiments demonstrating pixie 's ability to accurately account for resource availability at runtime and enable a range of both generic and application specific adaptations .
highly undersampled magnetic resonance image reconstruction via homotopic l ( <digit> ) minimization . <eos> in clinical magnetic resonance imaging ( mri ) , any reduction in scan time offers a number of potential benefits ranging from high temporal rate observation of physiological processes to improvements in patient comfort . following recent developments in compressive sensing ( cs ) theory , several authors have demonstrated that certain classes of mr images which possess sparse representations in some transform domain can be accurately reconstructed from very highly undersampled k space data by solving a convex l ( <digit> ) minimization problem . although l ( <digit> ) based techniques are extremely powerful , they inherently require a degree of oversampling above the theoretical minimum sampling rate to guarantee that exact reconstruction can be achieved . in this paper , we propose a generalization of the cs paradigm based on homotopic approximation of the l ( <digit> ) quasi norm and show how mr image reconstruction can be pushed even further below the nyquist limit and significantly closer to the theoretical bound . following a brief review of standard cs methods and the developed theoretical extensions , several example mri reconstructions from highly undersampled k space data are presented .
an incremental verification algorithm for real time systems . <eos> we present an incremental algorithm for model checking the red time systems against the requirements specified in the real time extension of modal mu calculus . using this algorithm , we avoid the repeated construction and analysis of the whole state space during the course of evolution of the system from time to time . we use a finite representation of the system , like most other algorithms on real time systems . we construct and update a graph ( called tsg ) that is derived from the region graph and the formula . this allows us to halt the construction of this graph when enough nodes have been explored to determine the truth of the formula . tsg is minimal in the sense of partitioning the infinite state space into regions and it expresses a relation on the set of regions of the partition . we use the structure of the formula to derive this partition . when a change is applied to the timed automaton of the system , we find a new partition from the current partition and the tsg with minimum cost .
a survey on transport protocols for wireless multimedia sensor networks . <eos> wireless networks composed of multimedia enabled resource constrained sensor nodes have enriched a large set of monitoring sensing applications . in such communication scenario , however , new challenges in data transmission and energy efficiency have arisen due to the stringent requirements of those sensor networks . generally , congested nodes may deplete the energy of the active congested paths toward the sink and incur in undesired communication delay and packet dropping , while bit errors during transmission may negatively impact the end to end quality of the received data . many approaches have been proposed to face congestion and provide reliable communications in wireless sensor networks , usually employing some transport protocol that address one or both of these issues . nevertheless , due to the unique characteristics of multimedia based wireless sensor networks , notably minimum bandwidth demand , bounded delay and reduced energy consumption requirement , communication protocols from traditional scalar wireless sensor networks are not suitable for multimedia sensor networks . in the last decade , such requirements have fostered research in adapting existing protocols or proposing new protocols from scratch . we survey the state of the art of transport protocols for wireless multimedia sensor networks , addressing the recent developments and proposed strategies for congestion control and loss recovery . future research directions are also discussed , outlining the remaining challenges and promising investigation areas .
two integrable couplings of the tu hierarchy and their hamiltonian structures . <eos> the double integrable couplings of the tu hierarchy are worked out by use of vector loop algebras g <digit> and g <digit> respectively . also the hamiltonian structures of the obtained system are given by the quadratic form identity .
dynamic simulation of bioreactor systems using orthogonal collocation on finite elements . <eos> the dynamics of continuous biological processes is addressed in this paper . numerical simulation of a conventional activated sludge process shows that despite the large differences in the dynamics of the species investigated . the orthogonal collocation on finite element technique with three internal collocation and four elements ( ocfe <digit> ) gives excellent numerical results for bioreactor models up to a peclet number of <digit> . it is shown that there is little improvement in numerical accuracy when a much larger internal collocation point is introduced . over and above peclet number of <digit> , considered to be large for this process . simulation with the global orthogonal collocation ( goc ) technique is infeasible . due to the banded nature of its structural matrix , the method of lines ( mol ) technique requires the lowest computing time , typically four times less than that required by the ocfe <digit> . validation of the hydraulics of an existing pilot scale subsurface flow ( ssf ) constructed wetland process using the aforementioned numerical techniques suggested that the ocfe is superior to the mol and goc in terms of numerical stability , ( c ) <digit> elsevier science ltd. all rights reserved .
detecting regularities on grammar compressed strings . <eos> we address the problems of detecting and counting various forms of regularities in a string represented as a straight line program ( slp ) which is essentially a context free grammar in the chomsky normal form . given an slp of size n that represents a string s of length n , our algorithm computes all runs and squares in s in o ( n3h ) o ( n <digit> h ) time and o ( n2 ) o ( n <digit> ) space , where h is the height of the derivation tree of the slp . we also show an algorithm to compute all gapped palindromes in o ( n3h gnhlog n ) o ( n <digit> h g n h log n ) time and o ( n2 ) o ( n <digit> ) space , where g is the length of the gap . as one of the main components of the above solution , we propose a new technique called approximate doubling which seems to be a useful tool for a wide range of algorithms on slps . indeed , we show that the technique can be used to compute the periods and covers of the string in o ( n2h ) o ( n <digit> h ) time and o ( nh ( n log2 n ) ) o ( n h ( n log <digit> n ) ) time , respectively .
achieving reusability and composability with a simulation conceptual model . <eos> reusability and composability ( r c ) are two important quality characteristics that have been very difficult to achieve in the modelling and simulation ( m s ) discipline . reuse provides many technical and economical benefits . composability has been increasingly crucial for m s of a system of systems , in which disparate systems are composed with each other . the purpose of this paper is to describe how r c can be achieved by using a simulation conceptual model ( cm ) in a community of interest ( coi ) . we address r c in a multifaceted manner covering many m s areas ( types ) . m s is commonly employed where r c are very much needed by many cois . we present how a cm developed for a coi can assist in r c for the design of any type of large scale complex m s application in that coi . a cm becomes an asset for a coi and offers significant economic benefits through its broader applicability and more effective utilization .
wavelength decomposition approach for computing blocking probabilities in multicast wdm optical networks . <eos> we present an approximate analytical method to evaluate the blocking probabilities in multicast wavelength division multiplexing ( wdm ) networks without wavelength converters . our approach is based on the wavelength decomposition approach in which the wdm network is divided into layers ( colors ) and the moment matching method is used to characterize the overflow traffic from one layer to another . analyzing blocking probabilities for unicast and multicast calls in each layer of the network is derived from an exact approach . we assume static routing with either first fit or random wavelength assignment algorithm . results are presented which indicate the accuracy of our method .
a new local meshless method for steady state heat conduction in heterogeneous materials . <eos> in this paper a truly meshless method based on the integral form of energy equation is presented to study the steady state heat conduction in the anisotropic and heterogeneous materials . the presented meshless method is based on the satisfaction of the integral form of energy balance equation for each sub particle ( sub domain ) inside the material . moving least square ( mls ) approximation is used for approximation of the field variable over the randomly located nodes inside the domain . in the absence of heat generation , the domain integration is eliminated from the formulation of presented method and the computational efforts are reduced substantially with respect to the conventional mlpg method . a direct method is presented for treatment of material discontinuity at the heterogeneous material in the presented meshless method . as a practical problem the heat conduction in fibrous composite material is studied and the steady state heat conduction in unidirectional fibermatrix composites is investigated . the solution domain includes a small area of the composite system called representative volume element ( rve ) . comparison of numerical results shows that the presented meshless method is simple , effective , accurate and less costly method for micromechanical analysis of heat conduction in heterogeneous materials .
a territory defining multiobjective evolutionary algorithms and preference incorporation . <eos> we have developed a steady state elitist evolutionary algorithm to approximate the pareto optimal frontiers of multiobjective decision making problems . the algorithms define a territory around each individual to prevent crowding in any region . this maintains diversity while facilitating the fast execution of the algorithm . we conducted extensive experiments on a variety of test problems and demonstrated that our algorithm performs well against the leading multiobjective evolutionary algorithms . we also developed a mechanism to incorporate preference information in order to focus on the regions that are appealing to the decision maker . our experiments show that the algorithm approximates the pareto optimal solutions in the desired region very well when we incorporate the preference information .
a holistic frame of reference for modelling social systems . <eos> purpose to outline a philosophical system of inquiry that may be used as a frame of reference for modelling social systems . design methodology approach the paper draws on insights from cognitive science , autopoiesis , management cybernetics and non linear dynamics . findings the outcome of this paper is an outline of a frame of reference to be used as a starting point ( or a frame of orientation ) for any problem solving modelling intent or act . the framework highlights the importance of epistemological reflection and the need to avoid any separation of the process of knowing from that of modelling . it also emphasises the importance of inquiry into the assumptions that underpin the methods , tools and techniques that we employ , and into the tacit beliefs of the human actors who use them . research limitations implications the presented frame of reference should be regarded as an evolving system of inquiry , one that seeks to incorporate contemporary human insight . practical implications exactly , how the frame of reference presented in this paper should be exploited within an organisational or educational context , is a question to which there is no single correct answer . what is primarily important , however , is that it should be used to raise the profile of , and disseminate the benefits that accrue from , inquiry which goes beyond the simple application of tools and methods . originality value this paper proposes a new framework of reference for modelling social systems that draws on insights from cognitive science , autopoiesis , management cybernetics and non linear dynamics .
a source synchronous double data rate parallel optical transceiver ic . <eos> source synchronous double data rate ( ddr ) signaling is widely used in electrical interconnects to eliminate clock recovery and to double communication bandwidth . this paper describes the design of a parallel optical transceiver integrated circuit ( ic ) that uses source synchronous ddr optical signaling . on the transmit side , two <digit> b electrical inputs are multiplexed , encoded , and sent over two high speed optical links . on the receive side , the procedure is reversed to produce two <digit> b electrical outputs . the proposed ic integrates analog vertical cavity surface emitting lasers ( vcsels ) , drivers and optical receivers with digital ddr multiplexing , serialization , and deserialization circuits . it was fabricated in a 0.5 mu m silicon on sapphire ( sos ) complementary metal oxide semiconductor ( cmos ) process . linear arrays of quad vcsels and photodetectors were attached to the proposed transceiver ic using hip chip bonding . a free space optical link system was constructed to demonstrate correct ic functionality . the test results show successful transceiver operation at a data rate of <digit> mb s with a <digit> mhz ddr clock , achieving a gigabit of aggregate bandwidth . while the proposed ddr scheme is well suited for low skew fiber ribbon , free space , and waveguide optical links , it can also be extended to links with higher skew with the addition of skew compensation circuitry . to the authors ' knowledge , this is the first demonstration of parallel optical transceivers that use source synchronous ddr signaling .
fpcode an efficient approach for multi modal biometrics . <eos> although face recognition technology has progressed substantially , its performance is still not satisfactory due to the challenges of great variations in illumination , expression and occlusion . this paper aims to improve the accuracy of personal identification , when only few samples are registered as templates , by integrating multiple modal biometrics , i.e. face and palmprint . we developed in this paper a feature code , namely fpcode , to represent the features of both face and palmprint . though feature code has been used for palmprint recognition in literature , it is first applied in this paper for face recognition and multi modal biometrics . as the same feature is used , fusion is much easier . experimental results show that both feature level and decision level fusion strategies achieve much better performance than single modal biometrics . the proposed approach uses fixed length <digit> <digit> bits coding scheme that is very efficient in matching , and at the same time achieves higher accuracy than other fusion methods available in literature .
kinetics and energetics during uphill and downhill carrying of different weights . <eos> during physically heavy work tasks the musculoskeletal tissues are exposed to both mechanical and metabolic loading . the aim of the present study was to test a biomechanical model for prediction of whole body energy turnover from kinematic and anthropometric data during load carrying . total loads of <digit> , <digit> and <digit> kg were carried symmetrically or asymmetrically in the hands , while walking on a treadmill ( 4.5 kmh <digit> ) horizontally , uphill , or downhill the slopes being <digit> % . mean values for the directly measured oxygen uptake ranged for all trials from 0.5 to 2.1 l o2min <digit> , and analysis of variance showed significant differences regarding slope , load carried , and symmetry . the calculated values of oxygen uptake based on the biomechanical model correlated significantly with the directly measured values , fitting to the line y 0.990 x 0.144 , where y is the estimated and x is the measured oxygen uptake in lmin <digit> . the close relationship between energy turnover rate measured directly and estimated based on a biomechanical model justifies the assessment of the metabolic load from kinematic data .
granular prototyping in fuzzy clustering . <eos> we introduce a logic driven clustering in which prototypes are formed and evaluated in a sequential manner . the way of revealing a structure in data is realized by maximizing a certain performance index ( objective function ) that takes into consideration an overall level of matching ( to be maximized ) and a similarity level between the prototypes ( the component to be minimized ) . the prototypes identified in the process come with the optimal weight vector that serves to indicate the significance of the individual features ( coordinates ) in the data grouping represented by the prototype . since the topologies of these groupings are in general quite diverse the optimal weight vectors are reflecting the anisotropy of the feature space , i.e. , they show some local ranking of features in the data space . having found the prototypes we consider an inverse similarity problem and show how the relevance of the prototypes translates into their granularity .
empirical evaluation of latency sensitive application performance in the cloud . <eos> cloud computing platforms enable users to rent computing and storage resources on demand to run their networked applications and employ virtualization to multiplex virtual servers belonging to different customers on a shared set of servers . in this paper , we empirically evaluate the efficacy of cloud platforms for running latency sensitive multimedia applications . since multiple virtual machines running disparate applications from independent users may share a physical server , our study focuses on whether dynamically varying background load from such applications can interfere with the performance seen by latency sensitive tasks . we first conduct a series of experiments on amazon 's ec2 system to quantify the cpu , disk , and network jitter and throughput fluctuations seen over a period of several days . we then turn to a laboratory based cloud and systematically introduce different levels of background load and study the ability to isolate applications under different settings of the underlying resource control mechanisms . we use a combination of micro benchmarks and two real world applications the doom <digit> game server and apple 's darwin streaming server for our experimental evaluation . our results reveal that the jitter and the throughput seen by a latency sensitive application can indeed degrade due to background load from other virtual machines . the degree of interference varies from resource to resource and is the most pronounced for disk bound latency sensitive tasks , which can degrade by nearly <digit> % under sustained background load . we also find that careful configuration of the resource control mechanisms within the virtualization layer can mitigate , but not eliminate , this interference .
the role of chineseamerican scientists in chinaus scientific collaboration a study in nanotechnology . <eos> in this paper , we use bibliometric methods and social network analysis to analyze the pattern of chinaus scientific collaboration on individual level in nanotechnology . results show that chineseamerican scientists have been playing an important role in chinaus scientific collaboration . we find that chinaus collaboration in nanotechnology mainly occurs between chinese and chineseamerican scientists . in the co authorship network , chineseamerican scientists tend to have higher betweenness centrality . moreover , the series of polices implemented by the chinese government to recruit oversea experts seems to contribute a lot to chinaus scientific collaboration .
localization of spherical fruits for robotic harvesting . <eos> the orange picking robot ( opr ) is a project for developing a robot that is able to harvest oranges automatically . one of the key tasks in this robotic application is to identify the fruit and to measure its location in three dimensions . this should be performed using image processing techniques which must be sufficiently robust to cope with variations in lighting conditions and a changing environment . this paper describes the image processing system developed so far to guide automatic harvesting of oranges , which here has been integrated in the first complete full scale prototype opr .
game based learning for computer science education . <eos> today , learners increasingly demand for innovative and motivating learning scenarios that strongly respond to their habits of using media . one of the many possible solutions to this demand is the use of computer games to support the acquisition of knowledge . this paper reports on chances and challenges of applying a game based learning scenario for the acquisition of it knowledge as realized by the german bmbf project spitkom . after briefly describing the learning potential of multiplayer browser games as well as the educational objectives and target group of the spitkom project , we will present the main results of a study that was carried out in the first phase of the project to guide the game design . in the course of the study , data were collected regarding ( a ) the computer game preferences of the target group and ( b ) the target group 's competencies in playing computer games . we will then introduce recommendations that were deduced from the study 's findings and that outline the concept and the prototype of the game .
efficient evaluation functions for evolving coordination . <eos> this paper presents fitness evaluation functions that efficiently evolve coordination in large multi component systems . in particular , we focus on evolving distributed control policies that are applicable to dynamic and stochastic environments . while it is appealing to evolve such policies directly for an entire system , the search space is prohibitively large in most cases to allow such an approach to provide satisfactory results . instead , we present an approach based on evolving system components individually where each component aims to maximize its own fitness function . though this approach sidesteps the exploding state space concern , it introduces two new issues ( <digit> ) how to create component evaluation functions that are aligned with the global evaluation function and ( <digit> ) how to create component evaluation functions that are sensitive to the fitness changes of that component , while remaining relatively insensitive to the fitness changes of other components in the system . if the first issue is not addressed , the resulting system becomes uncoordinated if the second issue is not addressed , the evolutionary process becomes either slow to converge or worse , incapable of converging to good solutions . this paper shows how to construct evaluation functions that promote coordination by satisfying these two properties . we apply these evaluation functions to the distributed control problem of coordinating multiple rovers to maximize aggregate information collected . we focus on environments that are highly dynamic ( changing points of interest ) , noisy ( sensor and actuator faults ) , and communication limited ( both for observation of other rovers and points of interest ) forcing the rovers to evolve generalized solutions . on this difficult coordination problem , the control policy evolved using aligned and component sensitive evaluation functions outperforms global evaluation functions by up to <digit> % . more notably , the performance improvements increase when the problems become more difficult ( larger , noisier , less communication ) . in addition we provide an analysis of the results by quantifying the two characteristics ( alignment and sensitivity discussed above ) leading to a systematic study of the presented fitness functions .
contention free communication scheduling for array redistribution . <eos> array redistribution is required often in programs on distributed memory parallel computers . it is essential to use efficient algorithms for redistribution , otherwise the performance of the programs may degrade considerably . the redistribution overheads consist of two parts index computation and interprocessor communication . if there is no communication scheduling in a redistribution algorithm , the communication contention may occur , which increases the communication waiting time . in order to solve this problem , in this paper , we propose a technique to schedule the communication so that it becomes contention free . our approach initially generates a communication table to represent the communication relations among sending nodes and receiving nodes . according to the communication table , we then generate another table named communication scheduling table . each column of communication scheduling table is a permutation of receiving node numbers in each communication step . thus the communications in our redistribution algorithm are contention free . our approach can deal with multi dimensional shape changing redistribution .
quadratic weighted median filters for edge enhancement of noisy images . <eos> quadratic volterra filters are effective in image sharpening applications . the linear combination of polynomial terms , however , yields poor performance in noisy environments . weighted median ( wm ) filters , in contrast , are well known for their outlier suppression and detail preservation properties . the wm sample selection methodology is naturally extended to the quadratic sample case , yielding a filter structure referred to as quadratic weighted median ( qwm ) that exploits the higher order statistics of the observed samples while simultaneously being robust to outliers arising in the higher order statistics of environment noise . through statistical analysis of higher order samples , it is shown that , although the parent gaussian distribution is light tailed , the higher order terms exhibit heavy tailed distributions . the optimal combination of terms contributing to a quadratic system , i.e. , cross and square , is approached from a maximum likelihood perspective which yields the wm processing of these terms . the proposed qwm filter structure is analyzed through determination of the output variance and breakdown probability . the studies show that the qwm exhibits lower variance and breakdown probability indicating the robustness of the proposed structure . the performance of the qwm filter is tested on constant regions , edges and real images , and compared to its weighted sum dual , the quadratic volterra filter . the simulation results show that the proposed method simultaneously suppresses the noise and enhances image details . compared with the quadratic volterra sharpener , the qwm filter exhibits superior qualitative and quantitative performance in noisy image sharpening .
elra european language resources association background , recent developments and future perspectives . <eos> the european language resources association ( elra ) was founded in <digit> with the mission of providing language resources ( lr ) to european research institutions and companies . in this paper we describe the background , the mission and the major activities since then .
multiple concurrence of multi partite quantum system . <eos> we propose a new way of description of the global entanglement property of a multi partite pure state quantum system . based on the idea of bipartite concurrence , by dividing the multi partite quantum system into two subsystems , a combination of all the bipartite concurrences of a multipartite quantum system is used to describe the entanglement property of the multi partite system . we derive the analytical results for ghz state , w state with arbitrary number of qubits , and cluster state with the number of particles no greater than <digit> .
tolerant information retrieval with backpropagation networks . <eos> neural networks can learn fi om human decisions and preferences . especially in , human computer interaction , adaptation to the behaviour and expectations of the user is necessary . ih information retrieval , an important area within human computer interaction , expectations are difficult to meet . the inherently vague nature of information retrieval has bed to the application of vague processing techniques . neural networks seem to have great potential to model the cognitive processes involved more appropriately . current models based on neural networks and their implications for human computer interaction ar e analysed . cosimir ( cognitive similarity learning in information retrieval ) , an innovative model integrating human knowledge into the core of the retrieval process , is presented . it applies backpropagation to information retrieval , integrating human centred and soft and tolerant computing into the core of the retrieval process . a further backpropagation model , the transformation network for heterogeneous data sources , is discussed . empirical evaluations have provided promising results .
approximation of mean time between failure when a system has periodic maintenance . <eos> this paper describes a simple technique for estimating the mean time between failure ( mtbf ) of a system that has periodic maintenance at regular intervals . this type of maintenance is typically found in high reliability , mission oriented applications where it is convenient to perform maintenance after the completion of the mission . this approximation technique can greatly simplify the mtbf analysis for large systems . the motivation for this analysis was to understand the nature of the error in the approximation and to develop a means for quantifying that error . this paper provides the derivation of the equations that bound the error that can result when using this approximation method . it shows that , for most applications , the mtbf calculations can be greatly simplified with only a very small sacrifice in accuracy .
supplying web 2.0 an empirical investigation of the drivers of consumer transmutation of culture oriented digital information goods . <eos> this paper describes an empirical study of behaviors associated with consumers ' creative modification of digital information goods found in web 2.0 and elsewhere . they are products of culture such as digital images , music , video , news and computer games . we will refer to them as digital culture products . how do consumers who transmute such products differ from those who do not , and from each other this study develops and tests a theory of consumer behavior in transmuting digital culture products , separating consumers into different groups based on how and why they transmute . with our theory , we posit these groups as having differences of motivation , as measured by product involvement and innovativeness , and of ability as measured by computer skills . a survey instrument to collect data from internet capable computer users on the relevant constructs , and on their transmutation activities , is developed and distributed using a web based survey hosting service . the data are used to test hypotheses that consumers ' enduring involvement and innovativeness are positively related to transmutation behaviors , and that computer self efficacy moderates those relationships . the empirical results support the hypotheses that enduring involvement and innovativeness do motivate transmutation behavior . the data analysis also supports the existence of a moderating relationship of computer self efficacy with respect to enduring involvement , but not of computer self efficacy with respect to innovativeness . the findings further indicate that transmutation activities should be expected to impact web 2.0 oriented companies , both incumbents and start ups , as they make decisions about how to incorporate consumers into their business models not only as recipients of content , but also as its producers . ( c ) <digit> elsevier b. v. all rights reserved .
polynomial cost for solving ivp for high index dae . <eos> we show that the cost of solving initial value problems for high index differential algebraic equations is polynomial in the number of digits of accuracy requested . the algorithm analyzed is built on a taylor series method developed by pryce for solving a general class of differential algebraic equations . the problem may be fully implicit , of arbitrarily high fixed index and contain derivatives of any order . we give estimates of the residual which are needed to design practical error control algorithms for differential algebraic equations . we show that adaptive meshes are always more efficient than non adaptive meshes . finally , we construct sufficiently smooth interpolants of the discrete solution .
a novel wavelength hopping passive optical network ( wh pon ) for provision of enhanced physical security . <eos> a novel secure wavelength hopping passive optical network ( wh pon ) is presented in which physical layer security is introduced to the access network . the wh pon design uses a pair of matched tunable lasers in the optical line terminal to create a time division multiplexed signal in which each data frame is transmitted at a unique wavelength . the transmission results for a <digit> channel wh pon operating at a data rate of 2.5 gb s are presented in this paper . the inherent security of the wh pon design is verified through an attempted cross channel eavesdropping attempt at an optical network unit . the results presented verify that the wh pon provides secure broadband service in the access network .
on the information flow required for tracking control in networks of mobile sensing agents . <eos> we design controllers that permit mobile agents with distributed or networked sensing capabilities to track ( follow ) desired trajectories , identify what trajectory information must be distributed to each agent for tracking , and develop methods to minimize the communication needed for the trajectory information distribution .
analysis of timing based mutual exclusion with random times . <eos> various timing based mutual exclusion algorithms have been proposed that guarantee mutual exclusion if certain timing assumptions hold . in this paper , we examine how these algorithms behave when the time for the basic operations is governed by probability distributions . in particular , we are concerned with how often such algorithms succeed in allowing a processor to obtain a critical region and how this success rate depends on the random variables involved . we explore this question in the case where operation times are governed by exponential and gamma distributions , using both theoretical analysis and simulations .
modeling virtual worlds in databases . <eos> a method of modeling virtual worlds in databases is presented . the virtual world model is conceptually divided into several distinct elements , which are separately represented in a database . the model pen nits to dynamically generate virtual scenes . ( c ) <digit> published by elsevier b.v.
an efficient scheduling algorithm for scalable video streaming over p2p networks . <eos> during recent years , the internet has witnessed rapid advancement in peer to peer ( p2p ) media streaming . in these applications , an important issue has been the block scheduling problem , which deals with how each node requests the media data blocks from its neighbors . in most streaming systems , peers are likely to have heterogeneous upload download bandwidths , leading to the fact that different peers probably perceive different streaming quality . layered ( or scalable ) streaming in p2p networks has recently been proposed to address the heterogeneity of the network environment . in this paper , we propose a novel block scheduling scheme that is aimed to address the p2p layered video streaming . we define a soft priority function for each block to be requested by a node in accordance with the blocks significance for video playback . the priority function is unique in that it strikes good balance between different factors , which makes the priority of a block well represent the relative importance of the block over a wide variation of block size between different layers . the block scheduling problem is then transformed to an optimization problem that maximizes the priority sum of the delivered video blocks . we develop both centralized and distributed scheduling algorithms for the problem . simulation of two popular scalability types has been conducted to evaluate the performance of the algorithms . the simulation results show that the proposed algorithm is effective in terms of bandwidth utilization and video quality .
a threshold for a polynomial solution of 2sat . <eos> the sat problem is a classical p complete problem even for monotone , horn and two conjunctive formulas ( the last known as 2sat ) . we present a novel branch and bound algorithm to solve the 2sat problem exactly . our procedure establishes a new threshold where 2sat can be computed in polynomial time . we show that for any <digit> cf formula f with n variables where 2sat ( f ) < p ( n ) , for some polynomial p , 2sat ( f ) is computed in polynomial time . this is a new way to measure the degree of difficulty for solving 2sat and , according to such measure our algorithm allows to determine a boundary between ' hard ' and ' easy ' instances of the 2sat problem .
bagging and boosting statistical machine translation systems . <eos> in this article we address the issue of generating diversified translation systems from a single statistical machine translation ( smt ) engine for system combination . unlike traditional approaches , we do not resort to multiple structurally different smt systems , but instead directly learn a strong smt system from a single translation engine in a principled way . our approach is based on bagging and boosting which are two instances of the general framework of ensemble learning . the basic idea is that we first generate an ensemble of weak translation systems using a base learning algorithm , and then learn a strong translation system from the ensemble . one of the advantages of our approach is that it can work with any of current smt systems and make them stronger almost for free . beyond this , most system combination methods are directly applicable to the proposed framework for generating the final translation system from the ensemble of weak systems . we evaluate our approach on chinese english translation in three state of the art smt systems , including a phrase based system , a hierarchical phrase based system and a syntax based system . experimental results on the nist mt evaluation corpora show that our approach leads to significant improvements in translation accuracy over the baselines . more interestingly , it is observed that our approach is able to improve the existing system combination systems . the biggest improvements are obtained by generating weak systems using bagging boosting , and learning the strong system using a state of the art system combination method . ( c ) <digit> elsevier b.v. all rights reserved .
functional dimensioning and tolerancing software for concurrent engineering applications . <eos> this paper describes the development of a prototype software package for solving functional dimensioning and tolerancing ( fd t ) problems in a concurrent engineering environment . it provides a systematic way of converting functional requirements of a product into dimensional specifications by means of the following steps firstly , the relationships necessary for solving fd t problems are represented in a matrix form , known as functional requirements dimensions ( fr d ) matrix . secondly , the values of dimensions and tolerances are then determined by satisfying all these relationships represented in a fr d matrix by applying a comprehensive strategy which includes tolerance allocation strategies for different types of fd t problems and for determining an optimum solution order for coupled functional equations . the prototype software is evaluated by its potential users , and the results indicate that it can be an effective computer based tool for solving fd t problems in a ce environment . ( c ) <digit> elsevier b.v. all rights reserved .
parametric model checking of stopwatch petri nets . <eos> at the border between control and verification , parametric verification can be used to synthesize constraints on the parameters to ensure that a system verifies given specifications . in this paper we propose a new framework for the parametric verification of time petri nets with stopwatches . we first introduce a parametric extension of time petri nets with inhibitor arcs ( itpns ) with temporal parameters and we define a symbolic representation of the parametric state space based on the classical state class graph method . then , we propose semi algorithms for the parametric model checking of a subset of parametric tctl formulae on itpns . these results have been implemented in the tool romeo and we illustrate them in a case study based on a scheduling problem .
modelling the interaction of catecholamines with the alpha ( 1a ) adrenoceptor towards a ligand induced receptor structure . <eos> adrenoceptors are members of the important g protein coupled receptor family for which the detailed mechanism of activation remains unclear . in this study , we have combined docking and molecular dynamics simulations to model the ligand induced effect on an homology derived human alpha ( 1a ) adrenoceptor . analysis of agonist alpha ( 1a ) adrenoceptor complex interactions focused on the role of the charged amine group , the aromatic ring , the n methyl group of adrenaline , the beta hydroxyl group and the catechol meta and para hydroxyl groups of the catecholamines . the most critical interactions for the binding of the agonists are consistent with many earlier reports and our study suggests new residues possibly involved in the agonist binding site , namely thr <digit> and cys <digit> . we further observe a number of structural changes that occur upon agonist binding including a movement of tm v away from tm iii and a change in the interactions of asp <digit> of the conserved dry motif . this may cause arg <digit> to move out of the tm helical bundle and change the orientation of residues in ic ii and ic iii , allowing for increased affinity of coupling to the g protein .
probabilistic string similarity joins . <eos> edit distance based string similarity join is a fundamental operator in string databases . increasingly , many applications in data cleaning , data integration , and scientific computing have to deal with fuzzy information in string attributes . despite the intensive efforts devoted in processing ( deterministic ) string joins and managing probabilistic data respectively , modeling and processing probabilistic strings is still a largely unexplored territory . this work studies the string join problem in probabilistic string databases , using the expected edit distance ( eed ) as the similarity measure . we first discuss two probabilistic string models to capture the fuzziness in string values in real world applications . the string level model is complete , but may be expensive to represent and process . the character level model has a much more succinct representation when uncertainty in strings only exists at certain positions . since computing the eed between two probabilistic strings is prohibitively expensive , we have designed efficient and effective pruning techniques that can be easily implemented in existing relational database engines for both models . extensive experiments on real data have demonstrated order of magnitude improvements of our approaches over the baseline .
a high performance simulator of the immune response . <eos> the application of concepts and methods of statistical mechanics to biological problems is one of the most promising frontiers of computational physics . for instance cellular automata ( ca ) , i.e. fully discrete dynamical systems evolving according to boolean laws , appear to be extremely well suited to the simulation of the immune system dynamics . a prominent example of immunological ca is represented by the celadaseiden automaton that has proven capable of providing several new insights into the dynamics of the immune system response . in the present paper we describe a parallel version of the celadaseiden automaton . details on the parallel implementation as well as performance data on the ibm sp2 parallel platform are presented and commented on .
speaker adaptation of language and prosodic models for automatic dialog act segmentation of speech . <eos> speaker dependent modeling has a long history in speech recognition , but has received less attention in speech understanding . this study explores speaker specific modeling for the task of automatic segmentation of speech into dialog acts ( das ) , using a linear combination of speaker dependent and speaker independent language and prosodic models . data come from <digit> frequent speakers in the icsi meeting corpus adaptation data per speaker ranges from 5k to 115k words . we compare performance for both reference transcripts and automatic speech recognition output . we find that ( <digit> ) speaker adaptation in this domain results both in a significant overall improvement and in improvements for many individual speakers , ( <digit> ) the magnitude of improvement for individual speakers does not depend on the amount of adaptation data , and ( <digit> ) language and prosodic models differ both in degree of improvement , and in relative benefit for specific da classes . these results suggest important future directions for speaker specific modeling in spoken language understanding tasks .
an efficient method for electromagnetic scattering analysis . <eos> we present a novel method to solve the magnetic field integral equation ( mfie ) using the method of moments ( mom ) efficiently . this method employs a linear combination of the divergence conforming raowiltonglisson ( rwg ) function and the curl conforming nrwg function to test the mfie in mom . the discretization process and the relationship of this new testing function with the previously employed rwg and nrwg testing functions are presented . numerical results of radar cross section ( rcs ) data for objects with sharp edges and corners show that accuracy of the mfie can be improved significantly through the use of the new testing functions . at the same time , only the commonly used rwg basis functions are needed for this method .
empirical mode decomposition synthesis of fractional processes in 1d and 2d space . <eos> we report here on image texture analysis and on numerical simulation of fractional brownian textures based on the newly emerged empirical mode decomposition ( emd ) . emd introduced by n.e. huang et al. is a promising tool to non stationary signal representation as a sum of zero mean am fm components called intrinsic mode functions ( imf ) . recent works published by p. flandrin et al. relate that , in the case of fractional gaussian noise ( fgn ) , emd acts essentially as a dyadic filter bank that can be compared to wavelet decompositions . moreover , in the context of fgn identification , p. handrin et al. show that variance progression across imfs is related to hurst exponent h through a scaling law . starting with these recent results , we propose a new algorithm to generate fgn , and fractional brownian motion ( fbm ) of hurst exponent h from imfs obtained from emd of a white noise , i.e. ordinary gaussian noise ( fgn with h <digit> <digit> ) . ( c ) <digit> elsevier b.v. all rights reserved .
flow topology in a steady three dimensional lid driven cavity . <eos> we present in this paper a thorough investigation of three dimensional flow in a cubical cavity , subject to a constant velocity lid on its roof . in this steady state analysis , we adopt the mixed formulation on tri quadratic elements to preserve mass conservation . to resolve difficulties in the asymmetric and indefinite large size matrix equations , we apply the bicgstab solution solver . to achieve stability , weighting functions are designed in favor of variables on the upstream side . to achieve accuracy , the weighting functions are properly chosen so that false diffusion errors can be largely suppressed by the equipped streamline operator . our aim is to gain some physical insight into the vortical flow using a theoretically rigorous topological theory . to broaden our understanding of the vortex dynamics in the cavity , we also study in detail the longitudinal spiralling motion in the flow interior . ( c ) <digit> elsevier science ltd. all rights reserved .
image object classification using saccadic search , spatio temporal pattern encoding and self organisation . <eos> a method for extracting features from photographic images is investigated . the input image is through a saccadic search algorithm divided into a set of sub images , segmented and coded by a spatio temporal encoding engine . the input image is thus represented by a set of characteristic pattern signatures , well suited for classification by an unsupervised neural network . a strategy using multiple self organising feature maps ( som ) in a hierarchical manner is used . with this approach , using a certain degree of user selection , a database of sub images is grouped according to similarities in signature space .
theoretical properties of lfsrs for built in self test . <eos> linear feedback shift registers have been studied for a long time as interesting solutions for error detection and correction techniques in transmissions . in the test domain , and principally in built in self test applications , they are often used as generators of pseudo random test sequences . conversely , their potential to generate prescribed deterministic test sequences is dealt within more recent works , and nowadays , allows the investigation of efficient test with a pseudo deterministic bist technique . pseudo deterministic test sequences are composed of both deterministic and pseudo random test patterns and offer high fault coverage with a tradeoff between test length and hardware cost . in this paper , synthesis techniques for lfsrs that embed such kind of sequences are described .
two fixed parameter algorithms for vertex covering by paths on trees . <eos> vertex covering by paths on trees with applications in machine translation is the task to cover all vertices of a tree t ( v , e ) by choosing a minimum weight subset of given paths in the tree . the problem is np hard and has recently been solved by an exact algorithm running in o ( <digit> ( c ) center dot vertical bar v vertical bar ( <digit> ) ) time , where c denotes the maximum number of paths covering a tree vertex . we improve this running time to o ( <digit> ( c ) center dot c center dot vertical bar v vertical bar ) . on the route to this , we introduce the problem tree like weighted hitting set which might be of independent interest . in addition , for the unweighted case of vertex covering by paths on trees , we present an exact algorithm using a search tree of size o ( <digit> ( k ) center dot k ) , where k denotes the number of chosen covering paths . finally , we briefly discuss the existence of a size o ( k ( <digit> ) ) problem kernel . ( c ) <digit> elsevier b.v. all rights reserved .
graphical dynamic linear models specification , use and graphical transformations . <eos> in this work , we propose a dynamic graphical model as a tool for bayesian inference and forecasting in dynamic systems described by a series which is dependent on a state vector evolving according to a markovian law . we build sequential algorithms for the probabilities propagation . this sequentiality turns out to be represented by the dynamic graphical structure alter carrying out several goal oriented sequential graphical transformations . ( c ) <digit> elsevier science inc. all rights reserved . msg 60j99 68t30 62h99 .
alignment with non overlapping inversions and translocations on two strings . <eos> an inversion and a translocation are important in bio sequence analysis and motivate researchers to consider the sequence alignment problem using these operations . based on inversion and translocation , we introduce a new alignment problem with non overlapping inversions and translocationsgiven two strings x and y , find an alignment with non overlapping inversions and translocations for x and y. this problem has interesting application for finding a common sequence from two mutated sequences . we , in particular , consider the alignment problem when non overlapping inversions and translocations are allowed for both x and y. we design an efficient algorithm that determines the existence of such an alignment and retrieves an alignment , if exists .
a twist to partial least squares regression . <eos> a modification of the pls1 algorithm is presented . stepwise optimization over a set of candidate loading weights obtained by taking powers of the y x correlations and x standard deviations generalizes the classical pls1 based on y x covariances and hence adds flexibility to the modelling . when good linear predictions can be obtained , the suggested approach often finds models with fewer and more interpretable components . good performance is demonstrated when compared with the classical pls1 on calibration benchmark data sets . an important part of the comparisons is managed by a novel model selection strategy . the selection is based on choosing the simplest model among those with a cross validation error smaller than the pre specified significance limit of a chi ( <digit> ) statistic . copyright ( c ) <digit> john wiley sons , ltd .
a robust and efficient finite volume scheme for the discretization of diffusive flux on extremely skewed meshes in complex geometries . <eos> in this paper an improved finite volume scheme to discretize diffusive flux on a non orthogonal mesh is proposed . this approach , based on an iterative technique initially suggested by khosla p.k. khosla , s.g. rubin , a diagonally dominant second order accurate implicit scheme , computers and fluids <digit> ( <digit> ) <digit> and known as deferred correction , has been intensively utilized by muzaferija s. muzaferija , adaptative finite volume method for flow prediction using unstructured meshes and multigrid approach , ph.d. thesis , imperial college , <digit> and later fergizer and peric j.h. fergizer , m. peric , computational methods for fluid dynamics , springer , <digit> to deal with the non orthogonality of the control volumes . using a more suitable decomposition of the normal gradient , our scheme gives accurate solutions in geometries where the basic idea of muzaferija fails . first the performances of both schemes are compared for a poisson problem solved in quadrangular domains where control volumes are increasingly skewed in order to test their robustness and efficiency . it is shown that convergence properties and the accuracy order of the solution are not degraded even on extremely skewed mesh . next , the very stable behavior of the method is successfully demonstrated on a randomly distorted grid as well as on an anisotropically distorted one . finally we compare the solution obtained for quadrilateral control volumes to the ones obtained with a finite element code and with an unstructured version of our finite volume code for triangular control volumes . no differences can be observed between the different solutions , which demonstrates the effectiveness of our approach .
evaluation of trend localization with multi variate visualizations . <eos> multi valued data sets are increasingly common , with the number of dimensions growing . a number of multi variate visualization techniques have been presented to display such data . however , evaluating the utility of such techniques for general data sets remains difficult . thus most techniques are studied on only one data set . another criticism that could be levied against previous evaluations of multi variate visualizations is that the task does n't require the presence of multiple variables . at the same time , the taxonomy of tasks that users may perform visually is extensive . we designed a task , trend localization , that required comparison of multiple data values in a multi variate visualization . we then conducted a user study with this task , evaluating five multi variate visualization techniques from the literature ( brush strokes , data driven spots , oriented slivers , color blending , dimensional stacking ) and juxtaposed grayscale maps . we report the results and discuss the implications for both the techniques and the task .
an efficient reconfigurable multiplier architecture for galois field gf ( 2m ) . <eos> this paper describes an efficient architecture of a reconfigurable bit serial polynomial basis multiplier for galois field gf ( 2m ) , where <digit> < m m. the value m , of the irreducible polynomial degree , can be changed and so , can be configured and programmed . the value of m determines the maximum size that the multiplier can support . the advantages of the proposed architecture are ( i ) the high order of flexibility , which allows an easy configuration for different field sizes , and ( ii ) the low hardware complexity , which results in small area . by using the gated clock technique , significant reduction of the total multiplier power consumption is achieved .
experiences in building a grid based platform to serve earth observation training activities . <eos> earth observation data processing and storing can be done nowadays only using distributed systems . experiments dealing with a large amount of data are possible within the timeframe of a lesson and can give trainees the freedom to innovate . following these trends and ideas , we have built a proof of the concept platform , named gisheo , for earth observation educational tasks . it uses grid computing technologies to analyze and store remote sensing data , and combines them with elearning facilities . this paper provides an overview of the gisheo 's platform architecture and of its technical and innovative solutions . ( c ) <digit> elsevier b.v. all rights reserved .
field d path finding on weighted triangulated and tetrahedral meshes . <eos> classic shortest path algorithms operate on graphs , which are suitable for problems that can be represented by weighted nodes or edges . finding a shortest path through a set of weighted regions is more difficult and only approximate solutions tend to scale well . the field d algorithm efficiently calculates an approximate , interpolated shortest path through a set of weighted regions and was designed for navigating robots through terrains with varying characteristics . field d operates on unit grid or quad tree data structures , which require high resolutions to accurately model the boundaries of irregular world structures . in this paper , we extend the field d cost functions to 2d triangulations and 3d tetrahedral meshes structures which model polygonal world structures more accurately . since robots typically have limited resources available for computation and storage , we pay particular attention to computation and storage overheads when detailing our extensions . we begin by providing analytic solutions to the minimum of each cost function for 2d triangles and 3d tetrahedra . our triangle implementation provides a <digit> % improvement in performance over an existing triangle implementation . while our 3d extension to tetrahedra is the first full analytic extension of field d to 3d , previous work only provided an approximate minimization for a single cost function on a 3d cube with unit lengths . each cost function is expressed in terms of a general function whose characteristics can be exploited to reduce the calculations required to find a minimum . these characteristics can also be exploited to cache the majority of cost functions , producing a speedup of up to <digit> % in the 3d tetrahedral case . we demonstrate that , in environments composed of non grid aligned data , multi resolution quad tree field d requires an order of magnitude more faces and between <digit> and <digit> times more node expansions , to produce a path of similar cost to one produced by a triangle implementation of field d on a lower resolution triangulation . we provide examples of 3d pathing through models of complex topology , including pathing through anatomical structures extracted from a medical data set . to summarise , this paper details a robust and efficient extension of field d pathing to data sets represented by weighted triangles and tetrahedra , and also provides empirical data which demonstrates the reduction in storage and computation costs that accrue when one chooses such a representation over the more commonly used quad tree and grid based alternatives .
correlation analysis of signal flow in a model prefrontal cortical circuit representing multiple target locations . <eos> in spite of the recent cross correlation analyses of the monkey prefrontal cortical neurons performing spatial working memory tasks ( j. neurosci . <digit> ( <digit> ) <digit> cerebr . cortex <digit> ( <digit> ) <digit> ) , it is uncertain as to how much degree the correlation data reflect the circuitry of highly recurrent networks . we did a computer simulation of a model cortical circuit , whose connectivity is fully known , and analyzed the cross correlations of the spikes of pairs of neurons in the model . the result shows that cross correlation histograms ( cchs ) of pyramidalpyramidal pairs tend to mask higher order synaptic interactions , yielding cchs with central peaks or almost flat cchs . however , cchs of pyramidalinterneuron pairs show displaced positive and or negative peaks , depending on the connectivity of these neurons .
slow dynamic finite element simulation of manufacturing processes . <eos> explicit time integration and dynamic finite element formulations are increasingly being used to analyze nonlinear static problems in solid and structural mechanics . this is particularly true in the simulation of sheer metal manufacturing processes . employment of slow dynamic , quasi static techniques in static problems can introduce undesirable dynamic effects that originate from the inertia forces of the governing equations . in this paper , techniques and guidelines are presented , analyzed and demonstrated , which enable the minimization of the undesirable dynamic effects . the effect of the duration and functional form of the time histories of the loads and boundary conditions is quantified by the analysis of a linear spring mass oscillator . the resulting guidelines and techniques are successfully demonstrated in the nonlinear finite element simulation of a sheet metal deep drawing operation . the accuracy of the quasi static , slow dynamic finite element analyses is evaluated by comparison to results of laboratory experiments and purely static analyses . various measures that quantify the dynamic effects , including kinetic energy , also are discussed . ( c ) <digit> elsevier science ltd .
improving tcp performance in integrated wireless communications networks . <eos> many analytical and simulation based studies of tcp performance in wireless environments assume an error free and congestion free reverse channel that has the same capacity as the forward channel . such an assumption does not hold in many real world scenarios , particularly in the hybrid networks consisting of various wireless lan ( wlan ) and cellular technologies . in this paper , we first study , through extensive simulations , the performance characteristics of four representative tcp schemes , namely tcp new reno , sack , veno , and westwood , under the network conditions of asymmetric end to end link capacities , correlated wireless errors , and link congestion in both forward and reverse directions . we then propose a new tcp scheme , called tcp new jersey , which is capable of distinguishing wireless packet losses from congestion packet losses , and reacting accordingly . tcp new jersey consists of two key components , the timestamp based available bandwidth estimation ( tabe ) algorithm and the congestion warning ( cw ) router configuration . tabe is a tcp sender side algorithm that continuously estimates the bandwidth available to the connection and guides the sender to adjust its transmission rate when the network becomes congested . tabe is immune to the ack drops as well as ack compression . cw is a configuration of network routers such that routers alert end stations by marking all packets when there is a sign of an incipient congestion . the marking of packets by the cw configured routers helps the sender of the tcp connection to effectively differentiate packet losses caused by network congestion from those caused by wireless link errors . our simulation results show that tcp new jersey is able to accurately estimate the available bandwidth of the bottleneck link of an end to end path and the tabe estimator is immune to link asymmetry , bi directional congestion , and the relative position of the bottleneck link in the multi hop end to end path . the proactive congestion avoidance control mechanism proposed in our scheme minimizes the network congestion , reduces the network volatility , and stabilizes the queue lengths while achieving more throughput than other tcp schemes .
a hopfield neural network based task mapping method . <eos> with a prior knowledge of a program , static mapping aims to identify an optimal clustering strategy that can produce the best performance . in this paper we present a static method that uses hopfield neural network to cluster the tasks of a parallel program for a given system . this method takes into account both load balancing and communication minimization . the method has been tested on a distributed shared memory system against other three clustering methods . four programs , sor , n body , gaussian elimination and vq , are used in the test . the result shows that our method is superior to the other three . ( c ) <digit> elsevier science b.v. all rights reserved .
investigating the evolution of code smells in object oriented systems . <eos> software design problems are known and perceived under many different terms , such as code smells , flaws , non compliance to design principles , violation of heuristics , excessive metric values and anti patterns , signifying the importance of handling them in the construction and maintenance of software . once a design problem is identified , it can be removed by applying an appropriate refactoring , improving in most cases several aspects of quality such as maintainability , comprehensibility and reusability . this paper , taking advantage of recent advances and tools in the identification of non trivial code smells , explores the presence and evolution of such problems by analyzing past versions of code . several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations , whether problems vanish by time or only by targeted human intervention , whether code smells occur in the course of evolution of a module or exist right from the beginning and whether refactorings targeting at smell removal are frequent . in contrast to previous studies that investigate the application of refactorings in the history of a software project , we attempt to analyze the evolution from the point of view of the problems themselves . to this end , we classify smell evolution patterns distinguishing deliberate maintenance activities from the removal of design problems as a side effect of software evolution . results are discussed for two open source systems and four code smells .
issues in parallelizing multigrid based substrate model extraction and analysis . <eos> accurate modeling of coupling effects via the substrate is an increasingly important concern in the design of mixed signal systems such as communication , biomedical and analog signal processing circuits . fast switching digital blocks inject noise into the common substrate hindering the performance of high precision sensible analog circuitry . miniaturization effects on ics complexity inevitably make the accuracy requirements for substrate coupling simulation increase . due in part to the global nature of such couplings , model extraction and analysis is a computation intensive task requiring the availability of fast and accurate substrate model extraction and analysis tools . one way to deal with this problem is to take further advantage of available computational technologies and distributed computing emerges as an interesting solution.in this paper we discuss several issues related to the parallelization of a multigrid based substrate model extraction and analysis tool . this tool is used as a proxy for generic computations on a 3d discretized volume . the results presented indicate potential avenues for successfully exploiting parallelism as well as pitfalls to avoid in such a quest .
hypoxia induced phrenic long term facilitation emergent properties . <eos> as in other neural systems , plasticity is a hallmark of the neural system controlling breathing . one spinal mechanism of respiratory plasticity is phrenic long term facilitation ( pltf ) following acute intermittent hypoxia . although cellular mechanisms giving rise to pltf occur within the phrenic motor nucleus , different signaling cascades elicit pltf under different conditions . these cascades , referred to as q and s pathways to phrenic motor facilitation ( pmf ) , interact via cross talk inhibition . whereas the q pathway dominates pltf after mild to moderate hypoxic episodes , the s pathway dominates after severe hypoxic episodes . the biological significance of multiple pathways to pmf is unknown . this review will discuss the possibility that interactions between pathways confer emergent properties to pltf , including pattern sensitivity and metaplasticity . understanding these mechanisms and their interactions may enable us to optimize intermittent hypoxia induced plasticity as a treatment for patients that suffer from ventilatory impairment or other motor deficits .
bivariate mellin convolution operators quantitative approximation theorems . <eos> in this paper we study some qualitative and quantitative versions of the voronovskaja approximation formulae for a class of bivariate mellin convolution operators of type ( t ( w ) f ) ( x , y ) integral ( r <digit> ) k ( w ) ( tx ( <digit> ) , vy ( <digit> ) ) f ( t , v ) dtdv tv . moreover we apply the general theory to some particular cases leading to various asymptotic formulae and involving various differential operators . ( c ) <digit> elsevier ltd. all rights reserved .
e government evolution in eu local governments a comparative perspective . <eos> purpose the purpose of this paper is to describe an empirical study of the advances and trends of e government in transparency , openness and hence accountability in european union ( eu ) local governments to determine the extent to which the internet promotes the convergence towards more transparent and accountable government . the paper also tests the extent to which different factors related to the implementation of information and communication technologies ( icts ) , the number of inhabitants and the type of public administration style have influenced e government developments in the cities studied . design methodology approach a comprehensive content analysis of <digit> local government web sites was conducted using a <digit> item evaluation questionnaire . the evaluations were performed in <digit> and <digit> and <digit> eu countries were covered ( five per country ) . to analyse the evolution of e government , several techniques were used tests of difference of means , multidimensional scaling and cluster analysis . the contribution of the different contextual factors to the development of government web sites was tested with ols regression analysis . findings the results show noticeable progress in the application of icts and increasing eu local government concern for bringing government closer to citizens and for giving an image of modernity and responsiveness , although few web sites show clear signs of real openness to encouraging citizen dialogue . the evolution of the e government initiatives analysed shows that , at present , they are still overlapped with the public administration style of each country as an extension of traditional front offices with potential benefits in speed and accessibility . originality value although a growing number of e government studies are appearing , previous research has not analysed the evolution of eu local governments from a comparative perspective .
soccer video processing for the detection of advertisement billboards . <eos> billboards are placed on the sides of a soccer field for advertisement during match telecast . unlike regular commercials , which are introduced during a break , on field billboards appear on the tv screen at uncertain time instances , in different sizes , and also for different durations . automated processing of soccer telecasts for detection and analysis of such billboards can provide important information on the effectiveness of this mode of advertising . we propose a method in which shot boundaries are first identified and the type of each shot is determined . frames within each shot are then segmented to locate possible regions of interests ( rois ) locations in a frame where billboards are potentially present . finally , we use a combination of local and global features for detecting individual billboards by matching with a set of given templates .
multi component image segmentation in homogeneous regions based on description length minimization application to speckle , poisson and bernoulli noise . <eos> in this article , a minimum description length ( mdl ) criterion adapted to independent multi component image segmentation into homogeneous regions is proposed . this approach , based on a deformable polygonal grid , allows us to segment noisy multi component images perturbed with spatially independent speckle , poisson or bernoulli noise . the advantages of using such a multi component approach rather than a mono component one is demonstrated on synthetic and real images . this segmentation method is also applicable to multi component images whose components do not follow the same noise statistics or have not been previously registered .
language dominance in interpersonal deception in computer mediated communication . <eos> dominance is not only a complicated social phenomenon that involves interpersonal dynamics , but also an effective strategy used in various applications such as deception detection , negotiation , and online community . the extensive literature on dominance has primarily focused on the personality traits and socio biological influence , as well as various nonverbal and paralinguistic behaviors associated with dominance . nonetheless , language dominance manifested through dynamically acquired linguistic capability and strategies has not been fully investigated . the exploration of language dominance in the context of deception is even rarer . with the increasing use of computer mediated communication ( cmc ) in all aspects of modern life , language dominance in cmc has emerged as an important issue . this study examines language dominance in the context of deception via cmc . the experimental results show that deceivers ( <digit> ) demonstrate a different trend of language dominance from truthtellers over time ( <digit> ) manipulate the level of language dominance by initiating communication with low dominance and gradually increasing the level over the course of interaction , and ( <digit> ) display higher levels of dominance in terms of some linguistic behaviors than truthtellers . they suggest that in cmc , deceivers not only adjust the level of language dominance more frequently , but also change it more remarkably than truthtellers .
benchmarking short sequence mapping tools . <eos> the development of next generation sequencing instruments has led to the generation of millions of short sequences in a single run . the process of aligning these reads to a reference genome is time consuming and demands the development of fast and accurate alignment tools . however , the current proposed tools make different compromises between the accuracy and the speed of mapping . moreover , many important aspects are overlooked while comparing the performance of a newly developed tool to the state of the art . therefore , there is a need for an objective evaluation method that covers all the aspects . in this work , we introduce a benchmarking suite to extensively analyze sequencing tools with respect to various aspects and provide an objective comparison .

causality of frontal and occipital alpha activity revealed by directed coherence . <eos> recently there has been increased attention to the causality among biomedical signals . the causality between brain structures involved in the generation of alpha activity is examined based on eeg signals acquired simultaneously in the frontal and occipital regions of the scalp . the concept of directed coherence ( dc ) is introduced as a means of resolving two signal observations into the constituent components of original signals , the interaction between signals and the influence of one signal source on the other , through autoregressive modeling . the technique was applied to eeg recorded from <digit> normal subjects with eyes closed . through an analysis of the directed coherence , it was found that in both the left and right hemispheres , alpha rhythms with relatively low frequency had a significantly higher correlation in the frontal occipital direction than in the opposite direction . in the upper alpha frequency band , a significantly higher dc was observed in the occipital frontal direction , and the right left dc in the occipital area was consistently higher . the activity of rhythms near <digit> hz was widespread . these results suggest that there is a difference in the genesis and the structure of information transmission in the lower and upper band , and for <digit> hz alpha waves .
learning finite binary sequences from half space data . <eos> the problem of inferring a finite binary sequence w is an element of ( <digit> , <digit> ) ( n ) is considered . it is supposed that at epochs t <digit> , <digit> , ... , the learner is provided with random half space data in the form of finite binary sequences u ( ( t ) is an element of ) <digit> , <digit> ( n ) which have positive inner product with w . the goal of the learner is to determine the underlying sequence w in an efficient , on line fashion from the data u ( ( t ) ) , t greater than or equal to <digit> . in this context , it is shown that the randomized , on line directed drift algorithm produces a sequence of hypotheses w ( ( t ) ) is an element of <digit> , <digit> ( n ) , t greater than or equal to <digit> which converges to w in finite time with probability <digit> . it is shown that while the algorithm has a minimal space complexity of 2n bits of scratch memory , it has exponential time complexity with an expected mistake bound of order ohm ( e ( 0.139 n ) ) . batch incarnations of the algorithm are introduced which allow for massive improvements in running time with a relatively small cost in space ( batch size ) . in particular , using a batch of o ( n log n ) examples at each update epoch reduces the expected mistake bound of the ( batch ) algorithm to o ( n ) ( in an asynchronous bit update mode ) and o ( <digit> ) ( in a synchronous bit update mode ) . the problem considered here is related to binary integer programming and to learning in a mathematical model of a neuron . ( c ) <digit> john wiley sons , inc .
projection based statistical analysis of full chip leakage power with non log normal distributions . <eos> in this paper we propose a novel projection based algorithm to estimate the full chip leakage power with consideration of both inter die and intra die process variations . unlike many traditional approaches that rely on log normal approximations , the proposed algorithm applies a novel projection method to extract a low rank quadratic model of the logarithm of the full chip leakage current and , therefore , is not limited to log normal distributions . by exploring the underlying sparse structure of the problem , an efficient algorithm is developed to extract the non log normal leakage distribution with linear computational complexity in circuit size . in addition , an incremental analysis algorithm is proposed to quickly update the leakage distribution after changes to a circuit are made . our numerical examples in a commercial 90nm cmos process demonstrate that the proposed algorithm provides 4x error reduction compared with the previously proposed log normal approximations , while achieving orders of magnitude more efficiency than a monte carlo analysis with <digit> samples .
hybrid numerical methods for convection diffusion problems in arbitrary geometries . <eos> the hybrid nodal integral finite element method ( ni fem ) and the hybrid nodal integral finite analytic method ( ni fam ) are developed to solve the steady state , two dimensional convection diffusion equation ( cde ) . the hybrid ni fam for the steady state problem is then extended to solve the more general time dependent , two dimensional , cde . these hybrid coarse mesh methods , unlike the conventional nodal integral approach , are applicable in arbitrary geometries and maintain the high efficiency of the conventional nodal integral method ( nim ) . in steady state problems , the computational domain for both hybrid methods is discretized using rectangular nodes in the interior of the domain and along vertical and horizontal boundaries , while triangular nodes are used along the boundaries that are not parallel to the x or y axes . in time dependent problems , the rectangular and triangular nodes become space time parallelepiped and wedge shaped nodes , respectively . the difference schemes for the variables on the interfaces of adjacent rectangular parallelepiped nodes are developed using the conventional nim . for the triangular nodes in the hybrid ni fem , a trial function is written in terms of the edge averaged concentration of the three edges and made to satisfy the cde in an integral sense . in the hybrid ni fam , the concentration over the triangular wedge shaped nodes is represented using a finite analytic approximation , which is based on the analytic solution of the one dimensional cde . the difference schemes for both hybrid methods are then developed for the interfaces between the rectangular parallelepiped and triangular wedge shaped nodes by imposing continuity of the flux across the interfaces . a formal derivation of these hybrid methods and numerical results for several test problems are presented and discussed . ( c ) <digit> elsevier science ltd. all rights reserved .
constrained ellipse fitting with center on a line . <eos> fitting an ellipse to given data points is a common optimization task in computer vision problems . however , the possibility of incorporating the prior constraint the ellipses center is located on a given line into the optimization algorithm has not been examined so far . this problem arises , for example , by fitting an ellipse to data points representing the path of the image positions of an adhesion inside a rotating vessel whose position of the rotational axis in the image is known . our new method makes use of a constrained algebraic cost function with the incorporated ellipse center on given line prior condition in a global convergent one dimensional optimization approach . further advantages of the algorithm are computational efficiency and numerical stability .
trellis portability across architectures with a high level framework . <eos> trellis shows programmability benefits of a common and portable set of directives . we illustrate descriptive capability of directives that can support portable codes . we enhance the openacc model with more efficient mapping and synchronization . we implement prototype source translation of trellis to openmp , openacc and cuda .
pedagogical content knowledge in programming education for secondary school . <eos> dissertation overview , addressing the concept of pedadogical content knowledge for the teaching and learning of programming for secondary education .
interference analysis for highly directional <digit> ghz mesh networks the case for rethinking medium access control . <eos> we investigate spatial interference statistics for multigigabit outdoor mesh networks operating in the unlicensed <digit> ghz millimeter ( mm ) wave band . the links in such networks are highly directional because of the small carrier wavelength ( an order of magnitude smaller than those for existing cellular and wireless local area networks ) , narrow beams are essential for overcoming higher path loss and can be implemented using compact electronically steerable antenna arrays . directionality drastically reduces interference , but it also leads to deafness , making implicit coordination using carrier sense infeasible . in this paper , we make a quantitative case for rethinking medium access control ( mac ) design in such settings . unlike existing mac protocols for omnidirectional networks , where the focus is on interference management , we contend that mac design for <digit> ghz mesh networks can essentially ignore interference and must focus instead on the challenge of scheduling half duplex transmissions with deaf neighbors . our main contribution is an analytical framework for estimating the collision probability in such networks as a function of the antenna patterns and the density of simultaneously transmitting nodes . the numerical results from our interference analysis show that highly directional links can indeed be modeled as pseudowired , in that the collision probability is small even with a significant density of transmitters . furthermore , simulation of a rudimentary directional slotted aloha protocol shows that packet losses due to failed coordination are an order of magnitude higher than those due to collisions , confirming our analytical results and highlighting the need for more sophisticated coordination mechanisms .
the influence of skeletal muscle anisotropy on electroporation in vivo study and numerical modeling . <eos> the aim of this study was to theoretically and experimentally investigate electroporation of mouse tibialis cranialis and to determine the reversible electroporation threshold values needed for parallel and perpendicular orientation of the applied electric field with respect to the muscle fibers . our study was based on local electric field calculated with three dimensional realistic numerical models , that we built , and in vivo visualization of electroporated muscle tissue . we established that electroporation of muscle cells in tissue depends on the orientation of the applied electric field the local electric field threshold values were determined ( pulse parameters <digit> s , 1hz ) to be 80v cm and 200v cm for parallel and perpendicular orientation , respectively . our results could be useful electric field parameters in the control of skeletal muscle electroporation , which can be used in treatment planning of electroporation based therapies such as gene therapy , genetic vaccination , and electrochemotherapy .
the role of commutativity in constraint propagation algorithms . <eos> constraint propagation algorithms form an important part of most of the constraint programming systems . we provide here a simple , yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way . in this framework we proceed in two steps . first , we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting . then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms . in particular , using the notions commutativity and semi commutativity , we show that the ac <digit> , pc <digit> , dac , and dpc algorithms for achieving ( directional ) are consistency and ( directional ) path consistency are instances of a single generic algorithm . the work reported here extends and simplifies that of apt 1999a .
smartrank a smart scheduling tool for mobile cloud computing . <eos> resource scarcity is a major obstacle for many mobile applications , since devices have limited energy power and processing potential . as an example , there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardwares capabilities , such as language translation , speech recognition , and face recognition . a new trend has been explored to tackle this problem , the use of cloud computing . this study presents smartrank , a scheduling framework to perform load partitioning and offloading for mobile applications using cloud computing to increase performance in terms of response time . we first explore a benchmarking of face recognition application using mobile cloud and confirm its suitability to be used as case study with smartrank . we have applied the approach to a face recognition process based on two strategies cloudlet federation and resource ranking through balanced metrics ( level of cpu utilization and round trip time ) . second , using a full factorial experimental design we tuned the smartrank with the most suitable partitioning decision calibrating scheduling parameters . nevertheless , smartrank uses an equation that is extensible to include new parameters and make it applicable to other scenarios .
new global exponential stability conditions for inertial cohengrossberg neural networks with time delays . <eos> in this paper , global exponential stability of inertial cohengrossberg neural networks with time delays is investigated . by using homeomorphism theorem and inequality technique , a lmi based global exponential stability condition and inequality form global exponential stability condition are obtained for the above neural networks . in our result , the assumptions for the differentiability and monotonicity on the behaved functions in ke and miao ( <digit> ) <digit> are removed . thus our results are less conservative than those obtained in ke and miao ( <digit> ) <digit> . hence , we obtain new global exponential stability for this neural network .
extracting the fetal heart rate variability using a frequency tracking algorithm . <eos> in this work , we propose an algorithm to extract the fetal heart rate variability from an ecg measured from the mother abdomen . the algorithm consists of two methods a separation algorithm based on second order statistics that extracts the desired signal in one shot through the data , and a heart instantaneous frequency ( hif ) estimator . the hif algorithm is used to extract the mother heart rate which serves as reference to extract the fetal heart rate . we carried out simulations where the signals overlap in frequency and time , and showed that it worked efficiently .
education and training in health informatics the it eductra project . <eos> in this contribution both the eductra project of the european advanced informatics in medicine programme and the it eductra project of the telematics applications programme ( health sector ) are described . eductra had as aim to investigate which gaps in knowledge health professionals have with respect to health informatics and to suggest ways to remedy this . it was assumed that health professionals had a basic understanding of health informatics and that additional educational material only had to cover the knowledge necessary for appreciating the new products coming from the aim programme . a state of the art survey revealed that the knowledge of health professionals with respect to health informatics was deplorable . guidelines for curricula were therefore proposed to enable potential teachers to design courses . it eductra is a continuation of the eductra project . it has as aim to create learning materials covering a broad area of health informatics .
methods for reasoning from geometry about anatomic structures injured by penetrating trauma . <eos> this paper presents the methods used for three dimensional ( 3d ) reasoning about anatomic structures affected by penetrating trauma in traumascan web , a platform independent decision support system for evaluating the effects of penetrating trauma to the chest and abdomen . in assessing outcomes for an injured patient , traumascan web utilizes 3d models of anatomic structures and 3d models of the regions of damage associated with stab and gunshot wounds to determine the probability of injury to anatomic structures . probabilities estimated from 3d reasoning about affected anatomic structures serve as input to a bayesian network which calculates posterior probabilities of injury based on these initial probabilities together with available information about patient signs , symptoms and test results . in addition to displaying textual descriptions of conditions arising from penetrating trauma to a patient , traumascan web allows users to visualize the anatomy suspected of being injured in 3d , in this way providing a guide to its reasoning process .
use of effective stiffness matrix for the free vibration analyses of a non uniform cantilever beam carrying multiple two degree of freedom springdampermass systems . <eos> this paper investigates the free vibration characteristics of a non uniform cantilever beam carrying multiple two degree of freedom ( dof ) springdampermass systems by means of two finite element methods , fem1 and fem2 . where the fem1 is the conventional finite element method ( fem ) with each two dof springdampermass system being considered as a finite element possessing stiffness , damping and mass matrices , while the fem2 is an alternative approach with each two dof springdampermass system being replaced by an effective stiffness matrix composed of four massless effective springs . instead of using both the real part ( ) and the imaginary part ( ) of a complex eigenvalue ( ) to derive the mathematical expressions , this paper directly employs the implicit form of the complex eigenvalue ( ) to formulate the problem . in the fem1 , since each springdampermass system has two dof , the total dof of the entire system increases two if the beam carries one more two dof springdampermass system . however , in the fem2 , the total dof of the entire system remains unchanged , because all dof of each springdampermass system are suppressed by the effective stiffness matrix . good agreement between the natural frequencies obtained from fem2 and the corresponding ones from fem1 confirms the reliability of the presented theory .
a dtc strategy dedicated to three switch three phase inverter fed induction motor drives . <eos> purpose the put pose of this paper is to describe the implementation of a direct torque control strategy dedicated to three switch three phase delta shaped inverter ( tstpi ) fed induction motor drives as well as the comparison of its performance with those yielded by six switch three phase inverter ( sstpi ) fed induction motor drives under the takahashi dtc strategy . design methodology approach referring to the asymmetrical stator voltage vectors and in order to reach high dynamic with low ripple of the electromagnetic torque response , the design of the vector selection table should include virtual voltage vectors by the subdivision of each sector into two equal sub sectors . findings it has been shown that the implementation of the proposed dtc strategy in tstpi fed induction motor drives leads to higher transient behaviour and better steady state features than those exhibited by the takahashi dtc strategy implemented in sstpi fed induction motor drives . research limitations implications the research should be extended to a comparison of the obtained simulation results with experimental measurements . practical implications a <digit> per cent reduction of cost and compactness associated with a <digit> per cent increase of reliability makes the tstpi an interesting candidate , especially in large scale production applications such as the automotive industry . originality value the paper proposes an approach to improve the cost effectiveness , the compactness and the reliability of tstpi fed induction motor drives , which represents a crucial benefit in electric and hybrid propulsion systems .
hybrid generative discriminative classifier for unconstrained character recognition . <eos> handwriting recognition for hand held devices like pdas requires very accurate and adaptive classifiers . it is such a complex classification problem that it is quite usual now to make co operate several classification methods . in this paper , we present an original two stages recognizer . the first stage is a model based classifier which store an exhaustive set of character models . the second stage is a pairwise classifier which separate the most ambiguous pairs of classes . this hybrid architecture is based on the idea that the correct class almost systematically belongs to the two more relevant classes found by the first classifier . experiments on a 80,000 examples database show a <digit> % improvement on a <digit> classes recognition problem . moreover , we show experimentally that such an architecture suits perfectly for incremental classification .
optimal sleep patterns for serving delay tolerant jobs . <eos> sleeping is an important method to reduce energy consumption in many information and communication systems . in this paper we focus on a typical server under dynamic load , where entering and leaving sleeping mode incurs an energy and a response time penalty . we seek to understand under what kind of system configuration and control method will sleep mode obtain a pareto optimal tradeoff between energy saving and average response time . we prove that the optimal sleeping policy has a simple hysteretic structure . simulation results then show that this policy results in significant energy savings , especially for relatively delay insensitive applications and under low traffic load . however , we demonstrate that seeking the maximum energy saving presents another tradeoff it drives up the peak temperature in the server , with potential reliability consequences .
on channel discontinuity constraint routing in wireless networks . <eos> multi channel wireless networks are increasingly deployed as infrastructure networks , e.g. in metro areas . network nodes frequently employ directional antennas to improve spatial throughput . in such networks , between two nodes , it is of interest to compute a path with a channel assignment for the links such that the path and link bandwidths are the same . this is achieved when any two consecutive links are assigned different channels , termed as channel discontinuity constraint ( cdc ) . cdc paths are also useful in tdma systems , where , preferably , consecutive links are assigned different time slots . in the first part of this paper , we develop a t spanner for cdc paths using spatial properties a sub network containing <digit> ( n10 ) links , for any <digit> > <digit> , such that cdc paths increase in cost by at most a factor t ( <digit> <digit> sin ( <digit> <digit> ) ) ( <digit> ) . we propose a novel distributed algorithm to compute the spanner using an expected number of <digit> ( n log n ) fixed size messages . in the second part , we present a distributed algorithm to find minimum cost cdc paths between two nodes using <digit> ( n ( <digit> ) ) fixed size messages , by developing an extension of edmonds ' algorithm for minimum cost perfect matching . in a centralized implementation , our algorithm runs in <digit> ( n ( <digit> ) ) time improving the previous best algorithm which requires <digit> ( n ( <digit> ) ) running time . moreover , this running time improves to <digit> ( n <digit> ) when used in conjunction with the spanner developed . ( c ) <digit> elsevier b.v. all rights reserved .
composition of aspects based on a relation model synergy of multiple paradigms . <eos> software composition for timely and affordable software development and evolution is one of the oldest pursuits of software engineering . in current software composition techniques , component based software development ( cbsd ) and aspect oriented software development ( aosd ) have attracted academic and industrial attention . blackbox composition used in cbsd provides simple and safe modularization for its strong information hiding , which is , however , the main obstacle for a black box composite to evolve later . this implies that an application developed through black box composition can not take advantage of aspect oriented programming ( aop ) used in aosd . on the contrary , aop enhances maintainability and comprehensibility by modularizing concerns crosscutting multiple components but lacks the support for the hierarchical and external composition of aspects themselves and compromises the important software engineering principles such as encapsulation , which is almost perfectly supported in black box composition . the role and role model have been recognized to have many similarities with cbsd and aop but have significant differences with those composition techniques as well . although each composition paradigm has its own advantages and disadvantages , there is no substantial support to realize the synergy of these composition paradigms the black box composition , aop , and role model . in this paper , a new composition technique based on representational abstraction of the relationship between component instances is introduced . the model supports the simple , elegant , and dynamic composition of components with its declarative form and provides the hooks through which an aspect can evolve and a parallel developed aspect can be merged at the instance level .
generalization performance of magnitude preserving semi supervised ranking with graph based regularization . <eos> semi supervised ranking is a relatively new and important learning problem inspired by many applications . we propose a novel graph based regularized algorithm which learns the ranking function in the semi supervised learning framework . it can exploit geometry of the data while preserving the magnitude of the preferences . the least squares ranking loss is adopted and the optimal solution of our model has an explicit form . we establish error analysis of our proposed algorithm and demonstrate the relationship between predictive performance and intrinsic properties of the graph . the experiments on three datasets for recommendation task and two quantitative structureactivity relationship datasets show that our method is effective and comparable to some other state of the art algorithms for ranking .
accessible haptic user interface design approach for users with visual impairments . <eos> with the number of people with visual impairments ( e.g. , low vision and blind ) continuing to increase , vision loss has become one of the most challenging disabilities . today , haptic technology , using an alternative sense to vision , is deemed an important component for effectively accessing information systems . the most appropriately designed assistive technology is critical for those with visual impairments to adopt assistive technology and to access information , which will facilitate their tasks in personal and professional life . however , most of the existing design approaches are inapplicable and inappropriate to such design contexts as users with visual impairments interacting with non graphical user interfaces ( i.e. , haptic technology ) . to resolve such design challenges , the present study modified a participatory design approach ( i.e. , pictive , plastic interface for collaborative technology initiatives video exploration ) to be applicable to haptic technologies , by considering the brain plasticity theory . the sense of touch is integrated into the design activity of pictive . participants with visual impairments were able to effectively engage in designing non visual interfaces ( e.g. , haptic interfaces ) through non visual communication methods ( e.g. , touch modality ) .
effect of probabilistic task allocation based on statistical analysis of bid values . <eos> this paper presents the effect of adaptively introducing appropriate strategies into the award phase of the contract net protocol ( cnp ) in a massively multi agent system ( mmas ) .
higher order concurrent programs with finite communication topology ( extended abstract ) . <eos> concurrent ml ( cml ) is an extension of the functional language standard ml ( sml ) with primitives for the dynamic creation of processes and channels and for the communication of values over channels . because of the powerful abstraction mechanisms the communication topology of a given program may be very complex and therefore an efficient implementation may be facilitated by knowledge of the topology . this paper presents an analysis for determining when a bounded number of processes and channels will be generated . the analysis proceeds in two stages . first we extend a polymorphic type system for sml to deduce not only the type of cml programs but also their communication behaviour expressed as terms in a new process algebra . next we develop an analysis that given the communication behaviour predicts the number of processes and channels required during the execution of the cml program . the correctness of the analysis is proved using a subject reduction property for the type system .
on the polyhedral structure of a multi item production planning model with setup times . <eos> we present and study a mixed integer programming model that arises as a substructure in many industrial applications . this model generalizes a number of structured mip models previously studied , and it provides a relaxation of various capacitated production planning problems and other fixed charge network flow problems . we analyze the polyhedral structure of the convex hull of this model , as well as of a strengthened lp relaxation . among other results , we present valid inequalities that induce facets of the convex hull under certain conditions . we also discuss how to strengthen these inequalities by using known results for lifting valid inequalities for <digit> <digit> continuous knapsack problems .

coherence between one random and one periodic signal for measuring the strength of responses in the electro encephalogram during sensory stimulation . <eos> coherence between a pulse train representing periodic stimuli and the eeg has been used in the objective detection of steady state evoked potentials . this work aimed to quantify the strength of the stimulus responses based on the statistics of coherence estimate between one random and one periodic signal focusing on the confidence limits and power of significance tests in detecting responses . to detect the responses in <digit> % of cases , a signal to noise ratio of about 7.9 db was required when using <digit> windows ( m ) in the coherence estimation . the ratio , however , increased to 1.2 db when m was <digit> . the results were tested in monte carlo simulations and applied to eegs obtained from <digit> subjects during visual stimulation . the method showed differences in the strength of responses at the stimulus frequency and its harmonics , as well as variations between individuals and over cortical regions . in contrast to those from the parietal and temporal regions , results for the occipital region gave confidence limits ( with m <digit> ) that were above zero for all subjects , indicating statistically significant responses . the proposed technique extends the usefulness of coherence as a measure of stimulus responses and allows statistical analysis that could also be applied usefully in a range of other biological signals .
exploring the dynamics of adaptation with evolutionary activity plots . <eos> evolutionary activity statistics and their visualization are introduced , and their motivation is explained . examples of their use are described , and their strengths and limitations are discussed . references to more extensive or general accounts of these techniques are provided .
repeated exposure to the abused inhalant toluene alters levels of neurotransmitters and generates peroxynitrite in nigrostriatal and mesolimbic nuclei in rat . <eos> toluene , a volatile hydrocarbon found in a variety of chemical compounds , is misused and abused by inhalation for its euphorigenic effects . toluene 's reinforcing properties may share a common characteristic with other drugs of abuse , namely , activation of the mesolimbic dopamine system . prior studies in our laboratory found that acutely inhaled toluene activated midbrain dopamine neurons in the rat . moreover , single systemic injections of toluene in rats produced a dose dependent increase in locomotor activity which was blocked by depletion of nucleus accumbens dopamine or by pretreatment with a d2 dopamine receptor antagonist . here we examined the effects of seven daily intraperitoneal injections of <digit> mg kg toluene on the content of serotonin and dopamine in the caudate nucleus ( cn ) and nucleus accumbens ( nac ) , substantia nigra , and ventral tegmental area at <digit> , <digit> , and <digit> h after the last injection . also , the roles of nitric oxide , peroxynitrite , and the production of <digit> nitrosotyrosine ( <digit> nt ) , in the cn and nac were assessed at the same time points . toluene treatments increased dopamine levels in the cn and nac , and serotonin levels in cn , nac , and ventral tegmental area . measurements of the dopamine metabolite dihydroxyphenylacetic acid ( dopac ) further suggested a change in transmitter utilization in cn and nac . lastly , <digit> nt levels also showed a differential change between cn and nac , but at different time points post toluene injection . these results point out the complexity of action of toluene on neurotransmitter function following a course of chronic exposure . changes in the production of <digit> nt also suggest that toluene induced neurotoxicity may mediate via generation of peroxynitrite .
supporting ad hoc ranking aggregates . <eos> this paper presents a principled framework for efficient processing of ad hoc top k ( ranking ) aggregate queries , which provide the k groups with the highest aggregates as results . essential support of such queries is lacking in current systems , which process the queries in a nave materialize group sort scheme that can be prohibitively inefficient . our framework is based on three fundamental principles . the upper bound principle dictates the requirements of early pruning , and the group ranking and tuple ranking principles dictate group ordering and tuple ordering requirements . they together guide the query processor toward a provably optimal tuple schedule for aggregate query processing . we propose a new execution framework to apply the principles and requirements . we address the challenges in realizing the framework and implementing new query operators , enabling efficient group aware and rank aware query plans . the experimental study validates our framework by demonstrating orders of magnitude performance improvement in the new query plans , compared with the traditional plans .
meaningful and meaningless solutions for cooperative n person games . <eos> game values often represent data that can be measured in more than one acceptable way ( e.g. , monetary amounts ) . we point out that in such a case a statement about cooperative n person game models might be meaningless in the sense that its truth or falsity depends on the choice of an acceptable way to measure game values . in particular , we analyze statements about solution concepts such as the core , stable sets , the nucleolus , the shapley value ( and some of its generalizations ) .
numerical optimization algorithm for rotationally invariant multi orbital slave boson method . <eos> we develop a generalized numerical optimization algorithm for the rotationally invariant multi orbital slave boson approach , which is applicable for arbitrary boundary constraints of high dimensional objective function by combining several classical optimization techniques . after constructing the calculation architecture of rotationally invariant multi orbital slave boson model , we apply this optimization algorithm to find the stable ground state and magnetic configuration of two orbital hubbard models . the numerical results are consistent with available solutions , confirming the correctness and accuracy of our present algorithm . furthermore , we utilize it to explore the effects of the transverse hunds coupling terms on metalinsulator transition , orbital selective mott phase and magnetism . these results show the quick convergency and robust stable character of our algorithm in searching the optimized solution of strongly correlated electron systems .
molecular dynamics simulation of large scale carbon nanotubes on a shared memory architecture . <eos> carbon nanotubes are expected to play a significant role in the design and manufacture of many nano mechanical and nano electronic devices of future . it is important , therefore , that atomic level elastomechanical response properties of both single and multiwall nanotubes be investigated in detail . classical molecular dynamics simulations employing brenner 's reactive potential with long range van der waals interactions have been used in mechanistic response studies of carbon nanotubes to external strains . the studies of single and multiwalled carbon nanotubes under compressive strains show the instabilities beyond elastic response . due to inclusion of non bonded long range interactions , the simulations also show the redistribution of strain and strain energy from sideways bucklng to the formation of highly localized strained kink sites . bond rearrangements occur at the kink sites , leading to formation of topological defects , preventing the tube from relaxing fully back to it 's original configuration . elastomechanic response behavior of single and multiwall carbon nanotubes to externally applied compressive strains is simulated and studied in detail . we will describe the results and discuss their implication towards the stability of any molecular mechanical structure made of carbon nanotubes .
nonprimitive recursive complexity and undecidability for petri net equivalences . <eos> the aim of this note is twofold . firstly , it shows that the undecidability result for bisimilarity in theor . comput . sci . <digit> ( <digit> ) <digit> <digit> can be immediately extended for the whole range of equivalences land preorders ) on labelled petri nets . secondly , it shows that restricting our attention to nets with finite reachable space , the respective ( decidable ) problems are nonprimitive recursive this approach also applies to mayr and meyer 's result j. acm <digit> ( <digit> ) <digit> <digit> for the reachability set equality , yielding a more direct proof . ( c ) <digit> elsevier science b.v. all rights reserved .
time decaying aggregates in out of order streams . <eos> processing large data streams is now a major topic in data management . the data involved can be truly massive , and the required analyses complex . in a stream of sequential events such as stock feeds , sensor readings , or ip traffic measurements , data tuples pertaining to recent events are typically more important than older ones . this can be formalized via time decay functions , which assign weights to data based on the age of data . decay functions such as sliding windows and exponential decay have been studied under the assumption of well ordered arrivals , i.e. , data arrives in non decreasing order of time stamps . however , data quality issues are prevalent in massive streams ( due to network asynchrony and delays etc. ) , and correct arrival order is not guaranteed . we focus on the computation of decayed aggregates such as range queries , quantiles , and heavy hitters on out of order streams , where elements do not necessarily arrive in increasing order of timestamps . existing techniques such as exponential histograms and waves are unable to handle out of order streams . we give the first deterministic algorithms for approximating these aggregates under popular decay functions such as sliding window and polynomial decay . we study the overhead of allowing out of order arrivals when compared to well ordered arrivals , both analytically and experimentally . our experiments confirm that these algorithms can be applied in practice , and compare the relative performance of different approaches for handling out of order arrivals .
the village telco project a reliable and practical wireless mesh telephony infrastructure . <eos> voip ( voice over ip ) over mesh networks could be a potential solution to the high cost of making phone calls in most parts of africa . the village telco ( vt ) is an easy to use and scalable voip over meshed wlan ( wireless local area network ) telephone infrastructure . it uses a mesh network of mesh potatoes to form a peer to peer network to relay telephone calls without landlines or cell phone towers . this paper discusses the village telco infrastructure , how it addresses the numerous difficulties associated with wireless mesh networks , and its efficient deployment for voip services in some communities around the globe . the paper also presents the architecture and functions of a mesh potato and a novel combined analog telephone adapter ( ata ) and wifi access point that routes calls . lastly , the paper presents the results of preliminary tests that have been conducted on a mesh potato . the preliminary results indicate very good performance and user acceptance of the mesh potatoes . the results proved that the infrastructure is deployable in severe and under resourced environments as a means to make cheap phone calls and render internet and ip based services . as a result , the vt project contributes to bridging the digital divide in developing areas .
general subspace learning with corrupted training data via graph embedding . <eos> we address the following subspace learning problem supposing we are given a set of labeled , corrupted training data points , how to learn the underlying subspace , which contains three components an intrinsic subspace that captures certain desired properties of a data set , a penalty subspace that fits the undesired properties of the data , and an error container that models the gross corruptions possibly existing in the data . given a set of data points , these three components can be learned by solving a nuclear norm regularized optimization problem , which is convex and can be efficiently solved in polynomial time . using the method as a tool , we propose a new discriminant analysis ( i.e. , supervised subspace learning ) algorithm called corruptions tolerant discriminant analysis ( ctda ) , in which the intrinsic subspace is used to capture the features with high within class similarity , the penalty subspace takes the role of modeling the undesired features with high between class similarity , and the error container takes charge of fitting the possible corruptions in the data . we show that ctda can well handle the gross corruptions possibly existing in the training data , whereas previous linear discriminant analysis algorithms arguably fail in such a setting . extensive experiments conducted on two benchmark human face data sets and one object recognition data set show that ctda outperforms the related algorithms .
closure properties of hyper minimized automata . <eos> two deterministic finite automata are almost equivalent if they disagree in acceptance only for finitely many inputs . an automaton a is hyper minimized if no automaton with fewer states is almost equivalent to a. a regular language l is canonical if the minimal automaton accepting l is hyper minimized . the asymptotic state complexity s ( l ) of a regular language l is the number of states of a hyper minimized automaton for a language finitely different from l. in this paper we show that ( <digit> ) the class of canonical regular languages is not closed under intersection , union , concatenation , kleene closure , difference , symmetric difference , reversal , homomorphism , and inverse homomorphism ( <digit> ) for any regular languages l ( <digit> ) and l ( <digit> ) the asymptotic state complexity of their sum l ( <digit> ) boolean or l ( <digit> ) , intersection l ( <digit> ) boolean and l ( <digit> ) , difference l ( <digit> ) l ( <digit> ) , and symmetric difference l ( <digit> ) circle plus l ( <digit> ) can be bounded by s ( l ( <digit> ) ) . s ( l ( <digit> ) ) . this bound is tight in binary case and in unary case can be met in infinitely many cases . ( <digit> ) for any regular language l the asymptotic state complexity of its reversal l ( r ) can be bounded by <digit> ( s ) ( l ) . this bound is tight in binary case . ( <digit> ) the asymptotic state complexity of kleene closure and concatenation can not be bounded . namely , for every k > <digit> , there exist languages k , l , and m such that s ( k ) s ( l ) s ( m ) <digit> and s ( k ) s ( l . m ) k. these are answers to open problems formulated by back et al. rairo theor . inf . appl . <digit> ( <digit> ) <digit> <digit>
on equivalent parameter learning in simplified feature space based on bayesian asymptotic analysis . <eos> parametric models for sequential data , such as hidden markov models , stochastic context free grammars , and linear dynamical systems , are widely used in time series analysis and structural data analysis . computation of the likelihood function is one of primary considerations in many learning methods . iterative calculation of the likelihood such as the model selection is still time consuming though there are effective algorithms based on dynamic programming . the present paper studies parameter learning in a simplified feature space to reduce the computational cost . simplifying data is a common technique seen in feature selection and dimension reduction though an oversimplified space causes adverse learning results . therefore , we mathematically investigate a condition of the feature map to have an asymptotically equivalent convergence point of estimated parameters , referred to as the vicarious map . as a demonstration to find vicarious maps , we consider the feature space , which limits the length of data , and derive a necessary length for parameter learning in hidden markov models .
a dual scale lattice gas automata model for gas solid two phase flow in bubbling fluidized beds . <eos> modelling the hydrodynamics of gas solid flow is important for the design and scale up of fluidized bed reactors . a novel gas solid dual scale model based on lattice gas cellular automata ( lgca ) is proposed to describe the macroscopic behaviour through microscopic gas solid interactions . solid particles and gas pseudo particles are aligned in lattices with different scales for solid and gas . in addition to basic lgca rules , additional rules for collision and propagation are specifically designed for gas solid systems . the solid 's evolution is then motivated by the temporal and spatial average momentum gained through solid solid and gas solid interactions . a statistical method , based on the similarity principle , is derived for the conversion between model parameters and hydrodynamic properties . simulations for bubbles generated from a vertical jet in a bubbling fluidized bed based on this model agree well with experimental results , as well as with the results of two fluid approaches and discrete particle simulations . ( c ) <digit> elsevier ltd. all rights reserved .
scalable algorithms for global snapshots in distributed systems . <eos> existing algorithms for global snapshots in distributed systems are not scalable when the underlying topology is complete . in a network with n processors , these algorithms require o ( n ) space and o ( n ) messages per processor . as a result , these algorithms are not efficient in large systems when the logical topology of the communication layer such as mpi is complete . in this paper , we propose three algorithms for global snapshot a grid based , a tree based and a centralized algorithm . the grid based algorithm uses o ( n ) space but only o ( n ) messages per processor . the tree based algorithm requires only o ( <digit> ) space and o ( log n log w ) messages per processor where w is the average number of messages in transit per processor . the centralized algorithm requires only o ( <digit> ) space and o ( log w ) messages per processor . we also have a matching lower bound for this problem . our algorithms have applications in checkpointing , detecting stable predicates and implementing synchronizers . we have implemented our algorithms on top of the mpi library on the blue gene l supercomputer . our experiments confirm that the proposed algorithms significantly reduce the message and space complexity of a global snapshot .
a smart tcp acknowledgment approach for multihop wireless networks . <eos> reliable data transfer is one of the most difficult tasks to be accomplished in multihop wireless networks . traditional transport protocols like tcp face severe performance degradation over multihop networks given the noisy nature of wireless media as well as unstable connectivity conditions in place . the success of tcp in wired networks motivates its extension to wireless networks . a crucial challenge faced by tcp over these networks is how to operate smoothly with the 802.11 wireless mac protocol which also implements a retransmission mechanism at link level in addition to short rts cts control frames for avoiding collisions . these features render tcp acknowledgments ( ack ) transmission quite costly . data and ack packets cause similar medium access overheads despite the much smaller size of the acks . in this paper , we further evaluate our dynamic adaptive strategy for reducing ack induced overhead and consequent collisions . our approach resembles the sender side 's congestion control . the receiver is self adaptive by delaying more acks under nonconstrained channels and less otherwise . this improves not only throughput but also power consumption . simulation evaluations exhibit significant improvement in several scenarios .
vasopressin and social odor processing in the olfactory bulb and anterior olfactory nucleus . <eos> central vasopressin facilitates social recognition and modulates numerous complex social behaviors in mammals , including parental behavior , aggression , affiliation , and pair bonding . in rodents , social interactions are primarily mediated by the exchange of olfactory information , and there is evidence that vasopressin signaling is important in brain areas where olfactory information is processed . we recently discovered populations of vasopressin neurons in the main and accessory olfactory bulbs and anterior olfactory nucleus that are involved in the processing of social odor cues . in this review , we propose a model of how vasopressin release in these regions , potentially from the dendrites , may act to filter social odor information to facilitate odor based social recognition . finally , we discuss recent human research linked to vasopressin signaling and suggest that our model of priming facilitated vasopressin signaling would be a rewarding target for further studies , as a failure of priming may underlie pathological changes in complex behaviors .
ticks , tick borne rickettsiae , and coxiella burnetii in the greek island of cephalonia . <eos> domestic animals are the hosts of several tick species and the reservoirs of some tick borne pathogens hence , they play an important role in the circulation of these arthropods and their pathogens in nature . they may act as vectors , but , also , as reservoirs of spotted fever group ( sfg ) rickettsiae , which are the causative agents of sfg rickettsioses . q fever is a worldwide zoonosis caused by coxiella burnetii ( c. burnetii ) , which can be isolated from ticks . a total of 1,848 ticks ( <digit> female , <digit> male , and <digit> nymph ) were collected from dogs , goats , sheep , cattle , and horses in <digit> different localities of the greek island of cephalonia . rhipicephalus ( rh . ) bursa , rh . turanicus , rh . sanguineus , dermacentor marginatus ( d. marginatus ) , ixodes gibbosus ( i. gibbosus ) , haemaphysalis ( ha . ) punctata , ha . sulcata , hyalomma ( hy . ) anatolicum excavatum and hy . marginatum marginatum were the species identified . c. burnetii and four different sfg rickettsiae , including rickettsia ( r. ) conorii , r. massiliae , r. rhipicephali , and r. aeschlimannii were detected using molecular methods . double infection with r. massiliae and c. burnetii was found in one of the positive ticks
quiver polynomials in iterated residue form . <eos> degeneracy loci polynomials for quiver representations generalize several important polynomials in algebraic combinatorics . in this paper we give a nonconventional generating sequence description of these polynomials when the quiver is of dynkin type .
the relationship among soft sets , soft rough sets and topologies . <eos> molodtsovs soft set theory is a newly emerging tool to deal with uncertain problems . based on the novel granulation structures called soft approximation spaces , feng et al. initiated soft rough approximations and soft rough sets . fengs soft rough sets can be seen as a generalized rough set model based on soft sets , which could provide better approximations than pawlaks rough sets in some cases . this paper is devoted to establishing the relationship among soft sets , soft rough sets and topologies . we introduce the concept of topological soft sets by combining soft sets with topologies and give their properties . new types of soft sets such as keeping intersection soft sets and keeping union soft sets are defined and supported by some illustrative examples . we describe the relationship between rough sets and soft rough sets . we obtain the structure of soft rough sets and the topological structure of soft sets , and reveal that every topological space on the initial universe is a soft approximating space .
a highly efficient vlsi architecture for h. <digit> avc cavlc decoder . <eos> in this paper , an efficient algorithm is proposed to improve the decoding efficiency of the context based adaptive variable length coding ( cavlc ) procedure . due to the data dependency among symbols in the decoding how , the cavlc decoder requires large computation time , which dominates the overall decoder system performance . to expedite its decoding speed , the critical path in the cavlc decoder is first analyzed and then reduced by forwarding the adaptive detection for succeeding symbols . with a shortened critical path , the cavlc architecture is further divided into two segments , which can be easily implemented by a pipeline structure . consequently , the overall performance is effectively improved . in the hardware implementation , a low power combined lut and single output buffer have been adopted to reduce the area as well as power consumption without affecting the decoding performance . experimental results show that the proposed architecture surpassing other recent designs can approximately reduce power consumption by <digit> % and achieve three times decoding speed in comparison to the original decoding procedure suggested in the h. <digit> standard . the maximum frequency can be larger than <digit> mhz , which can easily support the real time requirement for resolutions higher than the hd1080 format .
building database applications of virtual reality with x vrml . <eos> a new method of building active database driven virtual reality applications is presented . the term active is used to describe applications that allow server side user interaction , dynamic composition of virtual scenes , access to on line data , continuous visualization , and implementation of persistency.the use the x vrml language for building active applications of virtual reality is proposed . x vrml is a high level xml based language that overcomes the main limitations of the current virtual reality systems by providing convenient access to databases , object orientation , parameterization , and imperative programming techniques . applications of x vrml include on line data visualization , geographical information systems , scientific visualization , virtual games , and e commerce applications such as virtual shops . in this paper , methods of accessing databases from x vrml are described , architectures of x vrml systems for different application domains are discussed , and examples of database applications of virtual reality implemented in x vrml are presented .
distributed h infinity filtering for sensor networks with switching topology . <eos> in this article , the distributed h filtering problem is investigated for a class of sensor networks under topology switching . the main purpose is to design the distributed h filter that allows one to regulate the sensor 's working modes . firstly , a switched system model is proposed to reflect the working mode change of the sensors . then , a stochastic sequence is adopted to model the packet dropout phenomenon occurring in the channels from the plant to the networked sensors . by utilising the lyapunov functional method and stochastic analysis , some sufficient conditions are established to ensure that the filtering error system is mean square exponentially stable with a prescribed h performance level . furthermore , the filter parameters are determined by solving a set of linear matrix inequalities ( lmis ) . our results relates the decay rate of the filtering error system to the switching frequency of the topology directly and shows the existence of such a distributed filter when the topology is not varying very frequently , which is helpful for the sensor state regulation . finally , the effectiveness of the proposed design method is demonstrated by two numerical examples .
the evolution of goal based information modelling literature review . <eos> purpose the first in a series on goal based information modelling , this paper presents a literature review of two goal based measurement methods . the second article in the series will build on this background to present an overview of some recent case based research that shows the applicability of the goal based methods for information modelling ( as opposed to measurement ) . the third and concluding article in the series will present a new goal based information model the goal based information framework ( gbif ) that is well suited to the task of documenting and evaluating organisational information flow . design methodology approach following a literature review of the goal question metric ( gqm ) and goal question indicator measure ( gqim ) methods , the paper presents the strengths and weaknesses of goal based approaches . findings the literature indicates that the goal based methods are both rigorous and adaptable . with over <digit> years of use , goal based methods have achieved demonstrable and quantifiable results in both practitioner and academic studies . the down side of the methods are the potential expense and the expansiveness of goal based models . the overheads of managing the goal based process , from early negotiations on objectives and goals to maintaining the model ( adding new goals , questions and indicators ) , could make the method unwieldy and expensive for organisations with limited resources . an additional challenge identified in the literature is the narrow focus of top down ( i.e. goal based ) methods . since the methods limit the focus to a pre defined set of goals and questions , the opportunity for discovery of new information is limited . research limitations implications much of the previous work on goal based methodologies has been confined to software measurement contexts in larger organisations with well established information gathering processes . although the next part of the series presents goal based methods outside of this native context , and within low maturity organisations , further work needs to be done to understand the applicability of these methods in the information science discipline . originality value this paper presents ail overview of goal based methods . the next article in the series will present the method outside the native context of software measurement . with the universality of the method established , information scientists will have a new tool to evaluate and document organisational information flow .
a communication reduction approach to iteratively solve large sparse linear systems on a gpgpu cluster . <eos> finite element methods ( fem ) are widely used in academia and industry , especially in the fields of mechanical engineering , civil engineering , aerospace , and electrical engineering . these methods usually convert partial difference equations into large sparse linear systems . for complex problems , solving these large sparse linear systems is a time consuming process . this paper presents a parallelized iterative solver for large sparse linear systems implemented on a gpgpu cluster . traditionally , these problems do not scale well on gpgpu clusters . this paper presents an approach to reduce the communications between cluster compute nodes for these solvers . additionally , computation and communication are overlapped to reduce the impact of data exchange . the parallelized system achieved a speedup of up to 15.3 times on <digit> nvidia tesla gpus , compared to a single gpu . an analytical evaluation of the algorithm is conducted in this paper , and the analytical equations for predicting the performance are presented and validated .
transductive inference using multiple experts for brushwork annotation in paintings domain . <eos> many recent studies perform annotation of paintings based on brushwork . in these studies the brushwork is modeled indirectly as part of the annotation of high level artistic concepts such as the artist name using low level texture . in this paper , we develop a serial multi expert framework for explicit annotation of paintings with brushwork classes . in the proposed framework , each individual expert implements transductive inference by exploiting both labeled and unlabelled data . to minimize the problem of noise in the feature space , the experts select appropriate features based on their relevance to the brushwork classes . the selected features are utilized to generate several models to annotate the unlabelled patterns . the experts select the best performing model based on vapnik combined bound . the transductive annotation using multiple experts out performs the conventional baseline method in annotating patterns with brushwork classes .
on the integration of equations of motion for particle in cell codes . <eos> an area preserving implementation of the 2nd order runge kutta integration method for equations of motion is presented . for forces independent of velocity the scheme possesses the same numerical simplicity and stability as the leapfrog method , and is not implicit for forces which do depend on velocity . it can be therefore easily applied where the leapfrog method in general can not . we discuss the stability of the new scheme and test its performance in calculations of particle motion in three cases of interest . first , in the ubiquitous and numerically demanding example of nonlinear interaction of particles with a propagating plane wave , second , in the case of particle motion in a static magnetic field and , third , in a nonlinear dissipative case leading to a limit cycle . we compare computed orbits with exact orbits and with results from the leapfrog and other low order integration schemes . of special interest is the role of intrinsic stochasticity introduced by time diferencing , which can destroy orbits of an otherwise exactly integrable system and therefore constitutes a restriction on the applicability of an integration scheme in such a context a. friedman , s.p. auerbach , j. comput . phys . <digit> ( <digit> ) <digit> . in particular , we show that for a plane wave the new scheme proposed herein can be reduced to a symmetric standard map . this leads to the nonlinear stability condition delta t omega ( b ) < <digit> , where delta t is the time step and omega ( b ) the particle bounce frequency . ( c ) <digit> elsevier inc. all rights reserved .
system support for mobile augmented reality services . <eos> developing and deploying augmented reality ( ar ) services in pervasive computing environments is quite difficult because almost of all current systems require heavy and bulky head mounted displays ( hmds ) and are based on inflexible centralized architectures for detecting service locations and superimposing ar images . we propose a light weight mobile ar service framework that combines personal mobile devices most of people own nowadays , visual tags as inexpensive ar techniques , and mobile code that enables easy to deploy environments . our framework enables developers to easily deploy mobile ar services in pervasive computing environments and users to interact them in a both of practical and intuitive way .
fabrication of the wireless systems for controlling movements of the electrical stimulus capsule in the small intestines . <eos> diseases of the gastro intestinal tract are becoming more prevalent . new techniques and devices , such as the wireless capsule endoscope and the telemetry capsule , that are able to measure the various signals of the digestive organs ( temperature , ph , and pressure ) , have been developed for the observation of the digestive organs . in these capsule devices , there are no methods of moving and grasping them . in order to make a swift diagnosis and to give proper medication , it is necessary to control the moving speed of the capsule . this paper presents a wireless system for the control of movements of an electrical stimulus capsule . this includes an electrical stimulus capsule which can be swallowed and an external transmitting control system . a receiver , a receiving antenna ( small multi loop ) , a transmitter , and a transmitting antenna ( monopole ) were designed and fabricated taking into consideration the mpe , power consumption , system size , signal to noise ratio and the modulation method . the wireless system , which was designed and implemented for the control of movements of the electrical stimulus capsule , was verified by in vitro experiments which were performed on the small intestines of a pig . as a result , we found that when the small intestines are contracted by electrical stimuli , the capsule can move to the opposite direction , which means that the capsule can go up or down in the small intestines .
a continuous wavelet based approach to detect anisotropic properties in spatial point processes . <eos> a two dimensional stochastic point process can be regarded as a random measure and thus represented as a ( countable ) sum of delta dirac measures concentrated at some points . integration with respect to the point process itself leads to the concept of the continuous wavelet transform of a point process . applying then suitable translation , rotation and dilation operations through a non unitary operator , we obtain a transformed point process which highlights main properties of the original point process . the choice of the mother wavelet is relevant and we thus conduct a detailed analysis proposing three two dimensional mother wavelets . we use this approach to detect main directions present in the point process , and to test for anisotropy .
robust object tracking using joint color texture histogram . <eos> a novel object tracking algorithm is presented in this paper by using the joint color texture histogram to represent a target and then applying it to the mean shift framework . apart from the conventional color histogram features , the texture features of the object are also extracted by using the local binary pattern ( lbp ) technique to represent the object . the major uniform lbp patterns are exploited to form a mask for joint color texture feature selection . compared with the traditional color histogram based algorithms that use the whole target region for tracking , the proposed algorithm extracts effectively the edge and corner features in the target region , which characterize better and represent more robustly the target . the experimental results validate that the proposed method improves greatly the tracking accuracy and efficiency with fewer mean shift iterations than standard mean shift tracking . it can robustly track the target under complex scenes , such as similar target and background appearance , on which the traditional color based schemes may fail to track .
quasi resonant interconnects a low power , low latency design methodology . <eos> design and analysis guidelines for quasi resonant interconnect networks ( qrn ) are presented in this paper . the methodology focuses on developing an accurate analytic distributed model of the on chip interconnect and inductor to obtain both low power and low latency . excellent agreement is shown between the proposed model and spectras simulations . the analysis and design of the inductor , insertion point , and driver resistance for minimum power delay product is described . a case study demonstrates the design of a quasi resonant interconnect , transmitting a <digit> gb s data signal along a <digit> mm line in a tsmc 0.18 mu m cmos technology . as compared to classical repeater insertion , an average reduction of 91.1 % and 37.8 % is obtained in power consumption and delay , respectively . as compared to optical links , a reduction of 97.1 % and 35.6 % is observed in power consumption and delay , respectively .
combining hashing and enciphering algorithms for epidemiological analysis of gathered data . <eos> objectives compiling individual records coming from different sources is necessary for multi center studies . legal aspects can be satisfied by implementing anonymization procedures . when using these procedures with a different key for each study it becomes almost impossible to link records from separate data collections . methods the originality of the method relies on the way the combination of hashing and enciphering techniques is performed like in asymmetric encryption , two keys are used but the private key depends on the patient 's identity . results the combination of hashing and enciphering techniques provides a great improvement in the overall security of the proposed scheme . conclusion this methodology makes stored data available for use in the field of public health , while respecting legal security requirements .
a personalized english learning recommender system for esl students . <eos> this paper has developed an online personalized english learning recommender system capable of providing esl students with reading lessons that suit their different interests and therefore increase the motivation to learn . the system , using content based analysis , collaborative filtering , and data mining techniques , analyzes real students reading data and generates recommender scores , based on which to help select appropriate lessons for respective students . its performance having been tracked over a period of one year , this recommender system has proved to be very useful in heightening esl learners motivation and interest in reading .
graph based hierarchical conceptual clustering . <eos> hierarchical conceptual clustering has proven to be a useful , although under explored , data mining technique . a graph based representation of structural information combined with a substructure discovery technique has been shown to be successful in knowledge discovery . the subdue substructure discovery system provides one such combination of approaches . this work presents subdue and the development of its clustering functionalities . several examples are used to illustrate the validity of the approach both in structured and unstructured domains , as well as to compare subdue to the cobweb clustering algorithm . we also develop a new metric for comparing structurally defined clusterings . results show that subdue successfully discovers hierarchical clusterings in both structured and unstructured data .
an intelligent system employing an enhanced fuzzy c means clustering model application in the case of forest fires . <eos> fuzzy c means is a well established clustering algorithm . according to this approach instead of having each data point dpi ( x , y ) belonging only to a specific cluster in a crisp manner , each dpi belongs to all of the determined clusters with a different degree of membership . in this way cluster overlapping is allowed . this research effort enhances the fuzzy c means model in an intelligent manner , employing a flexible fuzzy termination criterion . the enhanced fuzzy c means clustering algorithm performs several iterations before the proper centers of the clusters more or less stabilize , which means that their coordinates remain almost equal to the previous ones . in this way the algorithm is expanded to perform in a more flexible and human like intelligent way , avoiding the chance of infinite loops and the performance of unnecessary iterations . a corresponding software system has been developed in c programming language applying the extended model . the system has been applied for the clustering of the greek forest departments according to their forest fire risk . two risk factors were taken into consideration , namely the number of forest fires and the annual burned forested areas . the design and the development of the innovative model system and the results of its application are presented and discussed in this research paper .
miniaturization of uwb antennas and its influence on antenna transceiver performance in impulse uwb communication . <eos> in this paper , a co design methodology and the effect of antenna miniaturization in an impulse uwb system transceiver is presented . modified small size printed tapered monopole antennas ( ptma ) are designed in different scaling sizes . in order to evaluate the performance and functionality of these antennas , the effect of each antenna is studied in a given impulse uwb system . the uwb system includes an impulse uwb transmitter and two kinds of uwb receivers are considered , one based on correlation detection and one on energy detection schemes . a tunable low power impulse uwb transmitter is designed and the benefit of co designing it with the ptma antenna is investigated for the 3.110.6 ghz band . a comparison is given between a <digit> ( omega ) design and a co designed version . our antenna transceiver co design methodology shows improvement in both transmitter efficiency and whole system performance . the simulation results show that the ptma antenna and its miniaturized geometries are suitable for uwb applications .
induced quasi arithmetic uncertain linguistic aggregation operator . <eos> induced quasi arithmetic aggregation operators are considered to aggregate uncertain linguistic information by using order inducing variables . we introduce the induced correlative uncertain linguistic aggregation operator with choquet integral and we also present the induced uncertain linguistic aggregation operator by using the dempster shafer theory of evidence . the special cases of the new proposed operators are investigated . many existing linguistic aggregation operators are special cases of our new operators and more new uncertain linguistic aggregation operators can be derived from them . decision making methods based on the new aggregation operators are proposed and architecture material supplier selection problems are presented to illustrate the feasibility and efficiency of the new methods .
on fuzzy congruence of a near ring module . <eos> the aim of this paper is to introduce fuzzy submodule and fuzzy congruence of an r module ( near ring module ) , to obtain the correspondence between fuzzy congruences and fuzzy submodules of an r module , to define quotient r module of an r module over a fuzzy submodule and to obtain correspondence between fuzzy congruences of an r module and fuzzy congruences of quotient r module over a fuzzy submodule of an r module . ( c ) <digit> elsevier science b.v. all rights reserved .
self bounded controlled invariant subspaces in measurable signal decoupling with stability minimal order feedforward solution . <eos> the structural properties of self bounded controlled invariant subspaces are fundamental to the synthesis of a dynamic feedforward compensator achieving insensitivity of the controlled output to a disturbance input accessible for measurement , on the assumption that the system is stable or pre stabilized by an inner feedback . the control system herein devised has several important features i ) minimum order of the feedforward compensator ii ) minimum number of unassignable dynamics internal to the feedforward compensator iii ) maximum number of dynamics , external to the feedforward compensator , arbitrarily assignable by a possible inner feedback . from the numerical point of view , the design method herein detailed does not involve any computation of eigenspaces , which may be critical for systems of high order . the procedure is first presented for left invertible systems . then , it is extended to non left invertible systems by means of a simple , original , squaring down technique .
hypergraph based multilevel matrix approximation for text information retrieval . <eos> in latent semantic indexing ( lsi ) , a collection of documents is often pre processed to form a sparse term document matrix , followed by a computation of a low rank approximation to the data matrix . a multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term document matrix representing the data . the main goal is to reduce the cost of the matrix approximation without sacrificing accuracy . because coarsening by multilevel hypergraph techniques is a form of clustering , the proposed approach can be regarded as a hybrid of factorization based lsi and clustering based lsi . experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost
balanced paths in acyclic networks tractable cases and related approaches . <eos> given a weighted acyclic network g and two nodes s and t in g , we consider the problem of computing k balanced paths from s to t , that is , k paths such that the difference in cost between the longest and the shortest path is minimized . the problem has several variants . we show that , whereas the general problem is solvable in pseudopolynomial time , both the arc disjoint and the node disjoint variants ( i.e. , the variants where the k paths are required to be arc disjoint and node disjoint , respectively ) are strongly np hard . we then address some significant special cases of such variants , and propose exact as well as approximate algorithms for their solution . the proposed approaches are also able to solve versions of the problem in which k origin destination pairs are provided , and a set of k paths linking the origin destination pairs has to be computed in such a way to minimize the difference in cost between the longest and the shortest path in the set . ( c ) <digit> wiley periodicals , inc. networks , vol . <digit> ( <digit> ) , 104 111 2005
the connected assignment problem . <eos> given a graph and costs of assigning to each vertex one of k different colors , we want to find a minimum cost assignment such that no color q induces a subgraph with more than a given number ( q ) of connected components . this problem arose in the context of contiguity constrained clustering , but also has a number of other possible applications . we show the problem to be np hard . nevertheless , we derive a dynamic programming algorithm that proves the case where the underlying graph is a tree to be solvable in polynomial time . next , we propose mixed integer programming formulations for this problem that lead to branch and cut and branch and price algorithms . finally , we introduce a new class of valid inequalities to obtain an enhanced branch and cut . extensive computational experiments are reported .
stable spaces for real time clothing . <eos> we present a technique for learning clothing models that enables the simultaneous animation of thousands of detailed garments in real time . this surprisingly simple conditional model learns and preserves the key dynamic properties of a cloth motion along with folding details . our approach requires no a priori physical model , but rather treats training data as a black box . we show that the models learned with our method are stable over large time steps and can approximately resolve cloth body collisions . we also show that within a class of methods , no simpler model covers the full range of cloth dynamics captured by ours . our method bridges the current gap between skinning and physical simulation , combining benefits of speed from the former with dynamic effects from the latter . we demonstrate our approach on a variety of apparel worn by male and female human characters performing a varied set of motions typically used in video games ( e.g. , walking , running , jumping , etc. ) .
using topes to validate and reformat data in end user programming tools . <eos> end user programming tools offer no data types except string for many categories of data , such as person names and street addresses . consequently , these tools can not automatically validate or reformat these data . to address this problem , we have developed a user extensible model for string like data . each tope in this model is a user defined abstraction that guides the interpretation of strings as a particular kind of data . specifically , each tope implementation contains software functions for recognizing and reformatting instances of that tope 's kind of data . this makes it possible at runtime to distinguish between invalid data , valid data , and questionable data that could be valid or invalid . once identified , questionable and or invalid data can be double checked and possibly corrected , thereby increasing the overall reliability of the data . valid data can be automatically reformatted to any of the formats appropriate for that kind of data . to show the general applicability of topes , we describe new features that topes have enabled us to provide in four tools .
rough sets and the role of the monetary policy in financial stability ( macroeconomic problem ) and the prediction of insolvency in insurance sector ( microeconomic problem ) . <eos> this paper faces two questions related with financial stability . the first one is a macroeconomic problem in which we try to further investigate the role of monetary policy in explaining banking sector fragility and , ultimately , systemic banking crisis . it analyses a large sample of countries in the period <digit> . we find that the degree of central bank independence is one of the key variables to explain financial crisis . however , the effects of the degree of independence are not linear . surprisingly , either a high degree of independence or a high degree of dependence are compatible with a situation of financial stability , while intermediate levels of independence are more likely associated with financial crisis . it seems that it is the uncertainty related with a non clear allocation of monetary policy responsibilities that contributes to financial crisis episodes . the second one is a microeconomic problem the prediction of insolvency in insurance companies . this question has been a concern of several parties stemmed from the perceived need to protect general public and to minimize the costs associated such as the effects on state insurance guaranty funds or the responsibilities for management and auditors . we have developed a bankruptcy prediction model for spanish non life insurance companies and the results obtained are very encouraging in comparison with previous analysis . this model could be used as an early warning system for supervisors in charge of the soundness of these entities and or in charge of the financial system stability . most methods applied in the past to tackle these two problems are techniques of statistical nature and , variables employed in these models do not usually satisfy statistical assumptions what complicates the analysis . we propose an approach to undertake these questions based on rough set theory .
classification of self dual codes of length <digit> . <eos> a complete classification of binary self dual codes of length <digit> is given .
supporting pervasive computing applications with active context fusion and semantic context delivery . <eos> future pervasive computing applications are envisioned to adapt the applications behaviors by utilizing various contexts of an environment and its users . such context information may often be ambiguous and also heterogeneous , which make the delivery of unambiguous context information to real applications extremely challenging . thus , a significant challenge facing the development of realistic and deployable context aware services for pervasive computing applications is the ability to deal with these ambiguous contexts . in this paper , we propose a resource optimized quality assured context mediation framework based on efficient context aware data fusion and semantic based context delivery . in this framework , contexts are first fused by an active fusion technique based on dynamic bayesian networks and ontology , and further mediated using a composable ontological rule based model with the involvement of users or application developers . the fused context data are then organized into an ontology based semantic network together with the associated ontologies in order to facilitate efficient context delivery . experimental results using sunspot and other sensors demonstrate the promise of this approach .
on computing the minimum <digit> path vertex cover and dissociation number of graphs . <eos> the dissociation number of a graph g is the number of vertices in a maximum size induced subgraph of g with vertex degree at most <digit> . a k path vertex cover of a graph g is a subset s of vertices of g such that every path of order k in g contains at least one vertex from s. the minimum <digit> path vertex cover is a dual problem to the dissociation number . for this problem , we present an exact algorithm with a running time of o ( 1.5171 ( n ) ) on a graph with n vertices . we also provide a polynomial time randomized approximation algorithm with an expected approximation ratio of <digit> <digit> for the minimum <digit> path vertex cover . ( c ) <digit> elsevier b.v. all rights reserved .
interval multiplicative transitivity for consistency , missing values and priority weights of interval fuzzy preference relations . <eos> in this paper , the concept of multiplicative transitivity of a fuzzy preference relation , as defined by tanino t. tanino , fuzzy preference orderings in group decision making , fuzzy sets and systems <digit> ( <digit> ) <digit> , is extended to discover whether an interval fuzzy preference relation is consistent or not , and to derive the priority vector of a consistent interval fuzzy preference relation . we achieve this by introducing the concept of interval multiplicative transitivity of an interval fuzzy preference relation and show that , by solving numerical examples , the test of consistency and the weights derived by the simple formulas based on the interval multiplicative transitivity produce the same results as those of linear programming models proposed by xu and chen z.s. xu , j. chen , some models for deriving the priority weights from interval fuzzy preference relations , european journal of operational research <digit> ( <digit> ) <digit> . in addition , by taking advantage of interval multiplicative transitivity of an interval fuzzy preference relation , we put forward two approaches to estimate missing value ( s ) of an incomplete interval fuzzy preference relation , and present numerical examples to illustrate these two approaches .
an o ( n log n ) algorithm for finding a shortest central link segment . <eos> a central link segment of a simple n vertex polygon p is a segment s inside p that minimizes the quantity max ( x epsilon p ) min ( y epsilon s ) d ( l ) ( x , y ) , where d ( l ) ( x , y ) is the link distance between points a and y of p. in this paper we present an o ( n log n ) algorithm for finding a central link segment of p. this generalizes previous results for finding an edge or a segment of p from which p is visible . moreover , in the same time bound , our algorithm finds a central link segment of minimum length . constructing a central link segment has applications to the problems of finding an optimal robot placement in a simply connected polygonal region and determining the minimum value k for which a given polygon is k visible from some segment .
deconstructing switch reference . <eos> this paper develops a new view on switch reference , a phenomenon commonly taken to involve a morphological marker on a verb indicating whether the subject of this verb is coreferent with or disjoint from the subject of another verb . ipropose a new structural source of switch reference marking , which centers around coordination at different heights of the clausal structure , coupled with distinct morphological realizations of the syntactic coordination head . conjunction of two vps has two independent consequences first , only a single external argument is projected second , the coordinator head is realized by some marker a ( the same subject marker ) . conjunction of two vps , by contrast , leads to projection of two independent external arguments and a different realization of the coordination by a marker b ( the different subject marker ) . the hallmark properties of this analysis are that ( i ) subject identity or disjointness is only indirectly tied to the switch reference markers , furnishing a straightforward account of cases where this correlation breaks down ( ii ) switch reference does not operate across fully developed clauses , which accounts for the widely observed featural defectiveness of switch reference clauses ( iii ) same subject and different subject constructions differ in their syntactic structure , thus accommodating cases where the choice of the switch reference markers has an impact on event structure . the analysis is mainly developed on the basis of evidence from the mexican language seri , the papuan language amele , and the north american language kiowa .
an optimized parallel lsqr algorithm for seismic tomography . <eos> the lsqr algorithm developed by paige and saunders ( <digit> ) is considered one of the most efficient and stable methods for solving large , sparse , and ill posed linear ( or linearized ) systems . in seismic tomography , the lsqr method has been widely used in solving linearized inversion problems . as the amount of seismic observations increase and tomographic techniques advance , the size of inversion problems can grow accordingly . currently , a few parallel lsqr solvers are presented or available for solving large problems on supercomputers , but the scalabilities are generally weak because of the significant communication cost among processors . in this paper , we present the details of our optimizations on the lsqr code for , but not limited to , seismic tomographic inversions . the optimizations we have implemented to our lsqr code include reordering the damping matrix to reduce its band width for simplifying the communication pattern and reducing the amount of communication during calculations adopting sparse matrix storage formats for efficiently storing and partitioning matrices using the mpi i o functions to parallelize the date reading and result writing processes providing different data partition strategies for efficiently using computational resources . a large seismic tomographic inversion problem , the full 3d waveform tomography for southern california , is used to explain the details of our optimizations and examine the performance on yellowstone supercomputer at the ncar wyoming supercomputing center ( nwsc ) . the results showed that the required wall time of our code for the same inversion problem is much less than that of the lsqr solver from the petsc library ( balay et al. , <digit> ) .
on computer assisted classification of coupled integrable equations . <eos> we show how the triangularization method of the second author can be successfully applied to the problem of classification of homogeneous coupled integrable equations . the classifications rely on the recent algorithm developed by the first author that requires solving <digit> systems of polynomial equations . we show that these systems can be completely resolved in the case of coupled korteweg de vries , sawada kotera and kaup kupershmidttype equations .
a novel method for fingerprint verification that approaches the problem as a two class pattern recognition problem . <eos> we present a system for fingerprint verification that approaches the problem as a two class pattern recognition problem . the distances of the test fingerprint to the reference fingerprints are normalized by the corresponding mean values obtained from the reference set , to form a five dimensional feature vector . this feature vector is then projected onto a one dimensional karhunen loeve space and then classified into one of the two classes ( genuine or impostor ) .
the uncovering of hidden structures by latent semantic analysis . <eos> latent semantic analysis ( lsa ) is a well known method for information retrieval . it has also been applied as a model of cognitive processing and word meaning acquisition . this dual importance of lsa derives from its capacity to modulate the meaning of words by contexts , dealing successfully with polysemy and synonymy . the underlying reasons that make the method work are not clear enough . we propose that the method works because it detects an underlying block structure ( the blocks corresponding to topics ) in the term by document matrix . in real cases this block structure is hidden because of perturbations . we propose that the correct explanation for lsa must be searched in the structure of singular vectors rather than in the profile of singular values . using the perronfrobenius theory we show that the presence of disjoint blocks of documents is marked by sign homogeneous entries in the vectors corresponding to the documents of one block and zeros elsewhere . in the case of nearly disjoint blocks , perturbation theory shows that if the perturbations are small , the zeros in the leading vectors are replaced by small numbers ( pseudo zeros ) . since the singular values of each block might be very different in magnitude , their order does not mirror the order of blocks . when the norms of the blocks are similar , lsa works fine , but we propose that when the topics have different sizes , the usual procedure of selecting the first k singular triplets ( k being the number of blocks ) should be replaced by a method that selects the perturbed perron vectors for each block .
computing monodromy groups defined by plane algebraic curves . <eos> we present a symbolic numeric method to compute the monodromy group of a plane algebraic curve viewed as a ramified covering space of the complex plane . following the definition , our algorithm is based on analytic continuation of algebraic functions above paths in the complex plane . our contribution is three fold first of all , we show how to use a minimum spanning tree to minimize the length of paths then , we propose a strategy that gives a good compromise between the number of steps and the truncation orders of puiseux expansions , obtaining for the first time a complexity result about the number of steps finally , we present an efficient numerical modular algorithm to compute puiseux expansions above critical points , which is a non trivial task .
stone like representation theorems and three valued filters in r <digit> algebras ( nilpotent minimum algebras ) . <eos> nilpotent minimum algebras ( nm algebras ) are algebraic counterpart of a formal deductive system where conjunction is modeled by the nilpotent minimum t norm , a logic also independently introduced by guo jun wang in the mid 1990s . such algebras are to this logic just what boolean algebras are to the classical propositional logic . in this paper , by introducing respectively the stone topology and a three valued fuzzy stone topology on the set of all maximal filters in an nm algebra , we first establish two analogues for an nm algebra of the well known stone representation theorem for a boolean algebra , which state that the boolean skeleton of an nm algebra is isomorphic to the algebra of all clopen subsets of its stone space and the three valued skeleton is isomorphic to the algebra of all clopen fuzzy subsets of its three valued fuzzy stone space , respectively . then we introduce the notions of boolean filter and of three valued filter in an nm algebra , and finally we prove that three valued filters and closed subsets of the stone space of an nm algebra are in one to one correspondence and boolean filters uniquely correspond to closed subsets of the subspace consisting of all ultrafilters . ( c ) <digit> elsevier b.v. all rights reserved .
an adaptive learning scheme for load balancing with zone partition in multi sink wireless sensor network . <eos> in many researches on load balancing in multi sink wsn , sensors usually choose the nearest sink as destination for sending data . however , in wsn , events often occur in specific area . if all sensors in this area all follow the nearest sink strategy , sensors around nearest sink called hotspot will exhaust energy early . it means that this sink is isolated from network early and numbers of routing paths are broken . in this paper , we propose an adaptive learning scheme for load balancing scheme in multi sink wsn . the agent in a centralized mobile anchor with directional antenna is introduced to adaptively partition the network into several zones according to the residual energy of hotspots around sink nodes . in addition , machine learning is applied to the mobile anchor to make it adaptable to any traffic pattern . through interactions with the environment , the agent can discovery a near optimal control policy for movement of mobile anchor . the policy can achieve minimization of residual energys variance among sinks , which prevent the early isolation of sink and prolong the network lifetime .
interactive visual tools to explore spatio temporal variation . <eos> commongis is a developing software system for exploratory analysis of spatial data . it includes a multitude of tools applicable to different data types and helping an analyst to find answers to a variety of questions . commongis has been recently extended to support exploration of spatio temporal data , i.e. temporally variant data referring to spatial locations . the set of new tools includes animated thematic maps , map series , value flow maps , time graphs , and dynamic transformations of the data . we demonstrate the use of the new tools by considering different analytical questions arising in the course of analysis of thematic spatio temporal data .
multiprocessor system on chip ( mpsoc ) technology . <eos> the multiprocessor system on chip ( mpsoc ) uses multiple cpus along with other hardware subsystems to implement a system . a wide range of mpsoc architectures have been developed over the past decade . this paper surveys the history of mpsocs to argue that they represent an important and distinct category of computer architecture . we consider some of the technological trends that have driven the design of mpsocs . we also survey computer aided design problems relevant to the design of mpsocs .
statistical behavior of joint least square estimation in the phase diversity context . <eos> the images recorded by optical telescopes are often degraded by aberrations that induce phase variations in the pupil plane . several wavefront sensing techniques have been proposed to estimate aberrated phases . one of them is phase diversity , for which the joint least square approach introduced by gonsalves et al. is a reference method to estimate phase coefficients from the recorded images . in this paper , we rely on the asymptotic theory of toeplitz matrices to show that gonsalves ' technique provides a consistent phase estimator as the size of the images grows . no comparable result is yielded by the classical joint maximum likelihood interpretation ( e.g. , as found in the work by paxman et al. ) . finally , our theoretical analysis is illustrated through simulated problems .
integrated in silico approaches for the prediction of ames test mutagenicity . <eos> the bacterial reverse mutation assay ( ames test ) is a biological assay used to assess the mutagenic potential of chemical compounds . in this paper approaches for the development of an in silico mutagenicity screening tool are described . three individual in silico models , which cover both structure activity relationship methods ( sars ) and quantitative structure activity relationship methods ( qsars ) , were built using three different modelling techniques ( <digit> ) an in house alert model which uses sar approach where alerts are generated based on experts judgements ( <digit> ) a knn approach ( k nearest neighbours ) , which is a qsar model where a prediction is given based on outcomes of its k chemical neighbours ( <digit> ) a naive bayesian model ( nb ) , which is another qsar model , where a prediction is derived using a bayesian formula through preselected identified informative chemical features ( e.g. , physico chemical , structural descriptors ) . these in silico models , were compared against two well known alert models ( derek and toxtree ) and also against three different consensus approaches ( categorical bayesian integration approach ( cbi ) , partial least squares discriminate analysis ( pls da ) and simple majority vote approach ) . by applying these integration methods on the validation sets it was shown that both integration models ( pls da and cbi ) achieved better performance than any of the individual models or consensus obtained by simple majority rule . in conclusion , the recommendation of this paper is that when obtaining consensus predictions for ames mutagenicity , approaches like pls da or cbi should be the first choice for the integration as compared to a simple majority vote approach .
visualization and clustering of categorical data with probabilistic self organizing map . <eos> this paper introduces a self organizing map dedicated to clustering , analysis and visualization of categorical data . usually , when dealing with categorical data , topological maps use an encoding stage categorical data are changed into numerical vectors and traditional numerical algorithms ( som ) are run . in the present paper , we propose a novel probabilistic formalism of kohonen map dedicated to categorical data where neurons are represented by probability tables . we do not need to use any coding to encode variables . we evaluate the effectiveness of our model in four examples using real data . our experiments show that our model provides a good quality of results when dealing with categorical data .
stiffness analysis of parallelogram type parallel manipulators using a strain energy method . <eos> stiffness analysis of a general ptpm using an algebraic method . result comparison between the proposed method and a finite element analysis method . a new stiffness index relating the stiffness property to the wrench experienced in a task .
simulation of natural and social process interactions an example from bronze age mesopotamia . <eos> new multimodel simulations of bronze age mesopotamian settlement system dynamics , using advanced object based simulation frameworks , are addressing fine scale interaction of natural processes ( crop growth , hydrology , etc. ) and social processes ( kinship driven behaviors , farming and herding practices , etc. ) on a daily basis across multi enerational model runs . key components of these simulations are representations of initial settlement populations that are demographically and socially plausible , and detailed models of social mechanisms that can produce and maintain realistic textures of social structure and dynamics over time . the simulation engine has broad applicability and is also being used to address modern problems such as agroeconomic sustainability in southeast asia . this article describes the simulation framework and presents results of initial studies , highlighting some social system representations .
newton like dynamics and forward backward methods for structured monotone inclusions in hilbert spaces . <eos> in a hilbert space setting we introduce dynamical systems , which are linked to newton and levenbergmarquardt methods . they are intended to solve , by splitting methods , inclusions governed by structured monotone operators m a b , where a is a general maximal monotone operator , and b is monotone and locally lipschitz continuous . based on the minty representation of a as a lipschitz manifold , we show that these dynamics can be formulated as differential systems , which are relevant to the cauchylipschitz theorem , and involve separately b and the resolvents of a. in the convex subdifferential case , by using lyapunov asymptotic analysis , we prove a descent minimizing property and weak convergence to equilibria of the trajectories . time discretization of these dynamics gives algorithms combining newtons method and forward backward methods for solving structured monotone inclusions .
damage identification of a target substructure with moving load excitation . <eos> this paper presents a substructural damage identification approach under moving vehicular loads based on a dynamic response reconstruction technique . the relationship between two sets of time response vectors from the substructure subject to moving loads is formulated with the transmissibility matrix based on impulse response function in the wavelet domain . only the finite element model of the intact target substructure and the measured dynamic acceleration responses from the target substructure in the damaged state are required . the time histories of moving loads and interface forces on the substructure are not required in the proposed algorithm . the dynamic response sensitivity based method is adopted for the substructural damage identification with the local damage modeled as a reduction in the elemental stiffness factor . the adaptive tikhonov regularization technique is employed to have an improved identification result when noise effect is included in the measurements . numerical studies on a three dimensional box section girder bridge deck subject to a single moving force or a two axle three dimensional moving vehicle are conducted to investigate the performance of the proposed substructural damage identification approach . the simulated local damage can be identified with <digit> % noise in the measured data .
randomized parallel communication ( preliminary version ) . <eos> using a simple finite degree interconnection network among n processors and a straightforward randomized algorithm for packet delivery , it is possible to deliver a set of n packets travelling to unique targets from unique sources in <digit> ( log n ) expected time . the expected delivery time is in other words the depth of the interconnection graph . the b way shufile networks are examples of such . this represents a crude analysis of the transient response to a sudden but very uniform request load on the network . variations in the uniformity of the load are also considered . consider s i packets with randomly chosen targets beginning at a source labelled i . the expected overall delay is then equation where the labelling is chosen so that s <digit> s <digit> . these ideas can be used to guage the asymptotic efficiency of various synchronous parallel algorithms which use such a randomized communications system . the only important assumption is that variations in the physical transmission time along any connection link are negligible in comparison to the amount of work done at a processor .
feature selection for fast speech emotion recognition . <eos> in speech based emotion recognition , both acoustic features extraction and features classification are usually time consuming , which obstruct the system to be real time . in this paper , we proposea novel feature selection ( fsalgorithm to filter out the low efficiency features towards fast speech emotion recognition.firstly , each acoustic feature 's discriminative ability , time consumption and redundancy are calculated . then , we map the original feature space into a nonlinear one to select nonlinear features , which can exploit the underlying relationship among the original features . thirdly , high discriminative nonlinear feature with low time consumption is initially preserved . finally , a further selection is followed to obtain low redundant features based on these preserved features . the final selected nonlinear features are used in features ' extraction and features ' classification in our approach , we call them qualified features . the experimental results demonstrate that recognition time consumption can be dramatically reduced in not only the extraction phase but also the classification phase . moreover , a competitive of recognition accuracy has been observed in the speech emotion recognition .
automated inspection planning of free form shape parts by laser scanning . <eos> the inspection operation accounts for a large portion of manufacturing lead time , and its importance in quality control can not be overemphasized . in recent years , due to the development of laser technology , the accuracy of laser scanners has been improved significantly so that they can be used in a production environment . they are noncontact type measuring devices and usually have the scanning speed that is <digit> times faster than that of coordinate measuring machines . this laser scanning technology provides us a platform that enables us to perform a <digit> % inspection of complicated shape parts . this research proposes algorithms that lead to the automation of laser scanner based inspection operations . the proposed algorithms consist of three steps firstly , all possible accessible directions at each sampled point on a part surface are generated considering constraints existing in a laser scanning operation . the constraints include satisfying the view angle , the depth of view , checking interference with a part , and avoiding collision with the probe . secondly , the number of scans and the most desired direction for each scan are calculated . finally , the scan path that gives the minimum scan time is generated . the proposed algorithms are applied to sample parts and the results are discussed .
gbf a grammar based filter for internet applications . <eos> observing network traffic is necessary for achieving different purposes such as system performance , network debugging and or information security . observations , as such , are obtained from low level monitors that may record a large volume of relevant and irrelevant events . thus adequate filters are needed to pass interesting information only . this work presents a multilayer system , gbf that integrates both packet ( low level ) and document ( high level ) filters . actually , the design of gbf is grammar based so that it relies upon a set of context free grammars to carry out various processes , specially the document reconstruction process . gbf consists of three layers , acquisition layer , packet filter layer , and reconstruction layer . the performance of the reconstruction process is evaluated in terms of the time consumed during service separation and session separation tasks .
enhanced particle swarm optimizer incorporating a weighted particle . <eos> this study proposes an enhanced particle swarm optimizer incorporating a weighted particle ( epsowp ) to improve the evolutionary performance for a set of benchmark functions . in conventional particle swarm optimizer ( pso ) , there are two principal forces to guide the moving direction of each particle . however , if the current particle lies too close to either the personal best particle or the global best particle , the velocity is mainly updated by only one term . as a result , search step becomes smaller and the optimization of the swarm is likely to be trapped into a local optimum . to address this problem , we define a weighted particle for incorporation into the particle swarm optimization . because the weighted particle has a better opportunity getting closer to the optimal solution than the global best particle during the evolution , the epsowp is capable of guiding the swarm to a better direction to search the optimal solution . simulation results show the effectiveness of the epsowp , which outperforms various evolutionary algorithms on a selected set of benchmark functions . furthermore , the proposed epsowp is applied to controller design and parameter identification for an inverted pendulum system as well as parameter learning of neural network for function approximation to show its viability to solve practical design problems .
media access protocol for a coexisting cognitive femtocell network . <eos> femtocell networks are widely deployed to extend cellular network coverage into indoor environments such as large office spaces and homes . cognitive radio functionality can be implemented in femtocell networks based on an overlay mechanism under the assumption of a hierarchical access scenario . this study introduces a novel femtocell network architecture , that is characterized by a completely autonomous femtocell bandwidth access and a distributed media access control protocol for supporting data and real time traffic . the detailed description of the architecture and media access protocol is presented . furthermore , in depth theoretical analysis is performed on the proposed media access protocol using discrete time markov chain modeling to validate the effectiveness of the proposed protocol and architecture .
integrating computer animation and multimedia . <eos> multimedia provides an immensely powerful tool for the dissemination of both information and entertainment . current multimedia presentations consist of synchronised excerpts of media ( such as sound , video gi text ) which are coordinated by an author to ensure a clear narrative is presented to the audience . however each of the segments of the presentation consist of previously recorded footage , only the timing and synchronisation are dynamically constructed . the next logical advance for such systems is therefore to include the capability of generating material ' on the fly ' in response to the actions of the audience . this paper describes a mechanism for using computer animation to generate this interactive material . unlike previous animation techniques the approach presented here is suitable for use in constructing a storyline which the author can control , but the user can influence . in order to allow such techniques to be used we also present a multimedia authoring gr playback system which incorporates interactive animation with existing media .
an ontology for supporting communities of practice . <eos> in the context of the palette project aimed at enhancingallindividual and organizational learning in communities of practice ( cops ) , we are developing knowledge management ( km ) services . our approach is based on an ontology dedicated to cops and built from analysis of information sources about eleven cops available in palette project . this ontology aims both at modeling the members of the cop and at annotating the cop knowledge resources . the paper describes our method for building this ontology , its structure and contents and it analyses our experience feedback from the cooperative building of this ontology .
a set of neural tools for human computer interactions application to the handwritten character recognition , and visual speech recognition problems . <eos> this paper presents a new technique of data coding and an associated set of homogenous processing tools for the development of human computer interactions ( hci ) . the proposed technique facilitates the fusion of different sensorial modalities and simplifies the implementations . the coding takes into account the spatio temporal nature of the signals to be processed in the framework of a sparse representation of data . neural networks adapted to such a representation of data are proposed to perform the recognition tasks . their development is illustrated by two examples one of on line handwritten character recognition and the other of visual speech recognition .
impact of sub optimal checkpoint intervals on application efficiency in computational clusters . <eos> as computational clusters rapidly grow in both size and complexity , system reliability and , in particular , application resilience have become increasingly important factors to consider in maintaining efficiency and providing improved computational performance over predecessor systems . one commonly used mechanism for providing application fault tolerance in parallel systems is the use of checkpointing . by making use of a multi cluster simulator , we study the impact of sub optimal checkpoint intervals on overall application efficiency . by using a model of a <digit> node cluster and workload statistics from los alamos national laboratory to parameterize the simulator , we find that dramatically overestimating the amtti has a fairly minor impact on application efficiency while potentially having a much more severe impact on user centric performance metrics such a queueing delay . we compare and contrast these results with the trends predicted by an analytical model .
an approach to a content based retrieval of multimedia data . <eos> this paper presents a data model tailored for multimedia data representation , along with the main characteristics of a multimedia query language that exploits the features of the proposed model . the model addresses data presentation , manipulation and content based retrieval . it consists of three parts a multimedia description model , which provides a structural view of raw multimedia data , a multimedia presentation model , and a multimedia interpretation model which allows semantic information to be associated with multimedia data . the paper focuses on the structuring of a multimedia data model which provides support for content based retrieval of multimedia data . the query language is an extension of a traditional query language which allows restrictions to be expressed on features , concepts , and the structural aspects of the objects of multimedia data and the formulation of queries with imprecise conditions . the result of a query is an approximate set of database objects which partially match such a query .
monte carlo em with importance reweighting and its applications in random effects models1 . <eos> in this paper we propose a new monte carlo em algorithm to compute maximum likelihood estimates in the context of random effects models . the algorithm involves the construction of efficient sampling distributions for the monte carlo implementation of the e step , together with a reweighting procedure that allows repeatedly using a same sample of random effects . in addition , we explore the use of stochastic approximations to speed up convergence once stability has been reached . our algorithm is compared with that of mcculloch ( <digit> ) . extensions to more general problems are discussed .
a perceptual approach for stereoscopic rendering optimization . <eos> the traditional way of stereoscopic rendering requires rendering the scene for left and right eyes separately which doubles the rendering complexity . in this study , we propose a perceptually based approach for accelerating stereoscopic rendering . this optimization approach is based on the binocular suppression theory , which claims that the overall percept of a stereo pair in a region is determined by the dominant image on the corresponding region . we investigate how binocular suppression mechanism of human visual system can be utilized for rendering optimization . our aim is to identify the graphics rendering and modeling features that do not affect the overall quality of a stereo pair when simplified in one view . by combining the results of this investigation with the principles of visual attention , we infer that this optimization approach is feasible if the high quality view has more intensity contrast . for this reason , we performed a subjective experiment , in which various representative graphical methods were analyzed . the experimental results verified our hypothesis that a modification , applied on a single view , is not perceptible if it decreases the intensity contrast , and thus can be used for stereoscopic rendering .
using traditional loop unrolling to fit application on a new hybrid reconfigurable architecture . <eos> this paper presents a strategy to modify a sequential implementation of an h. <digit> avc motion estimation to run on a new reconfigurable architecture called rosa . the modifications aim to provide more parallelism that will be exploited by the architecture . in the strategy presented in this paper we used traditional loop unrolling and profile information as techniques to modify the application and to generate a best fit solution to rosa architecture .
evaluating fluid semantics for passive stochastic process algebra cooperation . <eos> fluid modelling is a next generation technique for analysing massive performance models . passive cooperation is a popular cooperation mechanism frequently used by performance engineers . therefore having an accurate translation of passive cooperation into a fluid model is of direct practical application . we compare different existing styles of fluid model translations of passive cooperation in a stochastic process algebra and show how the previous model can be improved upon significantly . we evaluate the new passive cooperation fluid semantics and show that the first order fluid model is a good approximation to the dynamics of the underlying continuous time markov chain . we show that in a family of possible translations to the fluid model , there is an optimal translation which can be expected to introduce least error . finally , we use these new techniques to show how the scalability of a passively cooperating distributed software architecture could be assessed .
using new media to improve self help for clients and staff . <eos> one of the most common frustrations for any person looking for technical support is actually finding effective technical support . even if a solution seems clear , it can be misunderstood if the vernacular is not just right . a large part of a successful support call involves being able to determine the actual problem based on the information the client provides . help desk analysts must have the ability to translate non tech descriptions to identify a problem in technical terms and then communicate a solution using vernacular the client can understand . this process is always a little different . if we aim to be successful analysts , we must speak different languages in order to help our clients . based on this logic , it stands to reason that our self help documentation must do the same . providing a variety of methods to get self help ensures a message will be received by a wider audience . in the world of modern media , audiences are presented with many ways to consume information . this ensures the message is heard by the most people in a manner that is the most appealing and the most clear . new methods of consuming information have become possible as the face of mainstream media has become democratized over the last few years . this is thanks largely to the fact that the tools needed to create and distribute content have become affordable and readily available to anyone with a bit of technical skill . anyone with a laptop , a webcam and a little imagination can and do create content . considering all of this , we asked ourselves , why should n't we . we have found that creating content in new media is relatively easy and fun . finding and creating new methods to deliver content positively engages and challenges our help desk team . thinking about how to best use new media requires help desk analysts to rethink otherwise standardized and mundane processes and create fresh perspectives . the creation and production of new media establishes stronger ownership of procedures and process . we would like to share the following from our ongoing experiences with new media at our help desk general issues we see with clients finding help how creating new media creates stronger ownership and morale with staff expanding the technical skills of help desk staff how using new media improves our client experience casting a wider net ( ensuring a message gets to the most people ) how we use new media and what we have done with it how to make your own video podcast in 1,345 easy steps
a framework for preservation of cloud users data privacy using dynamic reconstruction of metadata . <eos> in the rising paradigm of cloud computing , attainment of sustainable levels of cloud users trust in using cloud services is directly dependent on effective mitigation of its associated impending risks and resultant security threats . among the various indispensible security services required to ensure effective cloud functionality leading to enhancement of users confidence in using cloud offerings , those related to the preservation of cloud users data privacy are significantly important and must be matured enough to withstand the imminent security threats , as emphasized in this research paper . this paper highlights the possibility of exploiting the metadata stored in cloud 's database in order to compromise the privacy of users data items stored using a cloud provider 's simple storage service . it , then , proposes a framework based on database schema redesign and dynamic reconstruction of metadata for the preservation of cloud users data privacy . using the sensitivity parameterization parent class membership of cloud database attributes , the database schema is modified using cryptographic as well as relational privacy preservation operations . at the same time , unaltered access to database files is ensured for the cloud provider using dynamic reconstruction of metadata for the restoration of original database schema , when required . the suitability of the proposed technique with respect to private cloud environments is ensured by keeping the formulation of its constituent steps well aligned with the recommendations proposed by various standards development organizations working in this domain .
a systematic literature review on soa migration . <eos> when service orientation was introduced as the solution for retaining and rehabilitating legacy assets , both researchers and practitioners proposed techniques , methods , and guidelines for soa migration . with so much hype surrounding soa , it is not surprising that the concept was interpreted in many different ways , and consequently , different approaches to soa migration were proposed . accordingly , soon there was an abundance of methods that were hard to compare and eventually adopt . against this backdrop , this paper reports on a systematic literature review that was conducted to extract the categories of soa migration proposed by the research community . we provide the state of the art in soa migration approaches , and discuss categories of activities carried out and knowledge elements used or produced in those approaches . from such categorization , we derive a reference model , called soa migration frame of reference , that can be used for selecting and defining soa migration approaches . as a co product of the analysis , we shed light on how soa migration is perceived in the field , which further points to promising future research directions . copyright <digit> john wiley sons , ltd .
application of projection pursuit learning to boundary detection and deblurring in images . <eos> projection pursuit learning networks ( pplns ) have been used in many fields of research but have not been widely used in image processing . in this paper we demonstrate how this highly promising technique may be used to connect edges and produce continuous boundaries . we also propose the application of ppln to deblurring a degraded image when little or no a priori information about the blur is available . the ppln was successful at developing an inverse blur filter to enhance blurry images . theory and background information on projection pursuit regression ( ppr ) and ppln are also presented .
learning to transform time series with a few examples . <eos> we describe a semisupervised regression algorithm that learns to transform one time series into another time series given examples of the transformation . this algorithm is applied to tracking , where a time series of observations from sensors is transformed to a time series describing the pose of a target . instead of defining and implementing such transformations for each tracking task separately , our algorithm learns a memoryless transformation of time series from a few example input output mappings . the algorithm searches for a smooth function that fits the training examples and , when applied to the input time series , produces a time series that evolves according to assumed dynamics . the learning procedure is fast and lends itself to a closed form solution . it is closely related to nonlinear system identification and manifold learning techniques . we demonstrate our algorithm on the tasks of tracking rfid tags from signal strength measurements , recovering the pose of rigid objects , deformable bodies , and articulated bodies from video sequences . for these tasks , this algorithm requires significantly fewer examples compared to fully supervised regression algorithms or semisupervised learning algorithms that do not take the dynamics of the output time series into account .
almost periodic solutions to abstract semilinear evolution equations with stepanov almost periodic coefficients . <eos> in this paper , almost periodicity of the abstract semilinear evolution equation u ' ( t ) a ( t ) u ( t ) f ( t , u ( t ) ) with stepanov almost periodic coefficients is discussed . we establish a new composition theorem of stepanov almost periodic functions and , with its help , we study the existence and uniqueness of almost periodic solutions to the above semilinear evolution equation . our results are even new for the case of a ( t ) equivalent to a.
phoenix based clone detection using suffix trees . <eos> a code clone represents a sequence of statements that are duplicated in multiple locations of a program . clones often arise in source code as a result of multiple cut paste operations on the source , or due to the emergence of crosscutting concerns . programs containing code clones can manifest problems during the maintenance phase . when a fault is found or an update is needed on the original copy of a code section , all similar clones must also be found so that they can be fixed or updated accordingly . the ability to detect clones becomes a necessity when performing maintenance tasks . however , if done manually , clone detection can be a slow and tedious activity that is also error prone . a tool that can automatically detect clones offers a significant advantage during software evolution . with such an automated detection tool , clones can be found and updated in less time . moreover , restructuring or refactoring of these clones can yield better performance and modularity in the program.this paper describes an investigation into an automatic clone detection technique developed as a plug in for microsoft 's new phoenix framework . our investigation finds function level clones in a program using abstract syntax trees ( asts ) and suffix trees . an ast provides the structural representation of the code after the lexical analysis process . the ast nodes are used to generate a suffix tree , which allows analysis on the nodes to be performed rapidly . we use the same methods that have been successfully applied to find duplicate sections in biological sequences to search for matches on the suffix tree that is generated , which in turn reveal matches in the code .
slimeware engineering devices with slime mold . <eos> the plasmodium of the acellular slime mold physarum polycephalum is a gigantic single cell visible to the unaided eye . the cell shows a rich spectrum of behavioral patterns in response to environmental conditions . in a series of simple experiments we demonstrate how to make computing , sensing , and actuating devices from the slime mold . we show how to program living slime mold machines by configurations of repelling and attracting gradients and demonstrate the workability of the living machines on tasks of computational geometry , logic , and arithmetic .
automated aspect oriented decomposition of process control systems for ultra high dependability assurance . <eos> this paper presents a method for decomposing process control systems . this decomposition method is automated , meaning that a series of principles that can be evolved to support automated tools are given to help a designer decompose complex systems into a collection of simpler components . each component resulting from the decomposition process can be designed and implemented independently of the other components . also , these components can be tested or verified by the end user independently of each other . moreover , the system properties , such as safety , stability , and reliability , can be mathematically inferred from the properties of the individual components . these components are referred to as ideal ( independently developable end user assessable logical ) components . this decomposition method is applied to a case study specified by the high integrity systems group at sandia national labs , which involves the control of a future version of the bay area rapid transit ( bart ) system .
diffusion confusion based light weight security for item rfid tag reader communication . <eos> in this paper we propose a challenge response protocol called dcstar , which takes a novel approach to solve security issues that are specific to low cost item rfid tags . our dcstar protocol is built upon light weight primitives such as <digit> bit random number generator , exclusive or , and cyclic redundancy check and utilizing these primitives it also provides a simple diffusion confusion cipher to encrypt the challenge and response from the tag to the rfid reader . as a result our protocol achieves rfid tag reader server mutual authentication , communicating data confidentiality and integrity , secure key distribution and key protection . it also provides an efficient way for consumers to verify whether tagged items are genuine or fake and to protect consumers ' privacy while carrying tagged items .
on solutions of functional integral equations of urysohn type on an unbounded interval . <eos> in this paper we establish the existence of solutions of functional integral and quadratic urysohn integral on the interval r ( ) <digit> , infinity ) . the technique of proving applied in this paper is based on the concept of measure of noncompactness and the fixed point theorem . some new results are given . ( c ) <digit> elsevier ltd. all rights reserved .
a cultural probes study on video sharing and social communication on the internet . <eos> the focus of this article is the link between video sharing and interpersonal communication on the internet . previous works on social television systems belong to two categories <digit> ) studies on how collocated groups of viewers socialize while watching tv , and <digit> ) studies on novel social tv applications ( e.g. experimental set ups ) and devices ( e.g. ambient displays ) that provide technological support for tv sociability over a distance . the main shortcoming of those studies is that they have not considered the dominant contemporary method of social tv . early adopters of technology have been watching and sharing video online . we employed cultural probes in order to gain in depth information about the social aspect of video sharing on the internet . our sample consisted of six heavy users of internet video , watching an average of at least one hour of internet video a day . in particular , we explored how they are integrating video into their daily social communication practices . we found that internet video is shared and discussed with distant friends . moreover , the results of the study indicate several opportunities and threats for the development of integrated mass and interpersonal communication applications and services .
phenotypic modulation of vascular smooth muscle cells . <eos> the smooth muscle myosin heavy chain ( mhc ) gene and its isoforms are excellent molecular markers that reflect smooth muscle phenotypes . the smemb nonmuscle myosin heavy chain b ( nmhc b ) is a distinct mhc gene expressed predominantly in phenotypically modulated smcs ( synthetic type smc ) . to dissect the molecular mechanisms governing phenotypic modulation of smcs , we analyzed the transcriptional regulatory mechanisms underlying expression of the smemb gene . we previously reported two transcription factors , bteb2 iklf and hex , which transactivate the smemb gene promoter based on the transient reporter transfection assays . bteb2 iklf is a zinc finger transcription factor , whereas hex is a homeobox protein . bteb2 iklf expression in smcs is downregulated with vascular development in vivo but upregulated in cultured smcs and in neointima in response to vascular injury after balloon angioplasty . bteb2 iklf and hex activate not only the smemb gene but also other genes activated in synthetic smcs including plasminogen activator inhibitor <digit> ( pai <digit> ) , inos , pdgf a , egr <digit> , and vegf receptors . mitogenic stimulation activates bteb2 iklf gene expression through mek1 and egr <digit> . elevation of intracellular camp is also important in phenotypic modulation of smcs , because the smemb promoter is activated under cooperatively by camp response element binding protein ( creb ) and hex .
intent specifications an approach to building human centered specifications . <eos> this paper examines and proposes an approach to writing software specifications , based on research in systems theory , cognitive psychology , and human machine interaction . the goal is to provide specifications that support human problem solving and the tasks that humans must perform in software development and evolution . a type of specification , called intent specifications , is constructed upon this underlying foundation .
point equivalence of second order odes maximal invariant classification order . <eos> we show that the local equivalence problem of second order ordinary differential equations under point transformations is completely characterized by differential invariants of order at most <digit> and that this upper bound is sharp . we also demonstrate that , modulo cartan duality and point transformations , the painlev i equation can be characterized as the simplest second order ordinary differential equation belonging to the class of equations requiring 10th order jets for their classification .
an improved evaluation of ladder logic diagrams and petri nets for the sequence controller design in manufacturing systems . <eos> sequence controller designs play a key role in advanced manufacturing systems . traditionally , the ladder logic diagram ( lld ) has been widely applied to programmable logic controllers ( plc ) , while recently the petri net ( pn ) has emerged as an alternative tool for the sequence control of complex systems . the evaluation of both approaches has become crucial and has thus attracted attention .
lightweight detection of node presence in manets . <eos> while mobility in the sense of node movement has been an intensively studied aspect of mobile ad hoc networks ( manets ) , another aspect of mobility has not yet been subjected to systematic research nodes may not only move around but also enter and leave the network . in fact , many proposed protocols for manets exhibit worst case behavior when an intended communication partner is currently not present . therefore , knowing whether a given node is currently present in the network can often help to avoid unnecessary overhead . in this paper , we present a solution to the presence detection problem . it uses a bloom filter based beaconing mechanism to aggregate and distribute information about the presence of network nodes . we describe the algorithm and discuss design alternatives . we assess the algorithms properties both analytically and through simulation , and thereby underline the effectiveness and applicability of our approach .
an integrated toolchain for model based functional safety analysis . <eos> we design a complete toolchain for integrating fault tolerance analysis into modeling . the goal of this work is to bridge the gap between the different specialized tools available . having an integrated environment will reduce errors , ensure coherence and simplify analysis .
scale invariant feature matching using rotation invariant distance for remote sensing image registration . <eos> scale invariant feature transform ( sift ) has been widely used in image matching . but when sift is introduced in the registration of remote sensing images , the keypoint pairs which are expected to be matched are often assigned two different value of main orientation owing to the significant difference in the image intensity between remote sensing image pairs , and therefore a lot of incorrect matches of keypoints will appear . this paper presents a method using rotation invariant distance instead of euclid distance to match the scale invariant feature vectors associated with the keypoints . in the proposed method , the feature vectors are reorganized into feature matrices , and fast fourier transform ( fft ) is introduced to compute the rotation invariant distance between the matrices . much more correct matches are obtained by the proposed method since the rotation invariant distance is independent of the main orientation of the keypoints . experimental results indicate that the proposed method improves the match performance compared to other state of art methods in terms of correct match rate and aligning accuracy .
computer related gender differences . <eos> computer related gender differences are examined using survey responses from <digit> college students . issues studied include gender differences regarding interest and enjoyment of both using a computer and computer programming . interesting gender differences with implications for teaching are examined for the groups ( family , teachers , friends , others ) that have the most influence on students ' interest in computers . traditional areas such as confidence , career understanding and social bias are also discussed . preliminary results for a small sample of technology majors indicate that computer majors have unique interests and attitudes compared to other science majors .
analysis of eeg signals by combining eigenvector methods and multiclass support vector machines . <eos> a new approach based on the implementation of multiclass support vector machine ( svm ) with the error correcting output codes ( ecoc ) is presented for classification of electroencephalogram ( eeg ) signals . in practical applications of pattern recognition , there are often diverse features extracted from raw data which needs recognizing . decision making was performed in two stages feature extraction by eigenvector methods and classification using the classifiers trained on the extracted features . the aim of the study is classification of the eeg signals by the combination of eigenvector methods and multiclass svm . the purpose is to determine an optimum classification scheme for this problem and also to infer clues about the extracted features . the present research demonstrated that the eigenvector methods are the features which well represent the eeg signals and the multiclass svm trained on these features achieved high classification accuracies .
lambda rbac programming with role based access control . <eos> we study mechanisms that permit program components to express role constraints on clients , focusing on programmatic security mechanism , which permit access controls to be expressed , in situ , as part of the code realizing basic functionality . in this setting , two questions immediately arise . ( <digit> ) the user of a component faces the issue of safety is a particular role sufficient to use the component ( <digit> ) the component designer faces the dual issue of protection is a particular role demanded in all execution paths of the component we provide a formal calculus and static analysis to answer both questions .
output only modal analysis using continuous scan laser doppler vibrometry and application to a 20kw wind turbine . <eos> continuous scan laser doppler vibrometry ( csldv ) is a technique where the measurement point continuously sweeps over a structure while measuring , capturing both spatial and temporal information . the continuous scan approach can greatly accelerate measurements , allowing one to capture spatially detailed mode shapes in the same amount of time that conventional methods require to measure the response at a single point . the method is especially beneficial when testing large structures , such as wind turbines , that have low natural frequencies and hence may require very long time records at each measurement point . several csldv methods have been presented that use sinusoidal excitation or impulse excitation , but csldv has not previously been employed with an unmeasured , broadband random input . this work extends csldv to that class of input , developing an output only modal analysis method ( oma csldv ) . a recently developed algorithm for linear time periodic system identification , which makes use of harmonic power spectra and the harmonic transfer function concept developed by wereley <digit> , is used in conjunction with csldv measurements . one key consideration , the choice of the scan frequency , is explored . the proposed method is validated on a randomly excited free free beam , where one dimensional mode shapes are captured by scanning the laser along the length of the beam . the first seven natural frequencies and mode shapes are extracted from the harmonic power spectrum of the vibrometer signal and show good agreement with the analytically derived modes of the beam . the method is then applied to identify the mode shapes of a parked 20kw wind turbine using a ground based laser and with only a light breeze providing excitation .
preferences in wikipedia abstracts empirical findings and implications for automatic entity summarization . <eos> we empirically study how wikipedians summarize entity descriptions in practice . we compare entity descriptions in dbpedia with their wikipedia abstracts . we analyze the length of a summary and the priorities of property values . we analyze the priorities of , diversity of , and correlation between properties . implications for automatic entity summarization are drawn from the findings .
multi organ localization with cascaded global to local regression and shape prior . <eos> we propose a fast and robust method for multiple organs localization . our method provides organ dedicated confidence maps for each organ . it extends the cascade of random forest with additional shape prior . the values of the testing and learning parameters can be explained physically . we evaluate our method on <digit> ct volumes and show its good accuracy .
ordered interval routing schemes . <eos> an interval routing scheme ( irs ) represents the routing tables in a network in a space efficient way by labeling each vertex with an unique integer address , and the outgoing edges at each vertex with disjoint subintervals of these addresses . an irs that has at most k intervals per edge label is called a k irs . in this paper , we propose a new type of interval routing scheme , called an ordered interval routing scheme ( oirs ) , that uses an ordering of the outgoing edges at each vertex and allows non disjoint intervals in the labels of those edges . we show for a number of graph classes that using an oirs instead of an irs reduces the size of the routing tables in the case of optimal routing , i.e. , routing along shortest paths . we show that optimal routing in any k tree is possible using an oirs with at most 2k <digit> <digit> k <digit> intervals per edge label , although the best known result for an irs is 2k <digit> <digit> k <digit> intervals per edge label . any torus has an optimal <digit> oirs , although it may not have an optimal <digit> irs . we present similar results for the petersen graph , k garland graphs and a few other graphs .
performance analysis in non rayleigh and non rician communications channels . <eos> this paper investigates the probability of erasure for mobile communication channels containing limited number of scatterers . two kinds of channels with and without line of sight are examined . the resultant data is depicted by graphs to express the differences in existing theoretical models more clearly . the results indicate that the probability of erasure is different from that of predicted by both rayleigh and rician models for small number of scatterers .
computational geometry column <digit> . <eos> the recent result that n congruent balls in r ( d ) have at most <digit> distinct geometric permutations is described .
trends of environmental information systems in the context of the european water framework directive . <eos> in europe , the development of environmental information systems for the water domain is heavily influenced by the need to support the processes of the european water framework directive ( wfd ) . the aim of the wfd is to ensure that all european waters , these being groundwater , surface or coastal waters , are protected according to a common standard . while the wfd itself does only include concrete information technology ( it ) recommendations on a very high level of data exchange , regional and or national environmental agencies build or adapt their information systems according to their specific requirements in order to deliver the results for the first wfd reporting phase on time . moreover , as the wfd requires a water management policy centered on natural river basin districts instead of administrative and political regions , the agencies have to co ordinate their work , possibly across national borders . on this background , the present article analyses existing it recommendations for the wfd implementation strategy and motivates the need to develop an it framework architecture that comprises different views such as an organisational , a process , a data and a functional view . after having presented representative functions of operational water body information systems for the thematic and the co operation layer , the article concludes with a summary of future it developments that are required to efficiently support the wfd implementation .
a finite volume method for viscous incompressible flows using a consistent flux reconstruction scheme . <eos> an incompressible navier stokes solver using curvilinear body fitted collocated grid has been developed to solve unconfined flow past arbitrary two dimensional body geometries . in this solver , the full navier stokes equations have been solved numerically in the physical plane itself without using any transformation to the computational plane . for the proper coupling of pressure and velocity field on collocated grid , a new scheme , designated ' consistent flux reconstruction ' ( cfr ) scheme , has been developed . in this scheme , the cell face centre velocities are obtained explicitly by solving the momentum equations at the centre of the cell faces . the velocities at the cell centres are also updated explicitly by solving the momentum equations at the cell centres . by resorting to such a fully explicit treatment considerable simplification has been achieved compared to earlier approaches . in the present investigation the solver has been applied to unconfined flow past a square cylinder at zero and non zero incidence at low and moderate reynolds numbers and reasonably good agreement has been obtained with results available from literature . copyright ( c ) <digit> john wiley sons , ltd .
practical online retrieval evaluation . <eos> online evaluation is amongst the few evaluation techniques available to the information retrieval community that is guaranteed to reflect how users actually respond to improvements developed by the community . broadly speaking , online evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural context . however , it is rarely employed outside of large commercial search engines due primarily to a perception that it is impractical at small scales . the goal of this tutorial is to familiarize information retrieval researchers with state of the art techniques in evaluating information retrieval systems based on natural user clicking behavior , as well as to show how such methods can be practically deployed . in particular , our focus will be on demonstrating how the interleaving approach and other click based techniques contrast with traditional offline evaluation , and how these online methods can be effectively used in academic scale research . in addition to lecture notes , we will also provide sample software and code walk throughs to showcase the ease with which interleaving and other click based methods can be employed by students , academics and other researchers .
a privacy preserving clustering approach toward secure and effective data analysis for business collaboration . <eos> the sharing of data has been proven beneficial in data mining applications . however , privacy regulations and other privacy concerns may prevent data owners from sharing information for data analysis . to resolve this challenging problem , data owners must design a solution that meets privacy requirements and guarantees valid data clustering results . to achieve this dual goal , we introduce a new method for privacy preserving clustering called dimensionality reduction based transformation ( drbt ) . this method relies on the intuition behind random projection to protect the underlying attribute values subjected to cluster analysis . the major features of this method are ( a ) it is independent of distance based clustering algorithms ( b ) it has a sound mathematical foundation and ( c ) it does not require cpu intensive operations . we show analytically and empirically that transforming a data set using drbt , a data owner can achieve privacy preservation and get accurate clustering with a little overhead of communication cost .
unified read requests . <eos> most work on multimedia storage systems has assumed that clients will be serviced using a round robin strategy . the server services the clients in rounds and each client is allocated a time slice within that round . furthermore , most such algorithms are evaluated on the basis of a tightly specified cost function . this is the basis for well known algorithms such as fcfs , scan , scan edf , etc. in this paper , we describe a request merging ( rm ) module that takes as input , a set of client requests , and a set of constraints on the desired performance such as client waiting time or maximum disk bandwidth , and a cost function . it produces as output , a unified read request ( urr ) , telling the storage server which data items to read , and when the clients would like these data items to be delivered to them . given a cost function cf , a urr is optimal if there is no other urr satisfying the constraints with a lower cost . we present three algorithms in this paper , each of which accomplishes this kind of request merging . the first algorithm opturr is guaranteed to produce minimal cost urrs with respect to arbitrary cost functions . in general , the problem of computing an optimal urr is np complete , even when only two data objects are considered . to alleviate this problem , we develop two other algorithms , called greedyurr and fasturr that may produce sub optimal urrs , but which have some nicer computational properties . we will report on the pros and cons of these algorithms through an experimental evaluation .
brain computer evolutionary multiobjective optimization a genetic algorithm adapting to the decision maker . <eos> the centrality of the decision maker ( dm ) is widely recognized in the multiple criteria decision making community . this translates into emphasis on seamless human computer interaction , and adaptation of the solution technique to the knowledge which is progressively acquired from the dm . this paper adopts the methodology of reactive search optimization ( rso ) for evolutionary interactive multiobjective optimization . rso follows to the paradigm of learning while optimizing , through the use of online machine learning techniques as an integral part of a self tuning optimization scheme . user judgments of couples of solutions are used to build robust incremental models of the user utility function , with the objective to reduce the cognitive burden required from the dm to identify a satisficing solution . the technique of support vector ranking is used together with a k fold cross validation procedure to select the best kernel for the problem at hand , during the utility function training procedure . experimental results are presented for a series of benchmark problems .
linear separability of gene expression data sets . <eos> we study simple geometric properties of gene expression data sets , where samples are taken from two distinct classes ( e.g. , two types of cancer ) . specifically , the problem of linear separability for pairs of genes is investigated . if a pair of genes exhibits linear separation with respect to the two classes , then the joint expression level of the two genes is strongly correlated to the phenomena of the sample being taken from one class or the other . this may indicate an underlying molecular mechanism relating the two genes and the phenomena ( e. g. , a specific cancer ) . we developed and implemented novel efficient algorithmic tools for finding all pairs of genes that induce a linear separation of the two sample classes . these tools are based on computational geometric properties and were applied to <digit> publicly available cancer data sets . for each data set , we computed the number of actual separating pairs and compared it to an upper bound on the number expected by chance and to the numbers resulting from shuffling the labels of the data at random empirically . seven out of these <digit> data sets are highly separable . statistically , this phenomenon is highly significant , very unlikely to occur at random . it is therefore reasonable to expect that it manifests a functional association between separating genes and the underlying phenotypic classes .
a language for representing and extracting 3d geometry semantics from paper based sketches . <eos> the key contribution is a visual language to formally represent form geometry semantics on paper . parsing the language allows for the automatic generation of 3d virtual models . a proof of concept prototype tool was implemented . the language is capable to roughly model forms with linear topological ordering . evaluation results show that practising designers would use the language .
decentralized list scheduling . <eos> classical list scheduling is a very popular and efficient technique for scheduling jobs for parallel and distributed platforms . it is inherently centralized . however , with the increasing number of processors , the cost for managing a single centralized list becomes too prohibitive . a suitable approach to reduce the contention is to distribute the list among the computational units each processor only has a local view of the work to execute . thus , the scheduler is no longer greedy and standard performance guarantees are lost .
antenna impedance matching with neural networks . <eos> impedance matching between transmission lines and antennas is an important and fundamental concept in electromagnetic theory . one definition of antenna impedance is the resistance and reactance seen at the antenna terminals or the ratio of electric to magnetic fields at the input . the primary intent of this paper is real time compensation for changes in the driving point impedance of an antenna due to frequency deviations . in general , the driving point impedance of an antenna or antenna array is computed by numerical methods such as the method of moments or similar techniques . some configurations do lend themselves to analytical solutions , which will be the primary focus of this work . this paper employs a neural control system to match antenna feed lines to two common antennas during frequency sweeps . in practice , impedance matching is performed off line with smith charts or relatively complex formulas but they rarely perform optimally over a large bandwidth . there have been very few attempts to compensate for matching errors while the transmission system is in operation and most techniques have been targeted to a relatively small range of frequencies . the approach proposed here employs three small neural networks to perform real time impedance matching over a broad range of frequencies during transmitter operation . double stub tuners are being explored in this paper but the approach can certainly be applied to other methodologies . the ultimate purpose of this work is the development of an inexpensive microcontroller based system .
cellular automata over group alphabets undergraduate education and the pascgalois project . <eos> this purpose of this note is to report efforts underway in the pascgalois project ( www.pascgalois.org ) to provide connections between standard courses in the undergraduate mathematics curriculum ( e.g. abstract algebra , number theory , discrete mathematics ) and cellular automata . the value of these connections to the mathematical education of undergraduates will be described . project course supplements , supporting software , and areas of student research will also be summarized .
from structured documents to novel query facilities . <eos> structured documents ( e.g. , sgml ) can benefit a lot from database support and more specifically from object oriented database ( oodb ) management systems . this paper describes a natural mapping from sgml documents into oodb 's and a formal extension of two oodb query languages ( one sql like and the other calculus ) in order to deal with sgml document retrieval . although motivated by structured documents , the extensions of query languages that we present are general and useful for a variety of other oodb applications . a key element is the introduction of paths as first class citizens . the new features allow to query data ( and to some extent schema ) without exact knowledge of the schema in a simple and homogeneous fashion .
determination of oxidized low density lipoproteins ( ox ldl ) versus ox ldl 2gpi complexes for the assessment of autoimmune mediated atherosclerosis . <eos> the immunolocalization of oxidized low density lipoproteins ( ox ldl ) , <digit> glycoprotein i ( 2gpi ) , cd4 cd8 immunoreactive lymphocytes , and immunoglobulins in atherosclerotic lesions strongly suggested an active participation of the immune system in atherogenesis . oxidative stress leading to ox ldl production is thought to play a central role in both the initiation and progression of atherosclerosis . ox ldl is highly proinflammatory and chemotactic for macrophage monocyte and immune cells . enzyme linked immunosorbent assays ( elisas ) to measure circulating ox ldl have been developed and are being currently used to assess oxidative stress as risk factor or marker of atherosclerotic disease . ox ldl interacts with 2gpi and circulating ox ldl 2gpi complexes have been demonstrated in patients with systemic lupus erythematosus ( sle ) and antiphospholipid syndrome ( aps ) . it has been postulated that 2gpi binds ox ldl to neutralize its proinflammatory and proatherosclerotic effects . because 2gpi is ubiquitous in plasma , its interaction with ox ldl may mask oxidized epitopes recognized by capture antibodies potentially interfering with immunoassays results . the measurement of ox ldl 2gpi complexes may circumvent this interference representing a more physiological and accurate way of measuring ox ldl
text document clustering based on frequent word meaning sequences . <eos> most of existing text clustering algorithms use the vector space model , which treats documents as bags of words . thus , word sequences in the documents are ignored , while the meaning of natural languages strongly depends on them . in this paper , we propose two new text clustering algorithms , named clustering based on frequent word sequences ( cfws ) and clustering based on frequent word meaning sequences ( cfwms ) . a word is the word form showing in the document , and a word meaning is the concept expressed by synonymous word forms . a word ( meaning ) sequence is frequent if it occurs in more than certain percentage of the documents in the text database . the frequent word ( meaning ) sequences can provide compact and valuable information about those text documents . for experiments , we used the reuters <digit> text collection , cisi documents of the classic data set classic data set , ftp ftp.cs.cornell.edu pub smart , and a corpus of the text retrieval conference ( trec ) high accuracy retrieval from documents ( hard ) track of text retrieval conference , <digit> . our experimental results show that cfws and cfwms have much better clustering accuracy than bisecting k means ( bkm ) m. steinbach , g. karypis , v. kumar , a comparison of document clustering techniques , kdd <digit> workshop on text mining , <digit> , a modified bisecting k means using background knowledge ( bbk ) a. hotho , s. staab , g. stumme , ontologies improve text document clustering , in proceedings of the 3rd ieee international conference on data mining , <digit> , pp. <digit> and frequent itemset based hierarchical clustering ( fihc ) b.c.m. fung , k. wang , m. ester , hierarchical document clustering using frequent itemsets , in proceedings of siam international conference on data mining , <digit> algorithms .
an online approach based on locally weighted learning for short term traffic flow prediction . <eos> traffic flow prediction is a basic function of intelligent transportation system . due to the complexity of traffic phenomenon , most existing methods build complex models such as neural networks for traffic flow prediction . as a model may lose effect with time lapse , it is important to update the model on line . however , the high computational cost of maintaining a complex model puts great challenge for model updating . the high computation cost lies in two aspects computation of complex model coefficients and huge amount training data for it . in this paper , we propose to use a nonparametric approach based on locally weighted learning to predict traffic flow . our approach incrementally incorporates new data to the model and is computationally efficient , which makes it suitable for online model updating and predicting . in addition , we adopt wavelet analysis to extract the periodic characteristic of the traffic data , which is then used for the input of the prediction model instead of the raw traffic flow data . the primary experiments on real data demonstrate the effectiveness and efficiency of our approach .
usable computing on open distributed systems . <eos> an open distributed system provides a best effort guarantee on the quality of service provided to applications . this has worked well for throughput based applications of the kind typically executed in condor or boincstyle environments . for other applications , the absence of timeliness of correctness guarantees limit the utility or appeal of this environment . computational results that are too late or erroneous are not usable to the application . we present techniques designed to efficiently promote usable computing in open distributed systems .
a note on the not <digit> choosability of some families of planar graphs . <eos> a graph g is l list colorable if for a given list assignment l l ( v ) v epsilon v , there exists a proper coloring c of g such that c ( v ) epsilon l ( v ) for all v epsilon v. if g is l list colorable for any list assignment with vertical bar l ( v ) vertical bar > k for all v epsilon v , then g is said k choosable . in m. voigt , a not <digit> choosable planar graph without <digit> cycles , discrete math . <digit> ( <digit> ) <digit> <digit> and m. voigt , a non <digit> choosable planar graph without cycles of length <digit> and <digit> , <digit> , manuscript , voigt gave a planar graph without <digit> cycles and a planar graph without <digit> cycles and <digit> cycles which are not <digit> choosable . in this note , we give smaller and easier graphs than those proposed by voigt and suggest an extension of erdos ' relaxation of steinberg 's conjecture to <digit> choosability . ( c ) <digit> elsevier b.v. all rights reserved .
impedance spectroscopy studies of moisture uptake in low k dielectrics and its relation to reliability . <eos> water incursion into low k beol capacitors was monitored via impedance spectroscopy . it is a non destructive , zero dc field , low ac field probe ( < 0.5 v ) . samples are tested at device operation conditions and are re testable . thermal activation energies related to water bonding with dielectric are measured . the increase in ac loss is correlated with poorer reliability , i.e. early failure .
a multi level depiction method for painterly rendering based on visual perception cue . <eos> increasing the level of detail ( lod ) in brushstrokes within areas of interest improved the realism of painterly rendering . using a modified quad tree , we segmented an image into areas with similar levels of saliency each of these segments was then used to control the brush strokes during rendering . we could also simulate real oil painting steps based on saliency information . our method runs in a reasonable fine and produces results that are visually appealing and competitive with previous techniques .
preventive replacement for systems with condition monitoring and additional manual inspections . <eos> researched a problem of both condition monitoring and inspection . defined two types of preventive replacements . utilized the delay time concept to model the failure process . formulated a decision problem of two decision variables simultaneously .
redundant and force differentiated systems in engineering and nature . <eos> sophisticated load carrying structures , in nature as well as man made , share some common properties . a clear differentiation of tension , compression and shear is in nature primarily manifested in the properties of materials adapted to the efforts , whereas they in engineering are distributed on different components . for stability and failure safety , redundancy on different levels is also commonly used . the paper aims at collecting and expanding previous methods for the computational treatment of redundant and force differentiated systems . a common notation is sought , giving and developing criteria for describing the diverse problems from a common structural mechanical viewpoint . from this , new criteria for the existence of solutions , and a method for treatment of targeted dynamic solutions are developed . added aspects to previously described examples aim at emphasizing similarities and differences between engineering and nature , in the forms of a tension truss structure and the human musculoskeletal system .
simultaneous optimization of the material properties and the topology of functionally graded structures . <eos> a level set based method is proposed for the simultaneous optimization of the material properties and the topology of functionally graded structures . the objective of the present study is to determine the optimal material properties ( via the material volume fractions ) and the structural topology to maximize the performance of the structure in a given application . in the proposed method , the volume fraction and the structural boundary are considered as the design variables , with the former being discretized as a scalar field and the latter being implicitly represented by the level set method . to perform simultaneous optimization , the two design variables are integrated into a common objective functional . sensitivity analysis is conducted to obtain the descent directions . the optimization process is then expressed as the solution to a coupled hamiltonjacobi equation and diffusion partial differential equation . numerical results are provided for the problem of mean compliance optimization in two dimensions .
the impact of head movements on user involvement in mediated interaction . <eos> we examine engagement within conversational behaviours of the subject when interacting with a socially expressive system . we found real time communication requires more than verbal communication , and head nodding . head nodding effects depend on precise on screen movement by synchronize the on screen movement with the head movement .
parsing images into regions , curves , and curve groups . <eos> in this paper , we present an algorithm for parsing natural images into middle level vision representations regions , curves , and curve groups ( parallel curves and trees ) . this algorithm is targeted for an integrated solution to image segmentation and curve grouping through bayesian inference . the paper makes the following contributions . ( <digit> ) it adopts a layered ( or <digit> . <digit> d sketch ) representation integrating both region and curve models which compete to explain an input image . the curve layer occludes the region layer and curves observe a partial order occlusion relation . ( <digit> ) a markov chain search scheme metropolized gibbs samplers ( mgs ) is studied . it consists of several pairs of reversible jumps to traverse the complex solution space . an mgs proposes the next state within the jump scope of the current state according to a conditional probability like a gibbs sampler and then accepts the proposal with a metropolis hastings step . this paper discusses systematic design strategies of devising reversible jumps for a complex inference task . ( <digit> ) the proposal probability ratios in jumps are factorized into ratios of discriminative probabilities . the latter are computed in a bottom up process , and they drive the markov chain dynamics in a data driven markov chain monte carlo framework . we demonstrate the performance of the algorithm in experiments with a number of natural images .
laparoscopic management of adnexal masses . <eos> suspected ovarian neoplasm is a common clinical problem affecting women of all ages . although the majority of adnexal masses are benign , the primary goal of diagnostic evaluation is the exclusion of malignancy . it has been estimated that approximately <digit> % of women in the united states will undergo a surgical procedure for a suspected ovarian neoplasm during their lifetime . despite the magnitude of the problem , there is still considerable disagreement regarding the optimal surgical management of these lesions . traditional management has relied on laparotomy to avoid undertreatment of a potentially malignant process . advances in detection , diagnosis , and minimally invasive surgical techniques make it necessary now to review this practice in an effort to avoid unnecessary morbidity among patients . here , we review the literature on the laparosopic approach to the treatment of the adnexal mass without sacrificing the principles of oncologic surgery . we highlight potentials of minimally invasive surgery and address the risks associated with the laparoscopic approach .
dealing with plagiarism in the information systems research community a look at factors that drive plagiarism and ways to address them . <eos> imagine yourself spending years conducting a research project and having it published as an article in a refereed journal , only to see a plagiarized copy of the article later published in another journal . then imagine yourself being left to fight for your rights alone , and eventually finding out that it would be very difficult to hold the plagiarist accountable for what he or she did . the recent decision by the association of information systems to create a standing committee on member misconduct suggests that while this type of situation may sound outrageous , it is likely to become uncomfortably frequent in the information systems research community if proper measures are not taken by a community backed organization . in this article , we discuss factors that can drive plagiarism , as well as potential measures to prevent it . our goal is to discuss alternative ways in which plagiarism can be prevented and dealt with when it arises . we hope to start a debate that provides the basis on which broader mechanisms to deal with plagiarism can be established , which we envision as being associated with and complementary to the committee created by the association for information systems .
percolation in the secrecy graph . <eos> the secrecy graph is a random geometric graph which is intended to model the connectivity of wireless networks under secrecy constraints . directed edges in the graph are present whenever a node can talk to another node securely in the presence of eavesdroppers , which , in the model , is determined solely by the locations of the nodes and eavesdroppers . in the case of infinite networks , a critical parameter is the maximum density of eavesdroppers that can be accommodated while still guaranteeing an infinite component in the network , i.e. , the percolation threshold . we focus on the case where the locations of the nodes and eavesdroppers are given by poisson point processes , and present bounds for different types of percolation , including in , out and undirected percolation .
evaluation of region of interest coders using perceptual image quality assessments . <eos> perceptual image assessment is proposed for coder performance evaluation . proposed assessment uses a linear combination of perceptual measures just based on features . region of interest coder perceptual evaluation aims at identifying coder behavior . some perceptual assessments are adequate to evaluate test coders .
achieving anycast in dtns by enhancing existing unicast protocols . <eos> many dtn environments , such as emergency response networks and pocket switched networks , are based on human mobility and communication patterns , which naturally lead to groups . in these scenarios , group based communication is central , and hence a natural and useful routing paradigm is anycast , where a node attempts to communicate with at least one member of a particular group . unfortunately , most existing anycast solutions assume connectivity , and the few specifically for dtns are single copy in nature and have only been evaluated in highly limited mobility models . in this paper , we propose a protocol independent method of enhancing a large number of existing dtn unicast protocols , giving them the ability to perform anycast communication . this method requires no change to the unicast protocols themselves and instead changes their world view by adding a thin layer beneath the routing layer . through a thorough set of simulations , we also evaluate how different parameters and network conditions affect the performance of these newly transformed anycast protocols .
a framework for supporting data integration using the materialized and virtual approaches . <eos> this paper presents a framework for data integration currently under development in the squirrel project . the framework is based on a special class of mediators , called squirrel integration mediators . these mediators can support the traditional virtual and materialized approaches , and also hybrids of them.in the squirrel mediators , a relation in the integrated view can be supported as ( a ) fully materialized , ( b ) fully virtual , or ( c ) partially materialized ( i.e. , with some attributes materialized and other attributes virtual ) . in general , ( partially ) materialized relations of the integrated view are maintained by incremental updates from the source databases . squirrel mediators provide two approaches for doing this ( <digit> ) materialize all needed auxiliary data , so that data sources do not have to be queried when processing the incremental updates or ( <digit> ) leave some or all of the auxiliary data virtual , and query selected source databases when processing incremental updates.the paper presents formal notions of consistency and freshness for integrated views defined over multiple autonomous source databases . it is shown that squirrel mediators satisfy these properties .
validation and verification of intelligent systems what are they and how are they different . <eos> researchers and practitioners in the field of expert systems all generally agree that to be useful , any fielded intelligent system must be adequately verified and validated . but what does this mean in concrete terms what exactly is verification what exactly is validation how are they different many authors have attempted to define these terms and , as a result , several interpretations have surfaced . it is our opinion that there is great confusion as to what these terms mean . how they are different , and how they are implemented . this paper . therefore , has two aims to clarify the meaning of the terms validation and verification as they apply to intelligent systems , and to describe how several researchers are implementing these . the second part of the paper , therefore , details some techniques that can be used to perform the verification and validation of systems . also discussed is the role of testing as part of the above mentioned processes .
multiple blocking sets and multisets in desarguesian planes . <eos> in ag ( <digit> , q ( <digit> ) ) , the minimum size of a minimal ( q <digit> ) fold blocking set is known to be q ( <digit> ) <digit> . here , we construct minimal ( q <digit> ) fold blocking sets of size q ( <digit> ) in ag ( <digit> , q ( <digit> ) ) . as a byproduct , we also obtain new two character multisets in pg ( <digit> , q ( <digit> ) ) . the essential idea in this paper is to investigate q ( <digit> ) sets satisfying the opposite of ebert 's discriminant condition .
a simple weighting scheme for classification in two group discriminant problems . <eos> this paper introduces a new weighted linear programming model , which is simple and has strong intuitive appeal for two group classifications . generally , in applying weights to solve a classification problem in discriminant analysis where the relative importance of every observation is known , larger weights ( penalties ) will be assigned to those more important observations . the perceived importance of an observation is measured here as the willingness of the decision maker to misclassify this observation . for instance , a decision maker is least willing to see a classification rule that misclassifies a top financially strong firm to the group that contains bankrupt firms . our weighted linear programming model provides an objective weighting scheme whereby observations can be weighted according to their perceived importance . the more important this observation , the heavier its assigned weight . results of a simulation experiment that uses contaminated data show that the weighted linear programming model consistently and significantly outperforms existing linear programming and standard statistical approaches in attaining higher average hit ratios in the <digit> replications for each of the <digit> cases tested . scope and purpose generally , in applying weights to solve a discriminant problem where the relative importance of every observation is known , larger weights ( penalties ) will be assigned to those more important observations . however , if decision makers do not have prior or additional information about the observations , it is very difficult to assign weights to the observations . subjective judgements from decision makers may be a way of obtaining those weights . an alternative way is to suggest an objective weighting scheme for obtaining classification weights of observations from the data matrix of the training sample . we suggest a new approach , which provides an objective weighting scheme whereby individual observations can be weighted according to their perceived importance . the more important the observation , the heavier its assigned weight will be . the importance of individual observation is first determined in one of two stages of our model using more than one discriminant function . simulation experiments are run to test this new approach .
hybrid intelligent packing system ( hips ) through integration of artificial neural networks , artificial intelligence , and mathematical programming . <eos> a successful solution to the packing problem is a major step toward material savings on the scrap that could be avoided in the cutting process and therefore money savings . although the problem is of great interest , no satisfactory algorithm has been found that can be applied to all the possible situations . this paper models a hybrid intelligent packing system ( hips ) by integrating artificial neural networks ( anns ) , artificial intelligence ( ai ) , and operations research ( or ) approaches for solving the packing problem . the hips consists of two main modules , an intelligent generator module and a tester module . the intelligent generator module has two components ( i ) a rough assignment module and ( ii ) a packing module . the rough assignment module utilizes the expert system and rules concerning cutting restrictions and allocation goals in order to generate many possible patterns . the packing module is an ann that packs the generated patterns and performs post solution adjustments . the tester module , which consists of a mathematical programming model , selects the sets of patterns that will result in a minimum amount of scrap .
distributed scheduling and resource allocation for cognitive ofdma radios . <eos> scheduling spectrum access and allocating power and rate resources are tasks affecting critically the performance of wireless cognitive radio ( cr ) networks . the present contribution develops a primal dual optimization framework to schedule any to any cr communications based on orthogonal frequency division multiple access and allocate power so as to maximize the weighted average sum rate of all users . fairness is ensured among cr communicators and possible hierarchies are respected by guaranteeing minimum rate requirements for primary users while allowing secondary users to access the spectrum opportunistically . the framework leads to an iterative channel adaptive distributed algorithm whereby nodes rely only on local information exchanges with their neighbors to attain global optimality . simulations confirm that the distributed online algorithm does not require knowledge of the underlying fading channel distribution and converges to the optimum almost surely from any initialization .
physically based hydraulic erosion simulation on graphics processing unit . <eos> visual simulation of natural erosion on terrains has always been a fascinating research topic in the field of computer graphics . while there are many algorithms already developed to improve the visual quality of terrain , the recent simulation methods revolve around physically based hydraulic erosion because it can generate realistic natural looking terrains . however , many of such algorithms were tested only on low resolution terrains . when simulated on a higher resolution terrain , most of the current algorithms become computationally expensive . this is why in many applications today , terrains are generated off line and loaded during the application runtime . this method restricts the number of terrains which can be stored if there is a limitation on storage capacity . recently , graphics hardware has evolved into an indispensable tool in improving the speed of computation . this has motivated us to develop an erosion algorithm to map to graphics hardware for faster terrain generation . in this paper , we propose a fast and efficient hydraulic erosion procedural technique that utilizes the gpus powerful computation capability in order to generate high resolution erosion on terrains . our method is based on the newtonian physics approach that is implemented on a two dimensional data structure which stores height fields , water amount , and dissolved sediment and water velocities . we also present a comprehensive comparison between the cpu and gpu implementations together with the visual results and the statistics on simulation time taken .
novel immune based framework for securing ad hoc networks . <eos> one of the main security issues in mobile ad hoc networks ( manets ) is a malicious node that can falsify a route advertisement , overwhelm traffic without forwarding it , help to forward corrupted data and inject false or uncompleted information , and many other security problems . mapping immune system mechanisms to networking security is the main objective of this paper which may significantly contribute in securing manets . in a step for providing secured and reliable broadband services , formal specification logic along with a novel immuneinspired security framework ( i <digit> manets ) are introduced . the different immune components are synchronized with the framework through an agent that has the ability to replicate , monitor , detect , classify , and block isolate the corrupted packets and or nodes in a federated domain . the framework functions as the human immune system in first response , second response , adaptability , distributability , and survivability and other immune features and properties . interoperability with different routing protocols is considered . the framework has been implemented in a real environment . desired and achieved results are presented .
slabpose columnsort a new oblivious algorithm for out of core sorting on distributed memory clusters . <eos> our goal is to develop a robust out of core sorting program for a distributed memory cluster . the literature contains two dominant paradigms for out of core sorting algorithms merging based and partitioning based . we explore a third paradigm , that of oblivious algorithms . unlike the two dominant paradigms , oblivious algorithms do not depend on the input keys and therefore lead to predetermined i o and communication patterns in an out of core setting . predetermined i o and communication patterns facilitate overlapping i o , communication , and computation for efficient implementation . we have developed several out of core sorting programs using the paradigm of oblivious algorithms . our baseline implementation , <digit> pass columnsort , was based on leighton 's columnsort algorithm . though efficient in terms of i o and communication , <digit> pass columnsort has a restriction on the maximum problem size . as our first effort toward relaxing this restriction , we developed two implementations subblock columnsort and m columnsort . both of these implementations incur substantial performance costs subblock columnsort performs additional disk i o , and m columnsort needs substantial amounts of extra communication and computation . in this paper we present slabpose columnsort , a new oblivious algorithm that we have designed explicitly for the out of core setting . slabpose columnsort relaxes the problem size restriction at no extra i o or communication cost . experimental evidence on a beowulf cluster shows that unlike subblock columnsort and m columnsort , slabpose columnsort runs almost as fast as <digit> pass columnsort . to the best of our knowledge , our implementations are the first out of core multiprocessor sorting algorithms that make no assumptions about the keys and produce output that is perfectly load balanced and in the striped order assumed by the parallel disk model .
a real time kinematics on the translational crawl motion of a quadruped robot . <eos> it is known that the kinematics of a quadruped robot is complex due to its topology and the redundant actuation in the robot . however , it is fundamental to compute the inverse and direct kinematics for the sophisticated control of the robot in real time . in this paper , the translational crawl gait of a quadruped robot is introduced and the approach to find the solution of the kinematics for such a crawl motion is proposed . since the resulting kinematics is simplified , the formulation can be used for the real time control of the robot . the results of simulation and experiment shows that the present method is feasible and efficient .
geographical classification of olive oils by the application of cart and svm to their ft ir . <eos> this paper reports the application of fourier transform infrared ( ft ir ) spectroscopy to the geographical classification of extra virgin olive oils . two chemometrical techniques , classification and regression trees ( cart ) and support vector machines ( svm ) based on the gaussian kernel and the recently introduced euclidean distance based pearson vii universal kernel ( puk ) , were applied to discriminate between italian and non italian and between ligurian and non ligurian olive oils . the puk is applied in literature with success on regression problems . in this paper the mapping power of this universal kernel for classification was investigated . in this study it was observed that svm performed better than cart . svm based on the puk provide models with a high selectivity and sensitivity ( thus a better accuracy ) as compared to those obtained using the gaussian kernel . the wave numbers selected in the classification trees were interpreted demonstrating that the trees were chemically justified . this study also shows that ft ir spectroscopy associated with svm and cart can be used to correctly discriminate between various origins of olive oils , demonstrating that the combination of techniques might be a powerful tool for supporting the claimed origin of olive oils . copyright ( c ) <digit> john wiley sons , ltd .
arfnns under different types svr for identification of nonlinear magneto rheological damper systems with outliers . <eos> this paper demonstrates different types support vector regression ( svr ) for annealing robust fuzzy neural networks ( arfnns ) to identification of nonlinear magneto rheological ( mr ) damper with outliers . a svr has the good performances to determine the number of rule in the simplified fuzzy inference system and initial weights for the fuzzy neural networks . in this paper , we independently proposed two different types svr for the arfnns . hence , a combination model that fuses simplified fuzzy inference system , svr and radial basis function networks is used . based on these initial structures , and then annealing robust learning algorithm ( arla ) can be used effectively to adjust the parameters of structures . simulation results show the superiority of the proposed method with the different types svr for the nonlinear mr damper systems with outliers .
fuzzy linear regression model based on fuzzy scalar product . <eos> the new concept and method of imposing imprecise ( fuzzy ) input and output data upon the conventional linear regression model is proposed in this paper . we introduce the fuzzy scalar ( inner ) product to formulate the fuzzy linear regression model . in order to invoke the conventional approach of linear regression analysis for real valued data , we transact the alpha level linear regression models of the fuzzy linear regression model . we construct the membership functions of fuzzy least squares estimators via the form of resolution identity which is a well known formula in fuzzy sets theory . in order to obtain the membership value of any given least squares estimate taken from the fuzzy least squares estimator , we transform the original problem into the optimization problems . we also provide two computational procedures to solve the optimization problems .
relating torque and slip in an odometric model for an autonomous agricultural vehicle . <eos> this paper describes a method of considering the slip that is experienced by the wheels of an agricultural autonomous guided vehicle such that the accuracy of dead reckoning navigation may be improved . traction models for off road locomotion are reviewed . using experimental data from an agricultural agv , a simplified form suitable for vehicle navigation is derived . this simplified model relates measurements of the torques applied to the wheels with wheel slip , and is used as the basis of an observation model for odometric sensor data in the vehicle 's extended kalman filter ( ekf ) navigation system . the slip model parameters are included as states in the vehicle ekf so that the vehicle may adapt to changing surface properties . results using real field data and a simulation of the vehicle ekf show that positional accuracy can be increased by a slip aware odometric model , and that when used as part of a multi sensor navigation system , the consistency of the ekf state estimator is improved .
evaluation of folksonomy induction algorithms . <eos> algorithms for constructing hierarchical structures from user generated metadata have caught the interest of the academic community in recent years . in social tagging systems , the output of these algorithms is usually referred to as folksonomies ( from folk generated taxonomies ) . evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards , lack of comprehensive methods and tools as well as a lack of research and empirical simulation studies applying these methods . in this article , we report results from a broad comparative study of state of the art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems . in addition to adopting semantic evaluation techniques , we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation . our work sheds new light on the properties and characteristics of state of the art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation , while at the same time identifying some important limitations and challenges of folksonomy evaluation . our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques . to the best of our knowledge , this work represents the largest and most comprehensive evaluation study of state of the art folksonomy induction algorithms to date .
teleportation of n qudit state . <eos> in this paper , we study the teleportation of arbitrary n qudit state with the tensor representation . the necessary and sufficient condition for realizing a successful or perfect teleportation is obtained , as will be shown , which is determined by the measurement matrix t delta and the quantum channel parameter matrix x. the general expressions of the measurement matrix t delta are written out and the quantum channel parameter matrix x are discussed . as an example , we show the details of three ququart state teleportation .
bio interactive healthcare service system using lifelog based context computing . <eos> intelligent bio sensor information processing was developed using lifelog based context aware technology to provide a flexible and dynamic range of diagnostic capabilities to satisfy healthcare requirements in ubiquitous and mobile computing environments . to accomplish this , various noise signals were grouped into six categories by context estimation and effectively reconfigured noise reduction filters by neural network and genetic algorithm . the neural network based control module effectively selected an optimal filter block by noise context based clustering in running mode , and filtering performance was improved by genetic algorithm in evolution mode . due to its adaptive criteria , genetic algorithm was used to explore the action configuration for each identified bio context to implement our concept . our proposed bio interactive healthcare service system adopts the concepts of biological context awareness with evolutionary computations in working environments modeled and identified as bio sensors based environmental contexts . we used an unsupervised learning algorithm for lifelog based context modeling and a supervised learning algorithm for context identification .
smb collision detection based on temporal coherence . <eos> the paper presents a novel collision detection algorithm , termed the sort moving boxes ( smb ) for large number of moving 2d 3d objects which are represented by their axis aligned bounding boxes ( aabbs ) . the main feature of the algorithm is the full exploitation of the temporal coherence of the objects exhibited in a dynamic environment . in the algorithm , the aabbs are first projected to each cartesian axis . the projected intervals on the axes are separately sorted by the diminishing increment sort ( dis ) and further divided into subsections . by processing all the intervals within the subsections to check if they overlap , a complete contact list can be built . the smb is a fast and robust collision detection algorithm , particularly for systems involving a large number of moving aabbs , and also supports for the dynamic insertion and deletion of objects . its performance in terms of both expected total detection time and memory requirements is proportional to the total number of aabbs , n , and is not influenced by size differences of aabbs , the space size and packing density over a large range up to ten times difference . the only assumption made is that the sorted list at one time step will remain an almost sorted list at the next time step , which is valid for most applications whose movement and deformation of each aabb and the dynamic change of the total number n are approximately continuous .
an international analysis of the extensions to the ieee lomv1 .0 metadata standard . <eos> we analyzed <digit> works using the ieee lomv1 .0 standard and found <digit> types of extensions made to it . due to mexico interoperability difficulties , we compared its extensions with the rest of the world . we found that local extensions do not help to increase the system 's interoperability ability . we found the action most important after implementing extensions is to publish them .
a unifying approach to goal directed evaluation . <eos> goal directed evaluation , as embodied in icon and snobol , is built on the notions of backtracking and of generating successive results , and therefore it has always been something of a challenge to specify and implement . in this article , we address this challenge using computational monads and partial evaluation . we consider a subset of icon and we specify it with a monadic semantics and a list monad . we then consider a spectrum of monads that also fit the bill , and we relate them to each other . for example , we derive a continuation monad as a church encoding of the list monad . the resulting semantics coincides with gudeman 's continuation semantics of icon . we then compile icon programs by specializing their interpreter ( i.e. , by using the first futamura projection ) , using type directed partial evaluation . through various back ends , including a run time code generator , we generate ml code , c code , and ocaml byte code . binding time analysis and partial evaluation of the continuation based interpreter automatically give rise to c programs that coincide with the result of proebsting 's optimized compiler .
an architectural history of metaphors . <eos> this paper presents a review and an historical perspective on the architectural metaphor . it identifies common characteristics and peculiaritiesas they apply to given historical periodsand analyses the similarities and divergences . the review provides a vocabulary , which will facilitate an appreciation of existing and new metaphors .
time based query performance predictors . <eos> query performance prediction is aimed at predicting the retrieval effectiveness that a query will achieve with respect to a particular ranking model . in this paper , we study query performance prediction for a ranking model that explicitly incorporates the time dimension into ranking . different time based predictors are proposed as analogous to existing keyword based predictors . in order to improve predicting performance , we combine different predictors using linear regression and neural networks . extensive experiments are conducted using queries and relevance judgments obtained by crowdsourcing .
asymptotically sufficient partitions and quantizations . <eos> we consider quantizations of observations represented by finite partitions of observation spaces . partitions usually decrease the sensitivity of observations to their probability distributions . a sequence of quantizations is considered to be asymptotically sufficient for a statistical problem if the loss of sensitivity is asymptotically negligible . the sensitivity is measured by f divergences of distributions or the closely related f informations including the classical shannon information . it is demonstrated that in some cases the maximization of f divergences means the same as minimization of distortion of observations in the classical sense considered in mathematical statistics and information theory . the main result of the correspondence is a general sufficient condition ford the asymptotic sufficiency of quantizations . selected applications of this condition are studied leading to new simple criteria of asymptotic optimality for quantizations of vector valued observations and observations on general poisson processes .
the topology aware file distribution problem . <eos> we present theoretical results for large file distribution on general networks of known topology ( known link bandwidths and router locations ) . we show that the problem of distributing a file in minimum time is np hard in this model , and we give an o ( log n ) approximation algorithm , where n is the number of workstations that require the file . we also characterize our method as optimal amongst the class of no link sharing algorithms .
achieving quality assurance functionality in the food industry using a hybrid case based reasoning and fuzzy logic approach . <eos> quality control of food inventories in the warehouse is complex as well as challenging due to the fact that food can easily deteriorate . currently , this difficult storage problem is managed mostly by using a human dependent quality assurance and decision making process . this has however , occasionally led to unimaginative , arduous and inconsistent decisions due to the injection of subjective human intervention into the process . therefore , it could be said that current practice is not powerful enough to support high quality inventory management . in this paper , the development of an integrative prototype decision support system , namely , intelligent food quality assurance system ( ifqas ) is described which will assist the process by automating the human based decision making process in the quality control of food storage . the system , which is composed of a case based reasoning ( cbr ) engine and a fuzzy rule based reasoning ( fbr ) engine , starts with the receipt of incoming food inventory . with the cbr engine , certain quality assurance operations can be suggested based on the attributes of the food received . further of this , the fbr engine can make suggestions on the optimal storage conditions of inventory by systematically evaluating the food conditions when the food is receiving . with the assistance of the system , a holistic monitoring in quality control of the receiving operations and the storage conditions of the food in the warehouse can be performed . it provides consistent and systematic quality assurance guidelines for quality control which leads to improvement in the level of customer satisfaction and minimization of the defective rate .
coverage and connectivity in three dimensional underwater sensor networks . <eos> unlike a terrestrial network . an underwater sensor network call have significant height which makes it a three dimensional network . there are many important sensor network design problems where the physical dimensionality of the network plays it significant role . one such problem is determining how to deploy minimum number of sensor nodes so that all points inside the network is within the sensing range of at least one sensor and all sensor nodes call communicate with each other , possibly over a multi hop path . the solution to this problem depends oil the ratio of the communication ran e and the sensing range of each sensor . under sphere based communication and sensing model , placing a node at the center of each virtual cell created by truncated octahedron based tessellation solves this problem when this ratio is greater than 1.7889 . however , for smaller values of this ratio , the solution depends on how much communication redundancy the network needs . we provide solutions for both limited and full communication redundancy requirements . copyright ( c ) <digit> john wiley sons , ltd .
error correction of voicemail transcripts in scanmail . <eos> despite its widespread use , voicemail presents numerous usability challenges people must listen to messages in their entirety , they can not search by keywords , and audio files do not naturally support visual skimming . scanmail overcomes these flaws by automatically generating text transcripts of voicemail messages and presenting them in an email like interface . transcripts facilitate quick browsing and permanent archive . however , errors from the automatic speech recognition ( asr ) hinder the usefulness of the transcripts . the work presented here specifically addresses these problems by evaluating user initiated error correction of transcripts . user studies of two editor interfaces a grammar assisted menu and simple replacement by typing reveal reduced audio playback times and an emphasis on editing important words with the menu , suggesting its value in mobile environments where limited input capabilities are the norm and user privacy is essential . the study also adds to the scarce body of work on asr confidence shading , suggesting that shading may be more helpful than previously reported .
study of stress waves in geomedia and effect of a soil cover layer on wave attenuation using a <digit> d finite difference method . <eos> the propagation and attenuation of blast induced stress waves differs between geomedia such as rock or soil mass . this paper numerically studies the propagation and attenuation of blast induced elastoplastic waves in deep geomedia by using a one dimensional ( i d ) finite difference code . firstly , the elastoplastic cap models for rock and soil masses are introduced into the governing equations of spherical wave motion and a fortran code based on the finite difference method is developed . secondly , an underground spherical blast is simulated with this code and verified by software , renewto . the propagation of stress waves in rock and soil masses is numerically investigated , respectively . finally , the effect of a soil cover layer on the attenuation of stress waves in the rear rock mass is studied . it is determined that large plastic deformation of geomedia can effectively dissipate the energy of stress waves inward and the developed i d finite difference code coupled with elastoplastic cap models is convenient and effective in the numerical simulations for underground spherical explosion . ( c ) <digit> elsevier ltd. all rights reserved .
keyed hash function based on a dynamic lookup table of functions . <eos> in this paper , we present a novel keyed hash function based on a dynamic lookup table of functions . more specifically , we first exploit the piecewise linear chaotic map ( pwlcm ) with secret keys used for producing four <digit> bit initial buffers and then elaborate the lookup table of functions used for selecting composite functions associated with messages . next , we convert the divided message blocks into ascii code values , check the equivalent indices and then find the associated composite functions in the lookup table of functions . for each message block , the four buffers are reassigned by the corresponding composite function and then the lookup table of functions is dynamically updated . after all the message blocks are processed , the final <digit> bit hash value is obtained by cascading the last reassigned four buffers . finally , we evaluate our hash function and the results demonstrate that the proposed hash algorithm has good statistical properties , strong collision resistance , high efficiency , and better statistical performance compared with existing chaotic hash functions .
exploring hierarchical multidimensional data with unified views of distribution and correlation . <eos> data analysts explore data by inspecting features such as clustering , distribution and correlation . much existing research has focused on different visualisations for different data exploration tasks . for example , a data analyst might inspect clustering and correlation with scatterplots , but use histograms to inspect a distribution . such visualisations allow an analyst to confirm prior expectations . for example , a scatterplot may confirm an expected correlation or may show deviations from the expected correlation . in order to better facilitate discovery of unexpected features in data , however , a combination of different perspectives may be needed . in this paper , we combine distributional and correlational views of hierarchical multidimensional data . our unified view supports the simultaneous exploration of data distribution and correlation . by presenting a unified view , we aim to increase the chances of discovery of unexpected data features , and to provide the means to explore such features in detail . further , our unified view is equipped with a small number of primitive interaction operators which a user composes to facilitate smooth and flexible exploration . ( c ) <digit> elsevier ltd. all rights reserved .
application driven network on chip architecture exploration refinement for a complex soc . <eos> this article presents an overview of the design process of an interconnection network , using the technology proposed by arteris . section <digit> summarizes the various features a noc is required to implement to be integrated in modern socs . section <digit> describes the proposed top down approach , based on the progressive refinement of the noc description , from its functional specification ( sect. <digit> ) to its verification ( sect. <digit> ) . the approach is illustrated by a typical use case of a noc embedded in a hand held gaming device . the methodology relies on the definition of the performance behavior and expectation ( sect. <digit> ) , which can be early and efficiently simulated against various noc architectures . the system architect is then able to identify bottle necks and converge towards the noc implementation fulfilling the requirements of the target application ( sect. <digit> ) .
real valued mvdr beamforming using spherical arrays with frequency invariant characteristic . <eos> complex valued minimum variance distortionless response ( mvdr ) beamforming for wideband signals has very high computational amount . in this paper , we design a novel real valued mvdr beamformer for spherical arrays . the dependence of the array steering matrix on source signal directions and frequencies is decoupled using spherical harmonic decomposition . then a compensation network is designed to solve the frequency dependence of the array response and to get a new array response only determined by the spherical harmonics of the source directions . all frequency bins of wideband signals can be used together instead of being processed independently . by exploiting the property of the conjugate spherical harmonics , a unitary transform can be found to acquire a real valued frequency invariant steering matrix ( fism ) . based on the fism , real valued mvdr ( rv mvdr ) is developed to obtain good performance with low computational amount . simulation results demonstrate the performance of our proposed method for beamforming and direction of arrival ( doa ) estimation by comparing with the complex valued and real weighted mvdr methods .
private database queries using quantum states with limited coherence times . <eos> we describe a method for private database queries using exchange of quantum states with bits encoded in mutually incompatible bases . for technology with limited coherence time , the database vendor can announce the encoding after a suitable delay to allow the user to privately learn one of two items in the database without the ability to also definitely infer the second item . this quantum approach also allows the user to choose to learn other functions of the items , such as the exclusive or of their bits , but not to gain more information than equivalent to learning one item , on average . this method is especially useful for items consisting of a few bits by avoiding the substantial overhead of conventional cryptographic approaches .
scheduling for information gathering on sensor network . <eos> we investigate a unique wireless sensor network scheduling problem in which all nodes in a cluster send exactly one packet to a designated sink node in an effort to minimize transmission time . however , node transmissions must be sufficiently isolated either in time or in space to avoid collisions . the problem is formulated and solved via graph representation . we prove that an optimal transmission schedule can be obtained efficiently through a pipeline like schedule when the underlying topology is either line or tree . the minimum time required for a line or tree topology with n nodes is <digit> ( n <digit> ) . we further prove that our scheduling problem is np hard for general graphs . we propose a heuristic algorithm for general graphs . our heuristic tries to schedule as many independent segments as possible to increase the degree of parallel transmissions . this algorithm is compared to an rts cts based distributed algorithm . preliminary simulated results indicate that our heuristic algorithm outperforms the rts cts based distributed algorithm ( up to <digit> % ) and exhibits stable behavior .
synchronization analysis and control in chaos system based on complex network . <eos> for a certain kind of complex network , lorenz chaos system is used to describe the state equation of nodes in network . by constructing a lyapunov function , it is proved that this network model can achieve synchronization under the adaptive control scheme . the control strategy is simple , effective and easy for the engineering design in the future . the simulation results show the effectiveness of control scheme .
improved property in organic light emitting diode utilizing two al alq3 layers . <eos> we reported on the fabrication of organic light emitting devices ( oleds ) utilizing the two al alq3 layers and two electrodes . this novel green device with structure of al ( 110nm ) tris ( <digit> hydroxyquinoline ) aluminum ( alq3 ) ( 65nm ) al ( 110nm ) alq3 ( 50nm ) n , n dipheny1 n , n bis ( <digit> methy1phyeny1 ) <digit> , <digit> bipheny1 <digit> , <digit> diamine ( tpd ) ( 60nm ) ito ( 60nm ) glass . tpd were used as holes transporting layer ( htl ) , and alq3 was used as electron transporting layer ( etl ) , at the same time , alq3 was also used as emitting layer ( el ) , al and ito were used as cathode and anode , respectively . the results showed that the device containing the two al alq3 layers and two electrodes had a higher brightness and electroluminescent efficiency than the device without this layer . at current density of 14ma cm2 , the brightness of the device with the two al alq3 layers reach 3693cd m2 , which is higher than the 2537cd m2 of the al alq3 tpd alq3 ito glass device and the 1504.0 cd m2 of the al alq3 tpd ito glass . turn on voltage of the device with two al alq3 layers was 7v , which is lower than the others .
concept development for kindergarten children through a health simulation . <eos> according to many dental professionals , the decay process resulting from the accumulation of sugar on teeth is a very difficult concept for young children to learn . playing the dental hygiene game with thinkingtags not only brings context into the classroom , but also allows children to work with digital manipulatives that provide rich personal experiences and instant feedback . instead of watching a demonstration of the accumulation of sugars on a computer screen , or being told about dental health , this simulation allows pre school children to experience improving or decaying dental health without any real adverse health effects . small , wearable , microprocessor driven tags were brought into the kindergarten classroom to simulate the decay process , providing information about sugars in foods and creating a discussion about teeth . preliminary analyses suggest that this program was effective and enthusiastically received by this age group .
high output impedance current mode four function filter with reduced number of active and passive elements using the dual output current conveyor . <eos> this paper reports a new single input multi output current mode multifunction filter which can simultaneously realise lp , hp , bp and br filter functions all at high impedance outputs . the circuit permits orthogonal adjustment of quality factor q and omega ( <digit> ) , employs only five grounded passive components and no element matching conditions are imposed . a second order all pass function can easily be obtained . the passive sensitivities are shown to be low .
constraint programming for itemset mining . <eos> the relationship between constraint based mining and constraint programming is explored by showing how the typical constraints used in pattern mining can be formulated for use in constraint programming environments . the resulting framework is surprisingly flexible and allows us to combine a wide range of mining constraints in different ways . we implement this approach in off the shelf constraint programming systems and evaluate it empirically . the results show that the approach is not only very expressive , but also works well on complex benchmark problems .
experiences mining open source release histories . <eos> software releases form a critical part of the life cycle of a software project . typically , each project produces releases in its own way , using various methods of versioning , archiving , announcing and publishing the release . understanding the release history of a software project can shed light on the project history , as well as the release process used by that project , and how those processes change . however , many factors make automating the retrieval of release history information difficult , such as the many sources of data , a lack of relevant standards and a disparity of tools used to create releases . in spite of the large amount of raw data available , no attempt has been made to create a release history database of a large number of projects in the open source ecosystem . this paper presents our experiences , including the tools , techniques and pitfalls , in our early work to create a software release history database which will be of use to future researchers who want to study and model the release engineering process in greater depth .
balancing throughput and response time in online scientific clouds via ant colony optimization ( sp2013 <digit> <digit> ) . <eos> the cloud computing paradigm focuses on the provisioning of reliable and scalable infrastructures ( clouds ) delivering execution and storage services . the paradigm , with its promise of virtually infinite resources , seems to suit well in solving resource greedy scientific computing problems . the goal of this work is to study private clouds to execute scientific experiments coming from multiple users , i.e. , our work focuses on the infrastructure as a service ( iaas ) model where custom virtual machines ( vm ) are launched in appropriate hosts available in a cloud . then , correctly scheduling cloud hosts is very important and it is necessary to develop efficient scheduling strategies to appropriately allocate vms to physical resources . the job scheduling problem is however np complete , and therefore many heuristics have been developed . in this work , we describe and evaluate a cloud scheduler based on ant colony optimization ( aco ) . the main performance metrics to study are the number of serviced users by the cloud and the total number of created vms in online ( non batch ) scheduling scenarios . besides , the number of intra cloud network messages sent are evaluated . simulated experiments performed using cloudsim and job data from real scientific problems show that our scheduler succeeds in balancing the studied metrics compared to schedulers based on random assignment and genetic algorithms .
exploring the cscw spectrum using process mining . <eos> process mining techniques allow for extracting information from event logs . for example , the audit trails of a workflow management system or the transaction logs of an enterprise resource planning system can be used to discover models describing processes , organizations , and products . traditionally , process mining has been applied to structured processes . in this paper , we argue that process mining can also be applied to less structured processes supported by computer supported cooperative work ( cscw ) systems . in addition , the prom framework is described . using prom a wide variety of process mining activities are supported ranging from process discovery and verification to conformance checking and social network analysis .
effects of spatial and temporal variation in environmental conditions on simulation of wildfire spread . <eos> implementation of a wildfire spread model based on the level set method . investigation of wildfire propagation under stochastic wind and fuel conditions . local variation in combustion condition slows the rate of propagation . local variation in wind direction is found to increase flank spread . a harmonic mean is preferential for spatially varying parameters in spread models .
utilization of spatial decision support systems decision making in dryland agriculture a tifton burclover case study . <eos> fsaw delineated wyoming agricultural land into relative ranks for burclover establishment . defuzzification produced final output map with crisp scores and calculated centroid . calculated centroid map demonstrated efficacy of sdss in agricultural decision making . effective land suitability ranking validated value of ex ante agricultural technologies . presented information has potential to determine burclover feasibility in wyoming .
propagation engine prototyping with a domain specific language . <eos> constraint propagation is at the heart of constraint solvers . two main trends co exist for its implementation variable oriented propagation engines and constraint oriented propagation engines . those two approaches ensure the same level of local consistency but their efficiency ( computation time ) can be quite different depending on the instance solved . however , it is usually accepted that there is no best approach in general , and modern constraint solvers implement only one . in this paper , we would like to go a step further providing a solver independent language at the modeling stage to enable the design of propagation engines . we validate our proposal with a reference implementation based on the choco solver and the minizinc constraint modeling language .
a projection pursuit framework for supervised dimension reduction of high dimensional small sample datasets . <eos> the analysis and interpretation of datasets with large number of features and few examples has remained as a challenging problem in the scientific community , owing to the difficulties associated with the curse of the dimensionality phenomenon . projection pursuit ( pp ) has shown promise in circumventing this phenomenon by searching low dimensional projections of the data where meaningful structures are exposed . however , pp faces computational difficulties in dealing with datasets containing thousands of features ( typical in genomics and proteomics ) due to the vast quantity of parameters to optimize . in this paper we describe and evaluate a pp framework aimed at relieving such difficulties and thus ease the construction of classifier systems . the framework is a two stage approach , where the first stage performs a rapid compaction of the data and the second stage implements the pp search using an improved version of the spp method ( guo et al. , <digit> , <digit> ) . in an experimental evaluation with eight public microarray datasets we showed that some configurations of the proposed framework can clearly overtake the performance of eight well established dimension reduction methods in their ability to pack more discriminatory information into fewer dimensions .
conservation functions for <digit> d automata efficient algorithms , new results , and a partial taxonomy . <eos> we present theorems that can be used for improved efficiency in the calculation of conservation functions for cellular automata . we report results obtained from implementations of algorithms based on these theorems that show conservation laws for <digit> d cellular automata of higher order than any previously known . we introduce the notion of trivial and core conservation functions to distinguish truly new conservation functions from simple extensions of lower order ones . we then present the complete list of conservation functions up to order <digit> for the <digit> elementary <digit> d binary cellular automata . these include cas that were not previously known to have nontrivial conservation functions .
a reference bacterial genome dataset generated on the minion portable single molecule nanopore sequencer . <eos> the minion is a new , portable single molecule sequencer developed by oxford nanopore technologies . it measures four inches in length and is powered from the usb 3.0 port of a laptop computer . the minion measures the change in current resulting from dna strands interacting with a charged protein nanopore . these measurements can then be used to deduce the underlying nucleotide sequence .
serial batching scheduling of deteriorating jobs in a two stage supply chain to minimize the makespan . <eos> for the scheduling problem with a buffer , an optimal algorithm is developed for solving it . for the scheduling problem without buffer , some useful properties are derived . a heuristic is designed for solving it , and a novel lower bound is also derived . two special cases are well analyzed , and two optimal algorithms are developed for solving them , respectively .
entanglement monotones and maximally entangled states in multipartite qubit systems . <eos> we present a method to construct entanglement measures for pure states of multipartite qubit systems . the key element of our approach is an antilinear operator that we call comb in reference to the hairy ball theorem . for qubits ( i.e. spin <digit> <digit> ) the combs are automatically invariant under sl ( <digit> , c ) . this implies that the filters obtained from the combs are entanglement monotones by construction . we give alternative formulae for the concurrence and the <digit> tangle as expectation values of certain antilinear operators . as an application we discuss inequivalent types of genuine four , five and six qubit entanglement .
automatic verification of java programs with dynamic frames . <eos> framing in the presence of data abstraction is a challenging and important problem in the verification of object oriented programs leavens et al. ( formal aspects comput ( facs ) 19 159 189 , <digit> ) . the dynamic frames approach is a promising solution to this problem . however , the approach is formalized in the context of an idealized logical framework . in particular , it is not clear the solution is suitable for use within a program verifier for a java like language based on verification condition generation and automated , first order theorem proving . in this paper , we demonstrate that the dynamic frames approach can be integrated into an automatic verifier based on verification condition generation and automated theorem proving . the approach has been proven sound and has been implemented in a verifier prototype . the prototype has been used to prove correctness of several programming patterns considered challenging in related work .
properties of the transmission of pulse sequences in a bistable chain of unidirectionally coupled neurons . <eos> we study the propagation of pulse sequences in a chain of neurons with sigmoidal inputoutput relations . the propagating speeds of pulse fronts depend on the widths of the preceding pulses and adjacent pulse fronts interact attractively . sequences of pulse widths are then modulated through transmission . equations for changes in pulse width sequences are derived with a kinematical model of propagating pulse fronts . the transmission of pulse width sequences in the chain is expressed as a linear system with additive noise . the gain of the system function increases exponentially with the number of neurons in a high frequency region . the power spectrum of variations in pulse widths due to spatiotemporal noise also increases in the same manner . further , the interaction between pulse fronts keeps the coherence and mutual information of initial and transmitted pulse sequences . results of an experiment on an analog circuit confirm these properties .
building geometric feature based maps for indoor service robots . <eos> this paper presents an efficient geometric approach to the simultaneous localization and mapping problem based on an extended kalman filter . the map representation and building process is formulated , fully implemented and successfully experimented in different indoor environments with different robots . the use of orthogonal shape constraints is proposed to deal with the inconsistency of the estimation . built maps are successfully used for the navigation of two different service robots an interactive tour guide robot , and an assistive walking aid for the frail elderly .
the cross entropy method with patching for rare event simulation of large markov chains . <eos> there are various importance sampling schemes to estimate rare event probabilities in markovian systems such as markovian reliability models and jackson networks . in this work , we present a general state dependent importance sampling method which partitions the state space and applies the cross entropy method to each partition . we investigate two versions of our algorithm and apply them to several examples of reliability and queueing models . in all these examples we compare our method with other importance sampling schemes . the performance of the importance sampling schemes is measured by the relative error of the estimator and by the efficiency of the algorithm . the results from experiments show considerable improvements both in running time of the algorithm and the variance of the estimator .
combined simulation for process control extension of a general purpose simulation tool . <eos> combined discrete event and continuous views of production processes are important in designing computer control systems for both process industries and manufacturing . the paper presents an extension of the popular matlab simulink simulation tool to facilitate the simulation of the discrete sequential control logic applied to continuous processes . the control system is modelled as a combined system where the discrete and the continuous parts of the system are separated and an interface is introduced between them . the sequential control logic is represented by a sequential function chart ( sfc ) . a sfc blockset is defined to enable graphical composition of the sfc and its integration into the simulink environment . a simulation mechanism is implemented which is called periodically from the standard simulink simulation engine and carries out the correct state transition sequence of the discrete model and executes corresponding sfc actions . two simulation case studies are given to illustrate the possible application of the developed simulation environment the simulation of a batch process cell , as an example from the area of process control and an example of a manufacturing system , i.e. the control of a laboratory scale modular production system . ( c ) <digit> elsevier science b.v. all rights reserved .
action recognition feedback based framework for human pose reconstruction from monocular images . <eos> a novel framework based on action recognition feedback for pose reconstruction of articulated human body from monocular images is proposed in this paper . the intrinsic ambiguity caused by perspective projection makes it difficult to accurately recover articulated poses from monocular images . to alleviate such ambiguity , we exploit the high level motion knowledge as action recognition feedback to discard those implausible estimates and generate more accurate pose candidates using large number of motion constraints during natural human movement . the motion knowledge is represented by both local and global motion constraints . the local spatial constraint captures motion correlation between body parts by multiple relevance vector machines while the global temporal constraint preserves temporal coherence between time ordered poses via a manifold motion template . experiments on the cmu mocap database demonstrate that our method performs better on estimation accuracy than other methods without action recognition feedback .
burr size reduction in drilling by ultrasonic assistance . <eos> accuracy and surface finish play an important role in modern industry . undesired projections of materials , known as burrs , reduce the part quality and negatively affect the assembly process . a recent and promising method for reducing burr size in metal cutting is the use of ultrasonic assistance , where high frequency and low amplitude vibrations are added in the feed direction during cutting . note that this cutting process is distinct from ultrasonic machining . this paper presents the design of an ultrasonically vibrated workpiece holder , and a two stage experimental investigation of ultrasonically assisted drilling of a1100 <digit> aluminum workpieces . the results of <digit> drilling experiments with uncoated and tin coated drills are reported and analyzed . the effect of ultrasonic assistance on burr size , chip formation , thrust forces and tool wear is studied . the results demonstrate that under suitable ultrasonic vibration conditions , the burr height and width can be reduced in comparison to conventional drilling .
selecting coherent and relevant plots in large scatterplot matrices . <eos> the scatterplot matrix ( splom ) is a well established technique to visually explore high dimensional data sets . it is characterized by the number of scatterplots ( plots ) of which it consists of . unfortunately , this number quadratically grows with the number of the data sets dimensions . thus , an splom scales very poorly . consequently , the usefulness of sploms is restricted to a small number of dimensions . for this , several approaches already exist to explore such small sploms . those approaches address the scalability problem just indirectly and without solving it . therefore , we introduce a new greedy approach to manage large sploms with more than <digit> dimensions . we establish a combined visualization and interaction scheme that produces intuitively interpretable sploms by combining known quality measures , a pre process reordering and a perception based abstraction . with this scheme , the user can interactively find large amounts of relevant plots in large sploms .
the random electrode selection ensemble for eeg signal classification . <eos> pattern classification methods are a crucial direction in the current study of braincomputer interface ( bci ) technology . a simple yet effective ensemble approach for electroencephalogram ( eeg ) signal classification named the random electrode selection ensemble ( rese ) is developed , which aims to surmount the instability demerit of the fisher discriminant feature extraction for bci applications . through the random selection of recording electrodes answering for the physiological background of user intended mental activities , multiple individual classifiers are constructed . in a feature subspace determined by a couple of randomly selected electrodes , principal component analysis ( pca ) is first used to carry out dimensionality reduction . successively fisher discriminant is adopted for feature extraction , and a bayesian classifier with a gaussian mixture model ( gmm ) approximating the feature distribution is trained . for a test sample the outputs from all the bayesian classifiers are combined to give the final prediction for its label . theoretical analysis and classification experiments with real eeg signals indicate that the rese approach is both effective and efficient .
robust schur stability of polynomials with polynomial parameter dependency . <eos> the paper considers the robust schur stability verification of polynomials with coefficients depending polynomially on parameters varying in given intervals . a new algorithm is presented which relies on the expansion of a multivariate polynomial into bernstein polynomials and is based on the decomposition of the family of polynomials into its symmetric and antisymmetric parts . it is shown how the inspection of both polynomial families on the upper half of the unit circle can be reduced to the analysis of two related polynomial families on the real interval <digit> , <digit> . then the bernstein expansion can be applied in order to check whether both polynomial families have a zero in this interval in common .
use of nano scale double gate mosfets in low power tunable current mode analog circuits . <eos> use of independently driven nano scale double gate ( dg ) mosfets for low power analog circuits is emphasized and illustrated . in independent drive configuration , the top gate response of dg mosfets can be altered by application of a control voltage on the bottom gate . we show that this could be a powerful method to conveniently tune the response of conventional cmos analog circuits especially for current mode design . several examples of such circuits , including current mirrors , a differential current amplifier and differential integrators are illustrated and their performance gauged using tcad simulations . the topologies and biasing schemes explored here show how the nano scale dg mosfets may pave way for efficient , mismatch tolerant and smaller circuits with tunable characteristics .
cooperative triangulation in msbns without revealing subnet structures . <eos> multiply sectioned bayesian networks ( msbns ) provide a coherent framework for probabilistic inference in a cooperative multiagent distributed interpretation system . inference in msbns can be performed effectively using a compiled representation . the compilation involves the triangulation of the collective dependency structure ( a graph ) defined in terms of the union of a set of local dependency structures ( a set of graphs ) . privacy of agents eliminates the option to assemble these graphs at a central location and to triangulate their union . earlier work solved distributed triangulation in a restricted case . the method is conceptually complex and the correctness of its extension to the general case is difficult to justify . in this paper , we present a new method that is conceptually simpler and is efficient . we prove its correctness in the general case and demonstrate its performance experimentally ( c ) <digit> john wiley sons , inc .
the complexity of parallel evaluation of linear recurrence . <eos> the concept of computers such as c.mmp and illiac iv is to achieve computational speed up by performing several operations simultaneously with parallel processors . this type of computer organization is referred to as a parallel computer . in this paper , we prove upper bounds on speed ups achievable by parallel computers for a particular problem , the solution of first order linear recurrences . we consider this problem because it is important in practice and also because it is simply stated so that we might obtain some insight into the nature of parallel computation by studying it .
leukocyte image segmentation using simulated visual attention . <eos> computer aided automatic analysis of microscopic leukocyte is a powerful diagnostic tool in biomedical fields which could reduce the effects of human error , improve the diagnosis accuracy , save manpower and time . however , it is a challenging to segment entire leukocyte populations due to the changing features extracted in the leukocyte image , and this task remains an unsolved issue in blood cell image segmentation . this paper presents an efficient strategy to construct a segmentation model for any leukocyte image using simulated visual attention via learning by on line sampling . in the sampling stage , two types of visual attention , bottom up and top down together with the movement of the human eye are simulated . we focus on a few regions of interesting and sample high gradient pixels to group training sets . while in the learning stage , the svm ( support vector machine ) model is trained in real time to simulate the visual neuronal system and then classifies pixels and extracts leukocytes from the image . experimental results show that the proposed method has better performance compared to the marker controlled watershed algorithms with manual intervention and thresholding based methods .
on the construction of an aggregated measure of the development of interval data . <eos> we analyse some possibilities for constructing an aggregated measure of the development of socio economical objects in terms of their composite phenomenon ( i.e. , phenomenon described by many statistical features ) if the relevant data are expressed as intervals . such a measure , based on the deviation of the data structure for a given object from the benchmark of development is a useful tool for ordering , comparing and clustering objects . we present the construction of a composite phenomenon when it is described by interval data and discuss various aspects of stimulation and normalization of the diagnostic features as well as a definition of a benchmark of development ( based usually on optimum or expected levels of these features ) . our investigation includes the following options for the realization of this purpose transformation of the interval model into a singlevalued version without any significant loss of its statistical properties , standardization of pure intervals as well as definition of the interval ideal object . for the determination of a distance between intervals , the hausdorff formula is applied . the simulation study conducted and the empirical analysis showed that the first two variants are especially useful in practice .
user requirements for a web based spreadsheet mediated collaboration . <eos> this paper reports the initial results of a research project to investigate how to develop a web based spreadsheet mediated business collaboration system that could notably enhance the business processes presently carried out by small to medium sized enterprises . using a scenario based design approach , a set of user 's requirements were extracted from an appropriate field study . these requirements were then analysed in the context of well known usability principles , and a set of design implications were derived based on a selected set of hci design patterns related to cooperative interaction design . starting from that knowledge , suitable interactive collaboration scenarios have been drawn , from which a list of user interface requirements for a web based spreadsheet mediated collaboration system has been formulated .
automated estimation and analyses of meteorological drought characteristics from monthly rainfall data . <eos> the paper describes a new software package for automated estimation , display and analyses of various drought indices continuous functions of precipitation that allow quantitative assessment of meteorological drought events to be made . the software at present allows up to five different drought indices to be estimated . they include the decile index ( di ) , the effective drought index ( edi ) , the standardized precipitation index ( spi ) and deviations from the long term mean and median value . each index can be estimated from point and spatially averaged rainfall data and a number of options are provided for months ' selection and the type of the analysis , including a running mean , single value or multiple annual values . the software also allows spell run analysis to be performed and maps of a specific index to be constructed . the software forms part of the comprehensive computer package , developed earlier and designed to perform the multitude of water resources analyses and hydro meteorological data processing . the <digit> step procedure of setting up and running a typical drought assessment application is described in detail . the examples of applications are given primarily in the specific context of south asia where the software has been used .
introduction to the special issue on statistical signal extraction and filtering . <eos> the papers of the special issue on statistical signal extraction and filtering are introduced briefly and the invitation to contribute to the next issue to be devoted to this topic is reiterated . there follows an account of the history and the current developments in the areas of wienerkolmogorov and kalman filtering , which is a leading topic of the present issue . other topics will be treated in like manner in subsequent introductions .
worst case optimal approximation algorithms for maximizing triplet consistency within phylogenetic networks . <eos> the study of phylogenetic networks is of great interest to computational evolutionary biology and numerous different types of such structures are known . this article addresses the following question concerning rooted versions of phylogenetic networks . what is the maximum value of p 0,1 p <digit> , <digit> such that for every input set t of rooted triplets , there exists some network n n such that at least p t p t of the triplets are consistent with n n we call an algorithm that computes such a network ( where p is maximum ) worst case optimal . here we prove that the set containing all triplets ( the full triplet set ) in some sense defines p . moreover , given a network n n that obtains a fraction p p for the full triplet set ( for any p p ) , we show how to efficiently modify n n to obtain a fraction p p for any given triplet set t . we demonstrate the power of this insight by presenting a worst case optimal result for level <digit> phylogenetic networks improving considerably upon the <digit> <digit> fraction obtained recently by jansson , nguyen and sung . for level <digit> phylogenetic networks we show that p 0.61 p 0.61 . we emphasize that , because we are taking t t as a ( trivial ) upper bound on the size of an optimal solution for each specific input t , the results in this article do not exclude the existence of approximation algorithms that achieve approximation ratio better than p. finally , we note that all the results in this article also apply to weighted triplet sets .
direct search of feasible region and application to a crashworthy helicopter seat . <eos> the paper proposes a novel approach to identify the feasible region for a constrained optimisation problem . in engineering applications the search for the feasible region turns out to be extremely useful in the understanding of the problem as the feasible region defines the portion of the domain where design parameters can be ranged to fulfil the constraints imposed on performances , manufacturing and regulations . the search for the feasible region is not a trivial task as non convex , irregular and disjointed shapes can be found . the algorithm presented in this paper moves from the above considerations and proposes a recursive feasible infeasible segment bisection algorithm combined with support vector machine ( svm ) techniques to reduce the overall computational effort . the method is discussed and then illustrated by means of three simple analytical test cases in the first part of the paper . a real world application is finally presented the search for the survivability zone of a crashworthy helicopter seat under different crash conditions . a finite element model , including an anthropomorphic dummy , is adopted to simulate impacts that are characterised by different deceleration pulses and the proposed algorithm is used to investigate the influence of pulse shape on impact survivability .
feasibly constructive proofs and the propositional calculus ( preliminary version ) . <eos> the motivation for this work comes from two general sources . the first source is the basic open question in complexity theory of whether p equals np ( see <digit> and <digit> ) . our approach is to try to show they are not equal , by trying to show that the set of tautologies is not in np ( of course its complement is in np ) . this is equivalent to showing that no proof system ( in the general sense defined in <digit> ) for the tautologies is super in the sense that there is a short proof for every tautology . extended resolution is an example of a powerful proof system for tautologies that can simulate most standard proof systems ( see <digit> ) . the main theorem ( 5.5 ) in this paper describes the power of extended resolution in a way that may provide a handle for showing it is not super . the second motivation comes from constructive mathematics . a constructive proof of , say , a statement a must provide an effective means of finding a proof of a for each value of x , but nothing is said about how long this proof is as a function of x. if the function is exponential or super exponential , then for short values of x the length of the proof of the instance of a may exceed the number of electrons in the universe . in section <digit> , i introduce the system pv for number theory , and it is this system which i suggest properly formalizes the notion of a feasibly constructive proof .
sina semantic interpretation of user queries for question answering on interlinked data . <eos> the architectural choices underlying linked data have led to a compendium of data sources which contain both duplicated and fragmented information on a large number of domains . one way to enable non experts users to access this data compendium is to provide keyword search frameworks that can capitalize on the inherent characteristics of linked data . developing such systems is challenging for three main reasons . first , resources across different datasets or even within the same dataset can be homonyms . second , different datasets employ heterogeneous schemas and each one may only contain a part of the answer for a certain user query . finally , constructing a federated formal query from keywords across different datasets requires exploiting links between the different datasets on both the schema and instance levels . we present sina , a scalable keyword search system that can answer user queries by transforming user supplied keywords or natural languages queries into conjunctive sparql queries over a set of interlinked data sources . sina uses a hidden markov model to determine the most suitable resources for a user supplied query from different datasets . moreover , our framework is able to construct federated queries by using the disambiguated resources and leveraging the link structure underlying the datasets to query . we evaluate sina over three different datasets . we can answer <digit> queries from the qald <digit> correctly . moreover , we perform as well as the best question answering system from the qald <digit> competition by answering <digit> questions correctly while also being able to answer queries on distributed sources . we study the runtime of sina in its mono core and parallel implementations and draw preliminary conclusions on the scalability of keyword search on linked data .
generalized median string computation by means of string embedding in vector spaces . <eos> in structural pattern recognition the median string has been established as a useful tool to represent a set of strings . however , its exact computation is complex and of high computational burden . in this paper we propose a new approach for the computation of median string based on string embedding . strings are embedded into a vector space and the median is computed in the vector domain . we apply three different inverse transformations to go from the vector domain back to the string domain in order to obtain a final approximation of the median string . all of them are based on the weighted mean of a pair of strings . experiments show that we succeed to compute good approximations of the median string .
efficient indexing of the historical , present , and future positions of moving objects . <eos> although significant effort has been put into the development of efficient spatio temporal indexing techniques for moving objects , little attention has been given to the development of techniques that efficiently support queries about the past , present , and future positions of objects . the provisioning of such techniques is challenging , both because of the nature of the data , which reflects continuous movement , and because of the types of queries to be supported . this paper proposes the bb x index structure , which indexes the positions of moving objects , given as linear functions of time , at any time . the index stores linearized moving object locations in a forest of b trees . the index supports queries that select objects based on temporal and spatial constraints , such as queries that retrieve all objects whose positions fall within a spatial range during a set of time intervals . empirical experiments are reported that offer insight into the query and update performance of the proposed technique .
towards model driven unit testing . <eos> the model driven architecture ( mda ) approach for constructing software systems advocates a stepwise refinement and transformation process starting from high level models to concrete program code . in contrast to numerous research efforts that try to generate executable function code from models , we propose a novel approach termed model driven monitoring . on the model level the behavior of an operation is specified with a pair of uml composite structure diagrams ( visual contract ) , a visual notation for pre and post conditions . the specified behavior is implemented by a programmer manually . an automatic translation from our visual contracts to jml assertions allows for monitoring the hand coded programs during their execution . in this paper we present how we extend our approach to allow for model driven unit testing , where we utilize the generated jml assertions as test oracles . further , we present an idea how to generate sufficient test cases from our visual contracts with the help of model checking techniques .
sliding window based frequent pattern mining over data streams . <eos> finding frequent patterns in a continuous stream of transactions is critical for many applications such as retail market data analysis , network monitoring , web usage mining , and stock market prediction . even though numerous frequent pattern mining algorithms have been developed over the past decade , new solutions for handling stream data are still required due to the continuous , unbounded , and ordered sequence of data elements generated at a rapid rate in a data stream . therefore , extracting frequent patterns from more recent data can enhance the analysis of stream data . in this paper , we propose an efficient technique to discover the complete set of recent frequent patterns from a high speed data stream over a sliding window . we develop a compact pattern stream tree ( cps tree ) to capture the recent stream data content and efficiently remove the obsolete , old stream data content . we also introduce the concept of dynamic tree restructuring in our cps tree to produce a highly compact frequency descending tree structure at runtime . the complete set of recent frequent patterns is obtained from the cps tree of the current window using an fp growth mining technique . extensive experimental analyses show that our cps tree is highly efficient in terms of memory and time complexity when finding recent frequent patterns from a high speed data stream .
modeling cryptographic properties of voice and voice based entity authentication . <eos> strong and or multi factor entity authentication protocols are of crucial importancein building successful identity management architectures . popular mechanisms to achieve these types of entity authentication are biometrics , and , in particular , voice , for which there are especially interesting business cases in the telecommunication and financial industries , among others . despite several studies on the suitability of voice within entity authentication protocols , there has been little or no formal analysis of any such methods . in this paper we embark into formal modeling of seemingly cryptographic properties of voice . the goal is to define a formal abstraction for voice , in terms of algorithms with certain properties , that are of both combinatorial and cryptographic type . while we certainly do not expect to achieve the perfect mathematical model for a human phenomenon , we do hope that capturing some properties of voice in a formal model would help towards the design and analysis of voice based cryptographic protocols , as for entity authentication . in particular , in this model we design and formally analyze two voice based entity authentication schemes , the first being a voice based analogue of the conventional password transmission entity authentication scheme . we also design and analyze , in the recently introduced bounded retrieval model <digit> , one voice and password based entity authentication scheme that is additionally secure against intrusions and brute force attacks , including dictionary attacks .
inference of finite state transducers from regular languages . <eos> finite state transducers are models that are being used in different areas of pattern recognition and computational linguistics . one of these areas is machine translation , where the approaches that are based on building models automatically from training examples are becoming more and more attractive . finite state transducers are very adequate to be used in constrained tasks where training samples of pairs of sentences are available . a technique to infer finite state transducers is proposed in this work . this technique is based on formal relations between finite state transducers and finite state grammars . given a training corpus of inputoutput pairs of sentences , the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic finite state grammar is inferred . this grammar is finally transformed into a resulting finite state transducer . the proposed methods are assessed through series of machine translation experiments within the framework of the eutrans project .
particle swarm optimization with preference order ranking for multi objective optimization . <eos> a new optimality criterion based on preference order ( po ) scheme is used to identify the best compromise in multi objective particle swarm optimization ( mopso ) . this scheme is more efficient than pareto ranking scheme , especially when the number of objectives is very large . meanwhile , a novel updating formula for the particles velocity is introduced to improve the search ability of the algorithm . the proposed algorithm has been compared with nsga ii and other two mopso algorithms . the experimental results indicate that the proposed approach is effective on the highly complex multi objective optimization problems .
real time deformation using modal analysis on graphics hardware . <eos> this paper presents an approach for fast simulating deformable objects that is suitable for interactive applications in computer graphics . linear modal analysis is often used to simulate small amplitude deformation . compared to traditional linear modal analysis where the cpu has been used to calculate the nodal displacements , the vertex program of gpu has been found widely adopted in the current applications . however the calculation suffers from great errors due to the limitation of the number of the input registers on gpu vertex pipeline . in our approach , we solve this problem by the fragment program . a series of 2d floating point textures are used to hold the model displacement matrix , the fragment program multiplies this matrix with the modal amplitude and sums up the results . experiments show that the proposed technique fully utilizes the parallelism nature of gpu , and runs in real time even for the complex models .
stability analysis of a class of general periodic neural networks with delays and impulses . <eos> based on the inequality analysis , matrix theory and spectral theory , a class of general periodic neural networks with delays and impulses is studied . some sufficient conditions are established for the existence and globally exponential stability of a unique periodic solution . furthermore , the results are applied to some typical impulsive neural network systems as special cases , with a real life example to show feasibility of our results .
neuroprotective properties of resveratrol and derivatives . <eos> stilbenoid compounds consist of a family of resveratrol derivatives . they have demonstrated promising activities in vitro and in vivo that indicate they may be useful in the prevention of a wide range of pathologies , such as cardiovascular diseases and cancers , as well have anti aging effects . more recently stilbenoid compounds have shown promise in the treatment and prevention of neurodegenerative disorders , such as huntingtons , parkinsons , and alzheimer 's diseases . this paper primarily focuses on the impact of stilbenoids in alzheimer 's disease and more specifically on the inhibition of amyloid peptide aggregation .
a new fuzzy multicriteria decision making method and its application in diversion of water . <eos> taking account of uncertainty in multicriteria decision making problems is crucial due to the fact that depending on how it is done , ranking of alternatives can be completely different . this paper utilizes linguistic values to evaluate the performance of qualitative criteria and proposes using appropriate shapes of fuzzy numbers to evaluate the performance of quantitative criteria for each problem with respect to its particular conditions . in addition , a process to determine the weights of criteria using fuzzy numbers , which considers their competition to gain greater weights and their influence on each other is described . a new fuzzy methodology is proposed to solve such a problem that utilizes parametric form of fuzzy numbers . the case study of diversion of water into lake urmia watershed , which is defined using triangular , trapezoidal , and bell shape fuzzy numbers demonstrates the utility of the proposed method . ( c ) <digit> elsevier ltd. all rights reserved .
scheduling divisible workloads on heterogeneous platforms . <eos> in this paper , we discuss several algorithms for scheduling divisible workloads on heterogeneous systems . our main contributions are ( i ) new optimality results for single round algorithms and ( ii ) the design of an asymptotically optimal multi round algorithm . this multi round algorithm automatically performs resource selection , a difficult task that was previously left to the user . because it is periodic , it is simpler to implement , and more robust to changes in the speeds of the processors and or communication links . on the theoretical side , to the best of our knowledge , this is the first published result assessing the absolute performance of a multi round algorithm . on the practical side , extensive simulations reveal that our multi round algorithm outperforms existing solutions on a large variety of platforms , especially when the communication to computation ratio is not very high ( the difficult case ) .
a connective ethnography of peer knowledge sharing and diffusion in a tween virtual world . <eos> prior studies have shown how knowledge diffusion occurs in classrooms and structured small groups around assigned tasks yet have not begun to account for widespread knowledge sharing in more native , unstructured group settings found in online games and virtual worlds . in this paper , we describe and analyze how an insider gaming practice spread across a group of tween players ages <digit> years in an after school gaming club that simultaneously participated in a virtual world called whyville.net . in order to understand how this practice proliferated , we followed the club members as they interacted with each other and members of the virtual world at large . employing connective ethnography to trace the movements in learning and teaching this practice , we coordinated data records from videos , tracking data , field notes , and interviews . we found that club members took advantage of the different spaces , people , and times available to them across whyville , the club , and even home and classroom spaces . by using an insider gaming practice , namely teleporting , rather than the more traditional individual person as our analytical lens , we were able to examine knowledge sharing and diffusion across the gaming spaces , including events in local small groups as well as encounters in the virtual world . in the discussion , we address methodological issues and design implications of our findings .
unsupervised classification of sar images using normalized gamma process mixtures . <eos> we propose an image prior for the model based nonparametric classification of synthetic aperture radar ( sar ) images that allows working with infinite number of mixture components . in order to enclose the spatial interactions of the pixel labels , the prior is derived by incorporating a conditional multinomial auto logistic random field into the normalized gamma process prior . in this way , we obtain an image classification prior that is free from the limitation on the number of classes and includes the smoothing constraint into classification problem . in this model , we introduced a hyper parameter that can control the preservation of the important classes and the extinction of the weak ones . the recall rates reported on the synthetic and the real terrasar x images show that the proposed model is capable of accurately classifying the pixels . unlike the existing methods , it applies a simple iterative update scheme without performing a hierarchical clustering strategy . we demonstrate that the estimation accuracy of the proposed method in number of classes outperforms the conventional finite mixture models .
two couple resolution blocking protocols on adaptive query splitting for rfid tag identification . <eos> how to accelerate tag identification is an important issue in radio frequency identification ( rfid ) systems . in some cases , the rfid reader repeatedly identifies the same tags since these tags always stay in its communication range . an anticollision protocol , called the adaptive query splitting protocol ( aqs ) , was proposed to handle these cases . this protocol reserves information obtained from the last process of tag identification so that the reader can quickly identify these staying tags again . this paper proposes two blocking protocols , a couple resolution blocking protocol ( crb ) and an enhanced couple resolution blocking protocol ( ecrb ) , based on aqs . crb and ecrb not only have the above mentioned capability as aqs but also use the blocking technique , which prohibits unrecognized tags from colliding with staying tags , to reduce the number of collisions . moreover , crb adopts a couple resolution technique to couple staying tags by simultaneously transmitting two id prefixes from the reader , while ecrb allows the reader to send only one id prefix to interrogate a couple of staying tags . thus , they only need half time to identify staying tags . we formally analyze the identification delay of crb and ecrb in the worst and average cases . our analytic and simulation results show that they obviously outperform aqs , and ecrb needs less transmitted bits than crb .
an active measurement system for shared environments . <eos> testbeds composed of end hosts deployed across the internet enable researchers to simultaneously conduct a wide variety of experiments . active measurement studies of internet path properties that require precisely crafted probe streams can be problematic in these environments . the reason is that load on the host systems from concurrently executing experiments ( as is typical in planetlab ) can significantly alter probe stream timings . in this paper we measure and characterize how packet streams from our local planetlab nodes are affected by experimental concurrency . we find that the effects can be extreme . we then set up a simple planetlab deployment in a laboratory testbed to evaluate these effects in a controlled fashion . we find that even relatively low load levels can cause serious problems in probe streams . based on these results , we develop a novel system called mad that can operate as a linux kernel module or as a stand alone daemon to support real time scheduling of probe streams . mad coordinates probe packet emission for all active measurement experiments on a node . we demonstrate the capabilities of mad , showing that it performs effectively even under very high levels of multiplexing and host system load .
policy based inconsistency management in relational databases . <eos> we define inconsistency management policies ( imps ) for real world applications . we show how imps relate to belief revision postulates , cqa , and relational algebra operators . we present several approaches to efficiently implement an imp based framework .
a new delay dependent stability criterion for linear neutral systems with norm bounded uncertainties in all system matrices . <eos> this paper deals with the problem of robust stability for a class of uncertain linear neutral systems . the uncertainties under consideration are of norm bounded type and appear in all system matrices . a new delay dependent stability criterion is obtained and formulated in the form of linear matrix inequalities ( lmis ) . neither model transformation nor bounding technique for cross terms is involved through derivation of the stability criterion . numerical examples show that the results obtained in this paper significantly improve the estimate of the stability limit over some existing results in the literature .
neutralization new insights into the problem of employee information systems security policy violations . <eos> employees ' failure to comply with information systems security policies is a major concern for information technology security managers . in efforts to understand this problem , is security researchers have traditionally viewed violations of is security policies through the lens of deterrence theory . in this article , we show that neutralization theory . a theory prominent in criminology but not yet applied in the context of is , provides a compelling explanation for is security policy violations and offers new insight into how employees rationalize this behavior . in doing so , we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory . our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices .
simplifying complex environments using incremental textured depth meshes . <eos> we present an incremental algorithm to compute image based simplifications of a large environment . we use an optimization based approach to generate samples based on scene visibility , and from each viewpoint create textured depth meshes ( tdms ) using sampled range panoramas of the environment . the optimization function minimizes artifacts such as skins and cracks in the reconstruction . we also present an encoding scheme for multiple tdms that exploits spatial coherence among different viewpoints . the resulting simplifications , incremental textured depth meshes ( itdms ) , reduce preprocessing , storage , rendering costs and visible artifacts . our algorithm has been applied to large , complex synthetic environments comprising millions of primitives . it is able to render them at <digit> <digit> frames a second on a pc with little loss in visual fidelity .
a neural approach to the underdetermined order recursive least squares adaptive filtering . <eos> the incorporation of the neural architectures in adaptive filtering applications has been addressed in detail . in particular , the underdetermined order recursive least squares ( urls ) algorithm , which lies between the well known normalized least mean square and recursive least squares algorithms , is reformulated via a neural architecture . the response of the neural network is seen to be identical to that of the algorithmic approach . together with the advantage of simple circuit realization , this neural network avoids the drawbacks of digital computation such as error propagation and matrix inversion , which is ill conditioned in most cases . it is numerically attractive because the quadratic optimization problem performs an implicit matrix inversion . also , the neural network offers the flexibility of easy alteration of the prediction order of the urls algorithm which may be crucial in some applications . it is rather difficult to achieve in the digital implementation , as one would have to use levinson recursions . the neural network can easily be integrated into a digital system through appropriate digital to analog and analog to digital converters .
bottleneck flows in unit capacity networks . <eos> the bottleneck network flow problem ( bnfp ) is a generalization of several well studied bottleneck problems such as the bottleneck transportation problem ( btp ) , bottleneck assignment problem ( bap ) , bottleneck path problem ( bpp ) , and so on . the bnfp can easily be solved as a sequence of o ( log n ) maximum flow problems on almost unit capacity networks . we observe that this algorithm runs in o ( min m ( <digit> <digit> ) . n ( <digit> <digit> ) m log n ) time by showing that the maximum flow problem on an almost unit capacity graph can be solved in o ( min m ( <digit> <digit> ) . n ( <digit> <digit> ) m ) time . we then propose a faster algorithm to solve the unit capacity bnfp in o ( min m ( n log n ) ( <digit> <digit> ) . m ( <digit> <digit> ) root log n ) time , an improvement by a factor of at least <digit> root log n. for dense graphs , the improvement is by a factor of root log n. on unit capacity simple graphs , we show that bnfp can be solved in o root n log n ) time , an improvement by a factor of root log n. as a consequence we have an o ( m root n log n ) algorithm for the btp with unit arc capacities . ( c ) <digit> elsevier b.v. all rights reserved .
taylor 's decomposition on four points for solving third order linear time varying systems . <eos> in the present paper , the use of three step difference schemes generated by taylor 's decomposition on four points for the numerical solutions of third order time varying linear dynamical systems is presented . the method is illustrated for the numerical analysis of an up converter used in communication systems .
bem formulation for von krmn plates . <eos> this work deals with nonlinear geometric plates in the context of von krmn 's theory . the formulation is written such that only the boundary in plane displacement and deflection integral equations for boundary collocations are required . at internal points , only out of plane rotation , curvature and in plane internal force representations are used . thus , only integral representations of these values are derived . the nonlinear system of equations is derived by approximating all densities in the domain integrals as single values , which therefore reduces the computational effort needed to evaluate the domain value influences . hyper singular equations are avoided by approximating the domain values using only internal nodes . the solution is obtained using a newton scheme for which a consistent tangent operator was derived .
on x variable filling and flipping for capture power reduction in linear decompressor based test compression environment . <eos> excessive test power consumption and growing test data volume are both serious concerns for the semiconductor industry . various low power x filling techniques and test data compression schemes were developed accordingly to address the above problems . these methods , however , often exploit the very same do n't care bits in the test cubes to achieve different objectives and hence may contradict each other . in this paper , we propose novel techniques to reduce scan capture power in linear decompressor based test compression environment , by employing algorithmic solutions to fill and flip x variables supplied to the linear decompressor . experimental results on benchmark circuits demonstrate that our proposed techniques significantly outperform existing solutions .
www based access to object oriented clinical databases the khospad project . <eos> khospad is a project aiming at improving the quality of the process of patient care concerning general practitionerpatienthospital relationships , using current information and networking technologies . the studied application field is a cardiology division , with hemodynamic laboratory and the population of ptca patients . data related to ptca patients are managed by arcadia , an object oriented database management system developed for the considered clinical setting . we defined a remotely accessible view of arcadia medical record , suitable for general practitioners ( gps ) caring patients after ptca , during the follow up period . using a pc , a modem and internet , an authorized gp can consult remotely the medical records of his ptca patients . main features of the application are related to the management and display of complex data , specifically characterized by multimedia and temporal features , based on an object oriented temporal data model .
fuzzy r subgroups with thresholds of near rings and implication operators . <eos> using the belongs to relation ( q ) and quasi coincidence with relation ( q ) between fuzzy points and fuzzy sets , the concept of ( alpha , beta ) fuzzy r subgroup of a near ring where alpha , beta are any two of epsilon , q , epsilon boolean and q , epsilon boolean or q with alpha not equal epsilon boolean and q is introduced and related properties are investigated . we also introduce the notion of a fuzzy r subgroup with thresholds which is a generalization of an ordinary fuzzy r subgroup and an ( epsilon , epsilon boolean or q ) fuzzy r subgroup . finally , we give the definition of an implication based fuzzy r subgroup .
a time accurate pseudo wavelet scheme for two dimensional turbulence . <eos> in this paper , we propose a wavelet taylor galerkin method for solving the two dimensional navier stokes equations . the discretization in time is performed before the spatial discretization by introducing second order generalization of the standard time stepping schemes with the help of taylor series expansion in time step . wavelet taylor galerkin schemes taking advantage of the wavelet bases capabilities to compress both functions and operators are presented . results for two dimensional turbulence are shown .
audio augmented paper for therapy and educational intervention for children with autistic spectrum disorder . <eos> physical tokens are artifacts which sustain cooperation between the children and therapists . therapists anchor children 's attention through physical tokens . therapists controlled children 's attention through physical tokens . the environment provides to the therapists the control of the flow of the therapeutic activity . the environment provides a good mean to stimulate fun and consequently to help children 's attention on listening tasks .
treating epilepsy via adaptive neurostimulation a reinforcement learning approach . <eos> this paper presents a now methodology for automatically learning an optimal neurostimulation strategy for the treatment of epilepsy . the technical challenge is to automatically modulate neurostimulation parameters , as a function of the observed eeg signal , so as to minimize the frequency and duration of seizures . the methodology leverages recent techniques from the machine learning literature , in particular the reinforcement learning paradigm , to formalize this optimization problem . we present an algorithm which is able to automatically learn an adaptive neurostimulation strategy directly from labeled training data acquired from animal brain tissues . our results suggest that this methodology can be used to automatically find a stimulation strategy which effectively reduces the incidence of seizures , while also minimizing the amount of stimulation applied . this work highlights the crucial role that modern machine learning techniques can play in the optimization of treatment strategies for patients with chronic disorders such as epilepsy .
load balanced parallel streamline generation on large scale vector fields . <eos> because of the ever increasing size of output data from scientific simulations , supercomputers are increasingly relied upon to generate visualizations . one use of supercomputers is to generate field lines from large scale flow fields . when generating field lines in parallel , the vector field is generally decomposed into blocks , which are then assigned to processors . since various regions of the vector field can have different flow complexity , processors will require varying amounts of computation time to trace their particles , causing load imbalance , and thus limiting the performance speedup . to achieve load balanced streamline generation , we propose a workload aware partitioning algorithm to decompose the vector field into partitions with near equal workloads . since actual workloads are unknown beforehand , we propose a workload estimation algorithm to predict the workload in the local vector field . a graph based representation of the vector field is employed to generate these estimates . once the workloads have been estimated , our partitioning algorithm is hierarchically applied to distribute the workload to all partitions . we examine the performance of our workload estimation and workload aware partitioning algorithm in several timings studies , which demonstrates that by employing these methods , better scalability can be achieved with little overhead .
an integrated research tool for x ray imaging simulation . <eos> this paper presents a software simulation package of the entire x ray projection radiography process including beam generation , absorber structure and composition , irradiation set up , radiation transport through the absorbing medium , image formation and dose calculation . phantoms are created as composite objects from geometrical or voxelized primitives and can be subjected to simulated irradiation process . the acquired projection images represent the two dimensional spatial distribution of the energy absorbed in the detector and are formed at any geometry , taking into account energy spectrum , beam geometry and detector response . this software tool is the evolution of a previously presented system , with new functionalities , user interface and an expanded range of applications . this has been achieved mainly by the use of combinatorial geometry for phantom design and the implementation of a monte carlo code for the simulation of the radiation interaction at the absorber and the detector .
requirements and solutions to software encapsulation and engineering in next generation manufacturing systems oooneida approach . <eos> this paper addresses the solutions enabling agile development , deployment and reconfiguration of software intensive automation systems both in discrete manufacturing and process technologies . as the key enabler for reaching the required level of flexibility of such systems , the paper discusses the issues of encapsulation , integration and re use of the automation intellectual property ( ip ) . the goals can be fulfilled by the use of a vendor independent concept of a reusable portable and scalable software module ( function block ) , as well as by a vendor independent automation device model . this paper also discusses the requirements of the methodology for the application of such modules in the time and cost effective specification , design , validation , realization and deployment of intelligent mechatronic components in distributed industrial automation and control systems . a new global initiative oooneida is presented , that targets these goals through the development of the automation object concept based on the recognized industrial standards iec61131 , iec61499 , iec61804 and unified modelling language ( uml ) and through the creation of the technological infrastructure for a new , open knowledge economy for automation components and automated industrial products . in particular , a web based repository for standardized automation solutions will be developed to serve as an electronic commerce facility in industrial automation businesses .
robust camera pose and scene structure analysis for service robotics . <eos> successful path planning and object manipulation in service robotics applications rely both on a good estimation of the robot 's position and orientation ( pose ) in the environment , as well as on a reliable understanding of the visualized scene . in this paper a robust real time camera pose and a scene structure estimation system is proposed . first , the pose of the camera is estimated through the analysis of the so called tracks . the tracks include key features from the imaged scene and geometric constraints which are used to solve the pose estimation problem . second , based on the calculated pose of the camera , i.e. robot , the scene is analyzed via a robust depth segmentation and object classification approach . in order to reliably segment the object 's depth , a feedback control technique at an image processing level has been used with the purpose of improving the robustness of the robotic vision system with respect to external influences , such as cluttered scenes and variable illumination conditions . the control strategy detailed in this paper is based on the traditional open loop mathematical model of the depth estimation process . in order to control a robotic system , the obtained visual information is classified into objects of interest and obstacles . the proposed scene analysis architecture is evaluated through experimental results within a robotic collision avoidance system . ( c ) <digit> elsevier b.v. all rights reserved .
nml , a schematic extension of f. esteva and l. godo 's logic mtl . <eos> a schematic extension nml of f.esteva and l.godo 's logic mtl is introduced in this paper . based on a new left continuous but discontinuous t norm , which was proposed by s.jenei and can be regarded as a kind of distorted nilpotent minimum , the semantics of nml is interpreted and the standard completeness theorem of nml is proved . the fact that the maximum and the minimum are definable from the negation and implication in nml and nm is discovered , which also leads to a modification of the nm axiom system . ( c ) <digit> elsevier b.v. all rights reserved .
verifying safety properties of concurrent java programs using <digit> valued logic . <eos> we provide a parametric framework for verifying safety properties of concurrent java programs . the framework combines thread scheduling information with information about the shape of the heap . this leads to error detection algorithms that are more precise than existing techniques . the framework also provides the most precise shape analysis algorithm for concurrent programs . in contrast to existing verification techniques , we do not put a bound on the number of allocated objects . the framework even produces interesting results when analyzing java programs with an unbounded number of threads . the framework is applied to successfully verify the following properties of a concurrent program concurrent manipulation of linked list based adt preserves the adt datatype invariant <digit> . the program does not perform inconsistent updates due to interference . the program does not reach a deadlock . the program does not produce run time errors due to illegal thread interactions . we also find bugs in erroneous versions of such implementations . a prototype of our framework has been implemented .
automatic discovery of theorems in elementary geometry . <eos> we present here a further development of the well known approach to automatic theorem proving in elementary geometry via algorithmic commutative algebra and algebraic geometry . rather than confirming refuting geometric statements ( automatic proving ) or finding geometric formulae holding among prescribed geometric magnitudes ( automatic derivation ) , in this paper we consider ( following kapur and mundy ) the problem of dealing automatically with arbitrary geometric statements ( i.e. , theses that do not follow , in general , from the given hypotheses ) aiming to find complementary hypotheses for the statements to become true . first we introduce some standard algebraic geometry notions in automatic proving , both for self containment and in order to focus our own contribution . then we present a rather successful but noncomplete method for automatic discovery that , roughly , proceeds adding the given conjectural thesis to the collection of hypotheses and then derives some special consequences from this new set of conditions . several examples are discussed in detail .
using support vector machines with a novel hybrid feature selection method for diagnosis of erythemato squamous diseases . <eos> in this paper , we developed a diagnosis model based on support vector machines ( svm ) with a novel hybrid feature selection method to diagnose erythemato squamous diseases . our proposed hybrid feature selection method , named improved f score and sequential forward search ( ifsfs ) , combines the advantages of filter and wrapper methods to select the optimal feature subset from the original feature set . in our ifsfs , we improved the original f score from measuring the discrimination of two sets of real numbers to measuring the discrimination between more than two sets of real numbers . the improved f score and sequential forward search ( sfs ) are combined to find the optimal feature subset in the process of feature selection , where , the improved f score is an evaluation criterion of filter method , and sfs is an evaluation system of wrapper method . the best parameters of kernel function of svm are found out by grid search technique . experiments have been conducted on different training test partitions of the erythemato squamous diseases dataset taken from uci ( university of california irvine ) machine learning database . our experimental results show that the proposed svm based model with ifsfs achieves 98.61 % classification accuracy and contains <digit> features . with these results , we conclude our method is very promising compared to the previously reported results . ( c ) <digit> elsevier ltd. all rights reserved .
domain specific languages from design to implementation application to video device drivers generation . <eos> domain specific languages ( dsl ) have many potential advantages in terms of software engineering ranging from increased productivity to the application of formal methods . although they have been used in practice for decades , there has been little study of methodology or implementation tools for the dsl approach . in this paper , we present our dsl approach and its application to a realistic domain the generation of video display device drivers . the presentation focuses on the validation of our proposed framework for domain specific languages , from design to implementation . the framework leads to a flexible design and structure , and provides automatic generation of efficient implementations of dsl programs . additionally , we describe an example of a complete dsl for video display adaptors and the benefits of the dsl approach for this application . this demonstrates some of the generally claimed benefits of using dsls increased productivity , higher level abstraction , and easier verification . this dsl has been fully implemented with our approach and is available . compose project url http www.irisa.fr compose gal .
mapping visual notations to mof compliant models with qvt relations . <eos> model centric methodologies rely on the definition of domain specific modeling languages for being able to create domain specific models . with mof the omg adopted a standard which provides the essential constructs for the definition of semantic language constructs ( abstract syntax ) . however , there are no specifications on how to define the notations ( concrete syntax ) for abstract syntax elements . usually , the concrete syntax of mof compliant languages is described informally . we propose to define mof based metamodels for abstract syntax and concrete syntax and to connect them by model transformations specified with qvt relations in a flexible , declarative way . using a qvt based transformation engine one can easily implement a model view controller architecture by integrating modeling tools and metadata repositories
financial early warning system model and data mining application for risk detection . <eos> one of the biggest problems of smes is their tendencies to financial distress because of insufficient finance background . in this study , an early warning system ( ews ) model based on data mining for financial risk detection is presented . chaid algorithm has been used for development of the ews . developed ews can be served like a tailor made financial advisor in decision making process of the firms with its automated nature to the ones who have inadequate financial background . besides , an application of the model implemented which covered <digit> smes based on turkish central bank ( tcb ) <digit> data . by using ews model , <digit> risk profiles , <digit> risk indicators , <digit> early warning signals , and <digit> financial road maps has been determined for financial risk mitigation .
a new wavelet algorithm to enhance and detect microcalcifications . <eos> we have proposed a new thresholding technique applied over wavelet coefficients for mammogram enhancement . we have utilized shannon entropy to find the best t in the wavelet domain . we have utilized tsallis entropy to find the best t in the wavelet domain . the proposed technique has better froc test with 96.5 % true positives and 0.36 false positives .
an overview of the bioasq large scale biomedical semantic indexing and question answering competition . <eos> this article provides an overview of the first bioasq challenge , a competition on large scale biomedical semantic indexing and question answering ( qa ) , which took place between march and september <digit> . bioasq assesses the ability of systems to semantically index very large numbers of biomedical scientific articles , and to return concise and user understandable answers to given natural language questions by combining information from biomedical articles and ontologies .
regreening the metropolis pathways to more ecological cities keynote address . <eos> eighty percent of the american population now lives in metropolitan regions whose geographic extent continues to expand even as many core cities and inner tier suburbs lose middle class populations , jobs , and tax base . urban sprawl and the socioeconomic polarizing of metropolitan america have been fostered by public policies including ( <digit> ) federal subsidies for new infrastructure on the urban fringe ( <digit> ) tax policies that favor home ownership over rental properties ( <digit> ) local zoning codes and ( <digit> ) federal and state neglect of older urban neighborhoods . in the face of diminished access to nature outside of metropolitan areas , locally based efforts to protect and restore greenspaces within urban areas seek to make older communities more habitable and more ecological . some pathways to more ecological cities include the following
a modified runs test for symmetry . <eos> we propose a modification of a modarresgastwirth test for the hypothesis of symmetry about a known center . by means of a monte carlo study we show that the modified test overtakes the original modarresgastwirth test for a wide spectrum of asymmetrical alternatives coming from the lambda family and for all assayed sample sizes . we also show that our test is the best runs test among the runs tests we have compared .
probability based approaches to vlsi circuit partitioning . <eos> iterative improvement two way min cut partitioning is an important phase in most circuit placement tools , and finds use in many other computer aided design ( cad ) applications . most iterative improvement techniques for circuit netlists like the fiduccia mattheyses ( fm ) method compute the gains of nodes using local netlist information that is only concerned with the immediate improvement in the cutset , this can lead to misleading gain information . krishnamurthy suggested a lookahead ( la ) gain calculation method to ameliorate this situation however , as we show , it leaves room for improvement . we present here a probabilistic gain computation approach called probabilistic partitioner ( prop ) that is capable of capturing the future implications of moving a node at the current time . we also propose an extended algorithm shrink prop that increases the provability of removing recently perturbed nets ( nets whose nodes have been moved for the first time ) from the cutset , experimental results on medium to large size acm sigda benchmark circuits show that prop and shrink prop outperform previous iterative improvement methods like fm ( bq . about <digit> % and <digit> % , respectively ) and la ( by about <digit> % and <digit> % , respectively ) . both prop and shrink prop also obtain much better cutsizes than many recent state of the art partitioners like eig1 , window melo , paraboli , gfm and cmetis ( by 4.5 % to <digit> % ) . our empirical timing results reveal that prop is appreciably faster than most recent techniques , we also obtain results on the more recent ispd <digit> benchmark suite that show similar substantial mincut improvements by prop and shrink prop over fm ( <digit> % and <digit> % , respectively ) . it is also noteworthy that shrink prop 's results are within 2.5 % of those obtained by hmetis . one of the best multilevel partitioners . however . the multilevel paradigm is orthogonal to shrink prop . further , since it is a flat partitioner , it has advantages over hmetis in partition driven placement applications .
shengbte a solver of the boltzmann transport equation for phonons . <eos> shengbte is a software package for computing the lattice thermal conductivity of crystalline bulk materials and nanowires with diffusive boundary conditions . it is based on a full iterative solution to the boltzmann transport equation . its main inputs are sets of second and third order interatomic force constants , which can be calculated using third party ab initio packages . dirac delta distributions arising from conservation of energy are approximated by gaussian functions . a locally adaptive algorithm is used to determine each process specific broadening parameter , which renders the method fully parameter free . the code is free software , written in fortran and parallelized using mpi . a complementary python script to help compute third order interatomic force constants from a minimum number of ab initio calculations , using a real space finite difference approach , is also publicly available for download . here we discuss the design and implementation of both pieces of software and present results for three example systems si , inas and lonsdaleite . program title shengbte catalogue identifier aesl_v1_0 program summary url http cpc.cs.qub.ac.uk summaries aesl_v1_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions gnu general public license , version <digit> no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language fortran <digit> , mpi . computer non specific . operating system unix linux . has the code been vectorized or parallelized yes , parallelized using mpi . ram up to several gb classification 7.9 . external routines lapack , mpi , spglib ( http spglib.sourceforge.net ) nature of problem calculation of thermal conductivity and related quantities , determination of scattering rates for allowed three phonon processes solution method iterative solution , locally adaptive gaussian broadening running time up to several hours on several tens of processors
tracing impact in a usability improvement process . <eos> analyzing usability improvement processes as they take place in real life organizations is necessary to understand the practice of usability work . this paper describes a case study where the usability of an information system is improved and a relationship between the improvements and the evaluation efforts is established . results show that evaluation techniques complemented each other by suggesting different kinds of usability improvement . among the techniques applied , a combination of questionnaires and metaphors of human thinking ( mot ) showed the largest mean impact and mot produced the largest number of impacts . logging of real life use of the system over <digit> months indicated six aspects of improved usability , where significant differences among evaluation techniques were found . concerning five of the six aspects think aloud evaluations and the above mentioned combination of questionnaire and mot performed equally well , and better than mot . based on the evaluations <digit> redesign proposals were developed and <digit> of these were implemented . four of the implemented redesigns where considered especially important . these evolved with inspiration from multiple evaluations and were informed by stakeholders with different kinds of expertise . our results suggest that practitioners should not rely on isolated evaluations . instead complementing techniques should be combined , and people with different expertise should be involved . ( c ) <digit> elsevier b.v. all rights reserved .
homan , a learning based negotiation method for holonic multi agent systems . <eos> holonic multi agent systems are a special category of multi agent systems that best fit to environments with numerous agents and high complexity . like in general multi agent systems , the agents in the holonic system may negotiate with each other . these systems have their own characteristics and structure , for which a specific negotiation mechanism is required . this mechanism should be simple , fast and operable in real world applications . it would be better to equip negotiators with a learning method which can efficiently use the available information . the learning method should itself be fast , too . additionally , this mechanism should match the special characteristics of the holonic multi agent systems . in this paper , we introduce such a negotiation method . experimental results demonstrate the efficiency of this new approach .
the portable common runtime approach to interoperability . <eos> operating system abstractions do not always reach high enough for direct use by a language or applications designer . the gap is filled by language specific runtime environments , which become more complex for richer languages ( commonlisp needs more than c , which needs more than c ) . but language specific environments inhibit integrated multi lingual programming , and also make porting hard ( for instance , because of operating system dependencies ) . to help solve these problems , we have built the portable common runtime ( pcr ) , a language independent and operating system independent base for modern languages . pcr offers four interrelated facilities storage management ( including universal garbage collection ) , symbol binding ( including static and dynamic linking and loading ) , threads ( lightweight processes ) , and low level i o ( including network sockets ) . pcr is common because these facilities simultaneously support programs in several languages . pcr supports c. cedar , scheme , and commonlisp intercalling and runs pre existing c and commonlisp ( kyoto ) binaries . pcr is portable because it uses only a small set of operating system features . the pcr source code is available for use by other researchers and developers .
efficient keyword search over virtual xml views . <eos> emerging applications such as personalized portals , enterprise search , and web integration systems often require keyword search over semi structured views . however , traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized . in this paper , we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual ( unmaterialized ) xml views . an interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results . another feature of the algorithm is that by solely using indices , we can still score the results of queries over the virtual view , and the resulting scores are the same as if the view was materialized . our performance evaluation using the inex data set in the quark ( bhaskar et al. in quark an efficient xquery full text implementation . in sigmod , <digit> ) open source xml database system indicates that the proposed approach is scalable and efficient .
a novel clustering method on time series data . <eos> time series is a very popular type of data which exists in many domains . clustering time series data has a wide range of applications and has attracted researchers from a wide range of discipline . in this paper a novel algorithm for shape based time series clustering is proposed . it can reduce the size of data , improve the efficiency and not reduce the effects by using the principle of complex network . firstly , one nearest neighbor network is built based on the similarity of time series objects . in this step , triangle distance is used to measure the similarity . of the neighbor network each node represents one time series object and each link denotes neighbor relationship between nodes . secondly , the nodes with high degrees are chosen and used to cluster . in clustering process , dynamic time warping distance function and hierarchical clustering algorithm are applied . thirdly , some experiments are executed on synthetic and real data . the results show that the proposed algorithm has good performance on efficiency and effectiveness . ( c ) <digit> elsevier ltd. all rights reserved .
an improved floating to fixed point conversion scheme for dct quantization algorithm . <eos> conventional fixed point implementation of the dct coefficients quantization algorithm in video compression may result in deteriorated image quality . the paper investigates this problem and proposes an improved floating to fixed point conversion scheme . with a proper scaling factor and a new established look up table , the proposed fixed point scheme can obtain bit wise consistence to the floating point realization . experimental results verify the validity of the proposed method .
a radial basis function network approach for the computation of inverse continuous time variant functions . <eos> this paper presents an efficient approach for the fast computation of inverse continuous time variant functions with the proper use of radial basis function networks ( rbfns ) . the approach is based on implementing rbfns for computing inverse continuous time variant functions via an overall damped least squares solution that includes a novel null space vector for singularities prevention . the singularities avoidance null space vector is derived from developing a sufficiency condition for singularities prevention that conduces to establish some characterizing matrices and an associated performance index .
cryptography on smart cards . <eos> this article presents an overview of the cryptographic primitives that are commonly implemented on smart cards . we also discuss attacks that can be mounted on smart cards as well as countermeasures against such attacks .
the antecedents of customer satisfaction and its link to complaint intentions in online shopping an integration of justice , technology , and trust . <eos> complaint behaviors are critical to maintaining customer loyalty in an online market . they provide insight into the customer 's experience of service failure and help to redress the failures . previous studies have shown the importance of customer satisfaction as a mediator for complaint intentions . it is important to examine the antecedents of customer satisfaction and its link to complaint intentions . online shoppers are both buyers of products services and users of web based systems . trust also plays a critical role in forming a psychological state with positive or negative feelings toward e vendors . in this context , there are three major concerns justice , technology and trust . this study proposes a research model to combine these issues , in order to investigate complaint intentions . data were collected from an online survey wherein subjects were encouraged to reflect on recent service failure experiences . the results from testing a structural equation model indicate that distributive and interactional justice contribute significantly to customer satisfaction and , in turn , to complaint intentions , but procedural justice does not . technology based features and trust are also important in determining the two target variables . the implications for managers and scholars are also discussed .
to divide and conquer search ranking by learning query difficulty . <eos> learning to rank plays an important role in information retrieval . in most of the existing solutions for learning to rank , all the queries with their returned search results are learnt and ranked with a single model . in this paper , we demonstrate that it is highly beneficial to divide queries into multiple groups and conquer search ranking based on query difficulty . to this end , we propose a method which first characterizes a query using a variety of features extracted from user search behavior , such as the click entropy , the query reformulation probability . next , a classification model is built on these extracted features to assign a score to represent how difficult a query is . based on this score , our method automatically divides queries into groups , and trains a specific ranking model for each group to conquer search ranking . experimental results on ranksvm and ranknet with a large scale evaluation dataset show that the proposed method can achieve significant improvement in the task of web search ranking .
defect reduction in pcb contract manufacturing operations . <eos> this study addresses the identification and improvement of a defect reducing process step in plated through hole ( pth ) technology of printed circuit board ( pcb ) assemblies . the process step discussed is a step in which the substrates are baked prior to assembly . while this step is developed to address defect problems faced by both oems and contract manufacturers alike , this paper discusses an experiment designed to improve the effect of the baking step that was performed at a pcb contract manufacturing facility . furthermore , due to the tremendous variations in product complexity , a relatively new statistical process control chart which tracks defects per millions of opportunities ( dpmo ) , was used to help evaluate the results . ( c ) <digit> elsevier science ltd .
a new form of dos attack in a cloud and its avoidance mechanism . <eos> data center networks are typically grossly under provisioned . this is not a problem in a corporate data center , but it could be a problem in a shared infrastructure , such as a co location facility or a cloud infrastructure . if an application is deployed in such an infrastructure , the application owners need to take into account the infrastructure limitations . they need to build in counter measures to ensure that the application is secure and it meets its performance requirements . in this paper , we describe a new form of dos attack , which exploits the network under provisioning in a cloud infrastructure . we have verified that such an attack could be carried out in practice in one cloud infrastructure . we also describe a mechanism to detect and avoid this new form of attack .
interdisciplinary applications of mathematical modeling . <eos> we demonstrate applications of numerical integration and visualization algorithms in diverse fields including psychological modeling ( biometrics ) in high energy physics for the study of collisions of elementary particles and in medical physics for regulating the dosage of proton beam radiation therapy . we discuss the problems and solution methods , as supported by numerical results .
the probability ranking principle revisited . <eos> a theoretic framework for multimedia information retrieval is introduced which guarantees optimal retrieval effectiveness . in particular . a ranking principle for distributed multimedia documents ( rpdm ) is described together with an algorithm that satisfies this principle . finally , the rpdm is shown to be a generalization of the probability ranking principle ( prp ) which guarantees optimal retrieval effectiveness in the case of text document retrieval . the prp justifies theoretically the relevance ranking adopted by modern search engines . in contrast to the classical prp . the new rpdm takes into account transmission and inspection time , and most importantly , aspectual recall rather than simple recall .
how users associate wireless devices . <eos> in a wireless world , users can establish connections between devices spontaneously , and unhampered by cables . however , in the absence of cables , what is the natural interaction to connect one device with another a wide range of device association techniques have been demonstrated , but it has remained an open question what actions users would spontaneously choose for device association . we contribute a study eliciting device association actions from non technical users without premeditation . over <digit> user defined actions were collected for <digit> different device combinations . we present a classification of user defined actions , and observations of the users ' rationale . our findings indicate that there is no single most spontaneous action instead five prominent categories of user defined actions were found .
analyticity of weighted central paths and error bounds for semidefinite programming . <eos> the purpose of this paper is two fold . firstly , we show that every cholesky based weighted central path for semidefinite programming is analytic under strict complementarity . this result is applied to homogeneous cone programming to show that the central paths defined by the known class of optimal self concordant barriers are analytic in the presence of strictly complementary solutions . secondly , we consider a sequence of primal dual solutions that lies within a prescribed neighborhood of the central path of a pair of primal dual semidefinite programming problems , and converges to the respective optimal faces . under the additional assumption of strict complementarity , we derive two necessary and sufficient conditions for the sequence of primal dual solutions to converge linearly with their duality gaps .
multi class blue noise sampling . <eos> sampling is a core process for a variety of graphics applications . among existing sampling methods , blue noise sampling remains popular thanks to its spatial uniformity and absence of aliasing artifacts . however , research so far has been mainly focused on blue noise sampling with a single class of samples . this could be insufficient for common natural as well as man made phenomena requiring multiple classes of samples , such as object placement , imaging sensors , and stippling patterns .
a note on the inventory models for deteriorating items with ramp type demand rate . <eos> in this research we study the inventory models for deteriorating items with ramp type demand rate . we first clearly point out some questionable results that appeared in ( mandal , b. , pal , a.k. , <digit> . order level inventory system with ramp type demand rate for deteriorating items . journal of interdisciplinary mathematics <digit> , <digit> and wu , k.s. , ouyang , l.y. , <digit> . a replenishment policy for deteriorating items with ramp type demand rate ( short communication ) . proceedings of national science council roc ( a ) <digit> , <digit> ) . and then resolve the similar problem by offering a rigorous and efficient method to derive the optimal solution . in addition , we also propose an extended inventory model with ramp type demand rate and its optimal feasible solution to amend the incompleteness in the previous work . moreover , we also proposed a very good inventory replenishment policy for this kind of inventory model . we believe that our work will provide a solid foundation for the further study of this sort of important inventory models with ramp type demand rate .
efficient multiple faces tracking based on relevance vector machine and boosting learning . <eos> a multiple faces tracking system was presented based on relevance vector machine ( rvm ) and boosting learning . in this system , a face detector based on boosting learning is used to detect faces at the first frame , and the face motion model and color model are created . the face motion model consists of a set of rvms that learn the relationship between the motion of the face and its appearance , and the face color model is the 2d histogram of the face region in crcb color space . in the tracking process different tracking methods ( rvm tracking , local search , giving up tracking ) are used according to different states of faces , and the states are changed according to the tracking results . when the full image search condition is satisfied , a full image search is started in order to find new coming faces and former occluded faces . in the full image search and local search , the similarity matrix is introduced to help matching faces efficiently . experimental results demonstrate that this system can ( a ) automatically find new coming faces ( b ) recover from occlusion , for example , if the faces are occluded by others and reappear or leave the scene and return ( c ) run with a high computation efficiency , run at about <digit> frames s. ( c ) <digit> elsevier inc. all rights reserved .
a design flow for application specific networks on chip with guaranteed performance to accelerate soc design and verification . <eos> systems on chip ( soc ) are composed of intellectual property blocks ( ip ) and interconnect . while mature tooling exists to design the former , tooling for interconnect design is still a research area . in this paper we describe an operational design flow that generates and configures application specific network on chip ( noc ) instances , given application communication requirements . the noc can be simulated in systemc and rtl vhdl . an independent performance verification tool verifies analytically that the noc instance ( hardware ) and its configuration ( software ) together meet the application performance requirements . the thereal noc 's guaranteed performance is essential to replace time consuming simulation by fast analytical performance validation . as a result , application specific nocs that are guaranteed to meet the application 's communication requirements are generated and verified in minutes , reducing the number of design iterations . a realistic mpeg soc example substantiates our claims .
homing pigeon based messaging multiple pigeon assisted delivery in delay tolerant networks . <eos> in this paper , we consider the applications of delay tolerant networks ( dtns ) , where the nodes in a network are located in separated areas , and in each separated area , there exists ( at least ) an anchor node that provides regional network coverage for the nearby nodes . the anchor nodes are responsible for collecting and distributing messages for the nodes in the vicinity . this work proposes to use a set of messengers ( named pigeons ) that move around the network to deliver messages among multiple anchor nodes . each source node ( anchor node or internet access point ) owns multiple dedicated pigeons , and each pigeon takes a round trip starting from its home ( i.e. , the source ) through the destination anchor nodes and then returns home , disseminating the messages on its way . we named this as a homing pigeon based messaging ( hopm ) scheme . the hopm scheme is different from the prior schemes in that each messenger is completely dedicated to its home node for providing messaging service . we obtained the average message delay of hopm scheme in dtn through theoretical analysis with three different pigeon scheduling schemes . the analytical model was validated by simulations . we also studied the effects of several key parameters on the system performance and compared the results with previous solutions . the results allowed us to better understand the impacts of different scheduling schemes on the system performance of hopm and demonstrated that our proposed scheme outperforms the previous ones . copyright ( c ) <digit> john wiley sons , ltd .
the west nile virus encephalitis outbreak in the united states ( <digit> <digit> ) . <eos> viruses cause most forms of encephalitis . the two main types responsible for epidemic encephalitis are enteroviruses and arboviruses . the city of new york reports about <digit> cases of encephalitis yearly . establishing a diagnosis is often difficult . in august <digit> , a cluster of five patients with fever , confusion , and weakness were admitted to a community hospital in flushing , new york . flaccid paralysis developed in four of the five patients , and they required ventilatory support . three , less severe , cases presented later in the same month . an investigation was conducted by the new york city ( nyc ) and new york state ( nys ) health departments and the national centers for disease control and prevention ( cdc ) . the west nile virus ( wnv ) was identified as the etiologic agent . wnv is an arthropod borne flavivirus , with a geographic distribution in africa , the middle east , and southwestern asia . it has also been isolated in australia and sporadically in europe but never in the americas . the majority of people infected have no symptoms . fever , severe myalgias , headache , conjunctivitis , lymphadenopathy , and a roseolar rash can occur . rarely , encephalitis or meningitis is seen . the nyc outbreak resulted in the first cases of wnv infection in the western hemisphere and the first arboviral infection in nyc since yellow fever in the nineteenth century . the wnv is now a public health concern in the united states .
existence results for impulsive neutral second order stochastic evolution equations with nonlocal conditions . <eos> in this paper we consider a class of impulsive neutral second order stochastic evolution equations with nonlocal initial conditions in a real separable hilbert space . sufficient conditions for the existence of mild solutions are established by operator theory and the sadovskii fixed point theorem . an example is provided to illustrate the theory . ( c ) <digit> elsevier ltd. all rights reserved .
distribution network design new problems and related models . <eos> we study some complex distribution network design problems , which involve facility location , warehousing , transportation and inventory decisions . several realistic scenarios are investigated . two kinds of mathematical programming formulations are proposed for all the introduced problems , together with a proof of their correctness . some formulations extend models proposed by perl and daskin ( <digit> ) for some warehouse location routing problems other formulations are based on flow variables and constraints .
lyapunov based nonlinear controllers for obstacle avoidance with a planar n link doubly nonholonomic manipulator . <eos> a mobile manipulator is a robotic system made up of two components a mobile platform and a manipulator mounted on the platform equipped with non deformable wheels . such a combined system requires complex design and control . this paper considers the autonomous navigation problem of a nonholonomic mobile platform and an n link nonholonomic manipulator fixed to the platform . for this planar n link doubly nonholonomic manipulator , we present the first ever set of nonlinear continuous controllers for obstacle avoidance . the controllers provide a collision free trajectory within a constrained workspace cluttered with fixed obstacles of different shapes and sizes whilst satisfying the nonholonomic and kinodynamic constraints associated with the robotic system . an advantage of the proposed method is the ease at which the acceleration based control laws can be derived from the lyapunov function . the effectiveness of the nonholonomic planner is demonstrated via computer simulations . ( c ) <digit> elsevier b.v. all rights reserved .
automatic analysis of trabecular bone structure from knee mri . <eos> we investigated the feasibility of quantifying osteoarthritis ( oa ) by analysis of the trabecular bone structure in low field knee mri . generic texture features were extracted from the images and subsequently selected by sequential floating forward selection ( sffs ) , following a fully automatic , uncommitted machine learning based framework . six different classifiers were evaluated in cross validation schemes and the results showed that the presence of oa can be quantified by a bone structure marker . the performance of the developed marker reached a generalization area under the roc ( auc ) of 0.82 , which is higher than the established cartilage markers known to relate to the oa diagnosis .
an application of fuzzy sets theory to the eoq model with imperfect quality items . <eos> this article investigates the inventory problem for items received with imperfect quality , where , upon the arrival of order lot , <digit> % screening process is performed and the items of imperfect quality are sold as a single batch at a discounted price , prior to receiving the next shipment . the objective is to determine the optimal order lot size to maximize the total profit . we first propose a model with fuzzy defective rate . then , the model with fuzzy defective rate and fuzzy annual demand is presented . for each case , we employ the signed distance , a ranking method for fuzzy numbers , to find the estimate of total profit per unit time in the fuzzy sense , and then derive the corresponding optimal lot size . numerical examples are provided to illustrate the results of proposed models .
maximin performance of binary input channels with uncertain noise distributions . <eos> we consider uncertainty classes of noise distributions defined by a bound on the divergence with respect to a nominal noise distribution . the noise that maximizes the minimum error probability for binary input channels is found . the effect of the reduction in uncertainty brought about by knowledge of the signal to noise ratio is also studied . the particular class of gaussian nominal distributions provides an analysis tool for near gaussian channels . asymptotic behavior of the least favorable noise distribution and resulting error probability are studied in a variety of scenarios , namely asymptotically small divergence with and without power constraint asymptotically large divergence with and without power constraint and asymptotically large signal to noise ratio .
alleviating the problem of local minima in backpropagation through competitive learning . <eos> the backpropagation ( bp ) algorithm is widely recognized as a powerful tool for training feedforward neural networks ( fnns ) . however , since the algorithm employs the steepest descent technique to adjust the network weights , it suffers from a slow convergence rate and often produces suboptimal solutions , which are the two major drawbacks of bp . this paper proposes a modified bp algorithm which can remarkably alleviate the problem of local minima confronted with by the standard bp ( sbp ) . as one output of the modified training procedure , a bucket of all the possible solutions of weights matrices found during training is acquired , among which the best solution is chosen competitively based upon their performances on a validation dataset . simulations are conducted on four benchmark classification tasks to compare and evaluate the classification performances and generalization capabilities of the proposed modified bp and sbp .
towards categorical models for fairness fully abstract presheaf semantics of sccs with finite delay . <eos> we present a presheaf model for the observation of infinite as well as finite computations . we give a concrete representation of the presheaf model as a category of generalised synchronisation trees and show that it is coreflective in a category of generalised transition systems , which are a special case of the general transition systems of hennessy and stirling . this can be viewed as a first step towards representing fairness in categorical models for concurrency . the open map bisimulation is shown to coincide with extended bisimulation of hennessy and stirling , which is essentially fair ctl bisimulation . we give a denotational semantics of milner 's sccs with finite delay in the presheaf model , which differs from previous semantics by giving the meanings of recursion by final coalgebras and meanings of finite delay by initial algebras of the process equations for delay . finally , we formulate milner 's operational semantics of sccs with finite delay in terms of generalised transition systems and prove that the presheaf semantics is fully abstract with respect to extended bisimulation . ( c ) <digit> published by elsevier science b.v.
determining efficient temperature sets for the simulated tempering method . <eos> in statistical physics , the efficiency of tempering approaches strongly depends on ingredients such as the number of replicas r r , reliable determination of weight factors and the set of used temperatures , tr t1 , t2 , , tr t r t <digit> , t <digit> , , t r . for the simulated tempering ( st ) in particularuseful due to its generality and conceptual simplicitythe latter aspect ( closely related to the actual r r ) may be a key issue in problems displaying metastability and trapping in certain regions of the phase space . to determine tr t r s leading to accurate thermodynamics estimates and still trying to minimize the simulation computational time , here a fixed exchange frequency scheme is considered for the st. from the temperature of interest t1 t <digit> , successive t t s are chosen so that the exchange frequency between any adjacent pair tr t r and tr <digit> t r <digit> has a same value f f . by varying the f f s and analyzing the tr t r s through relatively inexpensive tests ( e.g. , time decay towards the steady regime ) , an optimal situation in which the simulations visit much faster and more uniformly the relevant portions of the phase space is determined . as illustrations , the proposal is applied to three lattice models , beg , belllavis , and potts , in the hard case of extreme first order phase transitions , always giving very good results , even for r <digit> r <digit> . also , comparisons with other protocols ( constant entropy and arithmetic progression ) to choose the set tr t r are undertaken . the fixed exchange frequency method is found to be consistently superior , specially for small r r s. finally , distinct instances where the prescription could be helpful ( in second order transitions and for the parallel tempering approach ) are briefly discussed .
a numerical method for solving variable coefficient elliptic equation with interfaces . <eos> a new 2nd order accurate numerical method on non body fitting grids is proposed for solving the variable coefficient elliptic equation in disjoint subdomains separated by interfaces . the variable coefficients , the source term , and hence the solution itself and its derivatives may be discontinuous across the interfaces . jump conditions in solution and its co normal derivative at interface are prescribed . instead of smooth , the interfaces are only required to be lipschitz continuous as submanifold . a weak formulation is developed , the existence , uniqueness and regularity of the solutions are studied . the numerical method is derived by discretizing the weak formulation . the method is different from traditional finite element methods . extensive numerical experiments are presented and show that the method is 2nd order accurate in solution and 1st order accurate in its gradient in l norm if the interface is c2 and solutions are c2 on the closures of the subdomains . the method can handle the problems when the solutions and or the interfaces are weaker than c2 . for example , u h2 ( ) , is lipschitz continuous and their singularities coincide , see example <digit> in section <digit> . the accuracies of the method under various circumstances are listed in table <digit> .
attitudes of community pharmacists , university based pharmacists , and students toward on line information resources . <eos> the study sought to explore the attitudes of community pharmacists , university based pharmacists , and pharmacy students before and after exposure to computerized systems of on line information services . a <digit> item attitudinal survey was administered to <digit> community pharmacists , <digit> university clinical pharmacist faculty , and <digit> senior pharmacy students , prior to and at the end of a year of access to grateful med ( r ) and brs colleague ( r ) . few significant differences were noted among the participants at baseline . no significant interaction effect differences for type of participant or system used were found . participants were generally positive about computers in general , the accuracy of on line information services , their impact on knowledge and confidence , and their usefulness for pharmacists .
comparison of several approaches to the linear approximation of the yield condition and application to the robust design of plane frames for the case of uncertainty . <eos> since the yield condition for frame structures is non linear , piecewise linear approximations are needed in order to apply linear optimization methods . four approaches are presented and compared . after the theoretical consideration and comparison of the different approximation methods , they are applied to the robust design of an <digit> bar frame in case of uncertainty . here , the less restrictive methods yield the cheapest design , as expected . it will be shown , that the approximation from inside of first level does not cause much higher costs than the other methods . but since its constraints are sufficient in contrast to other approximations , it is recommended .

a class of differential vector variational inequalities in finite dimensional spaces . <eos> in this paper , we introduce and study a class of differential vector variational inequalities in finite dimensional euclidean spaces . we establish a relationship between differential vector variational inequalities and differential scalar variational inequalities . under various conditions , we obtain the existence and linear growth of solutions to the scalar variational inequalities . in particular we prove existence theorems for carathodory weak solutions of the differential vector variational inequalities . furthermore , we give a convergence result on euler time dependent procedure for solving the initial value differential vector variational inequalities .
adaptive hypermedia . <eos> adaptive hypermedia is a relatively new direction of research on the crossroads of hypermedia and user modeling . adaptive hypermedia systems build a model of the goals , preferences and knowledge of each individual user , and use this model throughout the interaction with the user , in order to adapt to the needs of that user . the goal of this paper is to present the state of the art in adaptive hypermedia at the eve of the year <digit> , and to highlight some prospects for the future . this paper attempts to serve both the newcomers and the experts in the area of adaptive hypermedia by building on an earlier comprehensive review ( brusilovsky , <digit> brusilovsky , <digit> ) .
framing design in the third paradigm . <eos> this paper develops vocabulary to discuss the phenomena related to the new design paradigm , which considers designing as a situated and constructive activity of meaning making rather than as problem solving . the paper studies how design projects proceed from the fuzzy early phases towards the issues of central relevance to designing . a central concept is framing , and it is elaborated with examples from two case studies . several aspects of framing are explicated , exploratory , anticipatory and social framing , and related concepts of ' focusing ' , ' priming ' , and ' grounding ' are explained . the paper concludes that understanding designing as a situated and constructive making of meaning has bearings on how designing needs to be supported .
interval evaluations in the analytic hierarchy process by possibility analysis . <eos> since a pairwise comparison matrix in the analytic hierarchy process ( ahp ) is based on human intuition , the given matrix will always include inconsistent elements violating the transitivity property . we propose the interval ai ip by which interval weights can be obtained . the widths of the estimated interval weights represent inconsistency in judging data . since interval weights can be obtained from inconsistent data , the proposed interval ai ip is more appropriate to human judgment . assuming crisp values in a pairwise comparison matrix , the interval comparisons including the given crisp comparisons can be obtained by applying the linear programming ( lp ) approach . using an interval preference relation , the interval ahp for crisp data can be extended to an approach for interval data allowing to express the uncertainty of human judgment in pairwise comparisons .
a model for real time failure prognosis based on hidden markov model and belief rule base . <eos> as one of most important aspects of condition based maintenance ( cbm ) , failure prognosis has attracted an increasing attention with the growing demand for higher operational efficiency and safety in industrial systems . currently there are no effective methods which can predict a hidden failure of a system real time when there exist influences from the changes of environmental factors and there is no such an accurate mathematical model for the system prognosis due to its intrinsic complexity and operating in potentially uncertain environment . therefore , this paper focuses on developing a new hidden markov model ( hmm ) based method which can deal with the problem . although an accurate model between environmental factors and a failure process is difficult to obtain , some expert knowledge can be collected and represented by a belief rule base ( brb ) which is an expert system in fact . as such , combining the hmm with the brb , a new prognosis model is proposed to predict the hidden failure real time even when there are influences from the changes of environmental factors . in the proposed model , the hmm is used to capture the relationships between the hidden failure and monitored observations of a system . the brb is used to model the relationships between the environmental factors and the transition probabilities among the hidden states of the system including the hidden failure , which is the main contribution of this paper . moreover , a recursive algorithm for online updating the prognosis model is developed . an experimental case study is examined to demonstrate the implementation and potential applications of the proposed real time failure prognosis method .
computing the volume of a union of balls a certified algorithm . <eos> balls and spheres are amongst the simplest 3d modeling primitives , and computing the volume of a union of balls is an elementary problem . although a number of strategies addressing this problem have been investigated in several communities , we are not aware of any robust algorithm , and present the first such algorithm . our calculation relies on the decomposition of the volume of the union into convex regions , namely the restrictions of the balls to their regions in the power diagram . theoretically , we establish a formula for the volume of a restriction , based on gauss ' divergence theorem . the proof being constructive , we develop the associated algorithm . on the implementation side , we carefully analyse the predicates and constructions involved in the volume calculation , and present a certified implementation relying on interval arithmetic . the result is certified in the sense that the exact volume belongs to the interval computed . experimental results are presented on hand crafted models illustrating various difficulties , as well as on the 58,898 models found in the tenth of july <digit> release of the protein data bank .
simple polynomial multiplication algorithms for exact conditional tests of linearity in a logistic model . <eos> the linear logistic model is often employed in the analysis of binary response data . the well known asymptotic chi square and likelihood ratio tests are usually used to detect the assumption of linearity in such a model . for small , sparse , or skewed data , the asymptotic theory is however dubious and exact conditional chi square and likelihood ratio tests may provide reliable alternatives . in this article , we propose efficient polynomial multiplication algorithms to compute exact significance levels as well as exact powers of these tests . two options , namely the cell and stage wise approaches , in implementing these algorithms will be discussed . when sample sizes are large , we propose an efficient monte carlo method for estimating the exact significance levels and exact powers . real data are used to demonstrate the performance with an application of the proposed algorithms .
on the definitions of anonymity for ring signatures . <eos> this paper studies the relations among several definitions of anonymity for ring signature schemes in the same attack environment . it is shown that one intuitive and two technical definitions we consider are asymptotically equivalent , and the indistinguishability based technical definition is the strongest , i.e. , the most secure when achieved . when the exact reduction cost is taken into account . we then extend our result to the threshold case where a subset of members cooperate to create a signature . the threshold setting makes the notion of anonymity more complex and yields a greater variety of definitions . we explore several notions and observe certain relation does not seem hold unlike the simple single signer case . nevertheless , we see that an indistinguishability based definition is the most favorable in the threshold case . we also study the notion of linkability and present a simple scheme that achieves both anonymity and linkability .
scalable proximity estimation and link prediction in online social networks . <eos> proximity measures quantify the closeness or similarity between nodes in a social network and form the basis of a range of applications in social sciences , business , information technology , computer networks , and cyber security . it is challenging to estimate proximity measures in online social networks due to their massive scale ( with millions of users ) and dynamic nature ( with hundreds of thousands of new nodes and millions of edges added daily ) . to address this challenge , we develop two novel methods to efficiently and accurately approximate a large family of proximity measures . we also propose a novel incremental update algorithm to enable near real time proximity estimation in highly dynamic social networks . evaluation based on a large amount of real data collected in five popular online social networks shows that our methods are accurate and can easily scale to networks with millions of nodes . to demonstrate the practical values of our techniques , we consider a significant application of proximity estimation link prediction , i.e. , predicting which new edges will be added in the near future based on past snapshots of a social network . our results reveal that ( i ) the effectiveness of different proximity measures for link prediction varies significantly across different online social networks and depends heavily on the fraction of edges contributed by the highest degree nodes , and ( ii ) combining multiple proximity measures consistently yields the best link prediction accuracy .
applications of regional strain energy in compliant structure design for energy absorption . <eos> topology optimization of regional strain energy is studied in this paper . unlike the conventional mean compliance formulation , this paper considers two main functions of structure rigidity and compliance . for normal usages , rigidity is chosen as the design objective . for compliant design , a portion of the structure absorbs energy , while another part maintains the structural integrity . therefore , we implemented a regional strain energy formulation for topology optimization . sensitivity to regional strain energy is derived from the adjoint method . numerical results from the proposed formulation are presented .
conversion of control dependence to data dependence . <eos> program analysis methods , especially those which support automatic vectorization , are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other . powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept allk <digit> , kklw <digit> . the dependence analysis in these systems is based on data dependence . in the presence of complex control flow , data dependence is not sufficient to transform programs because of the introduction of control dependences . a control dependence exists between two statements when the execution of one statement can prevent the execution of the other . control dependences do not fit conveniently into dependence based program translators.one solution is to convert all control dependences to data dependences by eliminating goto statements and introducing logical variables to control the execution of statements in the program . in this scheme , action statements are converted to if statements . the variables in the conditional expression of an if statement can be viewed as inputs to the statement being controlled . the result is that control dependences between statements become explicit data dependences expressed through the definitions and uses of the controlling logical variables.this paper presents a method for systematically converting control dependences to data dependences in this fashion . the algorithms presented here have been implemented in pfc , an experimental vectorizer written at rice university .
an integration scheme for electromagnetic scattering using plane wave edge elements . <eos> finite element techniques for the simulation of electromagnetic wave propagation are , like all conventional element based approaches for wave problems , limited by the ability of the polynomial basis to capture the sinusoidal nature of the solution . the partition of unity method ( pum ) has recently been applied successfully , in finite and boundary element algorithms , to wave propagation . in this paper , we apply the pum approach to the edge finite elements in the solution of maxwells equations . the electric field is expanded in a set of plane waves , the amplitudes of which become the unknowns , allowing each element to span a region containing multiple wavelengths . however , it is well known that , with pum enrichment , the burden of computation shifts from the solver to the evaluation of oscillatory integrals during matrix assembly . a full electromagnetic scattering problem is not simulated or solved in this paper . this paper is an addition to the work of ledger and concentrates on efficient methods of evaluating the oscillatory integrals that arise . a semi analytical scheme of the filon character is presented .
search based metamodel matching with structural and syntactic measures . <eos> metamodel matching using search based software engineering . the use of syntactic measures improve the results of metamodel matching we compared our approach to four ontology based approaches . our results show that our search based approach was significantly better than state of the art matching tools .
a recursion based broadcast paradigm in wormhole routed networks . <eos> a novel broadcast technique for wormhole routed parallel computers based on recursion is presented in this paper . it works by partitioning the interconnection graph into a number of higher level subgraphs . then , we identify the transmission subgraph ( tsg ) in each subgraph . both the higher level subgraphs and the tsgs are recursively defined , i.e. , we split each level i subgraph into several level i <digit> subgraphs and identify level i <digit> tsgs accordingly . we first split and scatter the source message into the tsg of the original graph . next , in each recursive round message transmissions are from lower level tsgs to higher level tsgs and all transmissions at the same level happen concurrently . the algorithm proceeds recursively from lower level subgraphs to higher level subgraphs until each highest level subgraph ( a single node ) gets the complete message . we have applied this general paradigm to a number of topologies including two or higher dimension mesh torus and hypercube . our results show considerable improvements over all other algorithms for a wide range of message sizes under both one port and all port models .
stress analysis of three dimensional contact problems using the boundary element method . <eos> this paper presents a technique based on the boundary element method <digit> to analyse three dimensional contact problems . the formulation is implemented for the frictionless and infinite friction conditions . following a review of the basic nature of contact problems , the analytical basis of the direct formulation of the boundary element method is described . the numerical implementation employs linear triangular elements for the representation of the boundary and variables of the bodies in contact . opposite nodal points in similar element pairs are defined on the two surfaces in the area which are expected to come into contact under the increasing load . the use of appropriate contact conditions enables the integral equations for the two bodies to be coupled together . following an iteration procedure , the size of the contact zone is determined by finding a boundary solution compatible with the contact conditions . different examples have been analysed in order to verify the applicability of the proposed method to various contact situations . the results have been compared with those obtained using the finite element method in conjunction with the abaqus <digit> and ideas <digit> packages which are shown to be in good agreement .
co evolving application code and design models by exploiting meta data . <eos> evolvability and adaptability are intrinsic properties of today 's software applications . unfortunately , the urgency of evolving adapting a system often drives the developer to directly modify the application code neglecting to update its design models . even , most of the development environments support the code refactoring without supporting the refactoring of the design information . refactoring , evolution and in general every change to the code should be reflected into the design models , so that these models consistently represent the application and can be used as documentation in the successive maintenance steps . the code evolution should not evolve only the application code but also its design models . unfortunately , to co evolve the application code and its design is a hard job to be carried out automatically , since there is an evident and notorious gap between these two representations . we propose a new approach to code evolution ( in particular to code refactoring ) that supports the automatic co evolution of the design models . the approach relies on a set of predefined metadata that the developer should use to annotate the application code and to highlight the refactoring performed on the code . then , these meta data are retrieved through reflection and used to automatically and coherently update the application design models .
differential effects of donepezil on methamphetamine and cocaine dependencies . <eos> donepezil , a choline esterase inhibitor , has been widely used as a medicine for alzheimer 's disease . recently , a study showed that donepezil inhibited addictive behaviors induced by cocaine , including cocaine conditioned place preference ( cpp ) and locomotor sensitization to cocaine . in the present study , we investigated the effects of donepezil on methamphetamine ( meth ) induced behavioral changes in mice . in counterbalanced cpp tests , the intraperitoneal ( i.p. ) administration of <digit> mg kg donepezil prior to <digit> mg kg meth i.p. failed to inhibit meth cpp , whereas pretreatment with <digit> mg kg donepezil abolished the cpp for cocaine ( <digit> mg kg , i.p. ) . similarly , in locomotor sensitization experiments , i.p. administration of <digit> mg kg donepezil prior to <digit> mg kg meth i.p. failed to inhibit locomotor sensitivity to meth , whereas pretreatment with <digit> mg kg donepezil significantly inhibited locomotor sensitivity to cocaine ( <digit> mg kg , i.p. ) . these results suggest that donepezil may be a useful tool for treating cocaine dependence but not for treating meth dependence . the differences in the donepezil effects on addictive behaviors induced by meth and cocaine might be due to differences in the involvement of acetylcholine in the mechanisms of meth and cocaine dependencies

a gaussian function model for simulation of complex environmental sensing . <eos> sensors can be used to sense not only simple behavior but also complex ones . previous work has demonstrated how agent based modeling can be used to model sensing of complex behavior in complex environments .
an integer programming based search technique for error prone structures of ldpc codes . <eos> in this paper , an efficient , general framework is presented for finding common , devastating error prone structures ( eps ) of any finite length low density parity check ( ldpc ) code . the smallest stopping set for the binary erasure channel ( bec ) , the smallest fully absorbing set , the smallest absorbing set , and the smallest elementary trapping set for the binary symmetric channel ( bsc ) are found and the dominant eps are enumerated . the method involves integer programming optimization techniques , which guarantees that the results are provably optimal .
chemosensitization of tumors by resveratrol . <eos> because tumors develop resistance to chemotherapeutic agents , the cancer research community continues to search for effective chemosensitizers . one promising possibility is to use dietary agents that sensitize tumors to the chemotherapeutics . in this review , we discuss that the use of resveratrol can sensitize tumor cells to chemotherapeutic agents . the tumors shown to be sensitized by resveratrol include lung carcinoma , acute myeloid leukemia , promyelocytic leukemia , multiple myeloma , prostate cancer , oral epidermoid carcinoma , and pancreatic cancer . the chemotherapeutic agents include vincristine , adriamycin , paclitaxel , doxorubicin , cisplatin , gefitinib , <digit> fluorouracil , velcade , and gemcitabine . the chemosensitization of tumor cells by resveratrol appears to be mediated through its ability to modulate multiple cell signaling molecules , including drug transporters , cell survival proteins , cell proliferative proteins , and members of the nf b and stat3 signaling pathways . interestingly , this nutraceutical has also been reported to suppress apoptosis induced by paclitaxel , vincristine , and daunorubicin in some tumor cells . the potential mechanisms underlying this dual effect are discussed . overall , studies suggest that resveratrol can be used to sensitize tumors to standard cancer chemotherapeutics .
towards scalable summarization of consumer videos via sparse dictionary selection . <eos> the rapid growth of consumer videos requires an effective and efficient content summarization method to provide a user friendly way to manage and browse the huge amount of video data . compared with most previous methods that focus on sports and news videos , the summarization of personal videos is more challenging because of its unconstrained content and the lack of any pre imposed video structures . we formulate video summarization as a novel dictionary selection problem using sparsity consistency , where a dictionary of key frames is selected such that the original video can be best reconstructed from this representative dictionary . an efficient global optimization algorithm is introduced to solve the dictionary selection model with the convergence rates as o ( <digit> root k <digit> ) ( where k is the iteration counter ) , in contrast to traditional sub gradient descent methods of o ( <digit> root k ) . our method provides a scalable solution for both key frame extraction and video skim generation , because one can select an arbitrary number of key frames to represent the original videos . experiments on a human labeled benchmark dataset and comparisons to the state of the art methods demonstrate the advantages of our algorithm .
sufficient completeness verification for conditional and constrained trs . <eos> we present a procedure for checking sufficient completeness of conditional and constrained term rewriting systems containing axioms for constructors which may be constrained ( by e.g. equalities , disequalities , ordering , membership , ... ) . such axioms allow to specify complex data structures like e.g. sets , sorted lists or powerlists . our approach is integrated into a framework for inductive theorem proving based on tree grammars with constraints , a formalism which permits an exact representation of languages of ground constructor terms in normal form . the procedure is presented by an inference system which is shown sound and complete . a precondition of one inference of this system refers to a ( undecidable ) property called strong ground reducibility which is discharged to the above inductive theorem proving system . we have successfully applied our method to several examples , yielding readable proofs and , in case of negative answer , a counter example suggesting how to complete the specification . moreover , we show that it is a decision procedure when the trs is unconditional but constrained , for an expressive class of constrained constructor axioms . ( c ) <digit> elsevier b.v. all rights reserved .
scheduling parallel programs by work stealing with private deques . <eos> work stealing has proven to be an effective method for scheduling parallel programs on multicore computers . to achieve high performance , work stealing distributes tasks between concurrent queues , called deques , which are assigned to each processor . each processor operates on its deque locally except when performing load balancing via steals . unfortunately , concurrent deques suffer from two limitations <digit> ) local deque operations require expensive memory fences in modern weak memory architectures , <digit> ) they can be very difficult to extend to support various optimizations and flexible forms of task distribution strategies needed many applications , e. g. , those that do not fit nicely into the divide and conquer , nested data parallel paradigm . for these reasons , there has been a lot recent interest in implementations of work stealing with non concurrent deques , where deques remain entirely private to each processor and load balancing is performed via message passing . private deques eliminate the need for memory fences from local operations and enable the design and implementation of efficient techniques for reducing task creation overheads and improving task distribution . these advantages , however , come at the cost of communication . it is not known whether work stealing with private deques enjoys the theoretical guarantees of concurrent deques and whether they can be effective in practice . in this paper , we propose two work stealing algorithms with private deques and prove that the algorithms guarantee similar theoretical bounds as work stealing with concurrent deques . for the analysis , we use a probabilistic model and consider a new parameter , the branching depth of the computation . we present an implementation of the algorithm as a c library and show that it compares well to cilk on a range of benchmarks . since our approach relies on private deques , it enables implementing flexible task creation and distribution strategies . as a specific example , we show how to implement task coalescing and steal half strategies , which can be important in fine grain , non divide and conquer algorithms such as graph algorithms , and apply them to the depth first search problem .
multilevel huffman coding an efficient test data compression method for ip cores . <eos> a new test data compression method suitable for cores of unknown structure is introduced in this paper . the proposed method encodes the test data provided by the core vendor using a new , very effective compression scheme based on multilevel huffman coding . each huffman codeword corresponds to three different kinds of information , and thus , significant compression improvements compared to the already known techniques are achieved . a simple architecture is proposed for decoding the compressed data on chip . its hardware overhead is very low and comparable to that of the most efficient methods in the literature . moreover , the major part of the decompressor can be shared among different cores , which reduces the hardware overhead of the proposed architecture considerably . additionally , the proposed technique offers increased probability of detection of unmodeled faults since the majority of the unknown values of the test sets are replaced by pseudorandom data generated by a linear feedback shift register .
variable selection in regression models using nonstandard optimisation of information criteria . <eos> the question of variable selection in a regression model is a major open research topic in econometrics . traditionally two broad classes of methods have been used . one is sequential testing and the other is information criteria . the advent of large datasets used by institutions such as central banks has exacerbated this model selection problem . a solution in the context of information criteria is provided in this paper . the solution rests on the judicious selection of a subset of models for consideration using nonstandard optimisation algorithms for information criterion minimisation . in particular , simulated annealing and genetic algorithms are considered . both a monte carlo study and an empirical forecasting application to uk cpi inflation suggest that the proposed methods are worthy of further consideration .
highly nonlinear photonic crystal fiber with ultrahigh birefringence using a nano scale slot core . <eos> a new type of slot photonic crystal fiber is proposed . ultrahigh nonlinear coefficient up to 3.5739104 w <digit> km <digit> can be achieved for the quasi tm mode . the modal birefringence at 1.55 m is up to 0.5015 . the proposed pcf is suitable for all optical signal processing .
what will system level design be when it grows up . <eos> we have seen a growing new interest in electronic system level ( esl ) architectures , design methods , tools and implementation fabrics in the last few years . but the picture of what types and approaches to building embedded systems will become the most widely accepted norms in the future remains fuzzy at best . everyone want to know where systems and system design is going when it grows up , if it ever grows up . some of the key questions that need to be answered include which applications will be key system drivers , what sw hw architectures will suit best , how programmable and configurable will they be , will systems designers need to deal with physical implementation issues or will that be hidden behind fabric abstractions and programming models , and what will those abstractions and models be moreover , will these abstractions stabilize and be still useful as the underlying technology keeps developing at high speed.this panel consists of proponents of a number of alternative visions for where we will end up , and how we will get there .
the effectiveness of bootstrap methods in evaluating skewed auditing populations a simulation study . <eos> this article describes a comparison among four bootstrap methods the percentile , reflective , bootstrap t , and variance stabilized bootstrap t using a simple new stabilization procedure . the four methods are employed in constructing upper confidence bounds for the mean error in a wide variety of audit populations . the simulation results indicate that the variance stabilized bootstrap t bound is to be preferred . it exhibits reliable coverage while maintaining reasonable tightness .
evaluation of arctic multibeam sonar data quality using nadir crossover error analysis and compilation of a full resolution data product . <eos> characterize uncertainty in multi source multibeam data sets . highest spatial resolution compilation for the canada basin and chukchi borderland . fully resolvable pdf for interpretation of arctic seafloor morphology .
software trace cache for commercial applications . <eos> in this paper we address the important problem of instruction fetch for future wide issue superscalar processors . our approach focuses on understanding the interaction between software and hardware techniques targeting an increase in the instruction fetch bandwidth . that is the objective , for instance , of the hardware trace cache ( htc ) . we design a profile based code reordering technique which targets a maximization of the sequentiality of instructions , while still trying to minimize instruction cache misses . we call our software approach , software trace cache ( stc ) . we evaluate our software approach , and then compare it with the htc and the combination of both techniques . our results on postgresql show that for large codes with few loops and deterministic execution sequences the stc offers better results than a htc . also , both the software and hardware approaches combine well to obtain improved results .
goal state optimization algorithm considering computational resource constraints and uncertainty in task execution time . <eos> a search methodology with goal state optimization considering computational resource constraints is proposed . the combination of an extended graph search methodology and parallelization of task execution and online planning makes it possible to solve the problem . the uncertainty of the task execution time is also considered . the problem can be solved by utilizing a random based and or a greedy based graph searching methodology . the proposed method is evaluated using a rearrangement problem of <digit> movable objects with uncertainty in the task execution time , and the effectiveness is shown with simulation results .
a <digit> mhz cmos quadrature modulator for a gsm transmitter . <eos> this paper describes a <digit> mhz cmos quadrature modulator ( qmod ) fur a global system for mobile communications ( gsm ) transmitter . qmod consists of two attenuators and two doubly balanced modulators ( dbm 's ) and fabricated by using 0.35 mu m cmos process . the carrier leakage level of 35.7 dbc and the image rejection level of 45.1 dbc are achieved . it 's total chip area is <digit> mu m x <digit> mu m and it consumes 1.0 ma with 3.0 v power supply .
mining multi tag association for image tagging . <eos> automatic media tagging plays a critical role in modern tag based media retrieval systems . existing tagging schemes mostly perform tag assignment based on community contributed media resources , where the tags are provided by users interactively . however , such social resources usually contain dirty and incomplete tags , which severely limit the performance of these tagging methods . in this paper , we propose a novel automatic image tagging method aiming to automatically discover more complete tags associated with information importance for test images . given an image dataset , all the near duplicate clusters are discovered . for each near duplicate cluster , all the tags occurring in the cluster form the cluster 's document . given a test image , we firstly initialize the candidate tag set from its near duplicate cluster 's document . the candidate tag set is then expanded by considering the implicit multi tag associations mined from all the clusters ' documents , where each cluster 's document is regarded as a transaction . to further reduce noisy tags , a visual relevance score is also computed for each candidate tag to the test image based on a new tag model . tags with very low scores can be removed from the final tag set . extensive experiments conducted on a real world web image dataset nus wide , demonstrate the promising effectiveness of our approach .
improve the performance of co training by committee with refinement of class probability estimations . <eos> semi supervised learning is a popular machine learning technique where only a small number of labeled examples are available and a large pool of unlabeled examples can be obtained easily . in co training by committee , a paradigm of semi supervised learning , it is necessary to pick out a fixed number of most confident examples according to the ranking of class probability values at each iteration . unfortunately , the class probability values may repeat , which results in the problem that some unlabeled instances share the same probability and will be picked out randomly . this brings a negative effect on the improvement of the performance of classifiers . in this paper , we propose a simple method to deal with this problem under the intuition that different probabilities are crucial . the distance metric between unlabeled instances and labeled instances can be combined with the probabilities of class membership of committee . two distance metrics are considered to assign each unlabeled example a unique probability value . in order to prove that our method can get higher quality examples and reduce the introduction of noise , a data editing technique is used to compare with our method . experimental results verify the effectiveness of our method and the data editing technique , and also confirm that the method for the first distance metric is generally better than the data editing technique .
a unified ransles model computational development , accuracy and cost . <eos> large eddy simulation ( les ) is computationally extremely expensive for the investigation of wall bounded turbulent flows at high reynolds numbers . a way to reduce the computational cost of les by orders of magnitude is to combine les equations with reynolds averaged navierstokes ( rans ) equations used in the near wall region . a large variety of such hybrid ransles methods are currently in use such that there is the question of which hybrid rans les method represents the optimal approach . the properties of an optimal hybrid ransles model are formulated here by taking reference to fundamental properties of fluid flow equations . it is shown that unified ransles models derived from an underlying stochastic turbulence model have the properties of optimal hybrid ransles models . the rest of the paper is organized in two parts . first , a priori and a posteriori analyses of channel flow data are used to find the optimal computational formulation of the theoretically derived unified ransles model and to show that this computational model , which is referred to as linear unified model ( lum ) , does also have all the properties of an optimal hybrid ransles model . second , a posteriori analyses of channel flow data are used to study the accuracy and cost features of the lum . the following conclusions are obtained . ( i ) compared to rans , which require evidence for their predictions , the lum has the significant advantage that the quality of predictions is relatively independent of the rans model applied . ( ii ) compared to les , the significant advantage of the lum is a cost reduction of high reynolds number simulations by a factor of 0.07 re 0.46 . for coarse grids , the lum has a significant accuracy advantage over corresponding les . ( iii ) compared to other usually applied hybrid ransles models , it is shown that the lum provides significantly improved predictions .
on kelly networks with shuffling . <eos> we consider kelly networks with shuffling of customers within each queue . specifically , each arrival , departure or movement of a customer from one queue to another triggers a shuffle of the other customers at each queue . the shuffle distribution may depend on the network state and on the customer that triggers the shuffle . we prove that the stationary distribution of the network state remains the same as without shuffling . in particular , kelly networks with shuffling have the product form . moreover , the insensitivity property is preserved for symmetric queues .
log based receiver reliable multicast for distributed interactive simulation . <eos> reliable multicast communication is important in large scale distributed applications . for example , reliable multicast is used to transmit terrain and environmental updates in distributed simulations . to date , proposed protocols have not supported these applications ' requirements , which include wide area data distribution , low latency packet loss detection and recovery , and minimal data and management over head within fine grained multicast groups , each containing a single data source.in this paper , we introduce the notion of log based receiver reliable multicast ( lbrm ) communication , and we describe and evaluate a collection of log based receiver reliable multicast optimizations that provide an efficient , scalable protocol for high performance simulation applications . we argue that these techniques provide value to a broader range of applications and that the receiver reliable model is an appropriate one for communication in general .
design of relational views over network schemas . <eos> an algorithm is presented for designing relational views over network schemas to ( <digit> ) support general query and update capability , ( <digit> ) preserve the information content of the data base and ( <digit> ) provide independence from its physical organization . the proposed solution is applicable to many existing codasyl databases without data or schema conversion . the particular declarations of a codasyl schema which supply sources of logical data definition are first identified . then the view design algorithm is derived on the basis of a formal analysis of the semantic constraints established by these declarations . a new form of data structure diagram is also introduced to visualize these constraints .
on the coverings by tolerance classes . <eos> a tolerance is a reflexive and symmetric , but not necessarily transitive , binary relation . contrary to what happens with equivalence relations , when dealing with tolerances one must distinguish between blocks ( maximal subsets where the tolerance is a total relation ) and classes ( the class of an element is the set of those elements tolerable with it ) . both blocks and classes of a tolerance on a set define coverings of this set , but not every covering of a set is defined in this way . the characterization of those coverings that are families of blocks of some tolerance has been known for more than a decade now . in this paper we give a characterization of those coverings of a finite set that are families of classes of some tolerance .
boundary conditions control for a shallow water model . <eos> a variational data assimilation technique was used to estimate optimal discretization of interpolation operators and derivatives in the nodes adjacent to the rigid boundary . assimilation of artificially generated observational data in the shallow water model in a square box and assimilation of real observations in the model of the black sea are discussed . it is shown in both experiments that controlling the discretization of operators near a rigid boundary can bring the model solution closer to observations as in the assimilation window and beyond the window . this type of control also allows to improve climatic variability of the model . copyright ( c ) <digit> john wiley sons , ltd .
a simple local smoothing scheme in strongly singular boundary integral representation of potential gradient . <eos> a new approach for computation of potential gradient at and near boundary is introduced . a strongly singular boundary integral representation of potential gradient , whose integral density is the potential gradient , is derived and analysed . applying the concept of the osculating circle , a local smoothing procedure which computes a continuous approximation of potential gradient from the results of a 2d boundary element method ( bem ) analysis using linear elements is proposed and evaluated . this approximation is used in the integral representation derived as an integral density which fulfills the continuity requirements . numerical experiments demonstrate , for quasiuniform meshes , an o ( h2 ) accuracy of potential gradient computed by both the local smoothing procedure on smooth parts of the boundary and by the integral representation on smooth boundary parts and near smooth boundary parts for points inside the domain . a consequence of the latter result is that no significant increase in the error appears near the boundary , boundary layer effect thus being eliminated in this approach .
functional modularity for genetic programming . <eos> in this paper we introduce , formalize , and experimentally validate a novel concept of functional modularity for genetic programming ( gp ) . we rely on module definition that is most natural for gp a piece of program code ( subtree ) . however , as opposed to syntax based approaches that abstract from the actual computation performed by a module , we analyze also its semantic using a set of fitness cases . in particular , the central notion of this approach is subgoal , an entity that embodies module 's desired semantic and is used to evaluate module candidates . as the cardinality of the space of all subgoals is exponential with respect to the number of fitness cases , we introduce monotonicity to assess subgoals ' potential utility for searching for good modules . for a given subgoal and a sample of modules , monotonicity measures the correlation of subgoal 's distance from module 's semantics and the fitness of the solution the module is part of . in the experimental part we demonstrate how these concepts may be used to describe and quantify the modularity of two simple problems of boolean function synthesis . in particular , we conclude that monotonicity usefully differentiates two problems with different nature of modularity , allows us to tell apart the useful subgoals from the other ones , and may be potentially used for problem decomposition and enhance the efficiency of evolutionary search .

answering approximate queries over autonomous web databases . <eos> to deal with the problem of empty or too little answers returned from a web database in response to a user query , this paper proposes a novel approach to provide relevant and ranked query results . based on the user original query , we speculate how much the user cares about each specified attribute and assign a corresponding weight to it . this original query is then rewritten as an approximate query by relaxing the query criteria range . the relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights . for the approximate query results , we generate users ' contextual preferences from database workload and use them to create a priori orders of tuples in an off line preprocessing step . only a few representative orders are saved , each corresponding to a set of contexts . then , these orders and associated contexts are used at query time to expeditiously provide ranked answers . results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user 's preferences effectively . the efficiency and effectiveness of our approach is also demonstrated by experimental result .
stochastic finite learning of the pattern languages . <eos> the present paper proposes a new learning model called stochastic finite learning and shows the whole class of pattern languages to be learnable within this model . this main result is achieved by providing a new and improved average case analysis of the lange wiehagen ( new generation computing , <digit> , <digit> <digit> ) algorithm learning the class of all pattern languages in the limit from positive data . the complexity measure chosen is the total learning time , i.e. , the overall time taken by the algorithm until convergence . the expectation of the total learning time is carefully analyzed and exponentially shrinking tail bounds for it are established for a large class of probability distributions . for every pattern pi containing k different variables it is shown that lange and wiehagen 's algorithm possesses an expected total learning time of o ( ) over cap > alpha ( k ) e lambda log ( <digit> beta ) ( k ) ) , where ) over cap > and beta are two easily computable parameters arising naturally from the underlying probability distributions , and e lambda is the expected example string length . finally , assuming a bit of domain knowledge concerning the underlying class of probability distributions , it is shown how to convert learning in the limit into stochastic finite learning .

probabilistic quantum key distribution . <eos> this work presents a new concept in quantum key distribution called the probabilistic quantum key distribution ( pqkd ) protocol , which is based on the measurement uncertainty in quantum phenomena . it allows two mutually untrusted communicants to negotiate an unpredictable key that has a randomness guaranteed by the laws of quantum mechanics . in contrast to conventional qkd ( e.g. , bb84 ) in which one communicant has to trust the other for key distribution or quantum key agreement ( qka ) in which the communicants have to artificially contribute subkeys to a negotiating key , pqkd is a natural and simple method for distributing a secure random key . the communicants in the illustrated pqkd take einstein podolsky rosen ( epr ) pairs as quantum resources and then use entanglement swapping and bell measurements to negotiate an unpredictable key .
an experiment with reflective middleware to support grid based flood monitoring . <eos> flooding is a growing problem , which affects more than <digit> % of the u.k. population . the cost of damage caused by flooding correlates closely with the warning time given before a flood event , making flood monitoring and prediction critical to minimizing the cost of flood damage . this paper describes a wireless sensor network ( wsn ) for flood warning , which is capable of not only integrating with remote fixed network grids for computationally intensive flood modelling purposes but also performing on site grid computation . this functionality is supported by the reflective and component based gridkit middleware , which provides support for both wsn and grid application domains . copyright ( c ) <digit> john wiley sons , ltd .
rate control for delay sensitive traffic in multihop wireless networks . <eos> we propose two multipath rate control algorithms that guarantee bounded end to end delay in multihop wireless networks . our work extends the previous research on optimal rate control and scheduling in multihop wireless networks , to support inelastic delay requirements . using the relationship between dual variables and packet delay , we develop two alternative solutions that are independent from any queuing model assumption , contrary to the previous research . in the first solution , we derive lower bounds on source rates that achieve the required delay bounds . we then develop a distributed algorithm comprising scheduling and rate control functions , which requires each source to primarily check the feasibility of its qos before initiating its session . in the second solution we eliminate the admission control phase by developing an algorithm that converges to the utility function weights that ensure the required delay bounds for all flows . both solutions carry out scheduling at slower timescale than rate control , and consequently are more efficient than previous cross layer algorithms . we show through numerical examples that even when there are no delay constraints , the proposed algorithms significantly reduce the delay compared to the previous solutions .
optimality of klt for high rate transform coding of gaussian vector scale mixtures application to reconstruction , estimation , and classification . <eos> the karhunen loeve transform ( klt ) is known to be optimal for high rate transform coding of gaussian vectors for both fixed rate and variable rate encoding . the klt is also known to be suboptimal for some non gaussian models . this paper proves high rate optimality of the klt for variable rate encoding of a broad class of non gaussian vectors gaussian vector scale mixtures ( gvsm ) , which extend the gaussian scale mixture ( gsm ) model of natural signals . a key concavity property of the scalar gsm ( same as the scalar gvsm ) is derived to complete the proof . optimality holds under a broad class of quadratic criteria , which include mean squared error ( mse ) as well as generalized f divergence loss in estimation and binary classification systems . finally , the theory is illustrated using two applications signal estimation in multiplicative noise and joint optimization of classification reconstruction systems .
the norepinephrine transporter and pheochromocytoma . <eos> pheochromocytomas are rare neuroendocrine tumors of chromaffin cell origin that synthesize and secrete excess quantities of catecholamines and other vasoactive peptides . pheochromocytomas also express the norepinephrine transporter ( net ) , a molecule that is used clinically as a means of incorporating radiolabelled substrates such as 131i mibg ( iodo metaiodobenzylguanidine ) into pheochromocytoma tumor cells . this allows the diagnostic localization of these tumors and , more recently , 131i mibg has been used in trials in the treatment of pheochromocytoma , potentially giving rise to net as a therapeutic target . however , because of varying levels or activities of the transporter , the ability of 131i mibg to be consistently incorporated into tumor cells is limited , and therefore various strategies to increase net functional activity are being investigated , including the use of traditional chemotherapeutic agents such as cisplatin or doxorubicin . other aspects of net discussed in this short review include the regulation of the transporter and how novel proteinprotein interactions between net and structures such as syntaxin 1a may hold the key to innovative ways to increase the therapeutic value of 131i mibg
metaeasy a meta analysis add in for microsoft excel . <eos> meta analysis is a statistical methodology that combines or integrates the results of several independent clinical trials considered by the analyst to be ' combinable ' ( huque <digit> ) . however , completeness and user friendliness are uncommon both in specialised meta analysis software packages and in mainstream statistical packages that have to rely on user written commands . we implemented the meta analysis methodology in a microsoft excel add in which is freely available and incorporates more meta analysis models ( including the iterative maximum likelihood and profile likelihood ) than are usually available , while paying particular attention to the user friendliness of the package .
adaptive data collection strategies for lifetime constrained wireless sensor networks . <eos> communication is a primary source of energy consumption in wireless sensor networks . due to resource constraints , the sensor nodes may not have enough energy to report every reading to the base station over a required network lifetime . this paper investigates data collection strategies in lifetime constrained wireless sensor networks . our objective is to maximize the accuracy of data collected by the base station over the network lifetime . instead of sending sensor readings periodically , the relative importance of the readings is considered in data collection the sensor nodes send data updates to the base station when the new readings differ more substantially from the previous ones . we analyze the optimal update strategy and develop adaptive update strategies for both individual and aggregate data collections . we also present two methods to cope with message losses in wireless transmission . to make full use of the energy budgets , we design an algorithm to allocate the numbers of updates allowed to be sent by the sensor nodes based on their topological relations . experimental results using real data traces show that , compared with the periodic strategy , adaptive strategies significantly improve the accuracy of data collected by the base station .
image fusion based contrast enhancement . <eos> the goal of contrast enhancement is to improve visibility of image details without introducing unrealistic visual appearances and or unwanted artefacts . while global contrast enhancement techniques enhance the overall contrast , their dependences on the global content of the image limit their ability to enhance local details . they also result in significant change in image brightness and introduce saturation artefacts . local enhancement methods , on the other hand , improve image details but can produce block discontinuities , noise amplification and unnatural image modifications . to remedy these shortcomings , this article presents a fusion based contrast enhancement technique which integrates information to overcome the limitations of different contrast enhancement algorithms . the proposed method balances the requirement of local and global contrast enhancements and a faithful representation of the original image appearance , an objective that is difficult to achieve using traditional enhancement methods . fusion is performed in a multi resolution fashion using laplacian pyramid decomposition to account for the multi channel properties of the human visual system . for this purpose , metrics are defined for contrast , image brightness and saturation . the performance of the proposed method is evaluated using visual assessment and quantitative measures for contrast , luminance and saturation . the results show the efficiency of the method in enhancing details without affecting the colour balance or introducing saturation artefacts and illustrate the usefulness of fusion techniques for image enhancement applications .
reachability analysis for uncertain ssps . <eos> stochastic shortest path problems ( ssps ) can be efficiently dealt with by the real time dynamic programming algorithm ( rtdp ) . yet , rtdp requires that a goal state is always reachable . this article presents an algorithm checking for goal reachability , especially in the complex case of an uncertain ssp where only a possible interval is known for each transition probability . this gives an analysis method for determining if ssp algorithms such as rtdp are applicable , even if the exact model is not known . as this is a time consuming algorithm , we also present a simple process that often speeds it up dramatically . yet , the main improvement still needed is to turn to a symbolic analysis in order to avoid a complete state space enumeration .
embodiment in brain computer interaction . <eos> with emerging opportunities for using brain computer interaction ( bci ) in gaming applications , there is a need to understand the opportunities and constraints of this interaction paradigm . to complement existing laboratory based studies , there is also a call for the study of bci in real world contexts . in this paper we present such a real world study of a simple bci game called mindflex , played as a social activity in the home . in particular , drawing on the philosophical traditions of embodied interaction , we highlight the importance of considering the body in bci and not simply what is going on in the head . the study shows how people use bodily actions to facilitate control of brain activity but also to make their actions and intentions visible to , and interpretable by , others playing and watching the game . it is the public availability of these bodily actions during bci that allows action to be socially organised , understood and coordinated with others and through which social relationships can be played out . we discuss the implications of this perspective and findings for bci .
formally measuring agreement and disagreement in ontologies . <eos> ontologies are conceptual models of particular domains , and domains can be modeled differently , representing different opinions , beliefs or perspectives . in other terms , ontologies may disagree with some particular pieces of information and among themselves . assessing such agreements and disagreements is very useful in a variety of scenarios , in particular when integrating external elements of information into existing ones . in this paper , we present a set of measures to evaluate the agreement and disagreement of an ontology with a statement or with other ontologies . while our work goes beyond the naive approach of checking for logical inconsistencies , it relies on a complete formal framework based on the semantics of the considered ontologies . the experiments realized on several concrete scenarios show the validity of our approach and the usefulness of measuring agreement and disagreement in ontologies .
minimal realizations of linear systems the shortest basis approach . <eos> given a discrete time linear system c , a shortest basis for is a set of linearly independent generators for c with the least possible lengths . a basis b is a shortest basis if and only if it has the predictable span property ( i.e. , has the predictable delay and degree properties , and is non catastrophic ) , or alternatively if and only if it has the subsystem basis property ( for any interval j , the generators in b whose span is in j is a basis for the subsystem c ( j ) ) . the dimensions of the minimal state spaces and minimal transition spaces of c are simply the numbers of generators in a shortest basis b that are active at any given state or symbol time , respectively . a minimal linear realization for c in controller canonical form follows directly from a shortest basis for c , and a minimal linear realization for c in observer canonical form follows directly from a shortest basis for the orthogonal system c ( perpendicular to ) . this approach seems conceptually simpler than that of classical minimal realization theory .
a low latency multi layer prefix grouping technique for parallel huffman decoding of multimedia standards . <eos> huffman coding is a popular and important lossless compression scheme for various multimedia applications . this paper presents a low latency parallel huffman decoding technique with efficient memory usage for multimedia standards . first , the multi layer prefix grouping technique is proposed for sub group partition . it exploits the prefix characteristic in huffman codewords to solve the problem of table size explosion . second , a two level table lookup approach is introduced which can promptly branch to the correct sub group by level <digit> table lookup and decode the symbols by level <digit> table lookup . third , two optimization approaches are developed one is to reduce the branch cycles and the other is parallel processing between two level table lookup and direct table lookup approaches to fully utilize the advantage of vliw parallel processing . an aac huffman decoding example is realized on the parallel architecture core dsp ( pac dsp ) processor . the simulation results show that the proposed method can further improve about <digit> % of decoding cycles and <digit> % of table size comparing to the linear search method .
analysis and numerical simulation of strong discontinuities in finite strain poroplasticity . <eos> this paper presents an analysis of strong discontinuities in coupled poroplastic media in the finite deformation range . a multi scale framework is developed for the characterization of these solutions involving a discontinuous deformation ( or displacement ) field in this coupled setting . the strong discontinuities are used as a tool for the modeling of the localized dissipative effects characteristic of the localized failures of typical poroplastic systems . this is accomplished through the inclusion of a cohesive frictional law relating the resolved stresses on the discontinuity and the accumulated fluid content on it with the displacement and fluid flow jumps across the discontinuity surface . the formulation considers the limit of vanishing small scales , hence recovering a problem in the large scale involving the usual regular displacement and pore pressure variables , while capturing correctly these localized dissipative mechanisms . all the couplings between the mechanical and fluid problems , from the modeling of the solid 's response through effective stresses and tractions to the geometric coupling consequence of the assumed finite deformation setting , are taken into account in these considerations . the multi scale structure of the theoretical formulation is fully employed in the development of new enhanced strain finite elements to capture these discontinuous solutions with no regularization of the singular fields appearing in the formulation . several numerical simulations are presented showing the properties and performance of the proposed localized models and the enhanced finite elements used in their numerical implementation .
toward real noon state sources . <eos> path entangled n photon systems described by noon states are the main ingredient of many quantum information and quantum imaging protocols . our analysis aims to lead the way toward the implementation of both noon state sources and their applications . to this end , we study the functionality of real noon state sources by quantifying the effect real experimental apparatuses have on the actual generation of the desired noon state . in particular , since the conditional generation of noon states strongly relies on photon counters , we evaluate the dependence of both the reliability and the signal to noise ratio of real noon state sources on detection losses . we find a surprising result noon state sources relying on nondetection are much more reliable than noon state sources relying on single photon detection . also the comparison of the resources required to implement these two protocols comes out to be in favor of noon state sources based on nondetection . a scheme to improve the performances of real noon state sources based on single photon detection is also proposed and analyzed .
using tpack as a framework to understand teacher candidates ' technology integration decisions . <eos> this research uses the technological pedagogical and content knowledge ( tpack ) framework as a lens for understanding how teacher candidates make decisions about the use of information and communication technology in their teaching . pre and post treatment assessments required elementary teacher candidates at brigham young university to articulate how and why they would integrate technology in three content teaching design tasks . researchers identified themes from student rationales that mapped to the tpack constructs . rationales simultaneously supported subcategories of knowledge that could be helpful to other researchers trying to understand and measure tpack . the research showed significant student growth in the use of rationales grounded in content specific knowledge and general pedagogical knowledge , while rationales related to general technological knowledge remained constant .
exploiting temporal coherence in global illumination . <eos> producing high quality animations featuring rich object appearance and compelling lighting effects is very time consuming using traditional frame by frame rendering systems . in this paper we present a number of global illumination and rendering solutions that exploit temporal coherence in lighting distribution for subsequent frames to improve the computation performance and overall animation quality . our strategy relies on extending into temporal domain well known global illumination techniques such as density estimation photon tracing , photon mapping , and bi directional path tracing , which were originally designed to handle static scenes only .
effectiveness of cognitive load based adaptive instruction in genetics education . <eos> research addressing the issue of instructional control in computer assisted instruction has revealed mixed results . prior knowledge level seems to play a mediating role in the students ability to effectively use given instructional control . this study examined the effects of three types of instructional control ( non adaptive program control , learner control , adaptive program control ) and prior knowledge ( high school , 1st year and 2nd year college students ) on effectiveness and efficiency of learning in a genetics training program . the results revealed that adaptive program control led to highest training performance but not to superior post test or far transfer performance . furthermore , adaptive program control proved to be more efficient in terms of learning outcomes of the test phase than the other two instructional control types . college students outperformed the high school students on all aspects of the study thereby strengthening the importance of prior knowledge in learning effectiveness and efficiency . lastly , the interaction effects showed that for each prior knowledge level different levels of support were beneficial to learning .
sub pixel mapping based on artificial immune systems for remote sensing imagery . <eos> we propose an artificial immune sub pixel mapping framework for remote sensing imagery . the sub pixel mapping problem is transformed to an optimization problem . the proposed algorithm can obtain better sub pixel mapping results by immune operators . experimental results demonstrate that the proposed approach outperforms the previous methods .
modeling of the quenching of blast products from energetic materials by expansion into vacuum . <eos> condensed phase energetic materials include propellants and explosives . their detonation or burning products generate dense , high pressure states that are often adjacent to regions that are at vacuum or near vacuum conditions . an important chemical diagnostic experiment is the time of flight mass spectroscopy experiment that initiates an energetic material sample via an impact from a flyer plate , whose products expand into a vacuum . the rapid expansion quenches the reaction in the products so that the products can be differentiated by molecular weight detection as they stream past a detector . analysis of this experiment requires a gas dynamic simulation of the products of a reacting multi component gas that flows into a vacuum region . extreme computational difficulties can arise if flow near the vacuum interface is not carefully and accurately computed . we modify an algorithm proposed by munz <digit> , that computed the fluxes appropriate to a gasvacuum interface for an inert ideal gas , and extend it to a multi component mixture of reacting chemical components reactions with general , non ideal equations of state . we illustrate how to incorporate that extension in the context of a complete set of algorithms for a general , cell based flow solver . a key step is to use the local exact solution for an isentropic expansion fan , for the mixture that connects the computed flow states to the vacuum . regularity conditions ( i.e. the liusmoller conditions ) are necessary conditions that must be imposed on the equation of state of the multicomponent fluid in the limit of a vacuum state . we show that the jones , wilkins , lee ( jwl ) equation of state meets these requirements .
modeling multiple event situations across news articles . <eos> readers interested in the context of an event covered in the news such as the dismissal of a lawsuit can benefit from easily finding out about the overall news situation , the legal trial , of which the event is a part . guided by abstract models of news situation types such as legal trials , corporate acquisitions , and kidnappings , brussell is a system that presents situation instances it creates by reading multiple articles about the specific events that comprise them . we discuss how these situation models are structured and how they drive the creation of particular instances .
model selection for least squares support vector regressions based on small world strategy . <eos> model selection plays a key role in the application of support vector machine ( svm ) . in this paper , a method of model selection based on the small world strategy is proposed for least squares support vector regression ( ls svr ) . in this method , the model selection is treated as a single objective global optimization problem in which generalization performance measure performs as fitness function . to get better optimization performance , the main idea of depending more heavily on dense local connections in small world phenomenon is considered , and a new small world optimization algorithm based on tabu search , called the tabu based small world optimization ( tswo ) , is proposed by employing tabu search to construct local search operator . therefore , the hyper parameters with best generalization performance can be chosen as the global optimum based on the powerful search ability of tswo . experiments on six complex multimodal functions are conducted , demonstrating that tswo performs better in avoiding premature of the population in comparison with the genetic algorithm ( ga ) and particle swarm optimization ( pso ) . moreover , the effectiveness of leave one out bound of ls svm on regression problems is tested on noisy sinc function and benchmark data sets , and the numerical results show that the model selection using tswo can almost obtain smaller generalization errors than using ga and pso with three generalization performance measures adopted .
a model of seepage field in the tailings dam considering the chemical clogging process . <eos> the radial collector well , an important water drainage construction , has been widely applied to the tailings dam . chemical clogging frequently occurs around the vertical shaft in radial collector well due to enough dissolved oxygen and some heavy metals in groundwater flow of tailings dam . considering the contribution of water discharge from both vertical shaft and horizontal screen laterals and chemical clogging occurring around vertical shaft well , a new model was developed on the basis of multi node well ( mnw2 ) package of modflow . moreover , two cases were calculated by the newly developed model . the results indicate that the model considering chemical clogging occurring around the vertical shaft well is reasonable . owing to the decrease in hydraulic conductivity caused by chemical clogging , the groundwater level in dam body increases constantly and water discharge of radial collector well declines by <digit> % . for ordinary vertical well , it decreases by <digit> % . therefore , chemical clogging occurring around radial collector well can arouse increases of groundwater level , and influence dambody safety .
a symmetrisation method for non associated unified hardening model . <eos> this paper presents a simple method for symmetrising the asymmetric elastoplastic matrix arising from non associated flow rules . the symmetrisation is based on mathematical transformation and does not alter the incremental stressstrain relationship . the resulting stress increment is identical to that obtained using the original asymmetrized elastoplastic matrix . the symmetrisation method is applied to integrate the unified hardening ( uh ) model where the elastoplastic matrix is asymmetric due to stress transformation . the performance of the method is verified through finite element analysis ( fea ) of boundary value problems such as triaxial extension tests and bearing capacity of foundations . it is found that the symmetrisation method can improve the convergence of the fea and reduce computational time significantly for non associated elastoplastic models .
rofl routing on flat labels . <eos> it is accepted wisdom that the current internet architecture conflates network locations and host identities , but there is no agreement on how a future architecture should distinguish the two . one could sidestep this quandary by routing directly on host identities themselves , and eliminating the need for network layer protocols to include any mention of network location . the key to achieving this is the ability to route on flat labels . in this paper we take an initial stab at this challenge , proposing and analyzing our rofl routing algorithm . while its scaling and efficiency properties are far from ideal , our results suggest that the idea of routing on flat labels can not be immediately dismissed .
a branch and cut approach for a generic multiple product , assembly system design problem . <eos> this paper presents two new models to deal with different tooling requirements in the generic multiple product assembly system design ( mpasd ) problem and proposes a new branch and cut solution approach , which adds cuts at each node in the search tree . it employs the facet generation procedure ( fgp ) to generate facets of underlying knapsack polytopes . in addition , it uses the fgp in a new way to generate additional cuts and incorporates two new methods that exploit special structures of the mpasd problem to generate cuts . one new method is based on a principle that can be applied to solve generic <digit> <digit> problems by exploiting embedded integral polytopes . the approach includes new heuristic and pre processing methods , which are applied at the root node to manage the size of each instance . this paper establishes benchmarks for mpasd through an experiment in which the approach outperformed ibm 's optimization subroutine library ( osl ) , a commercially available solver .
managing cognitive and mixed motive conflicts in concurrent engineering . <eos> in collaborative activities such as concurrent engineering ( ge ) , conflicts arise due to differences in goals , information available , and the understanding of the task . such conflicts can be categorized into two types mixed motive and cognitive . mixed motive conflicts are essentially due to interest differentials among stakeholders . cognitive conflicts can occur even when the stakeholders do not differ in their respective utilities , but simply because they offer multiple cognitive perspectives on the problem . because conflicts in ce occur under a wider context of cooperative problem solving , the imperative for solving conflicts in such situations is strong . this paper argues that mechanisms for managing conflicts in ce should bear a strong conceptual mapping with the nature of the underlying conflict . moreover , since ce activities are performed in collaborative settings , such mechanisms should accommodate information processing at multiple referent levels . we discuss the nature of both types of conflicts and the requirements of mechanisms for managing them , the functionalities of an implementation that addresses these requirements are illustrated through an example of a ce task
designing robust emergency medical service via stochastic programming . <eos> this paper addresses the problem of designing robust emergency medical services . under this respect , the main issue to consider is the inherent uncertainty which characterizes real life situations . several approaches can be used to design robust mathematical models which are able to hedge uncertain conditions . we are using here the stochastic programming framework and , in particular , the probabilistic paradigm . more specifically , we develop a stochastic programming model with probabilistic constraints aimed to solve both the location and the dimensioning problems , i.e. where service sites must be located and how many emergency vehicles must be assigned to each site , in order to achieve a reliable level of service and minimize the overall costs . in doing so , we consider the randomness of the system as far as the demand of emergency service is concerned . the numerical results , which have been collected on a large set of test problems , demonstrate the validity of the proposed model , particularly in dealing with the trade off between quality of service and costs management .
recommendation of optimized information seeking process based on the similarity of user access behavior patterns . <eos> differing from many studies of recommendation that provided the final results directly , our study focuses on providing an optimized process of information seeking to users . based on process mining , we propose an integrated adaptive framework to support and facilitate individualized recommendation based on the gradual adaptation model that gradually adapts to a target users transition of needs and behaviors of information access , including various search related activities , over different time spans . in detail , successful information seeking processes are extracted from the information seeking histories of users . furthermore , these successful information seeking processes are optimized as a series of action units to support the target users whose information access behavior patterns are similar to the reference users . based on these , the optimized information seeking processes are navigated to the target users according to their transitions of interest focus . in addition to describing some definitions and measures introduced , we go further to present an optimized process recommendation model and show the system architecture . finally , we discuss the simulation and scenario for the proposed system .
analytical mechanics solution for mechanism motion and elastic deformation hybrid problem of beam system . <eos> based on the dynamics of flexible multi body systems and finite element method , a beam system dynamics model is built for solving motiondeformation mixed problem and tracing the whole process of mechanism motion . kinetic control equation and constraint equation in which , mechanism motion and elastic deformation is described using hybrid coordinates , and the spatial position matrix of the element is described using euler quaternion are derived . numerical examples show that the method can trace and solve the track and internal force of the system .
availability analysis of shared backup path protection under multiple link failure scenario in wdm networks . <eos> dedicated protection and shared protection are the main protection schemes in optical wavelength division multiplexing ( wdm ) networks . shared protection techniques surpass the dedicated protection techniques by providing the same level of availability as dedicated protection with reduced spare capacity . satisfying the service availability levels defined by the users service level agreement ( sla ) in a cost effective and resource efficient way is a major challenge for networks operators . hence , evaluating the availability of the shared protection scheme has a great interest . we recently developed an analytical model to estimate network availability of a wdm network with shared link connections under multiple link failures . however , this model requires the information of all possible combinations of the unshared protection paths , which is somehow troublesome . in this paper , we propose a more practical analytical model for evaluating the availability of a wdm network with shared link connections under multiple link failures . the proposed model requires only an estimate of the set of shared paths of each protection path . the estimated availability of the proposed model accurately matched with that of the previous model . finally , we compare the previous model with the proposed model to demonstrate the merits and demerits of both models illustrating the threshold at which each model performs better based on the computational complexity . the proposed model significantly contributes to the related areas by providing network operators with a practical tool to evaluate quantitatively the system availability and , thus , the expected survivability degree of wdm optical networks with shared connections under multiple link failures .
evolving rbf neural networks for time series forecasting with evrbf . <eos> this paper is focused on determining the parameters of radial basis function neural networks ( number of neurons , and their respective centers and radii ) automatically . while this task is often done by hand , or based in hillclimbing methods which are highly dependent on initial values , in this work , evolutionary algorithms are used to automatically build a radial basis function neural networks ( rbf nn ) that solves a specified problem , in this case related to currency exchange rates forecasting . the evolutionary algorithm evrbf has been implemented using the evolutionary computation framework evolving object , which allows direct evolution of problem solutions . thus no internal representation is needed , and specific solution domain knowledge can be used to construct specific evolutionary operators , as well as cost or fitness functions . results obtained are compared with existent bibliography , showing an improvement over the published methods .
modified centralized rocof based load shedding scheme in an islanded distribution network . <eos> two new centralized adaptive under frequency load shedding methods are proposed . dg units operation and loads willing to pay ( wtp ) are considered . the objective is to minimize the resulting penalties of the load shedding .
horn ok please . <eos> road congestion is a common problem worldwide . existing intelligent transport systems ( its ) are mostly inapplicable in developing regions due to high cost and assumptions of orderly traffic . in this work , we develop a low cost technique to estimate vehicular speed , based on vehicular honks . honks are a characteristic feature of the chaotic road conditions common in many developing regions like india and south east asia . we envision a system where dynamic road traffic information is learnt using inexpensive , wireless enabled on road sensors . subsequent analyzed information can then be sent to mobile road users this would fit well with the burgeoning mobile market in developing regions . the core of our technique comprises a pair of road side acoustic sensors , separated by a distance . if a moving vehicle honks between the two sensors , its speed can be estimated from the doppler shift of the honk frequency . in this context , we have developed algorithms for honk detection , honk matching across sensors , and speed estimation . based on the speed estimates , we subsequently detect road congestion . we have done extensive experiments in semi controlled settings as well as real road scenarios under different traffic conditions . using over <digit> hours of road side recordings , we show that our speed estimation technique is effective in real conditions . further , we use our data to characterize traffic state as free flowing versus congested using a variety of metrics the vehicle speed distribution , the number and duration of honks . our results show clear statistical divergence of congested versus free flowing traffic states , and a threshold based classification accuracy of <digit> <digit> % in most situations .
triangular mesh offset for generalized cutter . <eos> in <digit> axis nc ( numerical control ) machining , various cutters are used and the offset compensation for these cutters is important for a gouge free tool path generation . this paper introduces triangular mesh offset method for a generalized cutter defined based on the apt ( automatically programmed tools ) definition or parametric curve . an offset vector is computed according to the geometry of a cutter and the normal vector of a part surface . a triangular mesh is offset to the cl ( cutter location ) surface by multiple normal vectors of a vertex and the offset vector computation method . a tool path for a generalized cutter is generated on the cl surface , and the machining test shows that the proposed offset method is useful for the nc machining .
the new fifa rules are hard complexity aspects of sports competitions . <eos> consider a soccer competition among various teams playing against each other in pairs ( matches ) according to a previously determined schedule . at some stage of the competition one may ask whether a particular team still has a ( theoretical ) chance to win the competition . the complexity of this question depends on the way scores are allocated according to the outcome of a match . for example , the problem is polynomially solvable for the ancient fifa rules ( resp . ) but becomes np hard if the new rules ( resp . ) are applied . we determine the complexity of the above problem for all possible score allocation rules .
a new segmentation method for phase change thermography sequence . <eos> a new segmentation method for image sequence is proposed in order to get the isotherm from phase change thermography sequence ( pcts ) . firstly , the pcts is transformed into a series of synthesized images by compression and conversion , so the isotherm extraction can be transformed into the segmentation of a series of synthesized images . secondly , a virtual illumination model is constructed to eliminate the glisten of the aerocraft model . in order to get the parameters of virtual illumination model , a coordination optimization method is employed and all parameters are obtained according to the similarity constraint . finally , the proving isotherms are gained after the threshold coefficients are compensated . the eventual results demonstrate the efficiency of the proposed segmentation method .
multi color continuous variable entangled optical beams generated by nopos . <eos> we propose an alternative scalable way to generate multi color entangled optical beams efficiently utilizing the tripartite entanglement existent between three fieldssignal , idler , and pumpfrom a nondegenerate optical parametric oscillator ( nopo ) operating above the threshold . the special case of two cascaded nopos is studied , as it is shown that the five beams with very different frequencies are generated by nopoa ( one of the retained signal and idler beams , and the reflected pump beam ) and nopob ( the output signal and idler beams , and the reflected pump beam ) . these beams are theoretically demonstrated to be continuous variable ( cv ) entangled with each other by applying the positivity of the partially transposed criterion for the inseparability of multipartite cv entanglement . the symplectic eigenvalues of the partial transposition covariance matrix of the obtained optical entangled state are numerically calculated in terms of experimentally reachable system parameters . the optimal operation conditions to achieve high five color entanglement are presented . as the cavity parameters and the nonlinear crystals of the two nopos can be chosen freely , the frequencies of the submodes in the entangled state thus are adjustable to match the transition frequencies of atoms or low loss fiber optic communication window . the calculated results provide direct references for future experiment to generate multi color entangled optical beams efficiently by means of nopos operating above the threshold .
bched energy balanced sub round local topology management for wireless sensor network . <eos> topology controlling based on cluster structure is an important method to improve the energy efficiency of wireless sensor network ( wsn ) systems . frequent clustering process of classical controlling methods , such as leach , is apt to cause serious energy consuming . some improved methods reduced re clustering frequency , but these methods sometimes lead to energy imbalance in the stable communication period . in this paper , a hierarchical topology controlling method bched is proposed . with double round clustering mechanism , bched activates a local re clustering process between two rounds of data transmission , and with optional cluster head exchanging mechanism , bched reorganize the node clusters according to their residual energy distribution . experimental results show that , with bched , the energy balance performance of wsn system is significantly improved , and the system lifetime can be effectively extended .
hierarchical reconstruction for discontinuous galerkin methods on unstructured grids with a weno type linear reconstruction and partial neighboring cells . <eos> the hierarchical reconstruction ( hr ) y. j. liu , c. w. shu , e. tadmor , m. p. zhang , central discontinuous galerkin methods on overlapping cells with a non oscillatory hierarchical reconstruction , siam j. numer . anal . <digit> ( <digit> ) <digit> <digit> is applied to the piecewise quadratic discontinuous galerkin method on two dimensional unstructured triangular grids . a variety of limiter functions have been explored in the construction of piecewise linear polynomials in every hierarchical reconstruction stage . we show that on triangular grids , the use of center biased limiter functions is essential in order to recover the desired order of accuracy . several new techniques have been developed in the paper ( a ) we develop a weno type linear reconstruction in each hierarchical level , which solves the accuracy degeneracy problem of previous limiter functions and is essentially independent of the local mesh structure ( b ) we find that hr using partial neighboring cells significantly reduces over under shoots , and further improves the resolution of the numerical solutions . the method is compact and therefore easy to implement . numerical computations for scalar and systems of nonlinear hyperbolic equations are performed . we demonstrate that the procedure can generate essentially non oscillatory solutions while keeping the resolution and desired order of accuracy for smooth solutions .
spatialtemporal model for demand and allocation of waste landfills in growing urban regions . <eos> shortage of land for waste disposal is a serious and growing potential problem in most large urban regions . however , no practical studies have been reported in the literature that incorporate the process of consumption and depletion of landfill space in urban regions over time and analyse its implications for the management of waste . an evaluation of existing models of waste management indicates that they can provide significant insights into the design of solid waste management activities . however , these models do not integrate spatial and temporal aspects of waste disposal that are essential to understand and measure the problem of shortage of land . the lack of adequate models is caused in part due to limitations of the methodologies the existing models are based upon , such as limitations of geographic information systems ( gis ) in handling dynamic processes , and the limitations of systems analysis in incorporating spatial physical properties . this indicates that new methods need to be introduced in waste management modelling . moreover , existing models generally do not link waste management to the process of urban growth . this paper presents a model to spatially and dynamically model the demand for and allocation of facilities for urban solid waste disposal in growing urban regions . the model developed here consists of a loose coupled system that integrates gis ( geographic information systems ) and cellular automata ( ca ) in order to give it spatial and dynamic capabilities . the model combines three sub systems ( <digit> ) a ca based model to simulate spatial urban growth over the future ( <digit> ) a spread sheet calculation for designing waste disposal options and hence evaluating demand for landfill space over time and ( <digit> ) a model developed within a gis to evaluate the availability and suitability of land for landfill over time and then simulate allocation of landfills in the available land . the proposed model has been tested and set up with data from a real source ( porto alegre city , brazil ) , and has successfully assessed the demand for landfills and their allocation over time under a range of scenarios of decision making regarding waste disposal systems , urban growth patterns and land evaluation criteria .
dynamic delamination modelling using interface elements . <eos> existing techniques in explicit dynamic finite element ( fe ) codes for the analysis of delamination in composite structures and components can be simplistic , using simple stress based failure function to initiate and propagate delaminations . this paper presents an interface modelling technique for explicit fe codes . the formulation is based on damage mechanics and uses only two constants for each delamination mode firstly , a stress threshold for damage to commence , and secondly , the critical energy release rate for the particular delamination mode . the model has been implemented into the llnl dyna3d finite element ( fe ) code and the ls dyna3d commercial fe code . the interface element modelling technique is applied to a series of common fracture toughness based delamination problems , namely the dcb , enf and mmb tests . the tests are modelled using a simple dynamic relaxation technique , and serves to validate the methodology before application to more complex problems . explicit finite elements codes , such as dyna3d , are commonly used to solve impact type problems . a modified boeing impact test at two energy levels is used to illustrate the application of the interface element technique , and its coupling to existing in plane failure models . simulations are also performed without interface elements to demonstrate the need to include the interface when modelling impact on composite components .
a new steiner patch based file format for additive manufacturing processes . <eos> a new steiner patch based additive manufacturing file format has been developed . steiner format uses triangular rational bezier representation of steiner patches . steiner format has high geometric fidelity and low approximation error . the steiner patches can be easily sliced and closed form solutions can be obtained . am parts manufactured using steiner format has very low profile and form errors .
empirical challenges and solutions in constructing a high performance metasearch engine . <eos> purpose this paper seeks to disclose the important role of missing documents , broken links and duplicate items in the results merging process of a metasearch engine in detail . it aims to investigate some related practical challenges and proposes some solutions . the study also aims to employ these solutions to improve an existing model for results aggregation . design methodology approach this research measures the amount of an increase in retrieval effectiveness of an existing results merging model that is obtained as a result of the proposed improvements . the <digit> queries of the <digit> trec web track were employed as a standard test collection based on a snapshot of the worldwide web to explore and evaluate the retrieval effectiveness of the suggested method . three popular web search engines ( ask , bing and google ) as the underlying resources of metasearch engines were selected . each of the <digit> queries was passed to all three search engines . for each query the top ten non sponsored results of each search engine were retrieved . the returned result lists of the search engines were aggregated using a proposed algorithm that takes the practical issues of the process into consideration . the effectiveness of the result lists generated was measured using a well known performance indicator called tsap ( trec style average precision ) . findings experimental results demonstrate that the proposed model increases the performance of an existing results merging system by 14.39 percent on average . practical implications the findings of this research would be helpful for metasearch engine designers as well as providing motivation to the vendors of web search engines to improve their technology . originality value this study provides some valuable concepts , practical challenges , solutions and experimental results in the field of web metasearching that have not been previously investigated .
maintaining awareness using policies enabling agents to identify relevance of information . <eos> the field of computer supported cooperative work aims at providing information technology models , methods , and tools that assist individuals to cooperate . the presented paper is based on three main observations from literature . first , one of the problems in utilizing information technology for cooperation is to identify the relevance of information , called awareness . second , research in computer supported cooperative work proposes the use of agent technologies to aid individuals to maintain their awareness . third , literature lacks the formalized methods on how software agents can identify awareness . this paper addresses the problem of awareness identification . the main contribution of this paper is to propose and evaluate a formalized structure , called policy based awareness management ( pam ) . pam extends the logic of general awareness in order to identify relevance of information . pam formalizes existing policies into directory enabled networks next generation structure and uses them as a source for awareness identification . the formalism is demonstrated by applying pam to the space shuttle columbia disaster occurred in <digit> . the paper also argues that efficacy and cost efficiency of the logic of general awareness will be increased by pam . this is evaluated by simulation of hypothetical scenarios as well as a case study . ( c ) <digit> elsevier inc. all rights reserved .
extension headers for ipv6 anycast . <eos> anycast is a new communication paradigm defined in ipv6 . different from unicast and multicast routing , routers on the internetwork deliver an anycast datagrant to the nearest available node . by shifting the task of resolving destinations from source node to internetwork , anycasting is highly flexible and cost effective on routing process and inherently load balanced and robust on server selection . to achieve these objectives , not only distance but also other metrics , such as load balance , reliability , qos , can and should be taken into account in anycast routing . the ipv6 basic header is designed in a simple and fixed length format for the purpose of efficient forwarding . extra data and options needed for packet processing are encoded into extension headers . such a design makes possible the adding of extension headers for special purposes . in this paper , we define routing extension headers for ipv6 anycasting to enable various types of anycast routing mechanism . scenarios are also provided to demonstrate how to apply them . ( c ) <digit> elsevier b.v. all rights reserved .
the calculus of constructions as a framework for proof search with set variable instantiation . <eos> we show how a procedure developed by bledsoe for automatically finding substitution instances for set variables in higher order logic can be adapted to provide increased automation in proof search in the calculus of constructions ( cc ) . bledsoe 's procedure operates on an extension of first order logic that allows existential quantification over set variables . this class of variables can also be identified in cc . the existence of a correspondence between higher order logic and higher order type theories such as cc is well known . cc can be viewed as an extension of higher order logic where the basic terms of the language , the simply typed lambda terms , are replaced with terms containing dependent types . we show how bledsoe 's techniques can be incorporated into a reformulation of a search procedure for cc given by dowek and extended to handle terms with dependent types . we introduce a notion of search context for cc which allows us to separate the operations of assumption introduction and backchaining . search contexts allow a smooth integration of the step which finds solutions to set variables . we discuss how the procedure can be restricted to obtain procedures for set variable instantiation in sublanguages of cc such as the logical framework ( lf ) and higher order hereditary harrop formulas ( hohh ) . the latter serves as the logical foundation of the lambda prolog logic programming language . ( c ) <digit> elsevier science b.v. all rights reserved .
learning temporal nodes bayesian networks . <eos> temporal nodes bayesian networks ( tnbns ) are an alternative to dynamic bayesian networks for temporal reasoning with much simpler and efficient models in some domains . tnbns are composed of temporal nodes , temporal intervals , and probabilistic dependencies . however , methods for learning this type of models from data have not yet been developed . in this paper , we propose a learning algorithm to obtain the structure and temporal intervals for tnbns from data . the method consists of three phases ( i ) obtain an initial approximation of the intervals , ( ii ) obtain a structure using a standard algorithm and ( iii ) refine the intervals for each temporal node based on a clustering algorithm . we evaluated the method with synthetic data from three different tnbns of different sizes . our method obtains the best score using a combined measure of interval quality and prediction accuracy , and a competitive structural quality with lower running times , compared to other related algorithms . we also present a real world application of the algorithm with data obtained from a combined cycle power plant in order to diagnose temporal faults . ( c ) <digit> elsevier inc. all rights reserved .
solving bilevel programs with the kkt approach . <eos> bilevel programs ( bl ) form a special class of optimization problems . they appear in many models in economics , game theory and mathematical physics . bl programs show a more complicated structure than standard finite problems . we study the so called kkt approach for solving bilevel problems , where the lower level minimality condition is replaced by the kkt or the fj condition . this leads to a special structured mathematical program with complementarity constraints . we analyze the kkt approach from a generic viewpoint and reveal the advantages and possible drawbacks of this approach for solving bl problems numerically .
modeling and evaluating of typical advanced peer to peer botnet . <eos> in this paper , we present a general model for an advanced peer to peer ( p2p ) botnet , in which the performance of the botnet can be systematically studied . from the model , we can derive five performance metrics to describe the robustness , security and efficiency of the botnet . additionally , we analyze the relationship between the performance metrics and the model feature metrics of the botnet , and it is helpful to study the botnet under different model feature metrics . furthermore , the proposed model can be easily applied to other types of botnets . finally , taking the robustness and security into consideration , an optimization scheme for designing an optimal p2p botnet is proposed .
learning a coverage set of maximally general fuzzy rules by rough sets . <eos> expert systems have been widely used in domains where mathematical models can not be easily built , human experts are not available or the cost of querying an expert is high . machine learning or data mining can extract desirable knowledge or interesting patterns from existing databases and ease the development bottleneck in building expert systems . in the past we proposed a method hong , t.p. , wang , t.t. , wang , s.l. ( <digit> ) . knowledge acquisition from quantitative data using the rough set theory . intelligent data analysis ( in press ) . , which combined the rough set theory and the fuzzy set theory to produce all possible fuzzy rules from quantitative data . in this paper , we propose a new algorithm to deal with the problem of producing a set of maximally general fuzzy rules for coverage of training examples from quantitative data . a rule is maximally general if no other rule exists that is both more general and with larger confidence than it . the proposed method first transforms each quantitative value into a fuzzy set of linguistic terms using membership functions and then calculates the fuzzy lower approximations and the fuzzy upper approximations . the maximally general fuzzy rules are then generated based on these fuzzy approximations by an iterative induction process . the rules derived can then be used to build a prototype knowledge base in a fuzzy expert system .
an ontological conceptualization approach for awareness in domain independent collaborative modeling systems application to a model driven development method . <eos> one of the most important aspects of collaborative systems is the concept of awareness , which refers to the perception and knowledge of the group and its activities . support for the design and automatic development of awareness mechanisms within collaborative systems is hard to find . furthermore , awareness conceptualizations are usually partial and differ greatly between the proposals of different authors . in response to these problems , we propose an awareness ontology that conceptualizes some of the most important aspects of awareness in a specific kind of system collaborative systems for carrying out modeling activities . the awareness ontology brings together and extends a series of ontologies we have developed in the past . the ontology is prepared to better meet the specific implementation needs of a model driven development approach . in order to validate the usefulness of this ontology , we relate its concepts to the awareness dimensions set out in gutwin and greenbergs framework , and we apply the ontology to two systems presently in use .
approximating k node connected subgraphs via critical graphs . <eos> we present two new approximation algorithms for the problem of finding a k node connected spanning subgraph ( directed or undirected ) of minimum cost . the best known approximation guarantees for this problem were o ( min k , n root ( n k ) ) for both directed and undirected graphs , and o ( ln k ) for undirected graphs with n > 6k ( <digit> ) , where n is the number of nodes in the input graph . our first algorithm has approximation ratio o ( n n k ln ( <digit> ) k ) , which is o ( ln ( <digit> ) k ) except for very large values of k , namely , k n o ( n ) . this algorithm is based on a new result on l connected p critical graphs , which is of independent interest in the context of graph theory . our second algorithm uses the primal dual method and has approximation ratio o ( v n ln k ) for all values of n , k. combining these two gives an algorithm with approximation ratio o ( ln k center dot min root k , n n k ln k ) , which asymptotically improves the best known approximation guarantee for directed graphs for all values of n , k , and for undirected graphs for k > root n <digit> . moreover , this is the first algorithm that has an approximation guarantee better than t ( k ) for all values of n , k. our approximation ratio also provides an upper bound on the integrality gap of the standard lp relaxation .
development of an autowep distributed hydrological model and its application to the upstream catchment of the miyun reservoir . <eos> based on the physically characterized distributed hydrological modeling scheme wep l a more generalized and expandable method autowep has been developed that is equipped with updated modules for pre processing and automatic parameter identification . sub basin scale classifications of land use and soil are undertaken by incorporating remote sensing data and geographic information system techniques . in the process of developing the autowep modeling scheme , a new concept of parameter partitioning is proposed and an automatic delineation of parameter partitions is achieved through programming . the sensitivity analysis algorithm , lh oat , and the parameter optimization algorithm , sce ua , are embedded in the model . its application to the upstream watershed of the miyun reservoir shows that autowep features time savings , improved efficiency and suitable generalizations , that result in a long series of acceptable simulations .
towards insider threat detection using web server logs . <eos> malicious insiders represent one of the most difficult categories of threats an organization must consider when mitigating operational risk . insiders by definition possess elevated privileges have knowledge about control measures and may be able to bypass security measures designed to prevent , detect , or react to unauthorized access . in this paper , we discuss our initial research efforts focused on the detection of malicious insiders who exploit internal organizational web servers . the objective of the research is to apply lessons learned in network monitoring domains and enterprise log management to investigate various approaches for detecting insider threat activities using standardized tools and a common event expression framework .
weight similarity measurement model based , object oriented approach for bug databases mining to detect similar and duplicate bugs . <eos> in this paper data mining is applied on bug database to discover the similar and duplicate bugs . whenever a new bug will be entered in the bug database through bug tracking system , it will be matched against the existing bugs and duplicate and similar bugs will be mined from the bug database . similar kind of bugs are resolved in almost in same manners . so if a bug is found somewhere similar to other existing bug which is already resolved then its resolution will take less time , since some of the bug analysis part is similar to existing one , hence it will save time . in the existing tradition developers must have to manually identify duplicate bug reports , but this identification process is time consuming and exacerbates the already high cost of software maintenance . so if the similar and duplicate bugs can be found out using some approach it will be a cost and time saving activity . based on this concept a weight similarity measurement model based object orinted approach is described here in this paper to discover similar and duplicate bugs in the bug database .
recursive modeling for completed code generation . <eos> model driven development is promising to software development because it can reduce the complexity and cost of developing large software systems . the basic idea is the use of different kinds of models during the software development process , transformations between them , and automatic code generation at the end of the development . but unlike the structural parts , fully automated code generation from the behavior parts is still hard , if it works at all , restricted to specific application areas using a domain specific language , dsl . this paper proposes an approach to model the behavior parts of a system and to embed them into the structural models . the underlying idea is recursive refinements of activity elements in an activity diagram . with this , the detail generated code depends on the depth at which the refinements are done , i.e. if the lowest level of activities is mapped into activities executors , the completed code can be obtained .
on the theoretical comparison of low bias steady state estimators . <eos> the time average estimator is typically biased in the context of steady state simulation , and its bias is of order <digit> t , where t represents simulated time . several low bias estimators have been developed that have a lower order bias , and , to first order , the same variance of the time average . we argue that this kind of first order comparison is insufficient , and that a second order asymptotic expansion of the mean square error ( mse ) of the estimators is needed . we provide such an expansion for the time average estimator in both the markov and regenerative settings . additionally , we provide a full bias expansion and a second order mse expansion for the meketon heidelberger low bias estimator , and show that its mse can be asymptotically higher or lower than that of the time average depending on the problem . the situation is different in the context of parallel steady state simulation , where a reduction in bias that leaves the first order variance unaffected is arguably an improvement in performance .
the multisymplectic numerical method for grosspitaevskii equation . <eos> for a boseeinstein condensate placed in a rotating trap and confined in the z axis , a multisymplectic difference scheme was constructed to investigate the evolution of vortices in this paper . first , we look for a steady state solution of the imaginary time g p equation . then , we numerically study the vortices 's development in real time , starting with the solution in imaginary time as initial value .
system design , data collection and evaluation of a speech dialog system . <eos> this paper describes design issues of a speech dialogue system , the evaluation of the system , and the data collection of spontaneous speech in a transportation guidance domain . as it is difficult to collect spontaneous speech and to use a real system for the collection and evaluation , the phenomena related with dialogues have not been quantitatively clarified yet . the authors constructed a speech dialogue system which operates in almost real time , with acceptable recognition accuracy and flexible dialogue control . the system was used for spontaneous speech collection in a transportation guidance domain . the system performance evaluated in the domain is the understanding rate of 84.2 % for the utterances within the predefined grammar and the lexicon . also some statistics of the spontaneous speech collected are given .
abstract convex evolutionary search . <eos> geometric crossover is a formal class of crossovers which includes many well known recombination operators across representations . in this paper , we present a general result showing that all evolutionary algorithms using geometric crossover with no mutation perform the same form of convex search regardless of the underlying representation , the specific selection mechanism , the specific offspring distribution , the specific search space , and the problem at hand . we then start investigating a few representation space independent geometric conditions on the fitness landscape various forms of generalized concavity that when matched with the convex evolutionary search guarantee , to different extents , improvement of offspring over parents for any choice of parents . this is a first step towards showing that the convexity relation between search and landscape may play an important role towards explaining the performance of evolutionary algorithms in a general setting across representations .
area measurement of large closed regions with a mobile robot . <eos> how can a mobile robot measure the area of a closed region that is beyond its immediate sensing range this problem , which we name as blind area measurement , is inspired from scout worker ants who assess potential nest cavities . we first review the insect studies that have shown that these scouts , who work in dark , seem to assess arbitrary closed spaces and reliably reject nest sites that are small for the colony . we briefly describe the hypothesis that these scouts use buffon 's needle method to measure the area of the nest . then we evaluate and analyze this method for mobile robots to measure large closed regions . we use a simulated mobile robot system to evaluate the performance of the method through systematic experiments . the results showed that the method can reliably measure the area of large and rather open , closed regions regardless of their shape and compactness . moreover , the method 's performance seems to be undisturbed by the existence of objects and by partial barriers placed inside these regions . finally , at a smaller scale , we partially verified some of these results on a real mobile robot platform .
minimum pilot power for service coverage in wcdma networks . <eos> pilot power management is an important issue for efficient resource utilization in wcdma networks . in this paper , we consider the problem of minimizing pilot power subject to a coverage constraint . the constraint can be used to model various levels of coverage requirement , among which full coverage is a special case . the pilot power minimization problem is np hard , as it generalizes the set covering problem . our solution approach for this problem consists of mathematical programming models and methods . we present a linear integer mathematical formulation for the problem . to solve the problem for large scale networks , we propose a column generation method embedded into an iterative rounding procedure . we apply the proposed method to a range of test networks originated from realistic network planning scenarios , and compare the results to those obtained by two ad hoc approaches . the numerical experiments show that our algorithm is able to find near optimal solutions with a reasonable amount of computing effort for large networks . moreover , optimized pilot power considerably outperforms the ad hoc approaches , demonstrating that efficient pilot power management is an important component of radio resource optimization . as another part of our numerical study , we examine the trade off between service coverage and pilot power consumption .
asymptotically stable multi valued many to many associative memory neural network and its application in image retrieval . <eos> as an important artificial neural network , associative memory model can be employed to mimic human thinking and machine intelligence . in this paper , first , a multi valued many to many gaussian associative memory model ( m ( <digit> ) gam ) is proposed by introducing the gaussian unidirectional associative memory model ( guam ) and gaussian bidirectional associative memory model ( gbam ) into hattori et al 's multi module associative memory model ( ( mma ) ( <digit> ) ) . second , the m ( <digit> ) gam 's asymptotical stability is proved theoretically in both synchronous and asynchronous update modes , which ensures that the stored patterns become the m ( <digit> ) gam 's stable points . third , by substituting the general similarity metric for the negative squared euclidean distance in m ( <digit> ) gam , the generalized multivalued many to many gaussian associative memory model ( gm ( <digit> ) gam ) is presented , which makes the m ( <digit> ) gam become its special case . finally , we investigate the m ( <digit> ) gam 's application in association based image retrieval , and the computer simulation results verify the m ( <digit> ) gam 's robust performance .
identity based threshold proxy signature from bilinear pairings . <eos> delegation of rights is a common practice in the real world . we present two identity based threshold proxy signature schemes , which allow an original signer to delegate her signing capability to a group of n proxy signers , and it requires a consensus of t or more proxy signers in order to generate a valid signature . in addition to identity based scheme , privacy protection for proxy singers and security assurance are two distinct features of this work . our first scheme provides partial privacy protection to proxy signers such that all signers ' identities are revealed , whereas none of those t participating signers is specified . on the other hand , all proxy signers remain anonymous in the second scheme . this provides a full privacy protection to all proxy signers however , each valid signature contains a tag that allows one to trace all the participating proxy signers . both our proposed schemes are secure against unforgeability under chosen message attack , and satisfy many other necessary conditions for proxy signature .
semi autonomous navigation of a robotic wheelchair . <eos> the present work considers the development of a wheelchair for people with special needs , which is capable of navigating semi autonomously within its workspace . this system is expected to prove useful to people with impaired mobility and limited fine motor control of the upper extremities . among the implemented behaviors of this robotic system are the avoidance of obstacles , the motion in the middle of the free space and the following of a moving target specified by the user ( e.g. , a person walking in front of the wheelchair ) . the wheelchair is equipped with sonars , which are used for distance measurement in preselected critical directions , and with a panoramic camera with a <digit> degree field of view , which is used for following a moving target . after suitably processing the color sequence of the panoramic images using the color histogram of the desired target , the orientation of the target with respect to the wheelchair is determined , while its distance is determined by the sonars . the motion control laws developed for the system use the sensory data and take into account the non holonomic kinematic constraints of the wheelchair , in order to guarantee certain desired features of the closed loop system , such as stability . moreover , they are as simplified as possible to minimize implementation requirements . an experimental prototype has been developed at ics forth , based on a commercially available wheelchair . the sensors , the computing power and the electronics needed for the implementation of the navigation behaviors and of the user interfaces ( touch screen , voice commands ) were developed as add on modules and integrated with the wheelchair .
learning protein secondary structure from sequential and relational data . <eos> we propose a method for sequential supervised learning that exploits explicit knowledge of short and long range dependencies . the architecture consists of a recursive and bi directional neural network that takes as input a sequence along with an associated interaction graph . the interaction graph models ( partial ) knowledge about long range dependency relations . we tested the method on the prediction of protein secondary structure , a task in which relations due to beta strand pairings and other spatial proximities are known to have a significant effect on the prediction accuracy . in this particular task , interactions can be derived from knowledge of protein contact maps at the residue level . our results show that prediction accuracy can be significantly boosted by the integration of interaction graphs .
document replication strategies for geographically distributed web search engines . <eos> large scale web search engines are composed of multiple data centers that are geographically distant to each other . typically , a user query is processed in a data center that is geographically close to the origin of the query , over a replica of the entire web index . compared to a centralized , single center search engine , this architecture offers lower query response times as the network latencies between the users and data centers are reduced . however , it does not scale well with increasing index sizes and query traffic volumes because queries are evaluated on the entire web index , which has to be replicated and maintained in all data centers . as a remedy to this scalability problem , we propose a document replication framework in which documents are selectively replicated on data centers based on regional user interests . within this framework , we propose three different document replication strategies , each optimizing a different objective reducing the potential search quality loss , the average query response time , or the total query workload of the search system . for all three strategies , we consider two alternative types of capacity constraints on index sizes of data centers . moreover , we investigate the performance impact of query forwarding and result caching . we evaluate our strategies via detailed simulations , using a large query log and a document collection obtained from the yahoo web search engine .
collision correction using a cross layer design architecture for dedicated short range communications vehicle safety messaging . <eos> this paper presents a new physical ( phy ) and medium access control ( mac ) cross layer design frame collision correction ( cc ) architecture for correction of dedicated short range communications ( dsrcs ) safety messages . conditions suitable for the use of this design are presented , which can be used for optimization . at its basic level , the cc at the phy uses a new decision making block that uses information from the mac layer for the channel estimator and equalizer . this requires a cache of previously received frames , and pre announcing frame repetitions from the mac . we present the theoretical equations behind cc mechanism , and describe the components required to implement the cross layer cc using deployment and sequence diagrams . simulation results show that especially under high user load , reception reliability of the dsrc safety messages increases and per decreases .
a general model of unit testing efficacy . <eos> much of software engineering is targeted towards identifying and removing existing defects while preventing the injection of new ones . defect management is therefore one important software development process whose principal aim is to ensure that the software produced reaches the required quality standard before it is shipped into the market place . in this paper , we report on the results of research conducted to develop a predictive model of the efficacy of one important defect management technique , that of unit testing . we have taken an empirical approach . we commence with a number of assumptions that led to a theoretical model which describes the relationship between effort expended and the number of defects remaining in a software code module tested ( the latter measure being termed correctness ) . this model is general enough to capture the possibility that debugging of a software defect is not perfect and could lead to new defects being injected . the model is examined empirically against actual data and validated as a good predictive model under specific conditions . the work has been done in such a way that models are derived not only for the case of overall correctness but also for specific types of correctness such as correctness arising from the removal of defects contributing to shortcoming in reliability ( r type ) , functionality ( f type ) , usability ( u type ) and maintainability ( m type ) aspects of the program subject to defect management .
constitutive modeling of materials and contacts using the disturbed state concept part <digit> background and analysis . <eos> computer methods have opened a new era for accurate and economic analysis and design of engineering problems . they account for many significant factors such as arbitrary geometries , nonhomogeneities in material composition , complex boundary conditions , nonlinear material behavior ( constitutive modeling ) and complex loading conditions , which were difficult to include in conventional and closed form solution procedures . constitutive modeling characterizes the mechanical behavior of solids and contacts ( e.g. interfaces and joints ) , and plays perhaps the most important role for realistic solutions from procedures in computational mechanics . a great number of constitutive models , from simple to the advanced , have been proposed . most of them account for specific characteristics of the material . however , a deforming material may experience , simultaneously , many characteristics such as elastic , plastic and creep strains , different loading ( stress ) paths , volume change under shear stress , microcracking leading to fracture and failure , strain softening or degradation , and healing or strengthening . hence , there is a need for developing unified models that account for these characteristics . the main objective of these two papers is to present a brief review of the available constitutive models , and identify their capabilities and limitations then a novel and unified approach , called the disturbed state concept ( dsc ) with hierarchical single surface ( hiss ) plasticity , is presented including its theoretical background , constitutive parameters and their determination , and validation at the test specimen and boundary value problem levels . the general capabilities of the dsc hiss approach are emphasized by its application for a wide range of materials and contacts ( interfaces and joints ) . because of its generality , the dsc contains many previous models as special cases . the presentation is divided in two papers . this paper ( part <digit> ) contains the review of various models , and then description of the dsc hiss model and its analysis for issues such as mesh dependence and localization . part <digit> also contains the capability of the dsc hiss model to define the behavior of both solids and contacts . validations of the dsc hiss model at the specimen and boundary value problem levels for a wide range of materials and contacts are included in the compendium paper , part <digit> . the idea of the dsc is considered to be relatively simple , and it can be easily implemented in computer procedures . it is believed that the dsc can provide a realistic and unified approach for constitutive modeling for a wide range of materials and contacts .
an efficient neumann series based algorithm for thermoacoustic and photoacoustic tomography with variable sound speed . <eos> we present an efficient algorithm for reconstructing an unknown source in thermoacoustic and photoacoustic tomography based on the recent advances in understanding the theoretical nature of the problem . we work with variable sound speeds that also might be discontinuous across some surface . the latter problem arises in brain imaging . the algorithmic development is based on an explicit formula in the form of a neumann series . we present numerical examples with nontrapping , trapping , and piecewise smooth speeds , as well as examples with data on a part of the boundary . these numerical examples demonstrate the robust performance of the neumann series based algorithm .
documentary genre and digital recordkeeping red herring or a way forward . <eos> the purpose of this paper is to provide a preliminary assessment of the utility of the genre concept for digital recordkeeping . the exponential growth in the volume of records created since the 1940s has been a key motivator for the development of strategies that do not involve the review or processing of individual documents or files . automation now allows processes at a level of granularity that is rarely , if at all , possible in the case of manual processes , without loss of cognisance of context . for this reason , it is timely to revisit concepts that may have been disregarded because of a perceived limited effectiveness in contributing anything to theory or practice . in this paper , the genre concept and its employability in the management of current and archival digital records are considered , as a form of social contextualisation of a document and as an attractive entry point of granularity at which to implement automation of appraisal processes . particular attention is paid to the structurational view of genre and its connections with recordkeeping theory .
existence and multiplicity of positive periodic solutions for a class of higher dimension functional differential equations with impulses . <eos> this paper deals with the existence of multiple periodic solutions for n dimensional functional differential equations with impulses . by employing the krasnoselskii fixed point theorem , we obtain some easily verifiable sufficient criteria which extend previous results . ( c ) <digit> elsevier ltd. all rights reserved .
modelling and querying geographical data warehouses . <eos> a number of proposals for integrating geographical ( geographical information systems gis ) and multidimensional ( data warehouse dw and online analytical processing olap ) processing are found in the database literature . however , most of the current approaches do not take into account the use of a cow ( geographical data warehouse ) metamodel or query language to make available the simultaneous specification of multidimensional and spatial operators . to address this , this paper discusses the uml class diagram of a gdw metamodel and proposes its formal specifications . we then present a formal metamodel for a geographical data cube and propose the geographical multidimensional query language ( geomdql ) as well . geomdql is based on well known standards such as the multidimensional expressions ( mdx ) language and ogc simple features specification for sql and has been specifically defined for spatial olap environments based on a gdw . we also present the geomdql syntax and a discussion regarding the taxonomy of geomdql query types . additionally , aspects related to the geomdql architecture implementation are described , along with a case study involving the brazilian public healthcare system in order to illustrate the proposed query language . ( c ) <digit> elsevier b.v. all rights reserved .
ddas distance and direction awareness system for intelligent vehicles . <eos> wireless technology has been widely used for applications of wireless internet access . with the matured wireless transmission technology , the new demand on wireless applications is toward the concept of deploying wireless devices on transportation systems such as buses , trains and vehicles . statistics of car accident cases show that car accidents are often caused from drivers unnoticing other approaching cars during driving . without the assistants of automotive personal computer system ( also called as auto pc ) , during high speed moving , driver always counts on himself herself to look for all vehicles around him her via limited vision and acoustic recognition . in case that the auto pc is able to provide useful surrounding information , such as the directions and distances to nearby vehicles , to drivers , unnecessary collisions could be obviously avoided , especially in cases of changing lane , crossing intersection and making a turn . in this paper , we will introduce the concept of automatic distance and direction awareness system ( ddas ) and describe the designed embedded ddas integrated with three wheel and four wheel robot cars .
investigating models for preservice teachers ' use of technology to support student centered learning . <eos> the study addressed two limitations of previous research on factors related to teachers integration of technology in their teaching . it attempted to test a structural equation model ( sem ) of the relationships among a set of variables influencing preservice teachers ' use of technology specifically to support student centered learning a review of literature led to a path model that provided the design and analysis for the study , which involved <digit> preservice teachers in the united states . the results show that the proposed model had a moderate fit to the observed data , and a more parsimonious model was found to have a better fit . in addition , preservice teachers ' self efficacy of teaching with technology had the strongest influence on technology use , which was mediated by their perceived value of teaching and learning with technology . school 's contextual factors had moderate influence on technology use . moreover , the effect of preservice teachers ' training on student centered technology use was mediated by both perceived value and self efficacy of technology . the implications for teacher preparation include close collaboration between teacher education program and field experience , focusing on specific technology uses ( c ) <digit> elsevier ltd all rights reserved
a risc approach to process groups . <eos> isis <digit> , developed at cornell university , is a system for building applications consisting of cooperating , distributed processes . group management and group communication are two basic building blocks provided by isis . isis has been very successful , and there is currently a demand for a version that will run on many different environments and transport protocols , and will scale to many process groups . furthermore , performance is an important issue . for this purpose , isis is being redesigned and rebuilt from scratch <digit> . of particular importance to us is getting the new isis system to run well on modern microkernel technology , notably mach <digit> and chorus <digit> . the basic reasoning behind these plans is that microkernels appear to offer satisfactory support for memory management and communication between processes on the same machine , but that support for applications that run on multiple machines is weak . the current ipc mechanisms are adequate only for the simpler distributed applications , as they do not address any of the internal management issues of distribution.the new isis system has several well defined layers . the lowest layers , which implement multicast transport and failure detection , are near completion and currently run on sun os using sun lwp threads , on mach using c threads , and on the x kernel <digit> . this system can use several different network protocols at the same time , such as ip , udp ( with or without multicast support ) , and raw ethernet . this enables processes on sun os , mach , and chorus to multicast among each other , even though the environments are very dissimilar . the system makes use of available hardware multicast if possible . it also queues messages if a backlog appears , so that multiple messages may be packed together in a single packet . using this strategy , the number of messages per second can become very large , and in the current ( simple ) implementation about 10,000 per second can be sent between distributed sun os user processes , a figure that approaches the speed of local light weight remote procedure call mechanisms . ( the current round trip time on sun os over ethernet is about <digit> milliseconds . )
on affine scaling algorithms for nonconvex quadratic programming . <eos> we investigate the use of interior algorithms , especially the affine scaling algorithm , to solve nonconvex indefinite or negative definite quadratic programming ( qp ) problems . although the nonconvex qp with a polytope constraint is a hard problem , we show that the problem with an ellipsoidal constraint is easy . when the hard qp is solved by successively solving the easy qp , the sequence of points monotonically converge to a feasible point satisfying both the first and the second order optimality conditions .
multi agent simulation of group behavior in e government policy decision . <eos> to research complex group behavior in e government policy decision , this study proposes a multi agent qualitative simulation approach using eggbm ( e government group behavior model ) . causal reasoning is employed to analyze it from the perspective of system . then , a multi agent simulation decision system based on java repast is developed . moreover , three validation experiments are designed to prove that eggbm can exactly represent the actual situation . at last , an example of application is given to show that this method can help policy makers choose appropriate policies to improve the level of accepting information technology ( lait ) of groups . it is shown that this approach could be a new attempt for the research of group behavior in governmental organization .
independent component analysis for unaveraged single trial meg data decomposition and single dipole source localization . <eos> this paper presents a novel method for decomposing and localizing unaveraged single trial magnetoencephalographic data based on the independent component analysis ( ica ) approach associated with pre and post processing techniques . in the pre processing stage , recorded single trial raw data are first decomposed into uncorrelated signals with the reduction of high power additive noise . in the stage of source separation , the decorrelated source signals are further decomposed into independent source components . in the post processing stage , we perform a source localization procedure to seek a single dipole map of decomposed individual source components , e.g. , evoked responses . the first results of applying the proposed robust ica approach to single trial data with phantom and auditory evoked field tasks indicate the following . ( <digit> ) a source signal is successfully extracted from unaveraged single trial phantom data . the accuracy of dipole estimation for the decomposed source is even better than that of taking the average of total trials . ( <digit> ) not only the behavior and location of individual neuronal sources can be obtained but also the activity strength ( amplitude ) of evoked responses corresponding to a stimulation trial can be obtained and visualized . moreover , the dynamics of individual neuronal sources , such as the trial by trial variations of the amplitude and location , can be observed .
the effects of learning style and hypermedia prior experience on behavioral disorders knowledge and time on task a case based hypermedia environment . <eos> this study involved <digit> graduate students enrolled in a behavioral disorders course . as a part of the course , they engaged in an extensive case based hypermedia program designed to enhance their ability to solve student emotional and behavioral problems . results include ( <digit> ) students increased their knowledge about behavioral disorders ( <digit> ) those students with more hypermedia experience spent more time using the hypermedia program ( <digit> ) those students who acquired greater knowledge also wrote better student reports and ( <digit> ) students , regardless of learning style ( as measured by kolb 's learning style inventory ) , benefited equally from using the hypermedia program .
rough sets , coverings and incomplete information . <eos> rough sets are often induced by descriptions of objects based on the precise observations of an insufficient number of attributes . in this paper , we study generalizations of rough sets to incomplete information systems , involving imprecise observations of attributes . the precise role of covering based approximations of sets that extend the standard rough sets in the presence of incomplete information about attribute values is described . in this setting , a covering encodes a set of possible partitions of the set of objects . a natural semantics of two possible generalisations of rough sets to the case of a covering ( or a non transitive tolerance relation ) is laid bare . it is shown that uncertainty due to granularity of the description of sets by attributes and uncertainty due to incomplete information are superposed , whereby upper and lower approximations themselves ( in pawlak 's sense ) become ill known , each being bracketed by two nested sets . the notion of measure of accuracy is extended to the incomplete information setting , and the generalization of this construct to fuzzy attribute mappings is outlined .
exploiting power budgeting in thermal aware dynamic placement for reconfigurable systems . <eos> in this paper , a novel thermal aware dynamic placement planner for reconfigurable systems is presented , which targets transient temperature reduction . rather than solving time consuming differential equations to obtain the hotspots , we propose a fast and accurate heuristic model based on power budgeting to plan the dynamic placements of the design statically , while considering the boundary conditions . based on our heuristic model , we have developed a fast optimization technique to plan the dynamic placements at design time . our results indicate that our technique is two orders of magnitude faster while the quality of the placements generated in terms of temperature and interconnection overhead is the same , if not better , compared to the thermal aware placement techniques which perform thermal simulations inside the search engine .
optimized independent components for parameter regression . <eos> in this paper , a modified icr algorithm is proposed for quality prediction purpose . the disadvantage of original independent component regression ( icr ) is that the extracted independent components ( ics ) are not informative for quality prediction and interpretation . in the proposed method , to enhance the causal relationship between the extracted ics and quality variables , a dual objective optimization which combines the cost function w ( t ) x ( t ) yv in partial least squares ( pls ) and the approximations of negentropy in independent component analysis ( ica ) is constructed in the first step for feature extraction . it simultaneously considers both the quality correlation and the independence , and then the icr mlr ( multiple linear regression ) method is used to obtain the regression coefficients . the proposed method is applied to the quality prediction in continuous annealing process and tennessee eastman process . applications indicate that the proposed approach effectively captures the relations in the process variables and use of proposed method instead of original pls and icr improves the regression matching and prediction ability . ( c ) <digit> elsevier b.v. all rights reserved .
discriminant bag of words based representation for human action recognition . <eos> human action recognition based on bag of words representation . discriminant codebook learning for better action class discrimination . unified framework for the determination of both the optimized codebook and linear data projections .
unsupervised connectionist algorithms for clustering an environmental data set a comparison . <eos> various unsupervised algorithms for vector quantization can be found in the literature . being based on different assumptions , they do not all yield exactly the same results on the same problem . to better understand these differences , this article presents an evaluation of some unsupervised neural networks , considered among the most useful for quantization , in the context of a real world problem radioelectric wave propagation . radio wave propagation is highly dependent upon environmental characteristics ( e.g. those of the city , country , mountains , etc. ) . within the framework of a cell net planning its radiocommunication strategy , we are interested in determining a set of environmental classes , sufficiently homogeneous , to which a specific prediction model of radio electrical field can be applied . of particular interest are techniques that allow improved analysis of results . firstly , mahalanobis distance , taking data correlation into account , is used to make assignments . secondly , studies of class dispersion and homogeneity , using both a data structure mapping representation and statistical analysis , emphasize the importance of the global properties of each algorithm . in conclusion , we discuss the advantages and disadvantages of each method on real problems .
preference based multi objective evolutionary algorithms for power aware application mapping on noc platforms . <eos> network on chip ( noc ) are considered the next generation of communication infrastructure in embedded systems . in the platform based design methodology , an application is implemented by a set of collaborative intellectual property ( ip ) blocks . the selection of the most suited set of ips as well as their physical mapping onto the noc infrastructure to implement efficiently the application at hand are two hard combinatorial problems that occur during the synthesis process of noc based embedded system implementation . in this paper , we propose an innovative preference based multi objective evolutionary methodology to perform the assignment and mapping stages . we use one of the well known and efficient multi objective evolutionary algorithms nsga ii and microga as a kernel . the optimization processes of assignment and mapping are both driven by the minimization of the required silicon area and imposed execution time of the application , considering that the decision makers preference is a pre specified value of the overall power consumption of the implementation .
2d dry granular free surface transient flow over complex topography with obstacles . part ii numerical predictions of fluid structures and benchmarking . <eos> dense granular flows are present in geophysics and in several industrial processes , which has lead to an increasing interest for the knowledge and understanding of the physics which govern their propagation . for this reason , a wide range of laboratory experiments on gravity driven flows have been carried out during the last two decades . the present work is focused on geomorphological processes and , following previous work , a series of laboratory studies which constitute a further step in mimicking natural phenomena are described and simulated . three situations are considered with some common properties a two dimensional configuration , variable slope of the topography and the presence of obstacles . the setup and measurement technique employed during the development of these experiments are deeply explained in the companion work . the first experiment is based on a single obstacle , the second one is performed against multiple obstacles and the third one studies the influence of a dike on which overtopping occurs . due to the impact of the flow against the obstacles , fast moving shocks appear , and a variety of secondary waves emerge . in order to delve into the physics of these types of phenomena , a shock capturing numerical scheme is used to simulate the cases . the suitability of the mathematical models employed in this work has been previously validated . comparisons between computed and experimental data are presented for the three cases . the computed results show that the numerical tool is able to predict faithfully the overall behavior of this type of complex dense granular flow .
context sharing in a real world ubicomp deployment . <eos> while the application of ubicomp systems to explore context sharing has received a large amount of interest , only a very small number of studies have been carried out which involve real world use outside of the lab . this article presents an in depth analysis of context sharing behaviours that built up around use of the hermes interactive office door display system received during deployment . the hermes system provided a groupware application supporting asynchronous messaging facilities , analogous to a digital form of post it notes , in order to explore the use of situated display systems to support awareness and coordination in an office environment . from this analysis we distil a set of issues relating to context sharing ranging from privacy concerns to ease of use each supported through qualitative data from user interviews and questionnaires .
holding time aware dynamic traffic grooming algorithms based on multipath routing for wdm optical networks . <eos> this paper investigates approaches for the traffic grooming problem that consider connection holding times and bandwidth availability . moreover , solutions can indicate the splitting of connections into two or more sub streams by multipath routing and fine tuned by traffic grooming to utilize network resources better . algorithms are proposed and the results of simulations using a variety of realistic scenarios indicate that the proposed algorithms significantly reduce the blocking of connection requests yet promote a fair distribution of the network resources in relation to the state of the art solutions .
three classes of maximal hyperclones . <eos> in this paper , we present three classes of maximal hyperclones . they are determined by three classes of rosenberg 's relations nontrivial equivalence relations , central relations and h regular relations .
the knowledge acquisition workshops a remarkable convergence of ideas . <eos> intense interest in knowledge acquisition research began <digit> years ago , stimulated by the excitement about knowledge based systems that emerged in the 1970s followed by the realities of the ai winter that arrived in the 1980s . the knowledge acquisition workshops that responded to this interest led to the formation of a vibrant research community that has achieved remarkable consensus on a number of issues . these viewpoints include ( <digit> ) the rejection of the notion of knowledge as a commodity to be transferred from one locus to another , ( <digit> ) an acceptance of the situated nature of human expertise , ( <digit> ) emphasis on knowledge acquisition as the modeling of problem solving , and ( <digit> ) the pursuit of reusable patterns in problem solving and in domain descriptions that can facilitate both modeling and system implementation . the semantic web community will benefit greatly by incorporating these perspectives in its work .
topological persistence for medium access control . <eos> the primary function of the medium access control ( mac ) protocol is managing access to the shared communication channel . from the viewpoint of the transmitters , the mac protocol determines each transmitter 's channel occupancy , the fraction of time that it spends transmitting over the channel . in this paper , we define a set of topological persistences that conform to both network topology and traffic load . we employ these persistences as target occupancies for the mac layer protocol . a centralized algorithm is developed for calculating topological persistences and its correctness is established . a distributed algorithm and implementation are developed that can operate within scheduled and contention based mac protocols . in the distributed algorithm , network resources are allocated through auctions at each receiver in which transmitters participate as bidders to converge on the topological allocation . very low overhead is achieved by piggybacking auction and bidder communication on existing data packets . the practicality of the distributed algorithm is demonstrated in a wireless network via simulation using the ns <digit> network simulator . simulation results show fast convergence to the topological solution and , once operating with topological persistences , improved performance compared to ieee 802.11 in delay , throughput , and drop rate .
plate on layered foundation analyzed by a semi analytical and semi numerical method . <eos> a semi analytical and semi numerical method is developed for the analysis of plate layered soil systems . applying a hankel transform , an expression relating the surface settlement and the reaction of the layered soil is derived . such a reaction can be treated as a load acting on the plate in addition to the applied external load . having the plate modeled by eight noded isoparametric elements , the governing equations of the plate can be formed and solved . numerical examples , including square , trapezoidal and circular plates resting on elastic layered soil , are given to demonstrate the advantages , accuracy and versatility of this method .
semi divisible triangular norms . <eos> semi divisibility of left continuous triangular norms is a weakening of the divisibility ( i.e. , continuity ) axiom for t norms . in this contribution we focus on the class of semi divisible t norms and show the following properties each semi divisible t norm with ran ( n ( t ) ) <digit> , <digit> is nilpotent . semi divisibility of an ordinal sum t norm is determined by the corresponding property of its first component ( which can be a proper t subnorm , too ) . finally , negations with finite range derived from semi divisible t norms are studied .
expert system for remnant life prediction of defected components under fatigue and creep fatigue loadings . <eos> life prediction and management of cracked high temperature structures is a matter of great importance for both economical and safe reasons . to implement such a task , many fields such as material science , structure engineering and mechanics science etc. are involved and expertise is generally required . in terms of the methodology of advanced time dependent fracture mechanics , this paper developed an expert system to realize an appropriate combination of material database , condition database and knowledge database . many assessment criteria including the multi defects interaction and combination , invalidation criterion and creep fatigue interaction are employed in the inference engine of expert system . the over conservativeness of life prediction from traditional method is reduced reasonably and therefore the accuracy of predicted life is improved . consequently , the intelligent and expert life management of cracked high temperature structures is realized which provides a powerful tool in practice . ( c ) <digit> elsevier ltd. all rights reserved .
packet mode scheduling in input queued cell based switches . <eos> we consider input queued switch architectures dealing at their interfaces with variable size packets , but internally operating on fixed size cells . packets are segmented into cells at input ports , transferred through the switching fabric , and reassembled at output ports . cell transfers are controlled by a scheduling algorithm , which operates in packet mode all cells belonging to the same packet are transferred from inputs to outputs without interruption . we prove that input queued switches using packet mode scheduling can achieve <digit> % throughput , and we show by simulation that , depending on the packet size distribution , packet mode scheduling may provide advantages over cell mode scheduling .
walkneta biologically inspired network to control six legged walking . <eos> to investigate walking we perform experimental studies on animals in parallel with software and hardware simulations of the control structures and the body to be controlled . therefore , the primary goal of our simulation studies is not so much to develop a technical device , but to develop a system which can be used as a scientific tool to study insect walking . to this end , the animat should copy essential properties of the animals . in this review , we will first describe the basic behavioral properties of hexapod walking , as the are known from stick insects . then we describe a simple neural network called walknet which exemplifies these properties and also shows some interesting emergent properties . the latter arise mainly from the use of the physical properties to simplify explicit calculations . the model is simple too , because it uses only static neuronal units . finally , we present some new behavioral results .
modelling the scatter of en curves using a serial hybrid neural network . <eos> if structural reliability is estimated by following a strain based approach , a materials strength should be represented by the scatter of the n ( en ) curves that link the strain amplitude with the corresponding statistical distribution of the number of cycles to failure . the basic shape of the n curve is usually modelled by the coffinmanson relationship . if a loading mean level also needs to be considered , the original coffinmanson relationship is modified to account for the non zero mean level of the loading , which can be achieved by using a smithwatsontopper modification of the original coffinmanson relationship . in this paper , a methodology for estimating the dependence of the statistical distribution of the number of cycles to failure on the smithwatsontopper modification is presented . the statistical distribution of the number of cycles to failure was modelled with a two parametric weibull probability density function . the core of the presented methodology is represented by a multilayer perceptron neural network combined with the weibull probability density function using a size parameter that follows the smithwatsontopper analytical model . the article presents the theoretical background of the methodology and its application in the case of experimental fatigue data . the results show that it is possible to model n curves and their scatter for different influential parameters , such as the specimens diameter and the testing temperature .
rate distortion problem for physics based distributed sensing . <eos> we consider the rate distortion problem for sensing the continuous space time physical temperature in a circular ring on which a heat source is applied over space and time , and which is also allowed to cool by radiation or convection to its surrounding medium . the heat source is modelled as a continuous space time stochastic process which is bandlimited over space and time . the temperature field is the result of a circular convolution over space and a continuous time causal filtering over time of the heat source with the green 's function corresponding to the heat equation , which is space and time invariant . the temperature field is sampled at uniform spatial locations by a set of sensors and it has to be reconstructed at a base station . the goal is to minimize the mean square error per second , for a given number of nats per second , assuming ideal communication channels between sensors and base station . we find a ) the centralized r c ( d ) function of the temperature field , where all the space time samples can be observed and encoded jointly . then , we obtain b ) the r s i ( d ) function , where each sensor , independently , encodes its samples optimally over time and c ) the r st i ( d ) function , where each sensor is constrained to encode also independently over time . we also study two distributed prediction based approaches a ) with perfect feedback from the base station , where temporal prediction is performed at the base station and each sensor performs differential encoding , and b ) without feedback , where each sensor locally performs temporal prediction .
developing a media space for remote synchronous parent child interaction . <eos> while supporting family communication has traditionally been a domain of interest for interaction designers , few research initiatives have explicitly investigated remote synchronous communication between children and parents . we discuss the design of the sharetable , a media space that supports synchronous interaction with children by augmenting videoconferencing with a camera projector system to allow for shared viewing of physical artifacts . we present an exploratory evaluation of this system , highlighting how such a media space may be used by families for learning and play activities . the sharetable was positively received by our participants and preferred over standard videoconferencing . informed by the results of our exploratory evaluation , we discuss the next design iteration of the sharetable and directions for future investigations in this area .
cross noise coupled architecture of complex bandpass delta sigma ad modulator . <eos> complex bandpass delta sigma ad modulators can provide superior performance to a pair of real bandpass delta sigma ad modulators of the same order . they process just input i and q signals , not image signals , and ad conversion can be realized with low power dissipation , so that they are desirable for such low if receiver applications . this paper proposes a new architecture for complex bandpass delta sigma ad modulators with cross noise coupled topology , which effectively raises the order of the complex modulator and achieves higher sqndr ( signal to quantization noise and distortion ratio ) with low power dissipation . by providing the cross coupled quantization noise injection to internal i and q paths , noise coupling between two quantizers can be realized in complex form , which enhances the order of noise shaping in complex domain , and provides a higher order ntf using a lower order loop filter in the complex delta sigma ad modulator . proposed higher order modulator can be realized just by adding some passive capacitors and switches , the additional integrator circuit composed of an operational amplifier is not necessary , and the performance of the complex modulator can be effectively raised without more power dissipation . we have performed simulation with matlab to verify the effectiveness of the proposed architecture . the simulation results show that the proposed architecture can achieve the realization of higher order enhancement , an improve sqndr of the complex bandpass delta sigma ad modulator .
facial motion cloning . <eos> we propose a method for automatically copying facial motion from one 3d face model to another , while preserving the compliance of the motion to the mpeg <digit> face and body animation ( fba ) standard . despite the enormous progress in the field of facial animation , producing a new animatable face from scratch is still a tremendous task for an artist . although many methods exist to animate a face automatically based on procedural methods , these methods still need to be initialized by defining facial regions or similar , and they lack flexibility because the artist can only obtain the facial motion that a particular algorithm offers . therefore a very common approach is interpolation between key facial expressions , usually called morph targets , containing either speech elements ( visemes ) or emotional expressions . following the same approach , the mpeg <digit> facial animation specification offers a method for interpolation of facial motion from key positions , called facial animation tables , which are essentially morph targets corresponding to all possible motions specified in mpeg <digit> . the problem of this approach is that the artist needs to create a new set of morph targets for each new face model . in case of mpeg <digit> there are <digit> morph targets , which is a lot of work to create manually . our method solves this problem by cloning the morph targets , i.e. by automatically copying the motion of vertices , as well as geometry transforms , from source face to target face while maintaining the regional correspondences and the correct scale of motion . it requires the user only to identify a subset of the mpeg <digit> feature points in the source and target faces . the scale of the movement is normalized with respect to mpeg <digit> normalization units ( fapus ) , meaning that the mpeg <digit> fba compliance of the copied motion is preserved . our method is therefore suitable not only for cloning of free facial expressions , but also of mpeg <digit> compatible facial motion , in particular the facial animation tables . we believe that facial motion cloning offers dramatic time saving to artists producing morph targets for facial animation or mpeg <digit> facial animation tables .
an analysis of the intel 80x86 security architecture and implementations . <eos> an in depth analysis of the 80x86 processor families identifies architectural properties that may have unexpected , and undesirable , results in secure computer systems . in addition , reported implementation errors in some processor versions render them undesirable for secure systems because of potential security and reliability problems . in this paper , we discuss the imbalance in scrutiny for hardware protection mechanisms relative to software , and why this imbalance is increasingly difficult to justify as hardware complexity increases . we illustrate this difficulty with examples of architectural subtleties and reported implementation errors .
realtime concatenation technique for skeletal motion in humanoid animation . <eos> in this paper , we propose a realtime concatenation technique between basic skeletal motions obtained ly the motion capture technique and etc. to generate a lifelike behavior for a humanoid character ( avatar ) . we execute several experiments to show the advantage and the property of our technique and also report the results . finally , we describe our applied system called wonderspace which leads participants to the exciting and attractive virtual worlds with humanoid characters in cyberspace . our concatenation technique has the following features ( <digit> ) based on a blending method between a preceding motion and a succeeding motion by a transition function , ( <digit> ) realizing smooth transition , monotone transition , and equivalent transition by the transition function called paste function , ( <digit> ) generating a connecting interval by making the backward and forward predictions for the preceding and succeeding motions , ( <digit> ) executing the prediction under the hypothesis of the smooth stopping state or the state of connecting motion , ( <digit> ) controlling the prediction intervals by the parameter indicating the importance of the motion , and ( <digit> ) realizing realtime calculation .
call by value is dual to call by name . <eos> the rules of classical logic may be formulated in pairs corresponding to de morgan duals rules about are dual to rules about v. a line of work , including that of filinski ( <digit> ) , griffin ( <digit> ) , parigot ( <digit> ) , danos , joinet , and schellinx ( <digit> ) , selinger ( 1998,2001 ) , and curien and herbelin ( <digit> ) , has led to the startling conclusion that call by value is the de morgan dual of call by name . this paper presents a dual calculus that corresponds to the classical sequent calculus of gentzen ( <digit> ) in the same way that the lambda calculus of church ( 1932,1940 ) corresponds to the intuitionistic natural deduction of gentzen ( <digit> ) . the paper includes crisp formulations of call by value and call by name that are obviously dual no similar formulations appear in the literature . the paper gives a cps translation and its inverse , and shows that the translation is both sound and complete , strengthening a result in curien and herbelin ( <digit> ) . note . this paper uses color to clarify the relation of types and terms , and of source and target calculi . if the url below is not in blue , please download the color version , which can be found in the acm digital library archive for icfp <digit> , at http portal.acm.org proceedings icfp archive , or by googling ' wadler dual ' .
universal automata and nfa learning . <eos> the aim of this paper is to develop a new algorithm that , with a complete sample as input , identifies the family of regular languages by means of nondeterministic finite automata . it is a state merging algorithm . one of its main features is that the convergence ( which is proved ) is achieved independently from the order in which the states are merged , that is , the merging of states may be done randomly . ( c ) <digit> elsevier b.v. all rights reserved .
effect of load models on assessment of energy losses in distributed generation planning . <eos> distributed generation ( dg ) is gaining in significance due to the keen public awareness of the environmental impacts of electric power generation and significant advances in several generation technologies which are much more environmentally friendly ( wind power generation , micro turbines , fuel cells , and photovoltaic ) than conventional coal , oil and gas fired plants . accurate assessment of energy losses when dg is connected is gaining in significance due to the developments in the electricity market place , such as increasing competition , real time pricing and spot pricing . however , inappropriate modelling can give rise to misleading results . this paper presents an investigation into the effect of load models on the predicted energy losses in dg planning . following a brief introduction the paper proposes a detailed voltage dependent load model , for dg planning use , which considers three categories of loads residential , industrial and commercial . the paper proposes a methodology to study the effect of load models on the assessment of energy losses based on time series simulations to take into account both the variations of renewable generation and load demand . a comparative study of energy losses between the use of a traditional constant load model and the voltage dependent load model and at various load levels is carried out using a <digit> node example power system . simulations presented in the paper indicate that the load model to be adopted can significantly affect the results of dg planning .
generational stack collection and profile driven pretenuring . <eos> this paper presents two techniques for improving garbage collection performance generational stack collection and profile driven pretenuring . the first is applicable to stack based implementations of functional languages while the second is useful for any generational collector . we have implemented both techniques in a generational collector used by the til compiler ( tarditi , morrisett , cheng , stone , harper , and lee <digit> ) , and have observed decreases in garbage collection times of as much as <digit> % and <digit> % , respectively.functional languages encourage the use of recursion which can lead to a long chain of activation records . when a collection occurs , these activation records must be scanned for roots . we show that scanning many activation records can take so long as to become the dominant cost of garbage collection . however , most deep stacks unwind very infrequently , so most of the root information obtained from the stack remains unchanged across successive garbage collections . generational stack collection greatly reduces the stack scan cost by reusing information from previous scans.generational techniques have been successful in reducing the cost of garbage collection ( ungar <digit> ) . various complex heap arrangements and tenuring policies have been proposed to increase the effectiveness of generational techniques by reducing the cost and frequency of scanning and copying . in contrast , we show that by using profile information to make lifetime predictions , pretenuring can avoid copying data altogether . in essence , this technique uses a refinement of the generational hypothesis ( most data die young ) with a locality principle concerning the age of data most allocations sites produce data that immediately dies , while a few allocation sites consistently produce data that survives many collections .
regulated secretion in chromaffin cells . <eos> arfs constitute a family of structurally related proteins that forms a subset of the ras gtpases . in chromaffin cells , secretagogue evoked stimulation triggers the rapid translocation of arf6 from secretory granules to the plasma membrane and the concomitant activation of pld in the plasma membrane . both pld activation and catecholamine secretion are strongly inhibited by a synthetic peptide corresponding to the n terminal domain of arf6 . arno , a potential guanine nucleotide exchange factor for arf6 , is expressed and localized in the plasma membrane of chromaffin cells . using permeabilized cells , we found that the introduction of anti arno antibodies into the cytosol inhibits both pld activation and catecholamine secretion . chromaffin cells express pld1 at the plasma membrane . we found that microinjection of the catalytically inactive pld1 ( k898r ) dramatically reduces catecholamine secretion monitored by amperometry , most likely by interfering with a late postdocking step of calcium regulated exocytosis . we propose that arno arf6 participate in the exocytotic reaction by controlling the plasma membrane bound pld1 . by generating fusogenic lipids at the exocytotic sites , pld1 may represent an essential component of the fusion machinery in neuroendocrine cells .
analysis of elastic wave propagation in a functionally graded thick hollow cylinder using a hybrid mesh free method . <eos> in this paper , a hybrid mesh free method based on generalized finite difference ( gfd ) and newmark finite difference ( nfd ) methods is presented to calculate the velocity of elastic wave propagation in functionally graded materials ( fgms ) . the physical domain to be considered is a thick hollow cylinder made of functionally graded material in which mechanical properties are graded in the radial direction only . a power law variation of the volume fractions of the two constituents is assumed for mechanical property variation . the cylinder is excited by shock loading to obtain the time history of the radial displacement . the velocity of elastic wave propagation in functionally graded cylinder is calculated from periodic behavior of the radial displacement in time domain . the effects of various grading patterns and various constitutive mechanical properties on the velocity of elastic wave propagation in functionally graded cylinders are studied in detail . numerical results demonstrate the efficiency of the proposed method in simulating the wave propagation in fgms .
a nonparametric methodology for evaluating convergence in a multi input multi output setting . <eos> the paper presents a novel nonparametric methodology to evaluate convergence . we develop two new indexes to evaluate convergence and convergence . the indexes developed allow evaluations using multiple inputs and outputs . the methodology complements productivity assessments based on the malmquist index . the methodology is applied to portuguese construction companies operating in <digit> .
the effects of interaction frequency on the optimization performance of cooperative coevolution . <eos> cooperative coevolution is often used to solve difficult optimization problems by means of problem decomposition . its performance on this task is influenced by many design decisions . it would be useful to have some knowledge of the performance effects of these decisions , in order to make the more beneficial ones . in this paper we study the effects on performance of the frequency of interaction between populations . we show them to be problem dependent and use dynamics analysis to explain this dependency .
brain inspired method for solving fuzzy multi criteria decision making problems ( bifmcdm ) . <eos> we propose a brain inspired method for solving fuzzy decision making problems . we study a websites ranking problem for an e alliance . processing fuzzy information as just abstract element could lead to wrong decision .
environmental model access and interoperability the geo model web initiative . <eos> the group on earth observation ( geo ) model web initiative utilizes a model as a service approach to increase model access and sharing . it relies on gradual , organic growth leading towards dynamic webs of interacting models , analogous to the world wide web . the long term vision is for a consultative infrastructure that can help address what if and other questions that decision makers and other users have . four basic principles underlie the model web open access , minimal barriers to entry , service driven , and scalability any implementation approach meeting these principles will be a step towards the long term vision . implementing a model web encounters a number of technical challenges , including information modelling , minimizing interoperability agreements , performance , and long term access , each of which has its own implications . for example , a clear information model is essential for accommodating the different resources published in the model web ( model engines , model services , etc. ) , and a flexible architecture , capable of integrating different existing distributed computing infrastructures , is required to address the performance requirements . architectural solutions , in keeping with the model web principles , exist for each of these technical challenges . there are also a variety of other key challenges , including difficulties in making models interoperable calibration and validation and social , cultural , and institutional constraints . although the long term vision of a consultative infrastructure is clearly an ambitious goal , even small steps towards that vision provide immediate benefits . a variety of activities are now in progress that are beginning to take those steps . ( c ) <digit> elsevier ltd. all rights reserved .
non uniform micro channel design for stacked 3d ics . <eos> micro channel cooling shows great potential in removing high density heat in 3d circuits . the current micro channel heat sink designs spread the entire surface to be cooled with micro channels . this approach , though might provide sufficient cooling , requires quite high pumping power . in this paper , we investigate the non uniform allocation of micro channels to provide sufficient cooling with less pumping power . specifically , we decide the count , location and pumping pressure drop flow rate of micro channels such that acceptable cooling is achieved at minimum pumping power . thermal wake effect and runtime pressure drop flow rate control are also considered . the experiments showed that , compared with the conventional design which spreads micro channels all over the chip , our non uniform microchannel design achieves <digit> <digit> % pumping power saving .
neurocomputing techniques to dynamically forecast spatiotemporal air pollution data . <eos> real time monitoring , forecasting and modeling air pollutants concentrations in major urban centers is one of the top priorities of all local and national authorities globally . this paper studies and analyzes the parameters related to the problem , aiming in the design and development of an effective machine learning model and its corresponding system , capable of forecasting dangerous levels of ozone ( o3 ) concentrations in the city center of athens and more specifically in the athinas air quality monitoring station . this is a multi parametric case , so an effort has been made to combine a vast number of data vectors from several operational nearby measurements stations . the final result was the design and construction of a group of artificial neural networks capable of estimating o3 concentrations in real time mode and also having the capacity of forecasting the same values for future time intervals of <digit> , <digit> , <digit> and 6h , respectively .
revisiting rational bubbles in the g <digit> stock markets using the fourier unit root test and the nonparametric rank test for cointegration . <eos> this paper re investigates whether rational bubbles existed in the g <digit> stock markets during the period of january <digit> june <digit> using the newly developed fourier unit root test and a nonparametric rank test for cointegration . the empirical results from our fourier unit test indicate that the null hypothesis of j ( <digit> ) unit root in stock prices can be rejected for canada , france , italy and the uk . however , the empirical results from the rank test reveal that rational bubbles did not exist in the g <digit> stock markets during the sample period . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
orchestrating stream graphs using model checking . <eos> in this article we use model checking to statically distribute and schedule synchronous dataflow ( sdf ) graphs on heterogeneous execution architectures . we show that model checking is capable of providing an optimal solution and it arrives at these solutions faster ( in terms of algorithm runtime ) than equivalent ilp formulations . furthermore , we also show how different types of optimizations such as task parallelism , data parallelism , and state sharing can be included within our framework . finally , comparison of our approach with the current state of the art heuristic techniques show the pitfalls of these techniques and gives a glimpse of how these heuristic techniques can be improved .
model averaged wald confidence intervals . <eos> the process of model averaging has become increasingly popular as a method for performing inference in the presence of model uncertainty . in the frequentist setting , a model averaged estimate of a parameter is calculated as the weighted sum of single model estimates , often using weights derived from an information criterion such as aic or bic . a standard method for calculating a model averaged confidence interval is to use a wald interval centered around the model averaged estimate . we propose a new method for construction of a model averaged wald confidence interval , based on the idea of model averaging tail areas of the sampling distributions of the single model estimates . we use simulation to compare the performance of the new method and existing methods , in terms of coverage rate and interval width . the new method consistently outperforms existing methods in terms of coverage , often for little increase in the interval width . we also consider choice of model weights , and find that aic weights are preferable to either aicc or bic weights in terms of coverage .
dynamics of the difference equation x n <digit> x n p x n k x n q . <eos> we study the invariant interval , the character of semicycles , the global stability , and the boundedness of the difference equation
cross validation based single response adaptive design of experiments for kriging metamodeling of deterministic computer simulations . <eos> a new approach for single response adaptive design of deterministic computer experiments is presented . the approach is called sfcvt , for space filling cross validation tradeoff . sfcvt uses metamodeling to obtain an estimate of cross validation errors , which are maximized subject to a constraint on space filling to determine sample points in the design space . the proposed method is compared , using a test suite of forty four numerical examples , with three doe methods from the literature . the numerical test examples can be classified into symmetric and asymmetric functions . symmetric examples refer to functions for which the extreme points are located symmetrically in the design space and asymmetric examples are those for which the extreme regions are not located in a symmetric fashion in the design space . based upon the comparison results for the numerical examples , it is shown that sfcvt performs better than an existing adaptive and a non adaptive doe method for asymmetric multimodal functions with high nonlinearity near the boundary , and is comparable for symmetric multimodal functions and other test problems . the proposed approach is integrated with a multi scale heat exchanger optimization tool to reduce the computational effort involved in the design of novel air to water heat exchangers . the resulting designs are shown to be significantly more compact than mainstream heat exchanger designs .
bottlenecks and hubs in inferred networks are important for virulence in salmonella typhimurium . <eos> recent advances in experimental methods have provided sufficient data to consider systems as large networks of interconnected components . high throughput determination of protein protein interaction networks has led to the observation that topological bottlenecks , proteins defined by high centrality in the network , are enriched in proteins with systems level phenotypes such as essentiality . global transcriptional profiling by microarray analysis has been used extensively to characterize systems , for example , examining cellular response to environmental conditions and effects of genetic mutations . these transcriptomic datasets have been used to infer regulatory and functional relationship networks based on co regulation . we use the context likelihood of relatedness ( clr ) method to infer networks from two datasets gathered from the pathogen salmonella typhimurium one under a range of environmental culture conditions and the other from deletions of <digit> regulators found to be essential in virulence . bottleneck and hub genes were identified from these inferred networks , and we show for the first time that these genes are significantly more likely to be essential for virulence than their non bottleneck or non hub counterparts . networks generated using simple similarity metrics ( correlation and mutual information ) did not display this behavior . overall , this study demonstrates that topology of networks inferred from global transcriptional profiles provides information about the systems level roles of bottleneck genes . analysis of the differences between the two clr derived networks suggests that the bottleneck nodes are either mediators of transitions between system states or sentinels that reflect the dynamics of these transitions .
semantic manipulation of users queries and modeling the health and nutrition preferences . <eos> people depend on popular search engines to look for the desired health and nutrition information . many search engines can not semantically interpret , enrich the users natural language queries easily and hence do not retrieve the personalized information that fits the users needs . one reason for retrieving irrelevant information is the fact that people have different preferences where each one likes and dislikes certain types of food . in addition , some people have specific health conditions that restrict their food choices and encourage them to take other foods . moreover , the cultures , where people live in , influence food choices while the search engines are not aware of these cultural habits . therefore , it will be helpful to develop a system that semantically manipulates users queries and models the users preferences to retrieve personalized health and food information . in this paper , we harness semantic web technology to capture users preferences , construct a nutritional and health oriented users profile , model the users preferences and use them to organize the related knowledge so that users can retrieve personalized health and food information . we present an approach that uses the personalization techniques based on integrated domain ontologies , pre constructed by domain experts , to retrieve relevant food and health information that is consistent with peoples needs . we implemented the system , and the empirical results show high precision and recall with a superior users satisfaction .
swift and stable polygon growth and broken line offset . <eos> the problem of object growing ( offsetting the object boundary by a certain distance ) is an important and widely studied problem . in this paper we propose a new approach for offsetting the boundary of an object described by segments which are not necessarily connected . this approach avoids many destructive special cases that arise in some heuristic based approaches . moreover , the method developed in this paper is stable in that it does not fail because of missing segments . also , the time required for the computation of the offset is relatively short and therefore inexpensive , i.e. it is expected to be o ( n log n ) .
ilp based multistage placement of pmus with dynamic monitoring constraints . <eos> a multistage planning of pmus placement for power systems is proposed . the methodology takes into account network expansion plans . system observability is maximized over time . the methodology identifies nodes to locate pmus based on security criteria . the stepwise approach allows the utilities to develop a path for pmu placement .
indoor solar energy harvesting for sensor network router nodes . <eos> a unique method has been developed to scavenge energy from monocrystaline solar cells to power wireless router nodes used in indoor applications . the systems energy harvesting module consists of solar cells connected in series parallel combination to scavenge energy from 34w fluorescent lights . a set of ultracapacitors were used as the energy storage device . two router nodes were used as a router pair at each route point to minimize power consumption . test results show that the harvesting circuit which acted as a plug in to the router nodes manages energy harvesting and storage , and enables near perpetual , harvesting aware operation of the router node .
detecting data records in semi structured web sites based on text token clustering . <eos> this paper describes a new approach to the use of clustering for automatic data detection in semi structured web pages . unlike most exiting web information extraction approaches that usually apply wrapper induction techniques to manually labelled web pages , this approach avoids the pattern induction process by using clustering techniques on unlabelled pages . in this approach , a variant hierarchical agglomerative clustering ( hac ) algorithm called k neighbours hac is developed which uses the similarities of the data format ( html tags ) and the data content ( text string values ) to group similar text tokens into clusters . we also develop a new method to label text tokens to capture the hierarchical structure of html pages and an algorithm for mapping labelled text tokens to xml . the new approach is tested and compared with several common existing wrapper induction systems on three different sets of web pages . the results suggest that the new approach is effective for data record detection and that it outperforms these common existing approaches examined on these web sites . compared with the existing approaches , the new approach does not require training and successfully avoids the explicit pattern induction process , and accordingly the entire data detection process is simpler .
the design of gsc fieldlog ontology based software for computer aided geological field mapping . <eos> databases containing geological field information are increasingly being constructed directly in the field . the design of such databases is often challenged by opposing needs ( <digit> ) the individual need to maintain flexibility of database structure and contents , to accommodate unexpected field situations and ( <digit> ) the corporate need to retain compatibility between distinct field databases , to accommodate their interoperability . the fieldlog mapping software balances these needs by exploiting a domain ontology developed for field information , one that enables field database flexibility and facilitates compatibility . the ontology consists of cartographic , geospatial , geological and metadata objects that form a common basis for interoperability and that can be instantiated by users into customized field databases . the design of the fieldlog software , its foundation on this ontology , and the resulting benefits to usability are presented in this paper . the discussion concentrates on satisfying the flexibility requirement by implementing the ontology as a generic data model within an object relational database environment issues of interoperability are not considered in detail . benefits of this ontologic driven approach are also developed within a description of the fieldlog application , including ( <digit> ) improved usability due to an user interface based on the geological components of the ontology , and ( <digit> ) diminished technical prerequisites as users are shielded from the many database and gis technicalities handled by the ontology .
systemic disease sequelae in chronic inflammatory diseases and chronic psychological stress comparison and pathophysiological model . <eos> in chronic inflammatory diseases ( cids ) , the neuroendocrineimmune crosstalk is important to allocate energy rich substrates to the activated immune system . since the immune system can request energy rich substrates independent of the rest of the body , i refer to it as the selfish immune system , an expression that was taken from the theory of the selfish brain , giving the brain a similar position . in cids , the theory predicts the appearance of long term disease sequelae , such as metabolic syndrome . since long standing energy requirements of the immune system determine disease sequelae , the question arose as to whether chronic psychological stress due to chronic activation of the brain causes similar sequelae . indeed , there are many similarities however , there are also differences . a major difference is the behavior of body weight ( constant in cids versus loss or gain in stress ) . to explain this discrepancy , a new pathophysiological theory is presented that places inflammation and stress axes in the middle .
setting parameters by example . <eos> we introduce a class of inverse parametric optimization problems , in which one is given both a parametric optimization problem and a desired optimal solution the task is to determine parameter values that lead to the given solution . we describe algorithms for solving such problems for minimum spanning trees , shortest paths , and other optimal subgraph problems and discuss applications in multicast routing , vehicle path planning , resource allocation , and board game programming .
pedvis a structured , space efficient technique for pedigree visualization . <eos> public genealogical databases are becoming increasingly populated with historical data and records of the current population 's ancestors . as this increasing amount of available information is used to link individuals to their ancestors , the resulting trees become deeper and more dense , which justifies the need for using organized , space efficient layouts to display the data . existing layouts are often only able to show a small subset of the data at a time . as a result , it is easy to become lost when navigating through the data or to lose sight of the overall tree structure . on the contrary , leaving space for unknown ancestors allows one to better understand the tree 's structure , but leaving this space becomes expensive and allows fewer generations to be displayed at a time . in this work , we propose that the h tree based layout be used in genealogical software to display ancestral trees . we will show that this layout presents an increase in the number of displayable generations , provides a nicely arranged , symmetrical , intuitive and organized fractal structure , increases the user 's ability to understand and navigate through the data , and accounts for the visualization requirements necessary for displaying such trees . finally , user study results indicate potential for user acceptance of the new layout .
the evolution of mobile communications in europe the transition from the second to the third generation . <eos> this paper analyses the evolution of the mobile communications industry in the european union . the research focuses its interest on the different roles played by the regulator in europe and in other regions of the world ( mainly the us ) . the diffusion of gsm was extraordinarily fast in europe , mainly due to the adoption of a unified standard from inception . this rapid diffusion has resulted in an important competitive advantage for european operators . interestingly , while the regulator acted similarly in the case of umts , the development of the latter has faced many problems and , presently , its diffusion is still low ( about <digit> % in the eu ) . the paper also offers basic information on market structure that may be useful for extracting some preliminary conclusions about the degree of rivalry within the industry and the differences that can be observed between european countries .
a decision support system for design of transmission system of low power tractors . <eos> a decision support system ( dss ) was developed in visual basic 6.0 programming language to design transmission system of low horsepower agricultural tractors , which involved the design of clutch and gearbox . the dss provided graphical user interface by linking databases to support decision on design of transmission system for low horsepower tractors on the basis of modified asabe draft model . the developed program for design of tractor transmission system calculated clutch size , gear ratios , number of teeth on each gear , and various gear design parameters . related deviation was computed for design of transmission system of tractors based on measured and predicted values ( simulated ) . the related deviation was less than <digit> % for design of clutch plate outer diameter and less than <digit> % for inner diameter . there was less than <digit> % variation between the predicted results by the developed dss and those obtained from actual measurement for design of gear ratio . the dss program was user friendly and efficient for predicting the design of transmission system for different tractor models to meet requirements of research institutions and industry . <digit> wiley periodicals , inc. comput . appl . eng . educ . comput appl eng educ 23 760770 , <digit> view this article online at wileyonlinelibrary.com journal cae doi 10.1002 cae .21648
distributed automation pabadis versus hms . <eos> distributed control systems ( dcs ) have gained huge interest in the automation business . several approaches have been made which aim at the design and application of dcs to improve system flexibility and robustness . important approaches are ( among others ) the holonic manufacturing systems ( hms ) and the plant automation based on distributed systems ( pabadis ) approach . pabadis deals with plant automation systems in a distributed way using generic mobile and stationary agents and plug and participate facilities within a flat structure as key points of the developed control architecture . hms deals with a similar structure , but aims more at a control hierarchy of special agents . this paper gives a description of the pabadis project and makes comparisons between the two concepts , showing advantages and disadvantages of both systems . based on this paper , it will be possible to observe the abilities and drawbacks of distributed agent based control systems .
a new supervised learning algorithm for multiple spiking neural networks with application in epilepsy and seizure detection . <eos> a new multi spiking neural network ( muspinn ) model is presented in which information from one neuron is transmitted to the next in the form of multiple spikes via multiple synapses . a new supervised learning algorithm , dubbed multi spikeprop , is developed for training muspinn . the model and learning algorithm employ the heuristic rules and optimum parameter values presented by the authors in a recent paper that improved the efficiency of the original single spiking spiking neural network ( snn ) model by two orders of magnitude . the classification accuracies of muspinn and multi spikeprop are evaluated using three increasingly more complicated problems the xor problem , the fisher iris classification problem , and the epilepsy and seizure detection ( eeg classification ) problem . it is observed that muspinn learns the xor problem in twice the number of epochs compared with the single spiking snn model but requires only one fourth the number of synapses . for the iris and eeg classification problems , a modular architecture is employed to reduce each <digit> class classification problem to three <digit> class classification problems and improve the classification accuracy . for the complicated eeg classification problem a classification accuracy in the range of 90.7 .8 % was achieved , which is significantly higher than the <digit> % classification accuracy obtained using the single spiking snn with spikeprop .
investigations about replication of empirical studies in software engineering a systematic mapping study . <eos> two recent mapping studies which were intended to verify the current state of replication of empirical studies in software engineering ( se ) identified two sets of studies empirical studies actually reporting replications ( published between <digit> and <digit> ) and a second group of studies that are concerned with definitions , classifications , processes , guidelines , and other research topics or themes about replication work in empirical software engineering research ( published between <digit> and <digit> ) . in this current article , our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research . we applied the systematic literature review method to build a systematic mapping study , in which the primary studies were collected by two previous mapping studies covering the period <digit> complemented by manual and automatic search procedures that collected articles published in <digit> . we analyzed <digit> papers reporting studies about replication published in the last <digit> years . these papers explore different topics related to concepts and classifications , presented guidelines , and discuss theoretical issues that are relevant for our understanding of replication in our field . we also investigated how these <digit> papers have been cited in the <digit> replication papers published between <digit> and <digit> . replication in se still lacks a set of standardized concepts and terminology , which has a negative impact on the replication work in our field . to improve this situation , it is important that the se research community engage on an effort to create and evaluate taxonomy , frameworks , guidelines , and methodologies to fully support the development of replications .
worst case analysis of memory allocation algorithms . <eos> various memory allocation problems can be modeled by the following abstract problem . given a list a equil ( agr <digit> , agr <digit> , ... agr n , ) of real numbers in the range ( <digit> , <digit> , place these in a minimum number of bins so that no bin holds numbers summing to more than <digit> . we let a be the smallest number of bins into which the numbers of list a may be placed . since a general placement algorithm for attaining a appears to be impractical , it is important to determine good heuristic methods for assigning numbers of bins . we consider four such simple methods and analyze the worst case performance of each , closely bounding the maximum of the ratio of the number of bins used by each method applied to list a to the optimal quantity a .
building a financial diagnosis system based on fuzzy logic production system . <eos> the purpose of this study is to build a financial expert system based on fuzzy theory and fuzzy logic production system ( flops ) , which is an expert tool for processing the ambiguity . the study consists if four parts . for the first part , the basic features of expert systems are presented . for the second part , fuzzy concepts and the evaluation of classical expert systems to fuzzy expert systems will be presented . for the third part , the expert system shell ( flops ) used in this study will be described . for the last part , it will be presented the financial diagnosis system , developed by using the wall 's seven ratios , traditional seven ratios and also <digit> ratios selected by a financial expert . alter analyzing and investigating these three kinds of methods , financial diagnosis system will be developed as a fuzzy expert system which used a membership function bared on averages and standard deviation . at the last step , the new approach will be tried by increasing the fuzzy sets far five membership functions . some practical examples will be given . throughout the paper , the way of budding i financial diagnosis system based on fuzzy expert system is stressed .
a hybrid boundary element wave based method for an efficient solution of bounded acoustic problems with inclusions . <eos> this paper presents a novel hybrid approach for the efficient solution of bounded acoustic problems with arbitrarily shaped inclusions . the hybrid method couples the wave based method ( wbm ) and the boundary element method ( bem ) in order to benefit from the prominent advantages of both . the wbm is based on an indirect trefftz approach as such , it uses exact solutions of the governing equations to approximate the field variables . it has a high computational advantage as compared to conventional element based methods , when applied on moderately complex geometries . the bem , on the other hand , can tackle complex geometries with ease . however , it can be computationally expensive . the hybrid boundary element wave based method ( be wbm ) combines the best properties of the two it makes use of the efficient wbm for the moderately complex bounded domains and utilizes the flexibility of the bem for the complex objects that reside in the bounded domains . the accuracy and the efficiency of the method is demonstrated with three numerical examples , where the hybrid be wbm is shown to be more efficient than a quadratic finite element method ( fem ) . while the hybrid method provides efficient solution for the bounded problems with inclusions , it also brings certain conceptual advantages over the fem . the fact that it is a boundary type method with an easy refinement concept reduces the modeling effort on the preprocessing step . moreover , for certain optimization scenarios such as optimization of the position of inclusions , the fem becomes disadvantageous because of its domain discretization requirements for each iteration . on the other hand , the hybrid method allows reusing of the fixed geometries and only needs recalculation of the coupling matrices without a further need of preprocessing . as such , the hybrid method combines efficiency with versatility .
a fuzzy clustering based binary threshold bispectrum estimation approach . <eos> a fuzzy clustering bispectrum estimation approach is proposed in this paper and applied on the rolling element bearing fault recognition . the method combines the basic higher order spectrum theory and fuzzy clustering technique in data mining . at first , all the bispectrum estimation results of the training samples and test samples are taken binarization threshold processing and turned into binary feature images . then , the binary feature images of the training samples are used to construct object templates including kernel images and domain images . every fault category has one object templates . at last , by calculating the distances between test samples binary feature images and the different object templates , the object classification and pattern recognition can be effectively accomplished . bearing is the most important and much easier to be damaged component in rotating machinery . furthermore , there exist large amounts of noise jamming and nonlinear coupling components in bearing vibration signals . higher order cumulants , which can quantitatively describe the nonlinear characteristic signals with close relationship between the mechanical faults , are introduced in this paper to de noise the raw bearing vibration signals and obtain the bispectrum estimation pictures . at last , the rolling bearing fault diagnosis experiment results showed that the classification was completely correct .
scalability of write ahead logging on multicore and multisocket hardware . <eos> the shift to multi core and multi socket hardware brings new challenges to database systems , as the software parallelism determines performance . even though database systems traditionally accommodate simultaneous requests , a multitude of synchronization barriers serialize execution . write ahead logging is a fundamental , omnipresent component in aries style concurrency and recovery , and one of the most important yet to be addressed potential bottlenecks , especially in oltp workloads making frequent small changes to data . in this paper , we identify four logging related impediments to database system scalability . each issue challenges different level in the software architecture ( a ) the high volume of small sized i o requests may saturate the disk , ( b ) transactions hold locks while waiting for the log flush , ( c ) extensive context switching overwhelms the os scheduler with threads executing log i os , and ( d ) contention appears as transactions serialize accesses to in memory log data structures . we demonstrate these problems and address them with techniques that , when combined , comprise a holistic , scalable approach to logging . our solution achieves a <digit> <digit> % speedup over a modern database system when running log intensive workloads , such as the tpc b and tatp benchmarks , in a single socket multiprocessor server . moreover , it achieves log insert throughput over 2.2 gb s for small log records on the single socket server , roughly <digit> times higher than the traditional way of accessing the log using a single mutex . furthermore , we investigate techniques on scaling the performance of logging to multi socket servers . we present a set of optimizations which partly ameliorate the latency penalty that comes with multi socket hardware , and then we investigate the feasibility of applying a distributed log buffer design at the socket level .
time driven priority router implementation analysis and experiments . <eos> low complexity solutions to provide deterministic quality over packet switched networks while achieving high resource utilization have been an open research issue for many years . service differentiation combined with resource overprovisioning has been considered an acceptable compromise and widely deployed given that the amount of traffic requiring quality guarantees has been limited . this approach is not viable , though , as new bandwidth hungry applications , such as video on demand , telepresence , and virtual reality , populate networks invalidating the rationale that made it acceptable so far . time driven priority represents a potentially interesting solution . however , the fact that the network operation is based on a time reference shared by all nodes raises concerns on the complexity of the nodes , from the point of view of both their hardware and software architecture . this work analyzes the implications that the timing requirements of time driven priority have on network nodes and shows how proper operation can be ensured even when system components introduce timing uncertainties . experimental results on a time driven priority router implementation based on a personal computer both validate the analysis and demonstrate the feasibility of the technology even on an architecture that is not designed for operating under timing constraints .
computationally sound symbolic security reduction analysis of the group key exchange protocols using bilinear pairings . <eos> the security of the group key exchange protocols has been widely studied in the cryptographic community in recent years . current work usually applies either the computational approach or the symbolic approach for security analysis . the symbolic approach is more efficient than the computational approach , because it can be easily automated . however , compared with the computational approach , it has to overcome three challenges ( <digit> ) the computational soundness is unclear ( <digit> ) the number of participants must be fixed and ( <digit> ) the advantage of efficiency disappears , if the number of participants is large . this paper proposes a computationally sound symbolic security reduction approach to resolve these three issues . on one hand , combined with the properties of the bilinear pairings , the universally composable symbolic analysis ( ucsa ) approach is extended from the two party protocols to the group key exchange protocols . meanwhile , the computational soundness of the symbolic approach is guaranteed . on the other hand , for the group key exchange protocols which satisfy the syntax of the simple protocols proposed in this paper , the security is proved to be unrelated with the number of participants . as a result , the symbolic approach just needs to deal with the protocols among three participants . this makes the symbolic approach has the ability to handle arbitrary number of participants . therefore , the advantage of efficiency is still guaranteed . the proposed approach can also be applied to other types of cryptographic primitives besides bilinear pairing for computationally sound and efficient symbolic analysis of group key exchange protocols .
dynamic gradient method for pebs detection in power system transient stability assessment . <eos> in methods for assessing the critical clearing time based on the transient energy function , the dominant procedures in use for detecting the exit point across the potential energy boundary surface ( pebs ) are the ray and the gradient methods . because both methods rely on the geometrical characteristics of the post fault potential energy surface , they may yield erroneous results . in this paper , a more reliable method for pebs detection is proposed . it is called the dynamic gradient method to indicate that from a given system state , a small portion of the trajectory of the gradient system is approximated and tested for convergence toward the post fault stable equilibrium point . it is shown that a trade off between computing time and reliability can be found as the number of machines in the system becomes greater . the method is illustrated on <digit> machine and <digit> machine systems .
exact matrix completion via convex optimization . <eos> we consider a problem of considerable practical interest the recovery of a data matrix from a sampling of its entries . suppose that we observe m entries selected uniformly at random from a matrix m. can we complete the matrix and recover the entries that we have not seen we show that one can perfectly recover most low rank matrices from what appears to be an incomplete set of entries . we prove that if the number m of sampled entries obeys m > cn ( 1.2 ) r log n for some positive numerical constant c , then with very high probability , most n x n matrices of rank r can be perfectly recovered by solving a simple convex optimization program . this program finds the matrix with minimum nuclear norm that fits the data . the condition above assumes that the rank is not too large . however , if one replaces the 1.2 exponent with 1.25 , then the result holds for all values of the rank . similar results hold for arbitrary rectangular matrices as well . our results are connected with the recent literature on compressed sensing , and show that objects other than signals and images can be perfectly reconstructed from very limited information .
a general framework for expressing preferences in causal reasoning and planning . <eos> we consider the problem of representing arbitrary preferences in causal reasoning and planning systems . in planning , a preference may be seen as a goal or constraint that is desirable , but not necessary , to satisfy . to begin , we define a very general query language for histories , or interleaved sequences of world states and actions . based on this , we specify a second language in which preferences are defined . a single preference defines a binary relation on histories , indicating that one history is preferred to the other . from this , one can define global preference orderings on the set of histories , the maximal elements of which are the preferred histories . the approach is very general and flexible thus it constitutes a base language in terms of which higher level preferences may be defined . to this end , we investigate two fundamental types of preferences that we call choice and temporal preferences . we consider concrete strategies for these types of preferences and encode them in terms of our framework . we suggest how to express aggregates in the approach , allowing , e.g. the expression of a preference for histories with lowest total action costs . last , our approach can be used to express other approaches and so serves as a common framework in which such approaches can be expressed and compared . we illustrate this by indicating how an approach due to son and pontelli can be encoded in our approach , as well as the language pddl3 .
a cost sensitive decision tree algorithm with two adaptive mechanisms . <eos> an adaptive selecting cut point mechanism is designed to build a classifier . adaptive removing attribute mechanism will remove the redundant attributes . we adopt two mechanisms to design algorithm which for classifier construction . experimental results show the effectiveness and feasibility of our algorithm .
solving the buckley leverett equation with gravity in a heterogeneous porous medium . <eos> immiscible two phase flow in porous media can be described by the fractional flow model . if capillary forces are neglected , then the saturation equation is a non linear hyperbolic conservation law , known as the buckley leverett equation . this equation can be numerically solved by the method of godunov , in which the saturation is computed from the solution of riemann problems at cell interfaces . at a discontinuity of permeability this solution has to be constructed from two flux functions . in order to determine a unique solution an entropy inequality is needed . in this article an entropy inequality is derived from a regularisation procedure , where the physical capillary pressure term is added to the buckley leverett equation . this entropy inequality determines unique solutions of riemann problems for all initial conditions . it leads to a simple recipe for the computation of interface fluxes for the method of godunov .
an exploratory study of enterprise resource planning adoption in greek companies . <eos> purpose to examine enterprise resource planning ( erp ) adoption in greek companies , and explore the effects of uncertainty on the performance of these systems and the methods used to cope with uncertainty . design methodology approach this research was exploratory and six case studies were generated . this work was part of a larger project on the adoption , implementation and integration of erp systems in greek enterprises . a taxonomy of erp adoption research was developed from the literature review and used to underpin the issues investigated in these cases . the results were compared with the literature on erp adoption in the usa and uk . findings there were major differences between erp adoption in greek companies and companies in other countries . the adoption , implementation and integration of erp systems were fragmented in greek companies . this fragmentation demonstrated that the internal enterprise 's culture , resources available , skills of employees , and the way erp systems are perceived , treated and integrated within the business and in the supply chain , play critical roles in determining the success failure of erp systems adoption . a warehouse management system was adopted by some greek enterprises to cope with uncertainty . research limitations implications a comparison of erp adoption was made between the usa , uk and greece , and may limit its usefulness elsewhere . practical implications practical advice is offered to managers contemplating adopting erp . originality value a new taxonomy of erp adoption research was developed , which refocused the erp implementation and integration into related critical success failure factors and total integration issues , thus providing a more holistic erp adoption framework .
classification of newborn eeg maturity with bayesian averaging over decision trees . <eos> eeg experts can assess a newborns brain maturity by visual analysis of age related patterns in sleep eeg . it is highly desirable to make the results of assessment most accurate and reliable . however , the expert analysis is limited in capability to provide the estimate of uncertainty in assessments . bayesian inference has been shown providing the most accurate estimates of uncertainty by using markov chain monte carlo ( mcmc ) integration over the posterior distribution . the use of mcmc enables to approximate the desired distribution by sampling the areas of interests in which the density of distribution is high . in practice , the posterior distribution can be multimodal , and so that the existing mcmc techniques can not provide the proportional sampling from the areas of interest . the lack of prior information makes mcmc integration more difficult when a model parameter space is large and can not be explored in detail within a reasonable time . in particular , the lack of information about eeg feature importance can affect the results of bayesian assessment of eeg maturity . in this paper we explore how the posterior information about eeg feature importance can be used to reduce a negative influence of disproportional sampling on the results of bayesian assessment . we found that the mcmc integration tends to oversample the areas in which a model parameter space includes one or more features , the importance of which counted in terms of their posterior use is low . using this finding , we proposed to cure the results of mcmc integration and then described the results of testing the proposed method on a set of sleep eeg recordings .
parametric estimation of the continuous non stationary spectrum and its dynamics in surface emg studies . <eos> frequency spectrum of surface electromyographic signals ( semgs ) exhibit a non stationary nature even in the case of constant level isometric muscle contractions due to changes related to muscle fatigue processes . these changes can be evaluated by methods for estimation of time varying ( tv ) spectrum . the most widely adopted non parametric approach is a short time fourier transform ( stft ) , from which changes of mean frequency ( mf ) as well as other parameters for qualitative description of spectrum variation can be calculated . similar idea of a sliding window generalisation can also be used in case of parametric spectrum analysis methods . we applied such approach to obtain tv linear models of semgs , although its large variance due to independence of estimations in consequent windows represents a major drawback . this variance causes unrealistic abrupt changes in the curve of overall spectrum dynamics , calculated either as the second derivative of the mf or , as we propose , autoregressive moving average ( arma ) distance between subsequent linear models forming the tv parametric spectrum . a smoother estimation is therefore sought and another method shows to be superior over a simple sliding window technique . it supposes that trajectories of tv linear model coefficients can be described as linear combinations of known basis functions . we demonstrate that the later method is very appropriate for description of slowly changing spectra of semgs and that dynamics measures obtained from such estimations can be used as an additional indication of the fatigue process .
a multiple case design methodology for studying mrp success and csfs . <eos> we used a multiple case design to study materials requirements planning ( mrp ) implementation outcome in <digit> manufacturing companies in singapore . using a two phased data collection approach ( pre interview questionnaires and personal interviews ) , we sought to develop a comprehensive and operationally acceptable measure of mrp success . our measure consists of two linked components . they are a satisfaction score ( a quantitative measure ) and a complementary measure based on comments from the interviewees regarding the level of usage and acceptance of the system . we also extended and consolidated a seven factor critical success factor ( csf ) framework using this methodology . csfs are important , but knowing the linkages between them is even more important , because these linkages tell us which csfs to emphasize at various stages of the project .
hybrid heuristic waterfilling game theory approach in mc cdma resource allocation . <eos> this paper discusses the power allocation with fixed rate constraint problem in multi carrier code division multiple access ( mc cdma ) networks , that has been solved through game theoretic perspective by the use of an iterative water filling algorithm ( iwfa ) . the problem is analyzed under various interference density configurations , and its reliability is studied in terms of solution existence and uniqueness . moreover , numerical results reveal the approach shortcoming , thus a new method combining swarm intelligence and iwfa is proposed to make practicable the use of game theoretic approaches in realistic mc cdma systems scenarios . the contribution of this paper is twofold ( i ) provide a complete analysis for the existence and uniqueness of the game solution , from simple to more realist and complex interference scenarios ( ii ) propose a hybrid power allocation optimization method combining swarm intelligence , game theory and iwfa . to corroborate the effectiveness of the proposed method , an outage probability analysis in realistic interference scenarios , and a complexity comparison with the classical iwfa are presented . ( c ) <digit> elsevier b.v. all rights reserved .
direct type specific conic fitting and eigenvalue bias correction . <eos> a new method to fit specific types of conics to scattered data points is introduced . direct , specific fitting of ellipses and hyperbolae is achieved by imposing a quadratic constraint on the conic coefficients , whereby an improved partitioning of the design matrix is devised so as to improve computational efficiency and numerical stability by eliminating redundant aspects of the fitting procedure . fitting of parabolas is achieved by determining an orthogonal basis vector set in the grassmannian space of the quadratic terms coefficients . the linear combination of the basis vectors that fulfills the parabolic condition and has a minimum residual norm is determined using lagrange multipliers . this is the first known direct solution for parabola specific fitting . furthermore , the inherent bias of a linear conic fit is addressed . we propose a linear method of correcting this bias , producing better geometric fits which are still constrained to specific conic type .
communication with www in czech . <eos> this paper describes uio , a multi domain question answering system for the czech language that looks for answers on the web . uio exploits two fields , namely natural language interface to databases and question answering . in its current version , uio can be used for asking questions about train and coach timetables , cinema and theatre performances , about currency exchange rates , name days and on the diderot encyclopaedia . much effort have been made into making addition of a new domain very easy . no limits concerning words or the form of a question need to be set in uio . users can ask syntactically correct as well as incorrect questions , or use keywords . a czech morphological analyser and a bottom up chart parser are employed for analysis of the question . the database of multi word expressions is automatically updated when a new item has been found on the web . for all domains uio has an accuracy rate about <digit> % .
efficient coloring of a large spectrum of graphs . <eos> we have developed a new algorithm and software for graph coloring by systematically combining several algorithm and software development ideas that had crucial impact on the algorithm 's performance . the algorithm explores the divide and conquer paradigm , global search for constrained independent sets using a computationally inexpensive objective function , assignment of most constrained vertices to least constraining colors , reuse and locality exploration of intermediate solutions , search time management , post processing lottery scheduling iterative improvement , and statistical parameter determination and validation . the algorithm was tested on a set of real life examples . we found that hard to color real life examples are common especially in domains where problem modeling results in denser graphs . systematic experimentations demonstrated that for numerous instances the algorithm outperformed all other implementations reported in literature in solution quality and run time .
boolean equations and boolean inequations . <eos> in this paper we consider boolean inequations of the form f ( x ) not equal <digit> . we also consider the system of boolean inequation and boolean equation f ( x ) not equal <digit> boolean and g ( x ) <digit> and we describe all the solutions of this system .
time efficient centralized gossiping in radio networks . <eos> in this paper we study the gossiping problem ( all to all communication ) in radio networks where all nodes are aware of the network topology . we start our presentation with a deterministic gossiping algorithm that works in at most n units of time in any radio network of size n. this algorithm is optimal in the worst case scenario since there exist radio network topologies , such as lines , stars and complete graphs in which radio gossiping can not be completed in less than n communication rounds . furthermore , we show that there does not exist any radio network topology in which the gossiping task can be solved in less than log ( n <digit> ) <digit> rounds . we also show that this lower bound can be matched from above for a fraction of all possible integer values of n , and for all other values of n we propose a solution which accomplishes gossiping in log ( n <digit> ) <digit> rounds . then we show an almost optimal radio gossiping algorithm in trees , which misses the optimal time complexity by a single round . finally , we study asymptotically optimal o ( d ) time gossiping ( where d is the diameter of the network ) in graphs with the maximum degree delta o ( d1 <digit> ( i <digit> ) log ( i ) n ) , for any integer constant i > <digit> and d large enough . ( c ) <digit> elsevier b.v. all rights reserved .
tree kernel based protein protein interaction extraction from biomedical literature . <eos> there is a surge of research interest in protein protein interaction ( ppi ) extraction from biomedical literature . while most of the state of the art ppi extraction systems focus on dependency based structured information , the rich structured information inherent in constituent parse trees has not been extensively explored for ppi extraction . in this paper , we propose a novel approach to tree kernel baled ppi extraction , where the tree representation generated from a constituent syntactic parser is further refined using the shortest dependency path between two proteins derived from a dependency parser . specifically , all the constituent tree nodes associated with the nodes on the shortest dependency path are kept intact , while other nodes are removed safely to make the constituent tree concise and precise for ppi extraction . compared with previously used constituent tree setups , our dependency motivated constituent tree setup achieves the best results across five commonly used ppi corpora . moreover , our tree kernel based method outperforms other single kernel based ones and performs comparably with some multiple kernel ones on the most commonly tested aimed corpus . ( c ) <digit> elsevier inc. all rights reserved .
a review of content based image retrieval systems in medical applications clinical benefits and future directions . <eos> content based visual information retrieval ( cbvir ) or content based image retrieval ( cbir ) has been one on the most vivid research areas in the field of computer vision over the last <digit> years . the availability of large and steadily growing amounts of visual and multimedia data , and the development of the internet underline the need to create thematic access methods that offer more than simple text based queries or requests based on matching exact database fields . many programs and toots have been developed to formulate and execute queries based on the visual or audio content and to help browsing large multimedia repositories . still , no general breakthrough has been achieved with respect to large varied databases with documents of differing sorts and with varying characteristics . answers to many questions with respect to speed , semantic descriptors or objective image interpretations are still unanswered . in the medical field , images , and especially digital images , are produced in ever increasing quantities and used for diagnostics and therapy . the radiology department of the university hospital of geneva alone produced more than 12,000 images a day in <digit> . the cardiology is currently the second largest producer of digital images , especially with videos of cardiac catheterization ( similar to1800 exams per year containing almost <digit> images each ) . the total amount of cardiologic image data produced in the geneva university hospital was around <digit> tb in <digit> . endoscopic videos can equally produce enormous amounts of data . with digital imaging and communications in medicine ( dicom ) , a standard for image communication has been set and patient information can be stored with the actual image ( s ) , although stilt a few problems prevail with respect to the standardization . in several articles , content based access to medical images for supporting clinical decision making has been proposed that would ease the management of clinical data and scenarios for the integration of content based access methods into picture archiving and communication systems ( pacs ) have been created . this article gives an overview of available literature in the field of content based access to medical image data and on the technologies used in the field . section <digit> gives an introduction into generic content based image retrieval and the technologies used . section <digit> explains the propositions for the use of image retrieval in medical practice and the various approaches . example systems and application areas are described . section <digit> describes the techniques used in the implemented systems , their datasets and evaluations . section <digit> identifies possible clinical benefits of image retrieval systems in clinical practice as well as in research and education . new research directions are being defined that can prove to be useful . this article also identifies explanations to some of the outlined problems in the field as it looks like many propositions for systems are made from the medical domain and research prototypes are developed in computer science departments using medical datasets . still , there are very few systems that seem to be used in clinical practice . it needs to be stated as well that the goal is not , in general , to replace text based retrieval methods as they exist at the moment but to complement them with visual search tools . ( c ) <digit> elsevier ireland ltd. all rights reserved .
infinitesimal plane based pose estimation . <eos> estimating the pose of a plane given a set of point correspondences is a core problem in computer vision with many applications including augmented reality ( ar ) , camera calibration and 3d scene reconstruction and interpretation . despite much progress over recent years there is still the need for a more efficient and more accurate solution , particularly in mobile applications where the run time budget is critical . we present a new analytic solution to the problem which is far faster than current methods based on solving pose from ( n ) points ( pnp ) and is in most cases more accurate . our approach involves a new way to exploit redundancy in the homography coefficients . this uses the fact that when the homography is noisy it will estimate the true transform between the model plane and the image better at some regions on the plane than at others . our method is based on locating a point where the transform is best estimated , and using only the local transformation at that point to constrain pose . this involves solving pose with a local non redundant 1st order pde . we call this framework infinitesimal plane based pose estimation ( ippe ) , because one can think of it as solving pose using the transform about an infinitesimally small region on the surface . we show experimentally that ippe leads to very accurate pose estimates . because ippe is analytic it is both extremely fast and allows us to fully characterise the method in terms of degeneracies , number of returned solutions , and the geometric relationship of these solutions . this characterisation is not possible with state of the art pnp methods .
evolutionary learning of spiking neural networks towards quantification of 3d mri brain tumor tissues . <eos> this paper presents a new classification technique for 3d mr images , based on a third generation network of spiking neurons . implementation of multi dimensional co occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed . the results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue specific structural patterns and achieve segmentation of tumor part . the spiking network classifier has been validated and tested for various real time and harvard benchmark datasets , where appreciable performance in terms of mean square error , accuracy and computational time is obtained . the spiking network employed izhikevich neurons as nodes in a multi layered structure . the classifier has been compared with computational power of multi layer neural networks with sigmoidal neurons . the results on misclassified tumors are analyzed and suggestions for future work are discussed .
inferential queueing and speculative push . <eos> communication latencies within critical sections constitute a major bottleneck in some classes of emerging parallel workloads . in this paper , we argue for the use of two mechanisms to reduce these communication latencies inferentially queued locks ( iqls ) and speculative push ( sp ) . with iqls , the processor infers the existence , and limits , of a critical section from the use of synchronization instructions and joins a queue of lock requestors , reducing synchronization delay . the sp mechanism extracts information about program structure by observing iqls . sp allows the cache controller , responding to a request for a cache line that likely includes a lock variable , to predict the data sets the requestor will modify within the associated critical section . the controller then pushes these lines from its own cache to the target cache , as well as writing them to memory . overlapping the protected data transfer with that of the lock can substantially reduce the communication latencies within critical sections . by pushing data in exclusive state , the mechanism can collapse a read modify write sequences within a critical section into a single local cache access . the write back to memory allows the receiving cache to ignore the push . neither mechanism requires any programmer or compiler support nor any instruction set changes . our experiments demonstrate that iqls and sp can improve performance of applications employing frequent synchronization .
the thermal failure process of the quantum cascade laser . <eos> we report the thermal failure process of the quantum cascade laser . firstly , high temperature and strain in the active region are verified by raman spectra , and the conspicuous catastrophically failed characteristics are observed by scanning electron microscope . secondly , the defects generate serious structure disorder due to the high temperature of the active region and the resulted strain relaxation . thirdly , the abundant atomic diffusion in the active region and substrate are observed . the structure disorder and the change of element composition in the active region directly lead to the quantum cascade laser failure . the theoretical analysis fits well with the results of experimental studies .
admira atomic decomposition for minimum rank approximation ( vol <digit> , pg <digit> , <digit> ) . <eos> in this correspondence , a corrected version of the convergence analysis given by lee and bresler is presented .
stable advection reaction diffusion with arbitrary anisotropy . <eos> turing first theorized that many biological patterns arise through the processes of reaction and diffusion . subsequently , reaction diffusion systems have been studied in many fields , including computer graphics . we first show that for visual simulation purposes , reaction diffusion equations can be made unconditionally stable using a variety of straightforward methods . second , we propose an anisotropy embedding that significantly expands the space of possible patterns that can be generated . third , we show that by adding an advection term , the simulation can be coupled to a fluid simulation to produce visually appealing flows . fourth , we couple fast marching methods to our anisotropy embedding to create a painting interface to the simulation . unconditional stability is maintained throughout , and our system runs at interactive rates . finally , we show that on the cell processor , it is possible to implement reaction diffusion on top of an existing fluid solver with no significant performance impact . copyright ( c ) <digit> john wiley sons , ltd .
alexander duality and moments in reliability modelling . <eos> there are strong connections between coherent systems in reliability for systems which have components with a finite number of states and certain algebraic structures . a special case is binary systems where there are two states fail and not fail . the connection relies on an order property in the system and a way of coding states alpha ( alpha ( <digit> ) , ... , alpha ( d ) ) with monomials x ( alpha ) ( x ( <digit> ) ( alpha1 ) , ... , x ( d ) ( alphad ) ) . the algebraic entities are the scarf complex and the associated alexander duality . the failure '' event '' can be studied using these ideas and identities and bounds derived when the algebra is combined with probability distributions on the states . the x ( alpha ) coding aids the use of moments mu ( alpha ) e ( x alpha ) with respect to the underlying distribution .
failure identification for linear repetitive processes . <eos> this paper investigates the fault detection and isolation ( fdi ) problem for discrete time linear repetitive processes using a geometric approach , starting from a <digit> d model for these processes that incorporates a representation of the failure . based on this model , the fdi problem is formulated in the geometric setting and sufficient conditions for solvability of this problem are given . moreover , the processess behaviour in the presence of noise is considered , leading to the development of a statistical approach for determining a decision threshold . finally , a fdi procedure is developed based on an asymptotic observer reconstruction of the state vector .
determinants of web site information by spanish city councils . <eos> purpose the purpose of this research is to analyse the web sites of large spanish city councils with the objective of assessing the extent of information disseminated on the internet and determining what factors are affecting the observed levels of information disclosure . design methodology approach the study takes as its reference point the existing literature on the examination of the quality of web sites , in particular the provisions of the web quality model ( wqm ) and the importance of content as a key variable in determining web site quality . in order to quantify the information on city council web sites , a disclosure index has been designed which takes into account the content , navigability and presentation of the web sites . in order to contrast which variables determine the information provided on the web sites , our investigation bases itself on the studies about voluntary disclosure in the public sector , and six lineal regressions models have been performed . findings the empirical evidence obtained reveals low disclosure levels among spanish city council web sites . in spite of this , almost <digit> per cent of the city councils have reached the approved level and of these , around a quarter obtained good marks . our results show that disclosure levels depend on political competition , public media visibility and the access to technology and educational levels of the citizens . practical implications the strategy of communication on the internet by local spanish authorities is limited in general to an ornamental web presence but one that does not respond efficiently to the requirements of the digital society . during the coming years , local spanish politicians will have to strive to take advantage of the opportunities that the internet offers to increase both the relational and informational capacity of municipal web sites as well as the digital information transparency of their public management . originality value the internet is a potent channel of communication that is modifying the way in which people access and relate to information and each other . the public sector is not unaware of these changes and is incorporating itself gradually into the new network society . this study systematises the analysis of local administration web sites , showing the lack of digital transparency , and orients politicians in the direction to follow in order to introduce improvements in their electronic relationships with the public .
the development of regional collaboration for resource efficiency a network perspective on industrial symbiosis . <eos> three development patterns of industrial symbiosis systems are proposed and empirically examined . industrial symbiosis networks build on and strengthen the disparity of firms capability on building symbiotic relations . due to this disparity , self organized industrial symbiosis networks favor the most capable firms and grow preferentially . coordinating agencies improve disadvantaged firms capabilities , and change the preferential growth to a homogeneous one . strong government engagement helps disadvantaged firms and facilitates non preferential symbiosis development in a region .
effective diagnosis of heart disease through neural networks ensembles . <eos> in the last decades , several tools and various methodologies have been proposed by the researchers for developing effective medical decision support systems . moreover , new methodologies and new tools are continued to develop and represent day by day . diagnosing of the heart disease is one of the important issue and many researchers investigated to develop intelligent medical decision support systems to improve the ability of the physicians . in this paper , we introduce a methodology which uses sas base software 9.1.3 for diagnosing of the heart disease . a neural networks ensemble method is in the centre of the proposed system . this ensemble based methods creates new models by combining the posterior probabilities or the predicted values from multiple predecessor models . so , more effective models can be created . we performed experiments with the proposed tool . we obtained 89.01 % classification accuracy from the experiments made on the data taken from cleveland heart disease database . we also obtained 80.95 % and 95.91 % sensitivity and specificity values , respectively , in heart disease diagnosis .
soil carbon model yasso07 graphical user interface . <eos> in this article , we present a graphical user interface software for the litter decomposition and soil carbon model yasso07 and an overview of the principles and formulae it is based on . the software can be used to test the model and use it in simple applications . yasso07 is applicable to upland soils of different ecosystems worldwide , because it has been developed using data covering the global climate conditions and representing various ecosystem types . as input information , yasso07 requires data on litter input to soil , climate conditions , and land use change if any . the model predictions are given as probability densities representing the uncertainties in the parameter values of the model and those in the input data the user interface calculates these densities using a built in monte carlo simulation .
time synchronization attacks in sensor networks . <eos> time synchronization is a critical building block in distributed wireless sensor networks . because sensor nodes may be severely resource constrained , traditional time synchronization protocols can not be used in sensor networks . various time synchronization protocols tailored for such networks have been proposed to solve this problem . however , none of these protocols have been designed with security in mind . if an adversary were able to compromise a node , he might prevent a network from effectively executing certain applications , such as sensing or tracking an object , or he might even disable the network by disrupting a fundamental service such as a tdma based channel sharing scheme . in this paper we give a survey of the most common time synchronization protocols and outline the possible attacks on each protocol . in addition , we discuss how different sensor network applications that are affected by time synchronization attacks , and we propose some countermeasures for these attack .
enhanced privacy id a direct anonymous attestation scheme with enhanced revocation capabilities . <eos> direct anonymous attestation ( daa ) is a scheme that enables the remote authentication of a trusted platform module ( tpm ) while preserving the user 's privacy . a tpm can prove to a remote party that it is a valid tpm without revealing its identity and without linkability . in the daa scheme , a tpm can be revoked only if the daa private key in the hardware has been extracted and published widely so that verifiers obtain the corrupted private key . if the unlinkability requirement is relaxed , a tpm suspected of being compromised can be revoked even if the private key is not known . however , with the full unlinkability requirement intact , if a tpm has been compromised but its private key has not been distributed to verifiers , the tpm can not be revoked . furthermore , a tpm can not be revoked from the issuer , if the tpm is found to be compromised after the daa issuing has occurred . in this paper , we present a new daa scheme called enhanced privacy id ( epid ) scheme that addresses the above limitations . while still providing unlinkability , our scheme provides a method to revoke a tpm even if the tpm private key is unknown . this expanded revocation property makes the scheme useful for other applications such as for driver 's license . our epid scheme is efficient and provably secure in the same security model as daa , i.e. , in the random oracle model under the strong rsa assumption and the decisional diffie hellman assumption .
using predicate path information in hardware to determine true dependences . <eos> predicated execution has been put forth as a method for improving processor performance by removing hard to predict branches . as part of the process of turning a set of basic blocks into a predicated region , both paths of a branch are combined into a single path . there can be multiple definitions from disjoint paths that reach a use . waiting to find out the correct definition that actually reaches the use can cause pipeline stalls.in this paper we examine a hardware optimization that dynamically collects and analyzes path information to determine valid dependences for predicated regions of code . we then use this information for an in order vliw predicated processor , so that instructions can continue towards execution without having to wait on operands from false dependences . our results show that using our disjoint path analysis system provides speedups over <digit> % and elimination of false raw dependences of up to <digit> % due to the detection of erroneous dependences in if converted regions of code .
simulation of dna damage clustering after proton irradiation using an adapted dbscan algorithm . <eos> in this work the density based spatial clustering of applications with noise ( dbscan ) algorithm was adapted to early stage dna damage clustering calculations . the resulting algorithm takes into account the distribution of energy deposit induced by ionising particles and a damage probability function that depends on the total energy deposit amount . proton track simulations were carried out in small micrometric volumes representing small dna containments . the algorithm was used to determine the damage concentration clusters and thus to deduce the dsb ssb ratios created by protons between 500kev and 50mev . the obtained results are compared to other calculations and to available experimental data of fibroblast and plasmid cells irradiations , both extracted from literature .
building the academic strategy program . <eos> purpose to present the application of a theory and best practice of a balanced scorecard bsc method , to create bsc strategic program for academic education decision makers , and to present framework for strategy program about research and education in faculty . methodology approach based on the investigation of number of successful project on this topic and on the authors exercise in balanced scorecard approach about educational strategy the program is created and modelled . findings the balanced scorecard strategic program is developed and it allows enhancing the leadership capability across university . practical implications the program can facilitate faculty and staff to formulate and measure strategic management decisions and to create competitive educational and research environment at the university . originality value the value of the program is in integrating competences , experience , best practices and tools within one new program design . the paper shows how to translate the academic strategy into different strategic objectives and goals , how to model them and how to communicate academic research and education processes to realize important improvements in cost and quality of academic services .
nonlinear transport in quantum point contact structures . <eos> we have investigated the magnetotransport properties under nonlinear conditions in quantum point contact structures fabricated on high mobility algaas gaas two dimensional electron gas ( 2deg ) layers . nonlinearities in the iv characteristics are observed at the threshold for conduction when biased initially from the tunneling regime as observed previously . we observe that this non ideality is enhanced by a magnetic field normal to the plane of the 2deg . this behavior is interpreted in terms of corrections to the landauer model extended to nonequilibrium conditions .
mrm a matrix representation and mapping approach for knowledge acquisition . <eos> knowledge acquisition plays a critical role in constructing a knowledge based system ( kbs ) . it is the most time consuming phase and has been recognized as the bottleneck of kbs development . this paper presents a matrix representation and mapping ( mrm ) approach to facilitate the effectiveness of knowledge acquisition in building a kbs . the proposed mrm approach , which is based on matrix representation and mapping operations , comprises six consecutive steps for generating rules . the procedure in each step is elaborated . a case study on primarily diagnosing an automotive system is employed to illustrate how the mrm approach works .
integrated obstacle avoidance and path following through a feedback control law . <eos> the article proposes a novel approach to path following in the presence of obstacles with unique characteristics . first , the approach proposes an integrated method for obstacle avoidance and path following based on a single feedback control law , which produces commands to actuators directly executable by a robot with unicycle kinematics . second , the approach offers a new solution to the well known dilemma that one has to face when dealing with multiple sensor readings , i.e. , whether it is better , to summarize a huge amount of sensor data , to consider only the closest sensor reading , to consider all sensor readings separately to compute the resulting force vector , or to build a local map . the approach requires very little memory and computational resources , thus being implementable even on simpler robots moving in unknown environments .
a review of learning vector quantization classifiers . <eos> in this work , we present a review of the state of the art of learning vector quantization ( lvq ) classifiers . a taxonomy is proposed which integrates the most relevant lvq approaches to date . the main concepts associated with modern lvq approaches are defined . a comparison is made among eleven lvq classifiers using one real world and two artificial datasets .
sketching concurrent data structures . <eos> we describe psketch , a program synthesizer that helps programmers implement concurrent data structures . the system is based on the concept of sketching , a form of synthesis that allows programmers to express their insight about an implementation as a partial program a sketch . the synthesizer automatically completes the sketch to produce an implementation that matches a given correctness criteria . psketch is based on a new counterexample guided inductive synthesis algorithm ( cegis ) that generalizes the original sketch synthesis algorithm from solar lezama et.al . to cope efficiently with concurrent programs . the new algorithm produces a correct implementation by iteratively generating candidate implementations , running them through a verifier , and if they fail , learning from the counterexample traces to produce a better candidate converging to a solution in a handful of iterations . psketch also extends sketch with higher level sketching constructs that allow the programmer to express her insight as a soup of ingredients from which complicated code fragments must be assembled . such sketches can be viewed as syntactic descriptions of huge spaces of candidate programs ( over <digit> <digit> candidates for some sketches we resolved ) . we have used the psketch system to implement several classes of concurrent data structures , including lock free queues and concurrent sets with fine grained locking . we have also sketched some other concurrent objects including a sense reversing barrier and a protocol for the dining philosophers problem all these sketches resolved in under an hour .
evaluating change in user error when using ruggedized handheld devices . <eos> there are no significant differences between user error and age . lack of corrective software may not impact user error as much as expected . keypad devices had more character errors while touchscreen devices had more word .
support for situation awareness in trustworthy ubiquitous computing application software . <eos> due to the dynamic and ephemeral nature of ubiquitous computing ( ubicomp ) environments , it is especially important that the application software in ubicomp environments is trustworthy . in order to have trustworthy application software in ubicomp environments , situation awareness ( saw ) in the application software is needed to enforce flexible security policies and detect violations of security policies . in this paper , an approach is presented to provide development and runtime support to incorporate saw in trustworthy ubicomp , application software . the development support is to provide saw requirement specification and automated code generation to achieve saw in trustworthy ubicomp application software , and the runtime support is for context acquisition , situation analysis and situation aware communication . to realize our approach , the improved reconfigurable context sensitive middleware ( rcsm ) is developed to provide the above development and runtime support . copyright ( c ) <digit> john wiley sons , ltd .
validation of temperature perturbation and cfd based modelling for the prediction of the thermal urban environment the lecce ( it ) case study . <eos> two modelling approaches for air temperature prediction in cities are evaluated . daily trends of air temperature are well captured . envi met requires an ad hoc tuning of surface boundary conditions . adms th model performance depends on the accuracy of energy balance terms . adms th shows an overall better performance than envi met .
comparison of software for computing the action of the matrix exponential . <eos> the implementation of exponential integrators requires the action of the matrix exponential and related functions of a possibly large matrix . there are various methods in the literature for carrying out this task . in this paper we describe a new implementation of a method based on interpolation at leja points . we numerically compare this method with other codes from the literature . as we are interested in applications to exponential integrators we choose the test examples from spatial discretization of time dependent partial differential equations in two and three space dimensions . the test matrices thus have large eigenvalues and can be nonnormal .
learning while designing . <eos> this paper describes how a computational system for designing can learn useful , reusable , generalized search strategy rules from its own experience of designing . it can then apply this experience to transform the design process from search based ( knowledge lean ) to knowledge based ( knowledge rich ) . the domain of application is the design of spatial layouts for architectural design . the processes of designing and learning are tightly coupled .
an algebraic construction of codes for slepian wolf source networks . <eos> this correspondence proposes an explicit construction of fixed length codes for slepian wolf ( sw ) source networks . the proposed code is linear , and has two step encoding and decoding procedures similar to the concatenated code used for channel coding . encoding and decoding of the code can be done in a polynomial order of the block length . the proposed code can achieve arbitrary small probability of error for ergodic sources with finite alphabets , if the pair of encoding rates is in the achievable region . further , if the sources are memoryless , the proposed code can be modified to become universal and the probability of error vanishes exponentially as the block length tends to infinity .
ann and anfis models for performance evaluation of a vertical ground source heat pump system . <eos> the aim of this study is to demonstrate the comparison of an artificial neural network ( ann ) and an adaptive neuro fuzzy inference system ( anfis ) for the prediction performance of a vertical ground source heat pump ( vgshp ) system . the vgshp system using r <digit> as refrigerant has a three single u tube ground heat exchanger ( ghe ) made of polyethylene pipe with a <digit> mm outside diameter . the ghes were placed in a vertical boreholes ( vbs ) with <digit> ( vb1 ) , <digit> ( vb2 ) and <digit> ( vb3 ) m depths and <digit> mm diameters . the monthly mean values of cop for vb1 , vb2 and vb3 are obtained to be 3.37 1.93 , 3.85 2.37 , and 4.33 3.03 , respectively , in cooling heating seasons . experimental performances were performed to verify the results from the ann and anfis approaches . ann model , multi layered perceptron back propagation with three different learning algorithms ( the levenbergmarquardt ( lm ) , scaled conjugate gradient ( scg ) and pola ribiere conjugate gradient ( cgp ) algorithms and the anfis model were developed using the same input variables . finally , the statistical values are given in as tables . this paper shows the appropriateness of anfis for the quantitative modeling of gshp systems .
effects of agent heterogeneity in the presence of a land market a systematic test in an agent based laboratory . <eos> representing agent heterogeneity is one of the main reasons that agent based models become increasingly popular in simulating the emergence of land use , land cover change and socioeconomic phenomena . however , the relationship between heterogeneous economic agents and the resultant landscape patterns and socioeconomic dynamics has not been systematically explored . in this paper , we present a stylized agent based land market model , land use in exurban environments ( luxe ) , to study the effects of multidimensional agents heterogeneity on the spatial and socioeconomic patterns of urban land use change under various market representations . we examined two sources of agent heterogeneity budget heterogeneity , which imposes constraints on the affordability of land , and preference heterogeneity , which determines location choice . the effects of the two dimensions of agents heterogeneity are systematically explored across different market representations by three experiments . agents heterogeneity exhibits a complex interplay with various forms of market institutions as indicated by macro measures ( landscape metrics , segregation index , and socioeconomic metrics ) . in general , budget heterogeneity has pronounced effect on socioeconomic results , while preference heterogeneity is highly pertinent to spatial outcomes . the relationship between agent heterogeneity and macro measures becomes more complex when more land market mechanisms are represented . in other words , appropriately simulating agent heterogeneity plays an important role in guaranteeing the fidelity of replicating empirical land use change process .
feature shaping for linear svm classifiers . <eos> linear classifiers have been shown to be effective for many discrimination tasks . irrespective of the learning algorithm itself , the final classifier has a weight to multiply by each feature . this suggests that ideally each input feature should be linearly correlated with the target variable ( or anti correlated ) , whereas raw features may be highly non linear . in this paper , we attempt to re shape each input feature so that it is appropriate to use with a linear weight and to scale the different features in proportion to their predictive value . we demonstrate that this pre processing is beneficial for linear svm classifiers on a large benchmark of text classification tasks as well as uci datasets .
a service driven development process ( sddp ) model for ultra large scale systems . <eos> achieving ultra large scale software systems will necessarily require new and special development processes . this position paper suggests overall structure of a process model to develop and maintain system of systems similar to ultra large scale ( uls ) systems . the proposed process model will be introduced in details and finally , we will evaluate it considering cmmi acq which has been presented by sei for acquirer organizations .
delineating white matter structure in diffusion tensor mri with anisotropy creases . <eos> geometric models of white matter architecture play an increasing role in neuroscientific applications of diffusion tensor imaging , and the most popular method for building them is fiber tractography . for some analysis tasks , however , a compelling alternative may be found in the first and second derivatives of diffusion anisotropy . we extend to tensor fields the notion from classical computer vision of ridges and valleys , and define anisotropy creases as features of locally extremal tensor anisotropy . mathematically , these are the loci where the gradient of anisotropy is orthogonal to one or more eigenvectors of its hessian . we propose that anisotropy creases provide a basis for extracting a skeleton of the major white matter pathways , in that ridges of anisotropy coincide with interiors of fiber tracts , and valleys of anisotropy coincide with the interfaces between adjacent but distinctly oriented tracts . the crease extraction algorithm we present generates high quality polygonal models of crease surfaces , which are further simplified by connected component analysis . we demonstrate anisotropy creases on measured diffusion mri data , and visualize them in combination with tractography to confirm their anatomic relevance .
a fully parallel method for the singular eigenvalue problem . <eos> in this paper , a fully parallel method for finding some or all finite eigenvalues of a real symmetric matrix pencil ( a , b ) is presented , where a is a symmetric tridiagonal matrix and b is a diagonal matrix with b ( <digit> ) > <digit> and b ( i ) > <digit> , i 2,3 , ... , n. the method is based on the homotopy continuation with rank <digit> perturbation . it is shown that there are exactly m disjoint , smooth homotopy paths connecting the trivial eigenvalues to the desired eigenvalues , where m is the number of finite eigenvalues of ( a , b ) , it is also shown that the homotopy curves are monotonic and easy to follow . ( c ) <digit> elsevier ltd. all rights reserved .
potential and requirements of it for ambient assisted living technologies results of a delphi study . <eos> objectives ambient assisted living ( aal ) technologies are developed to enable elderly to live independently and safely . innovative information technology ( it ) can interconnect personal devices and offer suitable user interfaces . often dedicated solutions are developed for particular projects . the aim of our research was to identify major it challenges for aal to enable generic and sustainable solutions . methods delphi survey . an online questionnaire was sent to <digit> members of the german innovation partnership aal . the first round was qualitative to collect statements . statements were reduced to items by qualitative content analysis . items were assessed in the following two rounds by a <digit> point likert scale . quantitative analyses for second and third round descriptive statistics , factor analysis and anova . results respondents <digit> in first , <digit> in second and <digit> in third round . all items got a rather high assessment . medical issues were rated as having a very high potential . items related to user friendliness were regarded as most important requirements . common requirements to all aal solutions are reliability , robustness , availability , data security , data privacy , legal issues , ethical requirements , easy configuration . the complete list of requirements can be used as framework for customizing future aal projects . conclusions a wide variety of it issues have been assessed important for aal . the extensive list of requirements makes obvious that it is not efficient to develop dedicated solutions for individual projects but to provide generic methods and reusable components . experiences and results from medical informatics research can be used to advance aal solutions ( e.g. ehealth and knowledge based approaches ) .
hybrid harmonic coding of speech at low bit rates . <eos> this paper presents a novel approach to sinusoidal coding of speech which avoids the use of a voicing detector . the proposed model represents the speech signal as a sum of sinusoids and bandpass random signals and it is denoted hybrid harmonic model in this paper . the use of two different sets of basis functions increases the robustness of the model since there is no need to switch between techniques tailored to particular classes of sounds . sinusoidal basis functions with harmonically related frequencies allow an accurate representation of the quasi periodic structure of voiced speech but show difficulties in representing unvoiced sounds . on the other hand , the bandpass random functions are well suited for high quality representation of unvoiced speech sounds , since their bandwidth is larger than the bandwidth of sinusoids . the amplitudes of both sets of basis functions are simultaneously estimated by a least squares algorithm and the output speech signal is synthesized in the time domain by the superposition of all basis functions multiplied by their amplitudes . experimental tests confirm an improved performance of the hybrid model for operation with noise corrupted input speech , relative to classic sinusoidal models which exhibit a strong dependency on voicing decision . finally , the implementation and test of a fully quantized hybrid coder at 4.8 kbit s is described .
semi supervised learning and condition fusion for fault diagnosis . <eos> manifold regularization based semi supervised learning is introduced to fault diagnosis . unlabeled condition data are also utilized to enhance the multi fault detection . a new single conditions and all conditions labeled mode is proposed to feed ssl . this ssl approach outperforms supervised learning in both labeled modes . the manifold fundamental of single conditions labeled mode is analyzed with dimensionality reduction .
a deductive system for proving workflow models from operational procedures . <eos> many modern business environments employ software to automate the delivery of workflows whereas , workflow design and generation remains a laborious technical task for domain specialists . several different approaches have been proposed for deriving workflow models . some approaches rely on process data mining approaches , whereas others have proposed derivations of workflow models from operational structures , domain specific knowledge or workflow model compositions from knowledge bases . many approaches draw on principles from automatic planning , but conceptual in context and lack mathematical justification . in this paper we present a mathematical framework for deducing tasks in workflow models from plans in mechanistic or strongly controlled work environments , with a focus around automatic plan generations . in addition , we prove an associative composition operator that permits crisp hierarchical task compositions for workflow models through a set of mathematical deduction rules . the result is a logical framework that can be used to prove tasks in workflow hierarchies from operational information about work processes and machine configurations in controlled or mechanistic work environments .
graph based signature classes for detecting polymorphic worms via content analysis . <eos> malicious softwares such as trojans , viruses , or worms can cause serious damage for information systems by exploiting operating system and application software vulnerabilities . worms constitute a significant proportion of overall malicious software and infect a large number of systems in very short periods . polymorphic worms combine polymorphism techniques with self replicating and fast spreading characteristics of worms . each copy of a polymorphic worm has a different pattern so it is not effective to use simple signature matching techniques . in this work , we propose a graph based classification framework of content based polymorphic worm signatures . this framework aims to guide researchers to propose new polymorphic worm signature schemes . we also propose a new polymorphic worm signature scheme , conjunction of combinational motifs ( ccm ) , based on the defined framework . ccm utilizes common substrings of polymorphic worm copies and also the relation between those substrings through dependency analysis . ccm is resilient to new versions of a polymorphic worm . ccm also automatically generates signatures for new versions of a polymorphic worm , triggered by partial signature matches . experimental results support that ccm has good flow evaluation time performance with low false positives and low false negatives .
polynomial time theory of matrix groups . <eos> we consider matrix groups , specified by a list of generators , over finite fields . the two most basic questions about such groups are membership in and the order of the group . even in the case of abelian groups it is not known how to answer these questions without solving hard number theoretic problems ( factoring and discrete log ) in fact , constructive membership testing in the case of <digit> <digit> matrices is precisely the discrete log problem . so the reasonable question is whether these problems are solvable in randomized polynomial time using number theory oracles . building on <digit> years of work , including remarkable recent developments by several groups of authors , we are now able to determine the order of a matrix group over a finite field of odd characteristic , and to perform constructive membership testing in such groups , in randomized polynomial time , using oracles for factoring and discrete log . one of the new ingredients of this result is the following . a group is called semisimple if it has no abelian normal subgroups . for matrix groups over finite fields , we show that the order of the largest semisimple quotient can be determined in randomized polynomial time ( no number theory oracles required and no restriction on parity ) . as a by product , we obtain a natural problem that belongs to bpp and is not known to belong either to rp or to corp. no such problem outside the area of matrix groups appears to be known . the problem is the decision version of the above given a list a of nonsingular d d matrices over a finite field and an integer n , does the group generated by a have a semisimple quotient of order > n we also make progress in the area of constructive recognition of simple groups , with the corollary that for a large class of matrix groups , our algorithms become las vegas .
delineation of the genomics field by hybrid citation lexical methods interaction with experts and validation process . <eos> in advanced methods of delineation and mapping of scientific fields , hybrid methods open a promising path to the capitalisation of advantages of approaches based on words and citations . one way to validate the hybrid approaches is to work in cooperation with experts of the fields under scrutiny . we report here an experiment in the field of genomics , where a corpus of documents has been built by a hybrid citation lexical method , and then clustered into research themes . experts of the field were associated in the various stages of the process lexical queries for building the initial set of documents , the seed citation based extension aiming at reducing silence final clustering to identify noise and allow discussion on border areas . the analysis of experts advices show a high level of validation of the process , which combines a high precision and low recall seed , obtained by journal and lexical queries , and a citation based extension enhancing the recall . this findings on the genomics field suggest that hybrid methods can efficiently retrieve a corpus of relevant literature , even in complex and emerging fields .
sampling correlation sources for timing yield analysis of sequential circuits with clock networks . <eos> analyzing timing yield under process variations is difficult because of the presence of correlations . reconvergent fan out nodes ( rfons ) within combinational subcircuits are a major source of topological correlation . we identify two more sources of topological correlation in clocked sequential circuit sequential rfons , which are nodes within a clock network where the clock paths to more than one flip flop branch out and sequential branch points , which are nodes within a combinational block where combinational paths to more than one capturing flip flop branch out . dealing with all sources of correlation is unacceptably complicated , and we therefore show how to sample a handful of correlation sources without sacrificing significant accuracy in the yield . a further reduction in computation time can be obtained by sampling only those nodes that are likely to affect the yield . these techniques are applied to yield analysis using statistical static timing analysis based on discrete random variables and also to yield analysis based on monte carlo simulation the accuracy and efficiency of both methods are assessed using example circuits . the sequential rfons suggest that timing yield may be improved by optimizing the clock network , and we address this possibility .
optimal search and one way trading online algorithms . <eos> this paper is concerned with the time series search and one way trading problems . in the ( time series ) search problem a player is searching for the maximum ( or minimum ) price in a sequence that unfolds sequentially , one price at a time . once during this game the player can decide to accept the current price p in which case the game ends and the player 's payoff is p. in the one way trading problem a trader is given the task of trading dollars to yen . each day , a new exchange rate is announced and the trader must decide how many dollars to convert to yen according to the current rate . the game ends when the trader trades his entire dollar wealth to yen and his payoff is the number of yen acquired . the search and one way trading are intimately related . any ( deterministic or randomized ) one way trading algorithm can be viewed as a randomized search algorithm . using the competitive ratio as a performance measure we determine the optimal competitive performance for several variants of these problems . in particular , we show that a simple threat based strategy is optimal and we determine its competitive ratio which yields , for realistic values of the problem parameters , surprisingly low competitive ratios . we also consider and analyze a one way trading game played against an adversary called nature where the online player knows the probability distribution of the maximum exchange rate and that distribution has been chosen by nature . finally , we consider some applications for a special case of portfolio selection called two way trading in which the trader may trade back and forth between cash and one asset .
interpretation of complex scenes using dynamic tree structure bayesian networks . <eos> this paper addresses the problem of object detection and recognition in complex scenes , where objects are partially occluded . the approach presented herein is based on the hypothesis that a careful analysis of visible object details at various scales is critical for recognition in such settings . in general , however , computational complexity becomes prohibitive when trying to analyze multiple sub parts of multiple objects in an image . to alleviate this problem , we propose a generative model framework namely , dynamic tree structure belief networks ( dtsbns ) . this framework formulates object detection and recognition as inference of dtsbn structure and image class conditional distributions , given an image . the causal ( markovian ) dependencies in dtsbns allow for design of computationally efficient inference , as well as for interpretation of the estimated structure as follows each root represents a whole distinct object , while children nodes down the sub tree represent parts of that object at various scales . therefore , within the dtsbn framework , the treatment and recognition of object parts requires no additional training , but merely a particular interpretation of the tree subtree structure . this property leads to a strategy for recognition of objects as a whole through recognition of their visible parts . our experimental results demonstrate that this approach remarkably outperforms strategies without explicit analysis of object parts . ( c ) <digit> elsevier inc. all rights reserved .
consistent group membership in ad hoc networks . <eos> the design of ad hoc mobile applications often requires the availability of a consistent view of the application state among the participating hosts . such views are important because they simplify both the programming and verification tasks . essential to constructing a consistent view is the ability to know what hosts are within proximity of each other , i.e. , form a group in support of the particular application . in this paper we propose an algorithm that allows hosts within communication range to maintain a consistent view of the group membership despite movement and frequent disconnections . the novel features of this algorithm are its reliance on location information and a conservative notion of logical connectivity that creates the illusion of announced disconnection . movement patterns and delays are factored in the policy that determines which physical connections are susceptible to disconnection .
a real time collision detection algorithm for mobile billiards game . <eos> collision detection is a key technique in game design . however , some algorithms employed in pc game are not suitable for mobile game because of the low performance and small screen size in mobile devices . combining with the features of the mobile devices , this paper proposes a quick and feasible collision detection algorithm . this algorithm makes use of the multi level collision detection and dynamic multi resolution grid subdivision to reduce the computing time for collision detection , which improves the algorithm performance greatly . in the collision response phase , this paper adopts the time step binary search algorithm to ensure both the computing precision and system efficiency . the mobile billiards game designed for the bird company indicates that this algorithm has good performance and real time interaction .
iterative execution feedback model directed gui testing . <eos> current fully automatic model based test case generation techniques for guis employ a static model . therefore they are unable to leverage certain state based relationships between gui events ( e.g. , one enables the other , one alters the others execution ) that are revealed at run time and non trivial to infer statically . we present alt a new technique to generate gui test cases in batches . because of its alternating nature , alt enhances the next batch by using gui run time information from the current batch . an empirical study on four fielded gui based applications demonstrated that alt was able to detect new <digit> and <digit> way gui interaction faults in contrast , previous techniques , due to their requirement of too many test cases , were unable to even test <digit> and <digit> way gui interactions .
a hierarchical refinement algorithm for fully automatic gridding in spotted dna microarray image processing . <eos> gridding , the first step in spotted dna microarray image processing , usually requires human intervention to achieve acceptable accuracy . we present a new algorithm for automatic gridding based on hierarchical refinement to improve the efficiency , robustness and reproducibility of microarray data analysis . this algorithm employs morphological reconstruction along with global and local rotation detection , non parametric optimal thresholding and local fine tuning without any human intervention . using synthetic data and real microarray images of different sizes and with different degrees of rotation of subarrays , we demonstrate that this algorithm can detect and compensate for alignment and rotation problems to obtain reliable and robust results .
genetic algorithms applied in bopp film scheduling problems minimizing total absolute deviation and setup times . <eos> the frequent changeovers in the production processes indicate the importance of setup time in many real world manufacturing activities . the traditional approaches in dealing with setup times are that either to omit or to merge into the processing times so as to simplify the problems . these approaches could reduce the complexity of the problem , but often generated unrealistic outcomes because of the assumed conditions . this situation motivated us to consider sequence dependent setup times in a real world bopp film scheduling problem . first , a setup time based heuristic method was developed to generate the initial solutions for the genetic algorithms ( gas ) . then , genetic algorithms with different mutation methods were applied . extensive experimental results showed that the setup time based heuristic method was relatively efficient . it was also found that a genetic algorithm with the variable mutation rate performed much effectively than one with the fixed mutation rate .
automatic grading of scots pine ( pinus sylvestris l. ) sawlogs using an industrial x ray log scanner . <eos> the successful running of a sawmill is dependent on its ability to achieve the highest possible value recovery from the sawlogs , i.e. to optimize the use of the raw material . such optimization requires information about the properties of every log . one method of measuring these properties is to use an x ray log scanner . the objective of the present study was to determine the accuracy when grading scots pine ( pinus sylvestris l. ) sawlogs using an industrial scanner known as the x ray logscanner . the study was based on <digit> scots pine sawlogs from a sawmill in northern sweden . all logs were scanned in the logscanner at a speed of <digit> m min . the x ray images were analyzed on line with measures of different properties as a result ( e.g. density and density variations ) . the logs were then sawn with a normal sawing pattern ( <digit> mm ) and the logs were graded depending on the result from the manual grading of the center boards . finally , partial least squares ( pls ) regression was used to calibrate statistical models that predict the log grade based on the properties measured by the x ray logscanner . the study showed that <digit> % of the logs were correctly sorted when using the scanner to sort logs into three groups according to the predicted grade of the center boards . after sawing the sorted logs , <digit> % of the boards had the correct grade . when scanning the same logs repeatedly , the relative standard deviation of the predicted grade was <digit> % . the study also showed that it is possible to sort out <digit> and <digit> % , respectively , of the material into two groups with high quality logs , without changing the grade distribution of the rest of the material to any great extent .
learning juntas in the presence of noise . <eos> we investigate the combination of two major challenges in computational learning dealing with huge amounts of irrelevant information and learning from noisy data . it is shown that large classes of boolean concepts that depend only on a small fraction of their variables so called juntas can be learned efficiently from uniformly distributed examples that are corrupted by random attribute and classification noise . we present solutions to cope with the manifold problems that inhibit a straightforward generalization of the noise free case . additionally , we extend our methods to non uniformly distributed examples and derive new results for monotone juntas and for parity juntas in this setting . it is assumed that the attribute noise is generated by a product distribution . without any restrictions of the attribute noise distribution , learning in the presence of noise is in general impossible . this follows from our construction of a noise distribution p and a concept class c such that it is impossible to learn c under p noise . ( c ) <digit> elsevier b.v. all rights reserved .
single allocation ordered median hub location problems . <eos> the discrete ordered median location model is a powerful tool in modeling classic and alternative location problems that have been applied with success to a large variety of discrete location problems . nevertheless , although hub location models have been analyzed from the sum , maximum and coverage point of views , as far as we know , they have never been considered under an alternative unifying point of view . in this paper we consider new formulations , based on the ordered median objective function , for hub location problems with new distribution patterns induced by the different users ' roles within the supply chain network . this approach introduces some penalty factors associated with the position of an allocation cost with respect to the sorted sequence of these costs . first we present basic formulations for this problem , and then develop stronger formulations by exploiting properties of the model . the performance of all these formulations is compared by means of a computational analysis . ( c ) <digit> elsevier ltd. all rights reserved .
how measuring student performances allows for measuring blended extreme apprenticeship for learning bash programming . <eos> many small exercises and few lectures can teach all programming . measuring student behavior in exercises assesses how they learn . the reported study logged student performances in programming exercises . metrics were defined for assessing overall programming performances . data show that all students tend to learn basic programming skills .
application of 3d wavelet statistics to video analysis . <eos> video activity analysis is used in various video applications such as human action recognition , video retrieval , video archiving . in this paper , we propose to apply 3d wavelet transform statistics to natural video signals and employ the resulting statistical attributes for video modeling and analysis . from the 3d wavelet transform , we investigate the marginal and joint statistics as well as the mutual information ( mi ) estimates . we show that marginal histograms are approximated quite well by generalized gaussian density ( ggd ) functions and the mi between coefficients decreases when the activity level increases in videos . joint statistics attributes are applied to scene activity grouping , leading to 87.3 % accurate grouping of videos . also , marginal and joint statistics features extracted from the video are used for human action classification employing support vector machine ( svm ) classifiers and 93.4 % of the human activities are properly classified .
modeling electrokinetic flows in microchannels using coupled lattice boltzmann methods . <eos> we present a numerical framework to solve the dynamic model for electrokinetic flows in microchannels using coupled lattice boltzmann methods . the governing equation for each transport process is solved by a lattice boltzmann model and the entire process is simulated through an iteration procedure . after validation , the present method is used to study the applicability of the poissonboltzmann model for electrokinetic flows in microchannels . our results show that for homogeneously charged long channels , the poissonboltzmann model is applicable for a wide range of electric double layer thickness . for the electric potential distribution , the poissonboltzmann model can provide good predictions until the electric double layers fully overlap , meaning that the thickness of the double layer equals the channel width . for the electroosmotic velocity , the poissonboltzmann model is valid even when the thickness of the double layer is <digit> times of the channel width . for heterogeneously charged microchannels , a higher zeta potential and an enhanced velocity field may cause the poissonboltzmann model to fail to provide accurate predictions . the ionic diffusion coefficients have little effect on the steady flows for either homogeneously or heterogeneously charged channels . however the ionic valence of solvent has remarkable influences on both the electric potential distribution and the flow velocity even in homogeneously charged microchannels . both theoretical analyses and numerical results indicate that the valence and the concentration of the counter ions dominate the debye length , the electrical potential distribution , and the ions transport . the present results may improve the understanding of the electrokinetic transport characteristics in microchannels .
a non iterative continuous model for switching window computation with crosstalk noise . <eos> proper modeling of switching windows leads to a better estimate of the noise induced delay variations . in this paper , we propose a new non iterative continuous switching model . the proposed new model employs an ordering technique combined with the principle of superposition of linear circuits . the principle of superposition considers the impact of aggressors one after the other . the ordering technique avoids convergence and multiple solution issues in many practical cases . our model surpasses the accuracy of the traditional discrete model and the speed of fixed point iteration method .
vibrational analysis of curved single walled carbon nanotube on a pasternak elastic foundation . <eos> continuum mechanics and an elastic beam model were employed in the nonlinear force vibrational analysis of an embedded , curved , single walled carbon nanotube . the analysis considered the effects of the curvature or waviness and midplane stretching of the nanotube on the nonlinear frequency . by utilizing hes energy balance method ( hebm ) , the relationships of the nonlinear amplitude and frequency were expressed for a curved , single walled carbon nanotube . the amplitude frequency response curves of the nonlinear free vibration were obtained for a curved , single walled carbon nanotube embedded in a pasternak elastic foundation . finally , the influence of the amplitude of the waviness , midplane stretching nonlinearity , shear foundation modulus , surrounding elastic medium , radius , and length of the curved carbon nanotube on the amplitude frequency response characteristics are discussed . as a result , the combination effects of waviness and stretching nonlinearity on the nonlinear frequency of the curved swcnt with a small outer radius were larger than the straight one .
fast bokeh effects using low rank linear filters . <eos> we present a method for faster and more flexible approximation of camera defocus effects given a focused image of a virtual scene and depth map . our method leverages the advantages of low rank linear filtering by reducing the problem of 2d convolution to multiple 1d convolutions , which significantly reduces the computational complexity of the filtering operation . in the case of rank <digit> filters ( e.g. , the box filter and gaussian filter ) , the kernel is described as separable since it can be implemented as a horizontal 1d convolution followed by a 1d vertical convolution . while many filter kernels which result in bokeh effects can not be approximated closely by separable kernels , they can be effectively approximated by low rank kernels . we demonstrate the speed and flexibility of low rank filters by applying them to image blurring , tilt shift postprocessing , and depth of field simulation , and also analyze the approximation error for several aperture shapes .
a novel method for cross species gene expression analysis . <eos> analysis of gene expression from different species is a powerful way to identify evolutionarily conserved transcriptional responses . however , due to evolutionary events such as gene duplication , there is no one to one correspondence between genes from different species which makes comparison of their expression profiles complex .
mimo radar signal design to improve the mimo ambiguity function via maximizing its peak . <eos> transmit signals are designed to maximize the ambiguity function s peak of a ws mimo radar . signal design is done for three cases of single target , multi target , and prioritized ambiguity function . it is shown that in spite of increasing the number of antennas of mimo radar , signal design does not provide diversity gain . through simulations , it is shown that better performance can be achieved by the proposed signal design to maximize the af s peak .
optimal design of radial basis function neural networks for fuzzy rule extraction in high dimensional data . <eos> the design of an optimal radial basis function neural network ( rbfnf ) is not a straightforward procedure . in this paper we take advantage of the functional equivalence between rbfn and fuzzy inference systems to propose a novel efficient approach to rbfn design for fuzzy rule extraction . the method is based on advanced fuzzy clustering techniques . solutions to practical problems are proposed . by combining these different solutions , a general methodology is derived . the efficiency of our method is demonstrated on challenging synthetic and real world data sets .
polarization properties of a turnstile antenna in the vicinity of the human body . <eos> polarization of a simple turnstile antenna situated close to the human body , for potential wban applications at 2.45 ghz band , is studied in detail by the use of electromagnetic simulator wipl d pro . circular polarization of the antenna ( when isolated ) is provided by adjusting the dipole impedances . full size , <digit> dimensional simplified homogeneous model of a human body is applied . polarization of both far and near field is studied , with various positions of the antenna and with without metallic reflector . in the far field significant degradation of the circular polarization , due to the vicinity of the body , was observed . in the near field , at points close to the surface of the torso , polarization ( of vector e ) was found to significantly deviate from circular . obtained results can be useful in designing on body sensor networks in which circularly polarized antennas are applied , for both far field communication between sensor nodes and the gateway and near field communication between sensors .
enforcing and defying associativity , commutativity , totality , and strong noninvertibility for worst case one way functions . <eos> rabi and sherman m. rabi , a. sherman , an observation on associative one way functions in complexity theory , information processing letters <digit> ( <digit> ) ( <digit> ) <digit> <digit> m. rabi , a. sherman , associative one way functions a new paradigm for secret key agreement and digital signatures , tech . rep. cs tr <digit> umiacs tr <digit> <digit> , department of computer science , university of maryland , college park , md , <digit> proved that the hardness of factoring is a sufficient condition for there to exist one way functions ( i.e. , p time computable , honest , p time noninvertible functions this paper is in the worst case model , not the average case model ) that are total , commutative , and associative but not strongly noninvertible . in this paper we improve the sufficient condition to p not equal np . more generally , in this paper we completely characterize which types of one way functions stand or fall together with ( plain ) one way functions equivalently , stand or fall together with p <digit> np . we look at the four attributes used in rabi and sherman 's seminal work on algebraic properties of one way functions ( see m. rabi , a. sherman , an observation on associative one way functions in complexity theory , information processing letters <digit> ( <digit> ) ( <digit> ) <digit> <digit> m. rabi , a. sherman , associative one way functions a new paradigm for secret key agreement and digital signatures , tech . rep. cs tr <digit> umiacs tr <digit> <digit> , department of computer science , university of maryland , college park , md , <digit> ) and subsequent papers strongness ( of noninvertibility ) , totality , commutativity , and associativity and for each attribute , we allow it to be required to hold , required to fail , or do n't care . in this categorization there are <digit> ( <digit> ) <digit> potential types of one way functions . we prove that each of these <digit> feature laden types stands or falls together with the existence of ( plain ) one way functions . ( c ) <digit> elsevier b.v. all rights reserved .
an efficient algorithm for constrained global optimization and application to mechanical engineering design league championship algorithm ( lca ) . <eos> the league championship algorithm ( lca ) is a new algorithm originally proposed for unconstrained optimization which tries to metaphorically model a league championship environment wherein artificial teams play in an artificial league for several weeks ( iterations ) . given the league schedule , a number of individuals , as sport teams , play in pairs and their game outcome is determined given known the playing strength ( fitness value ) along with the team formation ( solution ) . modelling an artificial match analysis , each team devises the required changes in its formation ( a new solution ) for the next week contest and the championship goes for a number of seasons . in this paper , we adapt lca for constrained optimization . in particular ( <digit> ) a feasibility criterion to bias the search toward feasible regions is included besides the objective value criterion ( <digit> ) generation of multiple offspring is allowed to increase the probability of an individual to generate a better solution ( <digit> ) a diversity mechanism is adopted , which allows infeasible solutions with a promising objective value precede the feasible solutions . performance of lca is compared with comparator algorithms on benchmark problems where the experimental results indicate that lca is a very competitive algorithm . performance of lca is also evaluated on well studied mechanical design problems and results are compared with the results of <digit> constrained optimization algorithms . computational results signify that with a smaller number of evaluations , lca ensures finding the true optimum of these problems . these results encourage that further developments and applications of lca would be worth investigating in the future studies .
statistical model training technique based on speaker clustering approach for hmm based speech synthesis . <eos> we propose an average voice model training technique using speaker class . the speaker class is obtained on the basis of speaker clustering . the average voice model is trained using the conventional contextual factors and the speaker class . in the speaker adaptation process , the target speakers speaker class is estimated . our proposal can synthesize speech with better similarity and naturalness .
a study of gradual transition detection in historic film material . <eos> the detection of gradual transitions focuses on two types of approaches unified approaches , i.e. one detector for all gradual transition types , and approaches that use specialized detectors for each gradual transition type . we present an overview on existing methods and extend an existing unified approach for the detection of gradual transitions in historic material . in an experimental study we evaluate our approach on complex and low quality historic material as well as on contemporary material from the trecvid evaluation . additionally we investigate different features , feature combinations and fusion strategies . we observe that the historic material requires the use of texture features in contrast to the contemporary material that in most of the cases requires the use of colour and luminance features .
electronic retention what does your mobile phone reveal about you . <eos> the global information rich society is increasingly dependent on mobile phone technology for daily activities . a substantial secondary market in mobile phones has developed as a result of a relatively short life cycle and recent regulatory measures on electronics recycling . these developments are , however , a cause for concern regarding privacy , since it is unclear how much information is retained on a device when it is re sold . the crucial question is what , despite your best efforts , does your mobile phone reveal about you . this research investigates the extent to which personal information continues to reside on mobile phones even when users have attempted to remove the information hence , passing the information into the secondary market . a total of <digit> re sold mobile devices were acquired from two secondary markets a local pawn shop and an online auction site . these devices were examined using three industry standard mobile forensic toolkits . data were extracted from the devices via both physical and logical acquisitions and the resulting information artifacts categorized by type and sensitivity . all mobile devices examined yielded some user information and in total 11,135 artifacts were recovered . the findings confirm that substantial personal information is retained on a typical mobile device when it is re sold . the results highlight several areas of potential future work necessary to ensure the confidentially of personal data stored on mobile devices .
reasoning about digital artifacts with acl2 . <eos> acl2 is both a programming language in which computing systems can be modeled and a tool to help a designer prove properties of such models . acl2 stands for a c omputational l ogic for a pplicative c ommon l isp '' and provides mechanized reasoning support for a first order axiomatization of an extended subset of functional common lisp . most often , acl2 is used to produce operational semantic models of artifacts . such models can be executed as functional lisp programs and so have dual use as both pre fabrication simulation engines and as analyzable mathematical models of intended ( or at least designed ) behavior . this project had its start <digit> years ago in edinburgh with the first boyer moore pure lisp theorem prover and has evolved proofs about list concatenation and reverse to proofs about industrial models . industrial use of theorem provers to answer design questions of critical importance is so surprising to people outside of the theorem proving community that it bears emphasis . in the 1980s , the earlier boyer moore theorem prover , nqthm , was used to verify the computational logic stack '' a hardware software stack starting with the ndl description of the netlist for a microprocessor and ascending through a machine code isa , an assembler , linker , and loader , two compilers ( for subsets of pascal and lisp ) , an operating system , and some simple applications . the system components were proved to compose so that properties proved of high level software were guaranteed by the binary image produced by the composition . at around the same time , nqthm was used to verify <digit> of the <digit> subroutines in the mc68020 binary machine code produced from the berkeley c string library by gcc o , identifying bugs in the library as a result . applications like these convinced us that ( a ) industrial scale formal methods was practical and ( b ) nqthm 's pure lisp produced uncompetitive results compared to c when used for simulation engines . we therefore designed acl2 , which initially was nqthm recoded to support applicative common lisp . the 1990s saw the first industrial application of acl2 , to verify the correspondence between a gate level description of the motorola cap dsp and its microcode engine . the lisp model of the microcode engine was proved to be bit and cycle accurate but operated several times faster than the gate level simulator in c because of the competitive execution speed of lisp and the higher level of trusted abstraction . furthermore , it was used to discover previously unknown microcode hazards . an executable lisp predicate was verified to detect all hazards and subsequently used by microcode programmers to check code . this project and a subsequent one at amd to verify the floating point division operation on the amd k5 microprocessor demonstrated the practicality of acl2 but also highlighted the need to develop better lisp system programming tools wedded to formal methods , formal modeling , proof development , and proof maintenance '' in the face of evolution of the modeled artifacts . much acl2 development in first decade of the 21st century was therefore dedicated to such tools and we have witnessed a cor responding increase in the use of acl2 to construct and reason about commercial artifacts . acl2 has been involved in the design of all amd desktop microprocessors since the athlon specifically , acl2 is used to verify floating point operations on those micro processors . centaur technology ( chipmaker for via technologies ) uses acl2 extensively in verifying its media unit and other parts of its x86 designs . researchers at rockwell collins have shown that acl2 models of microprocessors can run at <digit> % of the speed of c models of those microprocessors . rockwell collins has also used acl2 to do information flow proofs to establish process separation for the aamp7g cryptoprocessor and , on the basis of those proofs , obtained mils certification using formal methods techniques as specified by eal <digit> of the common criteria . ibm has used acl2 to verify floating point operations on the power <digit> and other chips . acl2 was also used to verify key properties of the sun java virtual machine 's class loader . in this talk i will sketch the <digit> year history of this project , showing how the techniques and applications have grown over the years . i will demonstrate acl2 on both some simple prob lems and a complicated one , and i will deal briefly with the question of how and with what tool one verifies a verifier . for scholarly details of some of how to use acl2 and some of its industrial applications see <digit> , <digit> . for source code , lemma li braries , and an online user 's manual , see the acl2 home page , http www.cs.utexas.edu users moore acl2 .
deformation and fracturing using adaptive shape matching with stiffness adjustment . <eos> this paper presents a fast method that computes deformations with fracturing of an object using a hierarchical lattice . our method allows numerically stable computation based on so called shape matching . during the simulation , the deformed shape of the object and the condition of fracturing are used to determine the appropriate detail level in the hierarchy of the lattices . our method modifies the computation of the stiffness of the object in different levels of the hierarchy so that the stiffness is maintained uniform by introducing a stiffness parameter that does not depend on the hierarchy . by merging the subdivided lattices , our method minimizes the increase of computational cost . copyright ( c ) <digit> john wiley sons , ltd .
using bp network for ultrasonic inspection of flip chip solder joints . <eos> flip chip technology has been used extensively in microelectronic packaging , where defect inspection for solder joints plays an extremely important role . in this paper , ultrasonic inspection , one of the non destructive methods , was used for inspection of flip chip solder joints . the image of the flip chip was captured by scanning acoustic microscope and segmented based on the flip chip structure information . then a back propagation network was adopted , and the geometric features extracted from the image were fed to the network for classification and recognition . the results demonstrate the high recognition rate and feasibility of the approach . therefore , this approach has high potentiality for solder joint defect inspection in flip chip packaging .
augmenting reflective middleware with an aspect orientation support layer . <eos> reflective middleware provides an effective way to support adaptation in distributed systems . however , as distributed systems become increasingly complex , certain drawbacks of the reflective middleware approach are becoming evident . in particular , reflective apis are found to impose a steep learning curve , and to place too much expressive power in the hands of developers . recently , researchers in the field of aspect oriented programming ( aop ) have argued that ' dynamic aspects ' show promise in alleviating these drawbacks . in this paper , we report on work that attempts to combine the reflective middleware and aop approaches . we build an aop support layer on top of an underlying reflective middleware substrate in such a way that it can be dynamically deployed undeployed where and when required , and imposes no overhead when it is not used . our aop approach involves aspects that can be dynamically ( un ) weaved across a distributed system on the basis of pointcut expressions that are inherently distributed in nature , and it supports the composition of advice that is remote from the advised joinpoint . an overall goal of the work is to effectively combine reflective middleware and aop in a way that maximises the benefits and minimises the drawbacks of each .
band pass filtering of the time sequences of spectral parameters for robust wireless speech recognition . <eos> in this paper we address the problem of automatic speech recognition when wireless speech communication systems are involved . in this context , three main sources of distortion should be considered acoustic environment , speech coding and transmission errors . whilst the first one has already received a lot of attention , the last two deserve further investigation in our opinion . we have found out that band pass filtering of the recognition features improves asr performance when distortions due to these particular communication systems are present . furthermore , we have evaluated two alternative configurations at different bit error rates ( ber ) typical of these channels band pass filtering the lp mfcc parameters or a modification of the rasta plp using a sharper low pass section perform consistently better than lp mfcc and rasta plp , respectively .
msoar a high throughput ortholog assignment system based on genome rearrangement . <eos> the assignment of orthologous genes between a pair of genomes is a fundamental and challenging problem in comparative genomics , since many computational methods for solving various biological problems critically rely on bona fide orthologs as input . while it is usually done using sequence similarity search , we recently proposed a new combinatorial approach that combines sequence similarity and genome rearrangement . this paper continues the development of the approach and unites genome rearrangement events and ( post speciation ) duplication events in a single framework under the parsimony principle . in this framework , orthologous genes are assumed to correspond to each other in the most parsimonious evolutionary scenario involving both genome rearrangement and ( post speciation ) gene duplication . besides several original algorithmic contributions , the enhanced method allows for the detection of inparalogs . following this approach , we have implemented a high throughput system for ortholog assignment on a genome scale , called msoar , and applied it to human and mouse genomes . as the result will show , msoar is able to find <digit> more true orthologs than the inparanoid program did . in comparison to the iterated exemplar algorithm on simulated data , msoar performed favorably in terms of assignment accuracy . we also validated our predicted main ortholog pairs between human and mouse using public ortholog assignment datasets , synteny information , and gene function classification . these test results indiate that our approach is very promising for genome wide ortholog assignment . supplemental material and msoar program are available at http msoar.cs.ucr.edu .
medlda maximum margin supervised topic models . <eos> a supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data . however , existing supervised topic models predominantly employ likelihood driven objective functions for learning and inference , leaving the popular and potentially powerful max margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus . in this paper , we propose the maximum entropy discrimination latent dirichlet allocation ( medlda ) model , which integrates the mechanism behind the max margin prediction models ( e.g. , svms ) with the mechanism behind the hierarchical bayesian topic models ( e.g. , lda ) under a unified constrained optimization framework , and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression . the principle underlying the medlda formalism is quite general and can be applied for jointly max margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available . efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided . our experimental results demonstrate qualitatively and quantitatively that medlda could <digit> ) discover sparse and highly discriminative topical representations <digit> ) achieve state of the art prediction performance and <digit> ) be more efficient than existing supervised topic models , especially for classification .
ls svm based image segmentation using pixel color texture descriptors . <eos> image segmentation remains an important , but hard to solve , problem since it appears to be application dependent with usually no a priori information available regarding the image structure . moreover , the increasing demands of image analysis tasks in terms of segmentation results quality introduce the necessity of employing multiple cues for improving image segmentation results . in this paper , we present a least squares support vector machine ( ls svm ) based image segmentation using pixel color texture descriptors , in which multiple cues such as edge saliency , color saliency , local maximum energy , and multiresolution texture gradient are incorporated . firstly , the pixel level edge saliency and color saliency are extracted based on the spatial relations between neighboring pixels in hsv color space . secondly , the image pixels texture features , local maximum energy and multiresolution texture gradient , are represented via nonsubsampled contourlet transform . then , both the pixel level edge color saliency and texture features are used as input of ls svm model ( classifier ) , and the ls svm model ( classifier ) is trained by selecting the training samples with arimoto entropy thresholding . finally , the color image is segmented with the trained ls svm model ( classifier ) . this image segmentation not only can fully take advantage of the human visual attention and local texture content of color image , but also the generalization ability of ls svm classifier . experimental results show that our proposed method has very promising segmentation performance compared with the state of the art segmentation approaches recently proposed in the literature .
geometric verification of swirling features in flow fields . <eos> in this paper , we present a verification algorithm for swirling features in flow fields , based on the geometry of streamlines . the features of interest in this case are vortices . without a formal definition , existing detection algorithms lack the ability to accurately identify these features , and the current method for verifying the accuracy of their results is by human visual inspection . our verification algorithm addresses this issue by automating the visual inspection process . it is based on identifying the swirling streamlines that surround the candidate vortex cores . we apply our algorithm to both numerically simulated and procedurally generated datasets to illustrate the efficacy of our approach .
on topology and dynamics of consensus among linear high order agents . <eos> consensus of a group of agents in a multi agent system with and without a leader is considered . all agents are modelled by identical linear n th order dynamical systems while the leader , when it exists , may evolve according to a different linear model of the same order . the interconnection topology between the agents is modelled as a directed weighted graph . we provide answers to the questions of whether the group converges to consensus and what consensus value the group eventually reaches . to that end , we give a detailed analysis of relevant algebraic properties of the graph laplacian . furthermore , we propose an lmi based design for group consensus in the general case .
human cognition in manual assembly theories and applications . <eos> human cognition in production environments is analyzed with respect to various findings and theories in cognitive psychology . this theoretical overview describes effects of task complexity and attentional demands on both mental workload and task performance as well as presents experimental data on these topics . a review of two studies investigating the benefit of augmented reality and spatial cueing in an assembly task is given . results demonstrate an improvement in task performance with attentional guidance while using contact analog highlighting . improvements were obvious in reduced performance times and eye fixations as well as in increased velocity and acceleration of reaching and grasping movements . these results have various implications for the development of an assistive system . future directions in this line of applied research are suggested . the introduced methodology illustrates how the analysis of human information processes and psychological experiments can contribute to the evaluation of engineering applications .
enabling warping on stereoscopic images . <eos> warping is one of the basic image processing techniques . directly applying existing monocular image warping techniques to stereoscopic images is problematic as it often introduces vertical disparities and damages the original disparity distribution . in this paper , we show that these problems can be solved by appropriately warping both the disparity map and the two images of a stereoscopic image . we accordingly develop a technique for extending existing image warping algorithms to stereoscopic images . this technique divides stereoscopic image warping into three steps . our method first applies the user specified warping to one of the two images . our method then computes the target disparity map according to the user specified warping . the target disparity map is optimized to preserve the perceived 3d shape of image content after image warping . our method finally warps the other image using a spatially varying warping method guided by the target disparity map . our experiments show that our technique enables existing warping methods to be effectively applied to stereoscopic images , ranging from parametric global warping to non parametric spatially varying warping .
markov chain modeling of intermittency chaos and its application to hopfield nn . <eos> in this study , a modeling method of the intermittency chaos using the markov chain is proposed . the performances of the intermittency chaos and the markov chain model are investigated when they are injected to the hopfield neural network for a quadratic assignment problem or an associative memory . computer simulated results show that the proposed modeling is good enough to gain similar performance of the intermittency chaos .
splitting integrators for nonlinear schrodinger equations over long times . <eos> conservation properties of a full discretization via a spectral semi discretization in space and a lie trotter splitting in time for cubic schrodinger equations with small initial data ( or small nonlinearity ) are studied . the approximate conservation of the actions of the linear schrodinger equation , energy , and momentum over long times is shown using modulated fourier expansions . the results are valid in arbitrary spatial dimension .
generalized scans and tridiagonal systems . <eos> motivated by the analysis of known parallel techniques for the solution of linear tridiagonal system , we introduce generalized scans , a class of recursively defined length preserving , sequence to sequence transformations that generalize the well known prefix computations ( scans ) . generalized scan functions are described in terms of three algorithmic phases , the reduction phase that saves data for the third of expansion phase and prepares data for the second phase which is a recursive invocation of the same function on one fewer variable . both the reduction and expansion phases operate on bounded number of variables , a key feature for their parallelization . generalized scans enjoy a property , called here protoassociativity , that gives rise to ordinary associativity when generalized scans are specialized to ordinary scans . we show that the solution of positive definite block tridiagonal linear systems can be cast as a generalized scan , thereby shedding light on the underlying structure enabling known parallelization schemes for this problem . we also describe a variety of parallel algorithms including some that are well known for tridiagonal systems and some that are much better suited to distributed computation . ( c ) <digit> elsevier science b.v. all rights reserved .

efficient normal basis multipliers in composite fields . <eos> it is well known that a class of finite fields gf ( <digit> ( n ) ) using an optimal normal basis is most suitable for a hardware implementation of arithmetic in finite fields . in this paper , we introduce composite fields of some hardware applicable properties resulting from the normal basis representation and the optimal condition . we also present a hardware architecture of the proposed composite fields including a hit parallel multiplier .
a stable second order scheme for fluidstructure interaction with strong added mass effects . <eos> in this paper , we present a stable second order time accurate scheme for solving fluidstructure interaction problems . the scheme uses so called combined field with explicit interface ( cfei ) advancing formulation based on the arbitrary lagrangianeulerian approach with finite element procedure . although loosely coupled partitioned schemes are often popular choices for simulating fsi problems , these schemes may suffer from inherent instability at low structure to fluid density ratios . we show that our second order scheme is stable for any mass density ratio and hence is able to handle strong added mass effects . energy based stability proof relies heavily on the connections among extrapolation formula , trapezoidal scheme for second order equation , and backward difference method for first order equation . numerical accuracy and stability of the scheme is assessed with the aid of two dimensional fluidstructure interaction problems of increasing complexity . we confirm second order temporal accuracy by numerical experiments on an elastic semi circular cylinder problem . we verify the accuracy of coupled solutions with respect to the benchmark solutions of a cylinder elastic bar and the navierstokes flow system . to study the stability of the proposed scheme for strong added mass effects , we present new results using the combined field formulation for flexible flapping motion of a thin membrane structure with low mass ratio and strong added mass effects in a uniform axial flow . using a systematic series of fluidstructure simulations , a detailed analysis of the coupled response as a function of mass ratio for the case of very low bending rigidity has been presented .
network information flow . <eos> a formal model for an analysis of an information flow in interconnection networks is presented . it is based on timed process algebra which can express also network properties . the information flow is based on a concept of deducibility on composition . robustness of systems against network timing attacks is defined . a variety of different security properties which reflect different security requirements are defined and investigated .
interactive reduct evolutional computation for aesthetic design . <eos> we propose a method of evolving designs based on the user 's personal preferences . the method works through an interaction between the user and a computer system . the method 's objective is to help the customer to set design parameters via a simple evaluation of displayed samples . an important feature is that the design attributes to which the user pays more attention ( favored features ) are estimated using reducts in rough set theory and reflected when refining the design . new design candidates are generated by the user 's evaluation of design samples generated at random . the values of attributes estimated as favored features are fixed in the refined samples , while other attributes are generated at random . this interaction continues until the samples converge to a satisfactory design . in this manner , the design process efficiently evaluates personal and subjective preferences . the method is applied to design a 3d cylinder model such as a cup or vase . the method is then compared with an interactive ga .
a multiple criteria sorting method where each category is characterized by several reference actions the electre tri nc method . <eos> this paper presents electre tri nc , a new sorting method which takes into account several reference actions for characterizing each category . this new method gives a particular freedom to the decision maker in the co construction decision aiding process with the analyst to characterize the set of categories , while there is no constraint for introducing only one reference action as typical of each category like in electre tri c ( almeida dias et al. , <digit> ) . as in such a sorting method , this new sorting method is composed of two joint rules . electre tri nc also fulfills a certain number of natural requirements . additional results on the behavior of the new method are also provided in this paper , namely the ones with respect to the addition or removal of the reference actions used for characterizing a certain category . a numerical example illustrates the manner in which electre tri nc can be used by a decision maker . a comparison with some related sorting procedures is presented and it allows to conclude that the new method is appropriate to deal with sorting problems .
estimating unique solutions of dc transistor circuits . <eos> for each natural n let f n denote the collection of mappings of r n onto itself defined by f is an element of f u if and only if there exist n strictly monotone increasing functions f ( k ) mapping r onto itself such that for each x x ( <digit> ) , ... , x ( n ) ( t ) is an element of r n , f ( x ) f ( <digit> ) ( x ( <digit> ) ) , ... , f ( n ) ( x ( n ) ) ( t ) . the following new property of the class p <digit> of matices is proved a real n x n matrix a belongs to p <digit> if and only if for every g , h is an element of f n the set s <digit> x is an element of r n g ( x ) less than or equal to ax less than or equal to h ( x ) is bounded . as an illustration of this property a method of estimating the unique solution of the nonlinear equation f ( x ) a ( x ) b describing the large class of dc transistor circuits is developed . this can improve the efficiency of known computation algorithms . numerical examples of transistor circuits illustrate in detail how the method works in practice .
multiple topic identification in human human conversations . <eos> a multiple classification methods for multiple theme hypothesization is proposed . four methods , one of which is new , are initially used and separately evaluated . a new sequential decision strategy for multiple theme hypothesization is introduced . a new hypothesis refinancing component is presented , based on asr word lattice . results show that the strategy makes it possible to obtain reliable service surveys .
towards a documentation maturity model . <eos> this paper presents preliminary work towards a maturity model for system documentation . the documentation maturity model ( dmm ) is specifically targeted towards assessing the quality of documentation used in aiding program understanding . software engineers and technical writers produce such documentation during regular product development lifecycles . the documentation can also be recreated after the fact via reverse engineering . the dmm has both process and product components this paper focuses on the product quality aspects .
numerical representation of product transitive complete fuzzy orderings . <eos> let x be a space of alternatives with a preference relation in the form of product transitive complete fuzzy ordering r. we prove existence of continuous utility functions for r. ( c ) <digit> elsevier ltd. all rights reserved .
design of wdm rof pon based on ofdm and optical heterodyne . <eos> in this paper , we propose a wdm radio over fiber ( rof ) passive optical network ( pon ) based on orthogonal frequency division multiplexing ( ofdm ) and optical heterodyne . with ofdm and coherent receiving technology , the system achieves high , elastic bandwidth allocation and excellent transporting property . using optical heterodyne , the network implements the wireless access without adding a radio source . we evaluate the performance of the system in terms of bit error rate , coverage area , and receiving eye diagram and obtain the network as an excellent wire wireless access property .
image retrieval via isotropic and anisotropic mappings . <eos> this paper presents an approach for content based image retrieval via isotropic and anisotropic mappings . isotropic mappings are defined as mappings invariant to the action of the planar euclidean group on the image spaceinvariant to the translation , rotation and reflection of image data , and hence , invariant to orientation and position . anisotropic mappings , on the other hand , are defined as those mappings that are correspondingly variant . structure extraction ( via a perceptual grouping process ) and color histogram are shown to be representations of isotropic mappings . texture analysis using a channel energy model comprised of even symmetric gabor filters is considered to be a representation of anisotropic mapping . an integration framework for these mappings is developed . results of retrieval of outdoor images by query and by classification using a nearest neighbor classifier are presented .
physical gestures for abstract concepts inclusive design with primary metaphors . <eos> designers in inclusive design are challenged to create interactive products that cater for a wide range of prior experiences and cognitive abilities of their users . but suitable design guidance for this task is rare . this paper proposes the theory of primary metaphor and explores its validity as a source of design guidance . primary metaphor theory describes how basic mental representations of physical sensorimotor experiences are extended to understand abstract domains . as primary metaphors are subconscious mental representations that are highly automated , they should be robustly available to people with differing levels of cognitive ability . their proposed universality should make them accessible to people with differing levels of prior experience with technology . these predictions were tested for <digit> primary metaphors that predict relations between spatial gestures and abstract interactive content . in an empirical study , <digit> participants from two age groups ( young and old ) were asked to produce two dimensional touch and three dimensional free form gestures in response to given abstract keywords and spatial dimensions of movements . the results show that across age groups in <digit> % of all cases users choose gestures that confirmed the predictions of the theory . although the two age groups differed in their cognitive abilities and prior experience with technology , overall they did not differ in the amount of metaphor congruent gestures they made . as predicted , only small or zero correlations of metaphor congruent gestures with prior experience or cognitive ability could be found . the results provide a promising step toward inclusive design guidelines for gesture interaction with abstract content on mobile multitouch devices . ( c ) <digit> elsevier b.v. all rights reserved .
mbs zone configuration schemes for wireless multicast and broadcast service . <eos> the multicast broadcast service ( mbs ) zone technology is proposed to provide mbs with high qos on mobile communications networks ( mcns ) . an mbs zone consists of a group of base stations ( bss ) synchronized to transmit the same mbs content using the same multicasting channel , which potentially reduces the time delay for mobile stations ( mss ) to handoff between different bss in the same mbs zone . however , significant time delay still incurs while mss handoff between different bss belonging to different mbs zones ( i.e. , the inter mbs zone handoff ) . to reduce the possibility for the inter mbs zone handoff , we may increase the size of an mbs zone ( i.e. , more bss contained in an mbs zone ) , which may result in poor multicasting channel utilization . this paper proposes the overlapping scheme ( ols ) and the enhanced overlapping scheme ( eols ) for more flexible mbs zone configuration to get better performance for mbs in terms of qos and radio resource utilization . we propose the analytical models for the original mbs zone technology ( namely the basic scheme ) , and the ols scheme , which are validated against the simulation experiments . based on the simulation results , we investigate the performance for the basic scheme , the ols scheme , and the eols scheme . copyright ( c ) <digit> john wiley sons , ltd .

olsr aware channel access scheduling in wireless mesh networks . <eos> wireless mesh networks ( wmns ) have emerged as a key technology having various advantages , especially in providing cost effective coverage and connectivity solutions in both rural and urban areas . wmns are typically deployed as backbone networks , usually employing spatial tdma ( stdma ) based access schemes which are suitable for the high traffic demands of wmns . this paper aims to achieve higher utilization of the network capacity and thereby aims to increase the application layer throughput of stoma based wmns . the central idea is to use optimized link state routing ( olsr ) specific routing layer information in link layer channel access schedule formation . this paper proposes two stdma based channel access scheduling schemes ( one distributed , one centralized ) that exploit olsr specific information to improve the application layer throughput without introducing any additional messaging overhead . to justify the contribution of using olsr specific information to the throughput , the proposed schemes are compared against one another and against their non olsr aware versions via extensive ns <digit> simulations . our simulation results verify that utilizing olsr specific information significantly improves the overall network performance both in distributed and in centralized schemes . the simulation results further show that olsr aware scheduling algorithms attain higher end to end throughput although their non olsr aware counterparts achieve higher concurrency in slot allocations . ( c ) <digit> elsevier inc. all rights reserved .
privacy preserving indexing of documents on the network . <eos> with the ubiquitous collection of data and creation of large distributed repositories , enabling search over this data while respecting access control is critical . a related problem is that of ensuring privacy of the content owners while still maintaining an efficient index of distributed content . we address the problem of providing privacy preserving search over distributed access controlled content . indexed documents can be easily reconstructed from conventional ( inverted ) indexes used in search . currently , the need to avoid breaches of access control through the index requires the index hosting site to be fully secured and trusted by all participating content providers . this level of trust is impractical in the increasingly common case where multiple competing organizations or individuals wish to selectively share content . we propose a solution that eliminates the need of such a trusted authority . the solution builds a centralized privacy preserving index in conjunction with a distributed access control enforcing search protocol . two alternative methods to build the centralized index are proposed , allowing trade offs of efficiency and security . the new index provides strong and quantifiable privacy guarantees that hold even if the entire index is made public . experiments on a real life dataset validate performance of the scheme . the appeal of our solution is twofold ( a ) content providers maintain complete control in defining access groups and ensuring its compliance , and ( b ) system implementors retain tunable knobs to balance privacy and efficiency concerns for their particular domains .
cross layer optimization for efficient data aggregation in multi hop wireless sensor networks . <eos> wireless sensor networks ( wsn ) is the most promising technological paradigm to support the next generation highly efficient emergency management systems . optimal design of wsn involves all the layers of the protocol stack from the physical ( phy ) , the medium access layer ( mac ) to the application layer . the design problem is conveniently cast in this paper for linear sensor network topologies where the terminals are equidistantly placed on the line between the source and the destination and are monitoring a correlated field . this simple topology can be adopted to provide insights to the performance of multihop networks used in several applications as monitoring systems , acoustic sensor arrays , seismic systems etc. . . the paper provides an analytical tool for performance analysis that takes into account both the statistical properties of the monitored field ( spatial and temporal correlation ) , the phy layer transceiver design ( rf power allocation and modulation ) and the medium access ( duty cycle , routing ) .
rank order polynomial subband decomposition for medical image compression . <eos> in this paper , the problem of progressive lossless image coding is addressed , a nonlinear decomposition for progressive lossless compression is presented . the decomposition into subbands is called rank order polynomial decomposition ( ropd ) according to the polynomial prediction models used . the decomposition method presented here is a further development and generalization of the morphological subband decomposition ( msd ) introduced earlier by the same research group . it is shown that ropd provides similar or slightly better results than the compared coding schemes such as the codec based on set partitioning in hierarchical trees ( spiht ) and the codec based on wavelet trellis coded quantization ( wtcq ) . our proposed method highly outperforms the standard jpeg . the proposed lossless compression scheme has the functionality of having a completely embedded bit stream , which allows for data browsing . it is shown that the ropd has a better lossless rate than the msd but it has also a much better browsing quality when only a part of the bit stream is decompressed . finally , the possibility of hybrid lossy lossless compression is presented using ultrasound images . as with other compression algorithms , considerable gain can be obtained if only the regions of interest are compressed losslessly .
linearn a new approach to nearest neighbour density estimator . <eos> reject the premise that a nn algorithm must find the nn for every instance . the first nn density estimator that has o ( n ) o ( n ) time complexity and o ( <digit> ) o ( <digit> ) space complexity . these complexities are achieved without using any indexing scheme . our asymptotic analysis reveals that it trades off between bias and variance . easily scales up to large data sets in anomaly detection and clustering tasks .
beauty or realism the dimensions of skin from cognitive sciences to computer graphics . <eos> as the most visible interface between the individual and the others , the skin is a key element of visually carried inter individual social information , since skin displays a wide array of information regarding gender , age , or health status . adequate skin perception is central in individual identification and social interactions . this topic elicited marked interest in artists since the first development of visual arts in antiquity . often performed in order to identify the biological correlates of attractiveness , psychological research on skin perception made a jump forward with the development of virtual image synthesis . here , we investigate how advances in both computer graphics and the psychology of skin perception may be turned to use in real time virtual worlds . we propose a model of skin perception based both on purely physical dimensions such as color , texture , and symmetry , and on dimensions carrying socially oriented information , such as perceived youth ( information regarding putative fertility ) , markers of sexual dimorphism ( information regarding hormonal status ) , and level of oxygenation ( information regarding health status ) . it appears that for almost all of the dimensions of skin , maximal attractiveness and realism are the two opposite extremities of a single perceptive continuum .
fault diagnosis by locality preserving discriminant analysis and its kernel variation . <eos> linear discriminant analysis ( lda ) and its nonlinear kernel variation generalized discriminant analysis ( gda ) are the most popular supervised dimensionality reduction methods for fault diagnosis . however , we argue that they probably provide suboptimal results for fault diagnosis due to the fisher 's criterion they use . this paper proposes a new supervised dimensionality reduction method named locality preserving discriminant analysis ( lpda ) and its kernel variation kernel lpda ( klpda ) for fault diagnosis . ( k ) lpda maximizes a new criterion such that local discriminant structure and local geometric structure in data are optimally preserved simultaneously in each dimension of the reduced space . the criterion directly targets at minimizing local overlapping between different classes . extensive simulations on the tennessee eastman ( te ) benchmark simulation process and a waste water treatment plant ( wwtp ) clearly demonstrate the superiority of our methods in terms of misclassification rate and making use of extra training data . ( c ) <digit> elsevier ltd. all rights reserved .
wireless distributed computing in cognitive radio networks . <eos> individual cognitive radio nodes in an ad hoc cognitive radio network ( crn ) have to perform complex data processing operations for several purposes , such as situational awareness and cognitive engine ( ce ) decision making . in an implementation point of view , each cognitive radio ( cr ) may not have the computational and power resources to perform these tasks by itself . in this paper , wireless distributed computing ( wdc ) is presented as a technology that enables multiple resource constrained nodes to collaborate in computing complex tasks in a distributed manner . this approach has several benefits over the traditional approach of local computing , such as reduced energy and power consumption , reduced burden on the resources of individual nodes , and improved robustness . however , the benefits are negated by the communication overhead involved in wdc . this paper demonstrates the application of wdc to crns with the help of an example ce processing task . in addition , the paper analyzes the impact of the wireless environment on wdc scalability in homogeneous and heterogeneous environments . the paper also proposes a workload allocation scheme that utilizes a combination of stochastic optimization and decision tree search approaches . the results show limitations in the scalability of wdc networks , mainly due to the communication overhead involved in sharing raw data pertaining to delegated computational tasks .
advertisement timeout driven bee 's mating approach to maintain fair energy level in sensor networks . <eos> in wireless sensor network , dynamic cluster based routing approach is widely used . such practiced approach , quickly depletes the energy of cluster heads and induces the execution of frequent re election algorithm . this repeated cluster head re election algorithm increases the number of advertisement messages , which in turn depletes the energy of overall sensor network . here , we proposed the advertisement timeout driven bee 's mating approach ( atdbma ) that reduces the cluster set up communication overhead and elects the standby node in advance for current cluster head , which has the capability to withstand for many rounds . our proposed atdbma method uses the honeybee mating behaviour in electing the standby node for current cluster head . this approach really outperforms the other methods in achieving reduced number of re election and maintaining fair energy nodes between the rounds .

protection against soft errors in the space environment a finite impulse response ( fir ) filter case study . <eos> the problem of radiation is a key issue in space applications , since it produces several negative effects on digital circuits . considering the high reliability expected in these systems , many techniques have been proposed to mitigate these effects . however , traditional protection techniques against soft errors , like triple modular redundancy ( tmr ) or edac codes ( for example hamming ) , normally result in a significant area and power overhead . in this paper we propose a specific technique to protect digital finite impulse response ( fir ) filters applying the system knowledge . this means to study and use the singularities in their structure in order to provide effective protection with minimal area and power . the results obtained in the experimental process have been compared with the protection offered by tmr and hamming codes , in order to prove the quality of the proposed solution .
minimizing the dynamic and sub threshold leakage power consumption using least leakage vector assisted technology mapping . <eos> power consumption due to the temperature dependent leakage current becomes a dominant part of the total power dissipation in systems using nanometer scale process technology . to obtain the minimum power consumption for different operating conditions , logic synthesis tools are required to take into consideration the leakage power as well as the operating characteristics during the optimization . conventional logic synthesis flows consider dynamic power only and use an over simplified cost function in modeling the total power consumption of the logic network . in this paper , we propose a complete model of the total power consumption of the logic network , which includes both the active and standby sub threshold leakage power , and the operating duty cycle of the applications . we also propose a least leakage vector ( llv ) assisted technology mapping algorithm to optimize the total power of the final mapped network . instead of finding the llv after the logic network is synthesized and mapped , we use the llv found in the technology decomposed network to help in obtaining the lowest total power match during technology mapping . experimental results on mcnc benchmarks show that on average more than <digit> % reduction in total power consumption is obtained comparing with the conventional low power technology mapping algorithm .
chaos breeds autonomy connectionist design between bias and baby sitting . <eos> in connectionism and its offshoots , models acquire functionality through externally controlled learning schedules . this undermines the claim of these models to autonomy . providing these models with intrinsic biases is not a solution , as it makes their function dependent on design assumptions . between these two alternatives , there is room for approaches based on spontaneous self organization . structural reorganization in adaptation to spontaneous activity is a well known phenomenon in neural development . it is proposed here as a way to prepare connectionist models for learning and enhance the autonomy of these models .
an improvement on the complexity of factoring read once boolean functions . <eos> read once functions have gained recent , renewed interest in the fields of theory and algorithms of boolean functions , computational learning theory and logic design and verification . in an earlier paper m.c. golumbic , a. mintz , u. rotics , factoring and recognition of read once functions using cographs and normality , and the readability of functions associated with partial k trees , discrete appl . math . <digit> ( <digit> ) <digit> , we presented the first polynomial time algorithm for recognizing and factoring read once functions , based on a classical characterization theorem of gurvich which states that a positive boolean function is read once if and only if it is normal and its co occurrence graph is p4 p <digit> free . in this note , we improve the complexity bound by showing that the method can be modified slightly , with two crucial observations , to obtain an o ( n f ) o ( n f ) implementation , where f f denotes the length of the dnf expression of a positive boolean function f , and n is the number of variables in f . the previously stated bound was o ( n2k ) o ( n <digit> k ) , where k is the number of prime implicants of the function . in both cases , f is assumed to be given as a dnf formula consisting entirely of the prime implicants of the function .
a dutch medical language processor part ii evaluation . <eos> this paper provides a preliminary evaluation of a general dutch medical language processor ( dmlp ) . four examples of different potential applications ( based on different linguistic modules ) are presented , each with its own evaluation method . finally , a critical review of the used evaluation methods is offered according to the state of the art in medical language processing .
privacy preserving distributed network troubleshooting bridging the gap between theory and practice . <eos> today , there is a fundamental imbalance in cybersecurity . while attackers act more andmore globally and coordinated , network defense is limited to examine local information only due to privacy concerns . to overcome this privacy barrier , we use secure multiparty computation ( mpc ) for the problem of aggregating network data from multiple domains . we first optimize mpc comparison operations for processing high volume data in near real time by not enforcing protocols to run in a constant number of synchronization rounds . we then implement a complete set of basic mpc primitives in the sepia library . for parallel invocations , sepia 's basic operations are between <digit> and several hundred times faster than those of comparable mpc frameworks . using these operations , we develop four protocols tailored for distributed network monitoring and security applications the entropy , distinct count , event correlation , and top k protocols . extensive evaluation shows that the protocols are suitable for near real time data aggregation . for example , our top k protocol pptks accurately aggregates counts for 180,000 distributed ip addresses in only a few minutes . finally , we use sepia with real traffic data from <digit> customers of a backbone network to collaboratively detect , analyze , and mitigate distributed anomalies . our work follows a path starting from theory , going to system design , performance evaluation , and ending with measurement . along this way , it makes a first effort to bridge two very disparate worlds mpc theory and network monitoring and security practices .
better gp benchmarks community survey results and proposals . <eos> we present the results of a community survey regarding genetic programming benchmark practices . analysis shows broad consensus that improvement is needed in problem selection and experimental rigor . while views expressed in the survey dissuade us from proposing a large scale benchmark suite , we find community support for creating a blacklist of problems which are in common use but have important flaws , and whose use should therefore be discouraged . we propose a set of possible replacement problems .
a tabular steganography scheme for graphical password authentication . <eos> authentication , authorization and auditing are the most important issues of security on data communication . in particular , authentication is the life of every individual essential closest friend . the user authentication security is dependent on the strength of user password . a secure password is usually random , strange , very long and difficult to remember . for most users , remember these irregular passwords are very difficult . to easily remember and security are two sides of one coin . in this paper , we propose a new graphical password authentication protocol to solve this problem . graphical password authentication technology is the use of click on the image to replace input some characters . the graphical user interface can help user easy to create and remember their secure passwords . however , in the graphical password system based on images can provide an alternative password , but too many images will be a large database to store issue . all the information can be steganography to achieve our scheme to solve the problem of database storage . furthermore , tabular steganography technique can achieve our scheme to solve the information eavesdropping problem during data transmission . our modified graphical password system can help user easily and friendly to memorize their password and without loss of any security of authentication . user 's chosen input will be hidden into image using steganography technology , and will be transferred to server security without any hacker problem . and then , our authentication server only needs to store only a secret key for decryption instead of large password database .
a framework for optimal correction of inconsistent linear constraints . <eos> the problem of inconsistency between constraints often arises in practice as the result , among others , of the complexity of real models or due to unrealistic requirements and preferences . to overcome such inconsistency two major actions may be taken removal of constraints or changes in the coefficients of the model . this last approach , that can be generically described as model correction is the problem we address in this paper in the context of linear constraints over the reals . the correction of the right hand side alone , which is very close to a fuzzy constraints approach , was one of the first proposals to deal with inconsistency , as it may be mapped into a linear problem . the correction of both the matrix of coefficients and the right hand side introduces non linearity in the constraints . the degree of difficulty in solving the problem of the optimal correction depends on the objective function , whose purpose is to measure the closeness between the original and corrected model . contrary to other norms , that provide corrections with quite rigid patterns , the optimization of the important frobenius norm was still an open problem . we have analyzed the problem using the kkt conditions and derived necessary and sufficient conditions which enabled us to unequivocally characterize local optima , in terms of the solution of the total least squares and the set of active constraints . these conditions justify a set of pruning rules , which proved , in preliminary experimental results , quite successful in a tree search procedure for determining the global minimizer .
user interface evaluation and empirically based evolution of a prototype experience management tool . <eos> experience management refers to the capture , structuring , analysis , synthesis , and reuse of an organization 's experience in the form of documents , plans , templates , processes , data , etc. the problem of managing experience effectively is not unique to software development , but the field of software engineering has had a high level approach to this problem for some time . the experience factory is an organizational infrastructure whose goal is to produce , store , and reuse experiences gained in a software development organization <digit> , <digit> , <digit> . this paper describes the q labs experience management system ( q labs ems ) , which is based on the experience factory concept and was developed for use in a multinational software engineering consultancy <digit> . a critical aspect of the q labs ems project is its emphasis on empirical evaluation as a major driver of its development and evolution . the initial prototype requirements were grounded in the organizational needs and vision of q labs , as were the goals and evaluation criteria later used to evaluate the prototype . however , the q labs ems architecture , data model , and user interface were designed to evolve , based on evolving user needs . this paper describes this approach , including the evaluation that was conducted of the initial prototype and its implications for the further development of systems to support software experience management .
energy aware performance analysis methodologies for hpc architecturesan exploratory study . <eos> performance analysis is a crucial step in hpc architectures including clouds . traditional performance analysis methodologies that were proposed , implemented , and enacted are functional with the objective of identifying bottlenecks or issues related to memory , programming languages , hardware , and virtualization aspects . however , the need for energy efficient architectures in highly scalable computing environments , such as , grid or cloud , has widened the research thrust on developing performance analysis methodologies that analyze the energy inefficiency of hpc applications or their associated hardware . this paper surveys the performance analysis methodologies that investigates into the available energy monitoring and energy awareness mechanisms for hpc architectures . in addition , the paper validates the existing tools in terms of overhead , portability , and user friendly parameters by conducting experiments at hpccloud research laboratory at our premise . this research work will promote hpc application developers to select an apt monitoring mechanism and hpc tool developers to augment required energy monitoring mechanisms which fit well with their basic monitoring infrastructures .
locating the tightest link of a network path . <eos> the tightest link of a network path is the link where the end to end available bandwidth is limited . we propose a new probe technique , called dual rate periodic streams ( drps ) , for finding the location of the tightest link . a drps probe is a periodic stream with two rates . initially , it goes through the path at a comparatively high rate . when arrived at a particular link , the probe shifts its rate to a lower level and keeps the rate . if proper rates are set to the probe , we can control whether the probe is congested or not by adjusting the shift time . when the point of rate shift is in front of the tightest link , the probe can go through the path without congestion , otherwise congestion occurs . thus , we can find the location of the tightest link by congestion detection at the receiver .
research methodology using online technology for secondary analysis of survey research data act globally , think locally . <eos> the purpose of the this article is to discuss the impact that online technologies are having and will continue to have on the way secondary analysis of survey research is performed . the authors discuss the validity of secondary analysis of survey research studies and the effect that online technology has on such analyses . before reviewing current online public opinion sources , the authors make the argument that online services are becoming increasingly important for secondary analysis . finally , the authors present a model indicating where online services can go in the future given the technology that is available today . ultimately , it is believed that the internet is currently underexploited for its capacity to aid secondary analysis . the authors advocate making survey data more easily available online to all potential users . this entails varying the format and depth of data so that users find sources suitable to their needs . it also entails the use of desktop technology to store and analyze survey research data and making that technology , or the applications that are developed through that technology , available to other users via computer networks , primarily via the internet .
free vibration analysis of multiple stepped beams by using adomian decomposition method . <eos> the adomian decomposition method ( adm ) is employed in this paper to investigate the free vibrations of the euler bernoulli beams with multiple cross section steps . the proposed adm method can be used to analyze the vibration of beams consisting of an arbitrary number of steps through a recursive way . the solution can be obtained by solving a set of algebraic equations with only three unknown parameters . furthermore , the method can be extended to obtain an approximate solution to vibration problems of any type of non uniform beams . several numerical examples are presented and compared to those given in the paper . it is shown that the adm offers an accurate and effective method of free vibration analysis of multiple stepped beams with arbitrary boundary conditions . ( c ) <digit> elsevier ltd. all rights reserved .
modelling and performance evaluation of mobile multimedia systems using qos gspn . <eos> quality of service ( qos ) measurement of multimedia applications is one of the most important issues for call handoff and call admission control in mobile networks . based on the qos measures , we propose a generalized stochastic petri net ( gspn ) based model , called qos gspn , which can express the real time behavior of qos measurement for mobile networks . qos gspn performance analysis methodology includes the formal expression and performance analysis environment . it offers the promise of providing real time behavior predictability for systems characterized by substantial stochastic behavior . with this methodology we model and analyze the call handoff and call admission control schemes in the different multimedia traffic environments of a mobile network . the results of simulation experiments are used to verify the optimal performance achievable for these schemes under the qos constraints in the given setting of design parameters .
watermarking of mpeg <digit> video in compressed domain using vlc mapping . <eos> in this work we propose a new algorithm for fragile , high capacity yet file size preserving watermarking of mpeg <digit> streams . watermarking is done entirely in the compressed domain , with no need for full or even partial decompression . the algorithm is based on a previously developed concept of vlc mapping for compressed domain watermarking . the entropy coded segment of the video is first parsed out and then analyzed in pairs . it is recognized that there are vlc pairs that never appear together in any intra coded block . the list of unused pairs is systematically generated by the intersection of pair trees . one of the trees is generated from the main vlc table given in iso iec <digit> <digit> <digit> standard . the other trees are dynamically generated for each intra coded blocks . forcing one vlc pairs in a block to one of the unused ones generates a watermark block . the change is done while maintaining run level change to a minimum . at the decoder , the main pair tree is created offline using publicly available vlc tables . through a secure key exchange , the indices to unused code pairs are communicated to the receiver . we show that the watermarked video is reasonably resistant to forgery attacks and remains secure to watermark detection attempts .
implementing monads for c plus plus template metaprograms . <eos> c template metaprogramming is used in various application areas , such as expression templates , static interface checking , active libraries , etc. its recognized similarities to pure functional programming languages like haskell make the adoption of advanced functional techniques possible . such a technique is using monads , programming structures representing computations . using them actions implementing domain logic can be chained together and decorated with custom code . c template metaprogramming could benefit from adopting monads in situations like advanced error propagation and parser construction . in this paper we present an approach for implementing monads in c template metaprograms . based on this approach we have built a monadic framework for c template metaprogramming . as real world examples we present a generic error propagation solution for c template metaprograms and a technique for building compile time parser generators . all solutions presented in this paper are implemented and available as an open source library . ( c ) <digit> elsevier b.v. all rights reserved .
non uniform data distribution for communication efficient parallel clustering . <eos> global communication requirements and load imbalance of some parallel data mining algorithms are the major obstacles to exploit the computational power of large scale systems . this work investigates how non uniform data distributions can be exploited to remove the global communication requirement and to reduce the communication cost in parallel data mining algorithms and , in particular , in the k means algorithm for cluster analysis . in the straightforward parallel formulation of the k means algorithm , data and computation loads are uniformly distributed over the processing nodes . this approach has excellent load balancing characteristics that may suggest it could scale up to large and extreme scale parallel computing systems . however , at each iteration step the algorithm requires a global reduction operation which hinders the scalability of the approach . this work studies a different parallel formulation of the algorithm where the requirement of global communication is removed , while maintaining the same deterministic nature of the centralised algorithm . the proposed approach exploits a non uniform data distribution which can be either found in real world distributed applications or can be induced by means of multi dimensional binary search trees . the approach can also be extended to accommodate an approximation error which allows a further reduction of the communication costs . the effectiveness of the exact and approximate methods has been tested in a parallel computing system with <digit> processors and in simulations with <digit> processing elements .
cost effective control of air quality and greenhouse gases in europe modeling and policy applications . <eos> environmental policies in europe have successfully eliminated the most visible and immediate harmful effects of air pollution in the last decades . however , there is ample and robust scientific evidence that even at present rates europes emissions to the atmosphere pose a significant threat to human health , ecosystems and the global climate , though in a less visible and immediate way . as many of the low hanging fruits have been harvested by now , further action will place higher demands on economic resources , especially at a time when resources are strained by an economic crisis . in addition , interactions and interdependencies of the various measures could even lead to counter productive outcomes of strategies if they are ignored . integrated assessment models , such as the gains ( greenhouse gas air pollution interactions and synergies ) model , have been developed to identify portfolios of measures that improve air quality and reduce greenhouse gas emissions at least cost . such models bring together scientific knowledge and quality controlled data on future socio economic driving forces of emissions , on the technical and economic features of the available emission control options , on the chemical transformation and dispersion of pollutants in the atmosphere , and the resulting impacts on human health and the environment . the gains model and its predecessor have been used to inform the key negotiations on air pollution control agreements in europe during the last two decades . this paper describes the methodological approach of the gains model and its components . it presents a recent policy analysis that explores the likely future development of emissions and air quality in europe in the absence of further policy measures , and assesses the potential and costs for further environmental improvements . to inform the forthcoming negotiations on the revision of the gothenburg protocol of the convention on long range transboundary air pollution , the paper discusses the implications of alternative formulations of environmental policy targets on a cost effective allocation of further mitigation measures .
costs assessments of european environmental policies . <eos> the evolution of energy production in the european union ( eu ) is going through a big change in recent years the incidence of traditional fuels is diminishing gradually for increasing renewable energy sources ( res ) , due to international concerns over climate change and for energy security reasons . the aim of this paper is to construct a simulation model that identifies and estimates costs that may arise for a community of negotiating countries from opportunistic behavior of some country when defining environmental policies . in this paper , the model is applied specifically to the new <digit> framework for climate and energy policies ( com ( <digit> ) <digit> ) ( ec , <digit> <digit> ) on the promotion of res that commits eu governments to a common goal to increase the share of res in final consumption to <digit> % by <digit> . costs faced by eu countries to achieve the res target are different due to their endowment heterogeneity , the availability of res , the diffusion process of cost improvements and the different instruments to support the development of the res technologies . given the still undefined participation agreement to reach the new overall res target by <digit> , we want to assess the potential cost penalty induced by free riding behavior . this could stem from some eu country , which avoids complying with the res directive . our policy simulation exercise shows that costs increase more than proportionally with the non participating country size , measured with gdp and co2 emissions . furthermore , we provide a model to analytically assess the likelihood each eu country may have to behave opportunistically within the negotiation process of the new proposal on eu res targets ( com ( <digit> ) <digit> ) .
design methodology for battery powered embedded systems in safety critical application . <eos> battery powered embedded system can be considered as a power aware system for a safety critical application . there is a need of saving the battery power for such power aware system so that it can be used more efficiently , particularly in safety critical applications . present paper describes power optimization procedure using real time scheduling technique having a specific dead line guided by the model based optimum current discharge profile of a battery . in any power aware system ' energy optimization ' is one of the major issues for a faithful operation . ( c ) <digit> elsevier b.v. all rights reserved .
reliability measures for two part partition of states for aggregated markov repairable systems . <eos> three models for the aggregated stochastic processes based on an underlying continuous time markov repairable system are developed in which two part partition of states is used . several availability measures such as interval availability , instantaneous availability and steady state availability are presented . some of these availabilities are derived by using laplace transforms , which are more compact and concise . other reliability distributions for these three models are given as well .
an innovative architecture for context foraging . <eos> nomadic computing is a term for describing computing environments where the nodes are mobile and have only ad hoc interactions with each other . evidently , context aware applications are a key ingredient in such environments . however , nomadic nodes may not always have the capability to sense their environment and infer their exact context . hence , applications carried by the nodes will not be able to execute properly . in this paper , we propose an architecture for collaborative exchange of contextual information in an ad hoc setting . this approach is called context foraging and is used for disseminating contextual information based on a publish subscribe scheme . we present the algorithms required for such architecture along with the dynamic event indexing techniques used by the system . the efficiency of the suggested approach is assessed through simulation results . our proposal is investigated and implemented in the context of the ict ipac project .
on the estimation and correction of bias in local atrophy estimations using example atrophy simulations . <eos> brain atrophy is considered an important marker of disease progression in many chronic neuro degenerative diseases such as multiple sclerosis ( ms ) . a great deal of attention is being paid toward developing tools that manipulate magnetic resonance ( mr ) images for obtaining an accurate estimate of atrophy . nevertheless , artifacts in mr images , inaccuracies of intermediate steps and inadequacies of the mathematical model representing the physical brain volume change , make it rather difficult to obtain a precise and unbiased estimate . this work revolves around the nature and magnitude of bias in atrophy estimations as well as a potential way of correcting them . first , we demonstrate that for different atrophy estimation methods , bias estimates exhibit varying relations to the expected atrophy and these bias estimates are of the order of the expected atrophies for standard algorithms , stressing the need for bias correction procedures . next , a framework for estimating uncertainty in longitudinal brain atrophy by means of constructing confidence intervals is developed . errors arising from mri artifacts and bias in estimations are learned from example atrophy simulations and anatomies . results are discussed for three popular non rigid registration approaches with the help of simulated localized brain atrophy in real mr images .
information technologies and intuitive expertise a method for implementing complex organizational change among new york city transit authoritys bus maintainers . <eos> this paper describes an attempt to implement a complex information technology system with the new york city transit authoritys ( nycta ) bus maintainers intended to help better track and coordinate bus maintenance schedules . it implementation is notorious for high failure rates among so called low level workers . we believe that many it implementation efforts make erroneous assumptions about front line workers expertise , which creates tension between the it implementation effort and the cultures of practice among the front line workers . we designed an aggressive learning intervention to address this issue and called operational simulation . rather than requiring the expected <digit> months for implementation , the hourly staff reached independence with the new system in <digit> weeks and line supervisors ( who do more ) managed in <digit> weeks . additionally , the nycta shifted from a reactive to a proactive maintenance approach , reduced cycle times , and increased the mean distance between failure , resulting in a estimated <digit> million cost savings . implications for cognition , expertise , and training are discussed .
mirrored disk organization reliability analysis . <eos> disk mirroring or raid level <digit> ( raid1 ) is a popular paradigm to achieve fault tolerance and a higher disk access bandwidth for read requests . we consider four raid1 organizations basic mirroring , group rotate declustering , interleaved declustering , and chained declustering , where the last three organizations attain a more balanced load than basic mirroring when disk failures occur . we first obtain the number of configurations , a ( n , i ) , which do not result in data loss when i out of n disks have failed . the probability of no data loss in this case is a ( n , i ) n i the reliability of each raid1 organization is the summation over <digit> < i < n <digit> of a ( n , i ) r ( n <digit> ) ( <digit> r ) ( i ) , where r denotes the reliability of each disk . a closed form expression for a ( n , i ) is obtained easily for the first three organizations . we present a relatively simple derivation of the expression for a ( n , i ) for the chained declustering method , which includes a correctness proof . we also discuss the routing of read requests to balance disk loads , especially when there are disk failures , to maximize the attainable throughput .
data processing in the early cosmic ray experiments in sydney . <eos> the cosmic ray air shower experiment set up at the university of sydney in the late 1950s was one of the first complex experiments in australia to utilize the power of an electronic computer to process and analyse the experimental data . the paper provides a brief overview of the design and construction of the equipment for the experiment and the use of the computer silliac in the processing and analysis of the data . the central role of chris wallace in this latter aspect is given special attention .
the impact of metadata in web resources discovering . <eos> purpose to explore the impact of using metadata in finding and ranking web pages through <digit> december <digit> search engines . design methodology approach the study has been divided into two phases . in phase one , the use of metadata schemes and the impact of overlapped documents have been examined by employing the usability technique . phase two examined the impact of adding metadata elements to web pages in their original rank order , using the experimental method . this study focuses on indexing web pages using metadata . and its impact on search engine 's rankings . findings meta tags are more widely used than dublin core . the overlapped pages tend to include metadata . the second phase shows that by adding metadata . elements to web pages , it raises its rank order . however , this depends on the quality of the description and the metadata schemes . the study shows no great difference in page ranking between adding meta tags and dublin core . practical implications to maximize the impact of metadata , more attention should be given to keyword and descriptive fields . originality value the hypothetical relationship between overlapped pages and the inclusion of metadata and indexing by search engines had not been previously examined .
a new density stiffness interpolation scheme for topology optimization of continuum structures . <eos> in this paper , a new density stiffness interpolation scheme for topology optimization of continuum structures is proposed , based on this new scheme , not only the so caged checkerboard pattern can be eliminated from the final optimal topology , but also the boundary smooth effect associated with the traditional sensitivity averaging approach can also be overcome . a proof of the existence of the solution of the optimization problem is also given , therefore mesh independent optimization results can be obtained numerical examples illustrate the effectiveness and the advantage of the proposed interpolation scheme .
pets and their users a critical review of the potentials and limitations of the privacy as confidentiality paradigm . <eos> privacy as confidentiality has been the dominant paradigm in computer science privacy research . privacy enhancing technologies ( pets ) that guarantee confidentiality of personal data or anonymous communication have resulted from such research . the objective of this paper is to show that such pets are indispensable but are short of being the privacy solutions they sometimes claim to be given current day circumstances . using perspectives from surveillance studies we will argue that the computer scientists conception of privacy through data or communication confidentiality is techno centric and displaces end user perspectives and needs in surveillance societies . we will further show that the perspectives from surveillance studies also demand a critical review for their human centric conception of information systems . last , we rethink the position of pets in a surveillance society and argue for the necessity of multiple paradigms for addressing privacy concerns in information systems design .
an approach to automated decomposition of volumetric mesh . <eos> mesh decomposition is critical for analyzing , understanding , editing and reusing of mesh models . although there are many methods for mesh decomposition , most utilize only triangular meshes . in this paper , we present an automated method for decomposing a volumetric mesh into semantic components . our method consists of three parts . first , the outer surface mesh of the volumetric mesh is decomposed into semantic features by applying existing surface mesh segmentation and feature recognition techniques . then , for each recognized feature , its outer boundary lines are identified , and the corresponding splitter element groups are setup accordingly . the inner volumetric elements of the feature are then obtained based on the established splitter element groups . finally , each splitter element group is decomposed into two parts using the graph cut algorithm each group completely belongs to one feature adjacent to the splitter element group . in our graph cut algorithm , the weights of the edges in the dual graph are calculated based on the electric field , which is generated using the vertices of the boundary lines of the features . experiments on both tetrahedral and hexahedral meshes demonstrate the effectiveness of our method .
algorithms for storytelling . <eos> we formulate a new data mining problem called storytelling as a generalization of redescription mining . in traditional redescription mining , we are given a set of objects and a collection of subsets defined over these objects . the goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects . storytelling , on the other hand , aims to explicitly relate object sets that are disjoint ( and , hence , maximally dissimilar ) by finding a chain of ( approximate ) redescriptions between the sets . this problem finds applications in bioinformatics , for instance , where the biologist is trying to relate a set of genes expressed in one experiment to another set , implicated in a different pathway . we outline an efficient storytelling implementation that embeds the cartwheels redescription mining algorithm in an a search procedure , using the former to supply next move operators on search branches to the latter . this approach is practical and effective for mining large data sets and , at the same time , exploits the structure of partitions imposed by the given vocabulary . three application case studies are presented a study of word overlaps in large english dictionaries , exploring connections between gene sets in a bioinformatics data set , and relating publications in the pubmed index of abstracts .
bifurcation study of a neural field competition model with an application to perceptual switching in motion integration . <eos> perceptual multistability is a phenomenon in which alternate interpretations of a fixed stimulus are perceived intermittently . although correlates between activity in specific cortical areas and perception have been found , the complex patterns of activity and the underlying mechanisms that gate multistable perception are little understood . here , we present a neural field competition model in which competing states are represented in a continuous feature space . bifurcation analysis is used to describe the different types of complex spatio temporal dynamics produced by the model in terms of several parameters and for different inputs . the dynamics of the model was then compared to human perception investigated psychophysically during long presentations of an ambiguous , multistable motion pattern known as the barberpole illusion . in order to do this , the model is operated in a parameter range where known physiological response properties are reproduced whilst also working close to bifurcation . the model accounts for characteristic behaviour from the psychophysical experiments in terms of the type of switching observed and changes in the rate of switching with respect to contrast . in this way , the modelling study sheds light on the underlying mechanisms that drive perceptual switching in different contrast regimes . the general approach presented is applicable to a broad range of perceptual competition problems in which spatial interactions play a role .
integration of fuzzy spatial relations in deformable models application to brain mri segmentation . <eos> this paper presents a general framework to integrate a new type of constraints , based on spatial relations , in deformable models . in the proposed approach , spatial relations are represented as fuzzy subsets of the image space and incorporated in the deformable model as a new external force . three methods to construct an external force from a fuzzy set representing a spatial relation are introduced and discussed . this framework is then used to segment brain subcortical structures in magnetic resonance images ( mri ) . a training step is proposed to estimate the main parameters defining the relations . the results demonstrate that the introduction of spatial relations in a deformable model can substantially improve the segmentation of structures with low contrast and ill defined boundaries . ( c ) <digit> pattern recognition society . published by elsevier ltd. all rights reserved .
an efficient animation of wrinkled cloth with approximate implicit integration . <eos> this paper presents an efficient method for creating the animation of flexible objects . the mass spring model was used to represent flexible objects . the easiest approach to creating animation with the mass spring model is the explicit euler method , but the method has a serious weakness in that it suffers from an instability problem . the implicit integration method is a possible solution , but a critical flaw of the implicit method is that it involves a large linear system . this paper presents an approximate implicit method for the mass spring model . the proposed technique updates with stability the state of n mass points in o ( n ) time when the number of total springs is o ( n ) . in order to increase the efficiency of simulation or reduce the numerical errors of the proposed approximate implicit method , the number of mass points must be as small as possible . however , coarse discretization with a small number of mass points generates an unrealistic appearance for a cloth model . by introducing a wrinkled cubic spline curve , we propose a new technique that generates realistic details of the cloth model , even though a small number of mass points are used for simulation .
does computer confidence relate to levels of achievement in ict enriched learning models . <eos> employer expectations have changed university students are expected to graduate with computer competencies appropriate for their field . educators are also harnessing technology as a medium for learning in the belief that information and communication technologies ( icts ) can enliven and motivate learning across a wide range of disciplines . alongside developing students computer skills and introducing them to the use of professional software , educators are also harnessing professional and scientific packages for learning in some disciplines . as the educational use of information and communication technologies increases dramatically , questions arise about the effects on learners . while the use of computers for delivery , support , and communication , is generally easy and unthreatening , higher level use may pose a barrier to learning for those who lack confidence or experience . computer confidence may mediate in how well students perform in learning environments that require interaction with computers . this paper examines the role played by computer confidence ( or computer self efficacy ) in a technology enriched science and engineering mathematics course in an australian university . findings revealed that careful and appropriate use of professional software did indeed enliven learning for the majority of students . however , computer confidence occupied a very different dimension to mathematics confidence and was not a predictor of achievement in the mathematics tasks , not even those requiring use of technology . moreover , despite careful and nurturing support for use of the software , students with low computer confidence levels felt threatened and disadvantaged by computer laboratory tasks . the educational implications of these findings are discussed with regard to teaching and assessment , in particular . the tcat scales used to measure technology attitudes , computer confidence self efficacy and mathematics confidence are included in an appendix . well established , reliable , internally consistent , they may be useful to other researchers . the development of the computer confidence scale is outlined , and guidelines are offered for the design of other discipline specific confidence self efficacy scales appropriate for use alongside the computer confidence scale .
dos protection for udp based protocols . <eos> since ip packet reassembly requires resources , a denial of service attack can be mounted by swamping a receiver with ip fragments . in this paper we argue how this attack need not affect protocols that do not rely on ip fragmentation , and argue how most protocols , e.g. , those that run on top of tcp , can avoid the need for fragmentation . however , protocols such as ipsec 's ike protocol , which both runs on top of udp and requires sending large packets , depend on ip packet reassembly . photuris , an early proposal for ike , introduced the concept of a stateless cookie , intended for dos protection . however , the stateless cookie mechanism can not protect against a dos attack unless the receiver can successfully receive the cookie , which it will not be able to do if reassembly resources are exhausted . thus , without additional design and or implementation defenses , an attacker can successfully , through a fragmentation attack , prevent legitimate ike handshakes from completing . defense against this attack requires both protocol design and implementation defenses . the ikev2 protocol was designed to make it easy to design a defensive implementation . this paper explains the defense strategy designed into the ikev2 protocol , along with the additional needed implementation mechanisms . it also describes and contrasts several other potential strategies that could work for similar udp based protocols .
on the parallel efficiency and scalability of the correntropy coefficient for image analysis . <eos> similarity measures have application in many scenarios of digital image processing . the correntropy is a robust and relatively new similarity measure that recently has been employed in various engineering applications . despite other competitive characteristics , its computational cost is relatively high and may impose hard to cope time restrictions for high dimensional applications , including image analysis and computer vision .
positive solution to a special singular second order boundary value problem . <eos> let lambda be a nonnegative parameter . the existence of a positive solution is studied for a semipositone second order boundary value problem u '' ( t ) lambda q ( t ) f ( t , u ( t ) , u ' ( t ) ) , alpha u ( <digit> ) beta u ' ( <digit> ) d , u ( <digit> ) <digit> , where d > <digit> , alpha > <digit> , beta > <digit> , alpha beta > <digit> , q ( t ) f ( t , u , v ) > <digit> on a suitable subset of <digit> , <digit> x <digit> , infinity ) x ( infinity , infinity ) and f ( t , u , v ) is allowed to be singular at t <digit> , t <digit> and u <digit> . the proofs are based on the leray schauder fixed point theorem and the localization method . ( c ) <digit> published by elsevier ltd .
report of research activities in fuzzy ai and medicine at usfcse . <eos> several projects involving the use of fuzzy and neuro fuzzy methods in medical applications , developed by members of the department of computer science and engineering , university of south florida , tampa , florida , are briefly reviewed . the successful applications are emphasized . ( c ) <digit> elsevier science b.v. all rights reserved .
societally connected multimedia across cultures . <eos> the advance of the internet in the past decade has radically changed the way people communicate and collaborate with each other . physical distance is no more a barrier in online social networks , but cultural differences ( at the individual , community , as well as societal levels ) still govern human human interactions and must be considered and leveraged in the online world . the rapid deployment of high speed internet allows humans to interact using a rich set of multimedia data such as texts , pictures , and videos . this position paper proposes to define a new research area called ' connected multimedia ' , which is the study of a collection of research issues of the super area social media that receive little attention in the literature . by connected multimedia , we mean the study of the social and technical interactions among users , multimedia data , and devices across cultures and explicitly exploiting the cultural differences . we justify why it is necessary to bring attention to this new research area and what benefits of this new research area may bring to the broader scientific research community and the humanity .
multiple object retrieval in image databases using hierarchical segmentation tree . <eos> with the rapid growth of information , efficient and robust information retrieval techniques have become increasingly more important . multiple object retrieval remains challenging due to the complex nature of this problem . the proposed research , unlike most existing works that are designed for single object retrieval or adopt heuristic multiple object matching scheme , aims at contributing to this field through the development of an image retrieval system that adopts a hierarchical region tree representation of image , and enables effective and efficient multiple object retrieval and automatic discovery of the objects of interest through users ' relevance feedback . we believe this is the first systematic attempt to formulate a comprehensive , intelligent , and interactive framework for multiple object retrieval in image databases that makes use of a hierarchical region tree representation .
delay dependent stability analysis for impulsive neural networks with time varying delays . <eos> in this paper , the global exponential stability and global asymptotic stability of the neural networks with impulsive effect and time varying delays is investigated . by using lyapunovkrasovskii type functional , the quality of negative definite matrix and cauchy criterion , we obtain the sufficient conditions for global exponential stability and global asymptotic stability of such model , in terms of linear matrix inequality ( lmi ) , which depend on the delays . two examples are given to illustrate the effectiveness of our theoretical results .
olap over uncertain and imprecise data . <eos> we extend the olap data model to represent data ambiguity , specifically imprecision and uncertainty , and introduce an allocation based approach to the semantics of aggregation queries over such data . we identify three natural query properties and use them to shed light on alternative query semantics . while there is much work on representing and querying ambiguous data , to our knowledge this is the first paper to handle both imprecision and uncertainty in an olap setting .
maximum skew symmetric flows and matchings . <eos> the maximum integer skew symmetric flow problem ( msfp ) generalizes both the maximum flow and maximum matching problems . it was introduced by tutte <digit> in terms of self conjugate flows in antisymmetrical digraphs . he showed that for these objects there are natural analogs of classical theoretical results on usual network flows , such as the flow decomposition , augmenting path , and max flow min cut theorems . we give unified and shorter proofs for those theoretical results . we then extend to msfp the shortest augmenting path method of edmonds and karp <digit> and the blocking flow method of dinits <digit> , obtaining algorithms with similar time bounds in general case . moreover , in the cases of unit arc capacities and unit node capacities our blocking skew symmetric flow algorithm has time bounds similar to those established in <digit> , <digit> for dinits ' algorithm . in particular , this implies an algorithm for finding a maximum matching in a nonbipartite graph in o ( rootnm ) time , which matches the time bound for the algorithm of micali and vazirani <digit> . finally , extending a clique compression technique of feder and motwani <digit> to particular skew symmetric graphs , we speed up the implied maximum matching algorithm to run in o ( rootnm log ( n ( <digit> ) m ) log n ) time , improving the best known bound for dense nonbipartite graphs . also other theoretical and algorithmic results on skew symmetric flows and their applications are presented .
efficient performance estimate for one class support vector machine . <eos> this letter proposes and analyzes a method ( estimate ) to estimate the generalization performance of one class support vector machine ( svm ) for novelty detection . the method is an extended version of the estimate method , which is used to estimate the generalization performance of standard svm for classification . our method is derived from analyzing the connection between one class svm and standard svm . without any computation intensive re sampling , the method is computationally much more efficient than leave one out method , since it can be computed immediately from the decision function of one class svm . using our method to estimate the error rate is more precise than using the fraction of support vectors and a parameter of one class svm . we also propose that the fraction of support vectors characterizes the precision of one class svm . a theoretical analysis and experiments on an artificial data and a widely known handwritten digit recognition set ( mnist ) show that our method can effectively estimate the generalization performance of one class svm for novelty detection .
comparison of recent methods for inference of variable influence in neural networks . <eos> neural networks ( nns ) belong to black box models and therefore suffer from interpretation difficulties . four recent methods inferring variable influence in nns are compared in this paper . the methods assist the interpretation task during different phases of the modeling procedure . they belong to information theory ( itss ) , the bayesian framework ( ard ) , the analysis of the network 's weights ( gim ) , and the sequential omission of the variables ( szw ) . the comparison is based upon artificial and real data sets of differing size , complexity and noise level . the influence of the neural network 's size has also been considered . the results provide useful information about the agreement between the methods under different conditions . generally , szw and gim differ from ard regarding the variable influence , although applied to nns with similar modeling accuracy , even when larger data sets sizes are used . itss produces similar results to szw and gim , although suffering more from the curse of dimensionality .
finite element modelling of reinforced concrete framed structures including catenary action . <eos> in this paper , a 1d discrete element is formulated for analysis of reinforced concrete frames with catenary action . a force based formulation is developed based on the total secant stiffness approach and an associated direct iterative solution scheme is derived . the effect of material nonlinearity as well as softening of concrete under compression is taken into account and a nonlocal averaging technique is employed to maintain the objectivity of displacement and force responses . concerning geometrical nonlinearities , the fibre strains are assumed to be small however , the effect of transverse displacement on the axial strain is large and it is taken into account as well as the effect of shear on the axial force . using a simpson integration scheme , together with a piecewise interpolation of curvature , the deformed shape of the element is consistently updated . the formulation is verified with numerical examples .
fostering a creative interest in computer science . <eos> in this paper , we describe activities undertaken at our university to revise our computer science program to develop an environment and curriculum which encourages creative , hands on learning by our students . our main changes were the development of laboratory space , increased hands on problem solving activities in the introductory course , open ended programming projects in the early courses including a requirement of an open ended project extension for an a grade , and the integration of a seminar into the senior project requirement . our results suggest that these changes have improved student skill and willingness to deal with new problems and technologies . an additional surprising side effect appears to be a dramatic increase in retention over the first two years , despite lower overall grade averages in those courses .
concave piecewise linear service curves and deadline calculations . <eos> the internet is gradually and constantly becoming a multimedia network that needs mechanisms to provide effective quality of service ( qos ) requirements to users . the service curve ( sc ) is an efficient description of qos and the service curve based earliest deadline first policy ( sced ) is a scheduling algorithm to guarantee scs specified by users . in sced , deadline calculation is the core . however , not every sc has a treatable deadline calculation currently the only known treatable sc is the concave piecewise linear sc ( cplsc ) . in this paper , we propose an algorithm to translate all kinds of scs into cplscs . in this way , the whole internet can have improved performance . moreover , a modification of the deadline calculation of the original sced is developed to obtain neat and precise results . the results combining with our proposed algorithm can make the deadline calculation smooth and the multimedia internet possible .
optimal retrial and timeout strategies for accessing network resources . <eos> the notion of timeout ( namely , the maximal time to wait before retrying an action ) turns up in many networking contexts , such as packet transmission , connection establishment , etc. usage of timeouts is encountered especially in large scale networks , where negative acknowledgments ( nacks ) on failures have significantly higher delays than positive acknowledgments ( acks ) and frequently are not employed at all . selection of a proper timeout involves a tradeoff between waiting too long and loading the network needlessly by waiting too little . the common approach is to set the timeout to a large value , such that , unless the action fails , it is acknowledged within the timeout duration with a high probability . this approach is conservative and leads to overly long , far from optimal , timeouts . we take a quantitative approach with the purpose of computing and studying the optimal timeout strategy . the above tradeoff is modeled by introducing a cost per unit time ( until success ) and a cost per repeated attempt . the optimal timeout strategy is then defined as one that a selfish user would follow to minimize its expected cost . we discuss the various practical interpretations that these costs may have . we then derive the formulas for the optimal timeout values and study some of their fundamental properties . in particular , we identify the conditions for making parallel attempts from the outset to be worthwhile . in addition , we demonstrate a striking property of positive feedback . this motivates us to study the interaction resulting when many users selfishly apply the optimal timeout strategy specifically , we use a noncooperative game model and show that it suffers from an inherent instability problem . some implications of these results on network design are discussed .
integrated modeling and analysis of dynamics for electric vehicle powertrains . <eos> this paper builds theoretical models for the entire powertrain of evs to describe ev dynamics with both mechanical and electrical systems . a matlab model of an ev is developed to verify the derived theoretical models for the entire powertrain of evs . a variety of final vehicle driving performances are analyzed and predicted as a function of electrical quantities .
wavelet synopses for general error metrics . <eos> several studies have demonstrated the effectiveness of the wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast , accurate approximate query answers . conventional wavelet synopses that greedily minimize the overall root mean squared ( i.e. , l <digit> norm ) error in the data approximation can suffer from important problems , including severe bias and wide variance in the quality of the data reconstruction , and lack of nontrivial guarantees for individual approximate answers . thus , probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control maximum approximation error metrics ( e.g. , maximum relative error ) . a key open problem is whether it is possible to design efficient deterministic wavelet thresholding algorithms for minimizing general , non l <digit> error metrics , that are relevant to approximate query processing systems , such as maximum relative or maximum absolute error . obviously , such algorithms can guarantee better maximum error wavelet synopses and avoid the pitfalls of probabilistic techniques ( e.g. , bad coin flip sequences ) leading to poor solutions in addition , they can be used to directly optimize the synopsis construction process for other useful error metrics , such as the mean relative error in data value reconstruction . in this article , we propose novel , computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing general approximation error metrics . we first consider the problem of constructing wavelet synopses optimized for maximum error , and introduce an optimal low polynomial time algorithm for one dimensional wavelet thresholding our algorithm is based on a new dynamic programming ( dp ) formulation , and can be employed to minimize the maximum relative or absolute error in the data reconstruction . unfortunately , directly extending our one dimensional dp algorithm to multidimensional wavelets results in a super exponential increase in time complexity with the data dimensionality . thus , we also introduce novel , polynomial time approximation schemes ( with tunable approximation guarantees ) for deterministic wavelet thresholding in multiple dimensions . we then demonstrate how our optimal and approximate thresholding algorithms for maximum error can be extended to handle a broad , natural class of distributive error metrics , which includes several important error measures , such as mean weighted relative error and weighted l p norm error . experimental results on real world and synthetic data sets evaluate our novel optimization algorithms , and demonstrate their effectiveness against earlier wavelet thresholding schemes .
on the correlation between children 's performances on electronic board tasks and nonverbal intelligence test measures . <eos> in this study it was investigated whether a tangible electronic console ( tagtiles ) can be used in principle to address a range of cognitive skills by examining the underlying basic psychometric properties of tagtiles tasks . this is a precursor to an intervention study on the impact of tagtiles on cognitive development or an instrument development study . the tasks implemented on the console consisted of abstract visual patterns , which were intended to target perception , spatial knowledge representation , eye hand coordination , reasoning and problem solving . the results of a pilot study ( n <digit> , children aged <digit> <digit> ) and an experiment ( n <digit> , children aged <digit> <digit> ) are presented . correlations between scores on tagtiles tasks on the one hand and a selection of wisc iiinl performance subtests , raven 's progressive matrices and rakit 's memory span on the other hand , were calculated . the results indicate that the tagtiles tasks cover similar skills as the applied wisc iiinl subtests , demonstrated by the moderate to large correlations between performance scores on sets of tagtiles tasks and sets of wisc iiinl tasks . the combined tagtiles task scores were also significantly correlated with the aggregated wisc iiinl subtest scores . significant correlations were found between the tagtiles tasks and the raven test scores , though for the rakit memory span no significant correlation with tagtiles tasks was found . after further refinement and validation , in particular with a larger sample size , the tasks can be applied to provide an indication of children 's skill levels , offering the benefits of a self motivating testing method to children , and avoiding inconsistencies in administration . as such , the tasks may become an effective tool for the training and assessment of nonverbal skills for children . ( c ) <digit> elsevier ltd. all rights reserved .
ontology design patterns for the semantic business processes . <eos> this paper discusses two research paradigms the first one is based on using the meta object facility ( mof ) to support any kind of metadata , whereas the second one emphasizes the role of the ontology design patterns ( odps ) to support knowledge transformation between the source and the target models . more precisely , in this paper we represent the business process odp , which reflects both the syntax and semantics of business process specification and brings an abstract solution to the typical problem of designing and modelling semantic business processes .
rematerialization based register allocation through reverse computing . <eos> reversible computing aims at keeping all information on input and intermediate values available at any step of the computation . rematerialization in register allocation is an alternate solution to spilling where values are recomputed from available data instead of held in registers . in this paper we present the basic ideas of our algorithm for rematerialization with reverse computing . we use the memory demanding lqcd ( lattice quantum chromodynamics ) application to demonstrate that important gains of up to <digit> % on register pressure can be obtained . this in turn enables an increase in instruction level parallelism or in thread level parallelism . we demonstrate a 16.8 % ( statically timed ) gain over a basic lqcd computation .
a modified fieller interval for the interval estimation of effective doses for a logistic dose response curve . <eos> interval estimation of the gamma % effective dose ( p , , say ) is often based on the asymptotic variance of the maximum likelihood estimator ( delta interval ) or fieller 's theorem ( fieller interval ) . sitter and wu ( j. amer . statist . assoc. <digit> ( <digit> ) <digit> ) compared the delta and fieller intervals for the median effective dose ( <digit> ) assuming a logistic dose response curve . their results indicated that although fieller intervals are generally superior to delta intervals , they appear to be conservative . here an adjusted form of the fieller interval for mu ( gamma ) termed an adjusted fieller ( af ) interval is introduced . a comparison of the af interval with the delta and fieller intervals is provided and the properties of these three interval estimation methods are investigated . ( c ) <digit> elsevier science b.v. all rights reserved .
constructing g ( <digit> ) bezier surfaces over a boundary curve network with t junctions . <eos> at junction occurs in a boundary curve network when one boundary curve ends in the middle of another . we show how to construct g ( <digit> ) bezier surfaces over a boundary curve network with t junctions . by treating the two micro patches which meet at the edge forming the upright of the t as a single macro patch , we reduce the problem to one of achieving continuity between this composite patch and the third patch which has the crossbar of the t as an edge . thus we avoid changes to the boundary network , or to any patches except those that meet at the t junction . also , we analyze the singularity of the g ( <digit> ) continuity system with the t junction , and give the constraint to make a consistent system using free variables of weight functions . this is the first method of surfacing the t junction . we present examples and verify continuity by drawing reflection lines and checking angles . ( c ) <digit> elsevier ltd. all rights reserved .
optimal input output reduction in production processes . <eos> while conventional data envelopment analysis ( dea ) models set targets for each operational unit , this paper considers the problem of input output reduction in a centralized decision making environment . the purpose of this paper is to develop an approach to input output reduction problem that typically occurs in organizations with a centralized decision making environment . this paper shows that dea can make an important contribution to this problem and discusses how dea based model can be used to determine an optimal input output reduction plan . an application in banking sector with limitation in it investment shows the usefulness of the proposed method .
ambiguous grammars and the chemical transactions of life part ii the hierarchy of life 's grammars . <eos> purpose this second part of a companion paper seeks to extend the theory proposed to apply the hierarchy of fuzzy formal language to cope with the three major phenomenon of life replication , control and shuffling of genetic information . design methodology approach in order to cope with the proposal , three new classes of ffg are proposed replicating grammars to formalize proper ties and consequences of dna duplication self controlled grammars to provide the tools to control the grammar ambiguity and to improve adaptability , and recombinant grammars to formalize properties and consequences of the sexual reproduction to life evolution . considering all these facts , ffg are proposed as the key instrument to formalize the basic properties of the chemical transactions supporting life . findings the formalism of the model provides a new way to analyze and interpret the findings of the different genome sequencing projects . originality value the theoretical framework developed here provides a new perspective of understanding the code of life and evolution .
a two pass rate control algorithm for h. <digit> avc high definition video coding . <eos> in this paper , we propose a novel two pass rate control algorithm to achieve constant quality for h. <digit> avc high definition video coding . with the first pass collected rate and distortion information and the built model of scene complexity , the encoder can determine the expected distortion which could be achieved in the second pass encoding under the target bit rate . according to the built linear distortion quantizer ( d q ) model , before encoding one frame , the quantization parameter can be solved to realize constant quality encoding . after encoding one frame , the model parameters will be updated with linear regression method to ensure the prediction accuracy of the quantization parameter of next encoded frame with the same coding type . in order to obtain the expected distortion of each frame under the target bit rate , a gop level bit allocation scheme is also designed to adjust the target bit rate of each gop based on the scene complexity of the gop in the second pass encoding . in addition , the effect of scene change on the updating of d q model is considered . the model will be re initialized at the scene change to minimize modeling error . the experimental results show that compared with the latest two pass rate control algorithm , our proposed algorithm can significantly improve the bit control accuracy at comparable coding performance in terms of constant quality and average psnr . on average , the improvement of bit control accuracy achieved about <digit> % .
verification and validation of a work domain analysis with turing machine task analysis . <eos> work domain models produced by work domain analysis need to be validated and verified . a method based on the turing machine formalism was proposed . an application to two domains allowed us to highlight some required changes . over or underspecification , omission or false inclusion of objects were noticed .
modelling and simulation of photosynthetic microorganism growth random walk vs. finite difference method . <eos> the paper deals with photosynthetic microorganism growth modelling and simulation in a distributed parameter system . main result concerns the development and comparison of two modeling frameworks for photo bioreactor modelling . the first classical approach is based on pde ( reaction turbulent diffusion system ) and finite difference method . the alternative approach is based on random walk model of transport by turbulent diffusion . the complications residing in modelling of multi scale transport and reaction phenomena in microalgae are clarified and the solution is chosen . it consists on phenomenological state description of microbial culture by the lumped parameter model of photosynthetic factory ( psf model ) in the re parametrized form , published recently in this journal by papacek , et al. ( <digit> ) . obviously both approaches lead to the same simulation results , nevertheless they provide different advantages . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
recent advances in natural language processing for biomedical applications . <eos> we survey a set a recent advances in natural language processing applied to biomedical applications , which were presented in geneva , switzerland , in <digit> at an international workshop . while text mining applied to molecular biology and biomedical literature can report several interesting achievements , we observe that studies applied to clinical contents are still rare . in general , we argue that clinical corpora , including electronic patient records , must be made available to fill the gap between bioinformatics and medical informatics .
a new probabilistic approach for distribution network reconfiguration applicability to real networks . <eos> power loss reduction can be considered as one of the main purposes for distribution system operators , especially for recent non governmental networks . reconfiguration is an operation process widely used for this optimization by means of changing the status of switches in a distribution network . some major points such as time varying loads and the number of switchings , which are often neglected or not applied simultaneously in most previous studies , are the main motivation behind this study . in this paper , a new probabilistic approach is proposed to perform an optimal reconfiguration in order to reduce the total cost of operation , including the cost of switching and benefit of loss reduction . considering time varying loads , the proposed method can obtain an optimal balance between the number of switchings and the power loss . the effectiveness of the suggested method is demonstrated through several experiments and the results are compared with those of other reliable methods in several cases . ( c ) <digit> elsevier ltd. all rights reserved .
a feasibility study of the classification of alpaca ( lama pacos ) wool samples from different ages , sex and color by means of visible and near infrared reflectance spectroscopy . <eos> the usefulness of classifying the alpaca wool samples according to their color , sex and location is associated with their economic value in the market , hence adequate methods for rapid classification are needed to assess the of wool value . this study evaluated the potential of the visible and near infrared ( visnir ) spectroscopy combined with multivariate statistical analysis to classify alpaca ( lama pacos ) fiber samples according to age ( <digit> and <digit> year old ) , sex ( male and female ) and color ( black , brown , lf and white ) . samples ( n <digit> ) were scanned in reflectance mode in the wavelength range of 4002500nm using a monochromator instrument ( foss nirsystems6500 , inc. , silver spring , md , usa ) . principal component analysis ( pca ) and partial least squares discriminant analysis ( pls da ) were used to classify fiber samples . cross validation was used for validation of classification models developed . results showed that pls da correctly classified <digit> % of fiber samples into ages , intermediate classification rates were obtained for color , while lower classification rates were obtained for the discrimination of wool samples according to sex . the results from this study suggested that visnir spectroscopy in combination with multivariate data analysis can be used as a rapid method to classify alpaca fiber samples according to age , sex and color .
the dissipative structure of variational multiscale methods for incompressible flows . <eos> in this paper , we present a precise definition of the numerical dissipation for the orthogonal projection version of the variational multiscale method for incompressible flows . we show that , only if the space of subscales is taken orthogonal to the finite element space , this definition is physically reasonable as the coarse and fine scales are properly separated . then we compare the diffusion introduced by the numerical discretization of the problem with the diffusion introduced by a large eddy simulation model . results for the flow around a surface mounted obstacle problem show that numerical dissipation is of the same order as the subgrid dissipation introduced by the smagorinsky model . finally , when transient subscales are considered , the model is able to predict backscatter , something that is only possible when dynamic les closures are used . numerical evidence supporting this point is also presented .
an integer linear programming based approach for parallelizing applications in on chip multiprocessors . <eos> with energy consumption becoming one of the first class optimization parameters in computer system design , compilation techniques that consider performance and energy simultaneously are expected to play a central role . in particular , compiling a given application code under performance and energy constraints is becoming an important problem . in this paper , we focus on an on chip multiprocessor architecture and present a parallelization strategy based on integer linear programming . given an array intensive application , our optimization strategy determines the number of processors to be used in executing each nest based on the objective function and additional compilation constraints provided by the user . our initial experience with this strategy shows that it is very successful in optimizing array intensive applications on on chip multiprocessors under energy and performance constraints .
sensorless direct torque and flux controlled ipm synchronous machine fed by matrix converter over a wide speed range . <eos> this paper proposes a new sensorless direct torque and flux controlled interior permanent magnet synchronous machine drive fed by a matrix converter . closed loop control of both torque and stator flux is achieved by using two pi controllers . the input and output voltage vectors are modulated with the indirect space vector modulation technique . additionally , unity power factor on the power supply side of matrix converter is achieved through closed loop compensation of the input displacement angle created by the input filter of matrix converter . the adaptive observer used for joint stator flux and rotor speed estimation is enhanced by hf signal injection scheme for stable operation at low speed including standstill . the stator resistance variation is compensated with the current estimation error . the operating range of the drive is extended into high speed region by incorporating field weakening . the sensorless drive exhibits high dynamic and steady state performances over a wide speed range . the implementation of digital control system for the proposed matrix converter drive is described in this paper . extensive experimental results confirming the effectiveness of the proposed method are also included .
integrated framework for assessing urban water supply security of systems with non traditional sources under climate change . <eos> an integrated framework for planning an urban water supply system is proposed . challenges of including non traditional sources under climate change are documented . desalination , stormwater and rainwater are augmentation options for a case study . rainwater is an expensive augmentation option for little supply security gain . reducing per capita consumption may increase robustness to future uncertainties .
an exploratory project expert system for eliciting correlation coefficient and sequential updating of duration estimation . <eos> this study proposes a framework for updating estimation of project duration in project networks . the first step of building a project expert system is to elicit the correlation coefficient of activity durations from experts ' knowledge and intuition . given the correlation coefficients elicited , the linear bayesian approach is used to update the distribution of activity duration . in particular , by reflecting the newly observed duration of completed activities , we can update the duration of upcoming activities repeatedly throughout the entire project period . this helps keep track of the constantly changing longest duration path within the networks . finally , it is shown that all these learning and updating schemes can be relatively easily implemented on an excel spreadsheet , so that field managers can apply the model into real projects .
structural effects of biologically relevant rhodamines on spectroscopy of fluorescence fluctuations . <eos> exciton coupling in complexes between the indole ring and other systems is known to enhance the efficiency of energy and electron transfer . rhodamines ' xanthylium rings allow the formation of weakly or nonfluorescent complexes with the amino acid tryptophan . thus , because of the short distance of the participating electronic clouds , intrinsic electron transferinduced fluorescence quenching occurs . in solution , the rate constant of electron transfer is known to be limited by collision interactions at the contact distance . by contrast , in protein local environments tryptophan residues can be either exposed or buried in hydrophobic regions . herein , i report on the properties of aromatic derivatized rhodamines , among which is one with a bound phenylalanine amino acid group . encompassed is the spectroscopic and kinetic information in bulk and at the single molecule levels both in free solution and in the presence of human serum albumin . spectroscopic characteristics are focused with special emphasis on enhanced fluorescence that is addressed considering optimized geometries and electronic spectra . the importance of the probes associated with peptides and metal ions both in condensed phase or interfaces and as substrates with proteins is put into perspective .
a novel approach for bit serial ab ( <digit> ) multiplication in finite fields gf ( <digit> ( m ) ) . <eos> this paper presents a new inner product ab ( <digit> ) multiplication algorithm and effective hardware architecture for exponentiation in finite fields cf ( <digit> ( m ) ) . exponentiation is more efficiently implemented by applying ab ( <digit> ) multiplication repeatedly rather than ab multiplication . thus , efficient ab ( <digit> ) multiplication algorithms and simple architectures are the key to implementing exponentiation . accordingly , this paper proposes an efficient inner product multiplication algorithm based on an irreducible all one polynomial ( aop ) and simple architecture , which has the same hardware equipment as fenn 's ab multiplier . the proposed bit serial multiplication algorithm and architecture are highly regular and simpler than those of previous works . ( c ) <digit> elsevier ltd. all rights reserved .
policy teaching through reward function learning . <eos> policy teaching considers a markov decision process setting in which an interested party aims to influence an agent 's decisions by providing limited incentives . in this paper , we consider the specific objective of inducing a pre specified desired policy . we examine both the case in which the agent 's reward function is known and unknown to the interested party , presenting a linear program for the former case and formulating an active , indirect elicitation method for the latter . we provide conditions for logarithmic convergence , and present a polynomial time algorithm that ensures logarithmic convergence with arbitrarily high probability . we also offer practical elicitation heuristics that can be formulated as linear programs , and demonstrate their effectiveness on a policy teaching problem in a simulated ad network setting . we extend our methods to handle partial observations and partial target policies , and provide a game theoretic interpretation of our methods for handling strategic agents .
turning telecommunications call details to churn prediction a data mining approach . <eos> as deregulation , new technologies , and new competitors open up the mobile telecommunications industry , churn prediction and management has become of great concern to mobile service providers . a mobile service provider wishing to retain its subscribers needs to be able to predict which of them may be at risk of changing services and will make those subscribers the focus of customer retention efforts . in response to the limitations of existing churn prediction systems and the unavailability of customer demographics in the mobile telecommunications provider investigated , we propose , design , and experimentally evaluate a churn prediction technique that predicts churning from subscriber contractual information and call pattern changes extracted from call details . this proposed technique is capable of identifying potential churners at the contract level for a specific prediction time period . in addition , the proposed technique incorporates the multi classifier class combiner approach to address the challenge of a highly skewed class distribution between churners and non churners . the empirical evaluation results suggest that the proposed call behavior based churn prediction technique exhibits satisfactory predictive effectiveness when more recent call details are employed for the churn prediction model construction . furthermore , the proposed technique is able to demonstrate satisfactory or reasonable predictive power within the one month interval between model construction and churn prediction . using a previous demographics based churn prediction system as a reference , the lift factors attained by our proposed technique appear largely satisfactory .
design and simulation of manufacturing systems facing imperfectly defined information . <eos> due to the constant evolution of the environment and to the complexity of the needs , the specifications of a manufacturing system are often imperfectly known . the initial design data are uncertain , inaccurate and even vague . we propose to represent the quantifiable needs using fuzzy quantities . the data are propagated during the activity of engineering to lead to the parameters of the target system . in this context , simulation techniques , based on fuzzy parameters , are used to verify the exactness of the design . we choose to use a commercial discrete event simulator and response surface methodology to perform fuzzy simulation .
the skeptical explorer a multiple hypothesis approach to visual modeling and exploration . <eos> the primary intent of this work is to present a method for sequentially associating three dimensional surface measurements acquired by an autonomous exploration agent with models that describe those surfaces . traditional multiple viewpoint registration approaches are concerned only with finding the transformation that maps data points to a chosen global frame . given a parts based object representation , and assuming that the view correspondence can be found , the problem of associating the registered data with the correct part models still needs to be solved . while traditional approaches are content to group segmented data sets that geometrically overlap one another with the same part , there are cases where this causes ambiguous situations . this paper addresses the model data association problem as it applies to three dimensional dynamic object modeling . by tracking the state of part models across subsequent views , we wish to identify possible events that explain model data association ambiguities and represent them in a bayesian framework . the model data association problem is therefore relaxed to allow multiple interpretations of the object 's structure , each being assigned a probability . rather than making a decision at every iteration about an ambiguous mapping , we look to the future for the information needed to disambiguate it . experimental results are presented to illustrate the effectiveness of the approach .
mobility and stability evaluation in wireless multi hop networks using multi player games . <eos> multi hop networks have gained a lot of interest in recent years . a lot of work was contributed in the field of protocol design and performance of multi hop networks . it is generally accepted that mobility has a huge impact on the protocol performance even more for multi hop networks . obtaining realistic measurements of mobility , however , is complex and expensive . thus , we adopt virtual world scenarios to explore the mobility issue , by using the well known multi player game , quake ii . the advantage of the quake ii engine is that users move within virtual worlds under realistic constraints , whereas other mobility models may offer insufficient accuracy or operate under unrealistic assumptions . moreover , it is very easy to create new virtual worlds and to adapt them to specialized needs . in this paper , we propose an analytical framework for mobility measurements in virtual worlds that could be adopted for the design of communication protocols . our framework enables the study of the impact of mobility on connectivity and stability of the network , giving useful insights for improving communication performance . an interesting application of our approach is the analysis of coverage extension of so called hotspots or emergency situations , where the fixed network infrastructure is insufficient or non existent . in these extreme cases , multi hop networks can be used to setup communication quickly . as these situations comprise a plethora of different cases and scenarios , our model is appropriate for their analysis , due to its generality . we use our framework to investigate the performance of multi hop networks based on ieee 802.11 a technology . in contrast to other contributions focusing only on connectivity , the ieee 802.11 a technology also considers multi rate connections . our framework covers the evaluation of simple connectivity as well as link quality stability in the presence of mobility , a combination that has not been considered thus far . therefore we introduce two simple routing schemes and highlight the performance of these protocols in presence of mobility . furthermore we come up with four definitions of stability and investigate protocols for multi hop networks in terms of this parameter . our other contributions are the changes to the quake ii engine and the availability of mobility trace files .
efficient method of achieving agreements between individuals and organizations about rfid privacy . <eos> this work presents novel technical and legal approaches that address privacy concerns for personal data in rfid systems . in recent years , to minimize the conflict between convenience and the privacy risk of rfid systems , organizations have been requested to disclose their policies regarding rfid activities , obtain customer consent , and adopt appropriate mechanisms to enforce these policies . however , current research on rfid typically focuses on enforcement mechanisms to protect personal data stored in rfid tags and prevent organizations from tracking user activity through information emitted by specific rfid tags . a missing piece is how organizations can obtain customers ' consent efficiently and flexibly . this study recommends that organizations obtain licenses automatically or semi automatically before collecting personal data via rfid technologies rather than deal with written consents . such digitalized and standard licenses can be checked automatically to ensure that collection and use of personal data is based on user consent . while individuals can easily control who has licenses and license content , the proposed framework provides an efficient and flexible way to overcome the deficiencies in current privacy protection technologies for rfid systems .
people are doing it for themselves . <eos> to date , the objective of creating pleasurable products has concentrated on designers articulating and interpreting user needs as part of the product creation process . this paper explores approaches to enable users to adapt , modify , specify or create products to match their needs directly . using the potential of new technologies , active consumers can now become product creators , paralleling developments in graphics , music and digital media production . empowered users , self builders , recreational manufacturers , web connected silver surfers ( retired individuals using the web ) and punk manufacturers <digit> all exemplify this new relationship between users and products , and the evolving role of designers <digit> .
low cost networks and gateways for teaching data communications . <eos> the growing importance of communications in computer science has resulted in many undergraduate computer science programmes offering courses in data communications . although data communications courses can be taught in a practical manner , the cost of data communications hardware often restricts the amount of actual hands on experience that students can gain . in this paper we describe the hardware and software requirements of several low cost networks that can be used by students to gain experience in a wide variety of data communication topics including local area networks ( such as bus networks and ring networks ) , wide area networks ( i.e. store and forward networks ) , and gateways .
on the application of genetic programming for software engineering predictive modeling a systematic review . <eos> the objective of this paper is to investigate the evidence for symbolic regression using genetic programming ( gp ) being an effective method for prediction and estimation in software engineering , when compared with regression machine learning models and other comparison groups ( including comparisons with different improvements over the standard gp algorithm ) . we performed a systematic review of literature that compared genetic programming models with comparative techniques based on different independent project variables . a total of <digit> primary studies were obtained after searching different information sources in the time span <digit> <digit> . the results of the review show that symbolic regression using genetic programming has been applied in three domains within software engineering predictive modeling ( i ) software quality classification ( eight primary studies ) . ( ii ) software cost effort size estimation ( seven primary studies ) . ( iii ) software fault prediction software reliability growth modeling ( eight primary studies ) . while there is evidence in support of using genetic programming for software quality classification , software fault prediction and software reliability growth modeling the results are inconclusive for software cost effort size estimation . ( c ) <digit> elsevier ltd. all rights reserved .
a unified framework for dynamic pari mutuel information market design . <eos> recently , coinciding with and perhaps driving the increased popularity of prediction markets , several novel pari mutuel mechanisms have been developed such as the logarithmic market scoring rule ( lmsr ) , the cost function formulation of market makers , and the sequential convex parimutuel mechanism ( scpm ) . in this work , we present a unified convex optimization framework which connects these seemingly unrelated models for centrally organizing contingent claims markets . the existing mechanisms can be expressed in our unified framework using classic utility functions . we also show that this framework is equivalent to a convex risk minimization model for the market maker . this facilitates a better understanding of the risk attitudes adopted by various mechanisms . the utility framework also leads to easy implementation since we can now find the useful cost function of a market maker in polynomial time through the solution of a simple convex optimization problem . in addition to unifying and explaining the existing mechanisms , we use the generalized framework to derive necessary and sufficient conditions for many desirable properties of a prediction market mechanism such as proper scoring , truthful bidding ( in a myopic sense ) , efficient computation , controllable risk measure , and guarantees on the worst case loss . as a result , we develop the first proper , truthful , risk controlled , loss bounded ( in number of states ) mechanism none of the previously proposed mechanisms possessed all these properties simultaneously . thus , our work could provide an effective tool for designing new market mechanisms .
interaction design for supporting communication between chinese sojourners . <eos> in our global village , distance is not a barrier anymore for traveling . people experience new cultures and face accompanying difficulties in order to live anywhere . social support can help these sojourners to cope with difficulties , such as culture shock . in this paper , we investigate how computer mediated communication ( cmc ) tools can facilitate social support when living physically separated from loved ones in different cultures . the goal is to understand the design considerations necessary to design new cmc tools . we studied communication practices of chinese sojourners living in the netherlands and the use of a technology probe with a novel video communication system . these results led to recommendations which can help designers to design interactive communication tools that facilitate communication across cultures . we conclude the paper with an interactive communication device called circadian , which was designed based on these recommendations . we experienced the design recommendations to be abstract enough to leave space for creativity while providing a set of clear requirements which we used to base design decisions upon .
backward penalty schemes for monotone inclusion problems . <eos> in this paper , we are concerned with solving monotone inclusion problems expressed by the sum of a set valued maximally monotone operator with a single valued maximally monotone one and the normal cone to the nonempty set of zeros of another set valued maximally monotone operator . depending on the nature of the single valued operator , we propose two iterative penalty schemes , both addressing the set valued operators via backward steps . the single valued operator is evaluated via a single forward step if it is cocoercive , and via two forward steps if it is monotone and lipschitz continuous . the latter situation represents the starting point for dealing with complexly structured monotone inclusion problems from algorithmic point of view .
3d video and free viewpoint videofrom capture to display . <eos> this paper gives an end to end overview of 3d video and free viewpoint video , which can be regarded as advanced functionalities that expand the capabilities of a 2d video . free viewpoint video can be understood as the functionality to freely navigate within real world visual scenes , as it is known for instance from virtual worlds in computer graphics . 3d video shall be understood as the functionality that provides the user with a 3d depth impression of the observed scene , which is also known as stereo video . in that sense as functionalities , 3d video and free viewpoint video are not mutually exclusive but can very well be combined in a single system . research in this area combines computer graphics , computer vision and visual communications . it spans the whole media processing chain from capture to display and the design of systems has to take all parts into account , which is outlined in different sections of this paper giving an end to end view and mapping of this broad area . the conclusion is that the necessary technology including standard media formats for 3d video and free viewpoint video is available or will be available in the future , and that there is a clear demand from industry and user for such advanced types of visual media . as a consequence we are witnessing these days how such technology enters our everyday life
e a brainiac theorem prover . <eos> we describe the superposition based theorem prover e. e is a sound and complete prover for clausal first order logic with equality . important properties of the prover include strong redundancy elimination criteria , the discount loop proof procedure , a very flexible interface for specifying search control heuristics , and an efficient inference engine . we also discuss strength and weaknesses of the system .
the trefftz method for solving eigenvalue problems . <eos> for laplace 's eigenvalue problems , this paper presents new algorithms of the trefftz method ( i.e. the boundary approximation method ) , which solve the helmholtz equation and then use an iteration process to yield approximate eigenvalues and eigenfunctions . the new iterative method has superlinear convergence rates and gives a better performance in numerical testing , compared with the other popular methods of rootfinding . moreover , piecewise particular solutions are used for a basic model of eigenvalue problems on the unit square with the dirichlet condition . numerical experiments are also conducted for the eigenvalue problems with singularities . our new algorithms using piecewise particular solutions are well suited to seek very accurate solutions of eigenvalue problems , in particular those with multiple singularities , interfaces and those on unbounded domains . using piecewise particular solutions has also the advantage to solve complicated problems because uniform particular solutions may not always exist for the entire solution domain .
exertion interfaces for computer videogames using smartphones as input controllers . <eos> as mobile phones become smarter and include a wider and more powerful array of sensory components , the opportunity to leverage those capabilities in contexts other than telephony grows . we have in particular identified those sensory capabilities as key components for modern user interfaces that can detect movement , actions and intentions to enrich human computer interaction in a natural way . in this work , we present research around using smartphones as input controllers in the context of exertion videogames . we propose a conceptual framework that identifies the core elements of such interfaces , regardless of the underlying technological platforms , and provides a design pattern for their integration into existing videogames without having to change the games source code . we present a proof of concept implementation for the framework , with two smartphone input controllers , which using a soft button and accelerometer data , interface to a target shooting exertion game played while exercising on a stationary bicycle . we present findings from a user experience evaluation .
local feature based multi object recognition scheme for surveillance . <eos> in this paper , we propose an efficient multi object recognition scheme for surveillance based on interest points of objects and their feature descriptors . in this scheme , we first define a set of object types of interest and collect their sample images . for each sample image , we detect interest points and construct their feature descriptors using surf . next , we perform a statistical analysis of the local features to select representative points among them . intuitively , the representative points of an object are the interest points that best characterize the object . finally , we calculate thresholds of each object for object recognition . user query is processed in a similar way . a given query image 's local feature descriptors are extracted and then compared with the representative points of objects in the database . especially , to reduce the number of comparisons required , we propose a method for merging descriptors of similar representative points into a single descriptor . this descriptor is different from typical surf descriptor in that each element represents not a single value but a range . by using this merged descriptor , we can calculate the similarity between input image descriptor and multiple descriptors in database efficiently . in addition , since our scheme treats all the objects independently , it can recognize multiple objects simultaneously .
entertainment modeling through physiology in physical play . <eos> this paper is an extension of previous work on capturing and modeling the affective state of entertainment ( fun ) grounded on children 's physiological state during physical game play . the goal is to construct , using representative statistics computed from children 's physiological signals , an estimator of the degree to which games provided by the playground engage the players . previous studies have identified the difficulties of isolating elements of physical activity attributed to reported entertainment derived ( solely ) from heart rate ( hr ) recordings . in the present article , a survey experiment on a larger scale and a physical activity control experiment for surmounting those difficulties are devised . in these experiments , children 's hr , blood volume pulse ( bvp ) and skin conductance ( sc ) signals , as well as their expressed preferences of how much fun particular game variants are , are obtained using games implemented on the playware physical interactive playground . given effective data collection , a set of numerical features is computed from these measurements of the child 's physiological state . a comprehensive statistical analysis shows that children 's reported entertainment preferences correlate well with specific features of the recorded signals . preference learning techniques combined with feature set selection methods permit the construction of user models that predict reported entertainment preferences given suitable signal features . the most accurate models are obtained through evolving artificial neural networks and are demonstrated and evaluated on a playware game and a control task requiring physical activity . the best network is able to correctly match expressed preferences in 69.64 % of cases on previously unseen data ( p value 0.0022 p value 0.0022 ) and indicates two dissimilar classes of children those that prefer constantly energetic play of low mental emotional load and those that report as fun a dynamic play that involves high mental emotional load independently of physical effort . the generality of the methodology , its limitations , its usability as a real time feedback mechanism for entertainment augmentation and as a validation tool are discussed .
discrete program size dependent software reliability assessment modeling , estimation , and goodness of fit comparisons . <eos> in this paper we propose a discrete program size dependent software reliability growth model flexibly describing the software failure occurrence phenomenon based on a discrete weibull distribution . we also conduct model comparisons of our discrete srgm with existing discrete srgms by using actual data sets . the program size is one of the important metrics of software complexity . it is known that flexible discrete software reliability growth modeling is difficult due to the mathematical manipulation under a conventional modeling framework in which the time dependent behavior of the cumulative number of detected faults is formulated by a difference equation . our discrete srgm is developed under an existing unified modeling framework based on the concept of general order statistics , and can incorporate the effect of the program size into software reliability assessment . further , we discuss the method of parameter estimation , and derive software reliability assessment measures of our discrete srgm . finally , we show numerical examples of discrete software reliability analysis based on our discrete srgm by using actual data .
implications of the fit between organizational structure and erp a structural contingency theory perspective . <eos> despite the tremendous popularity and great potential , the field of enterprise resource planning ( erp ) adoption and implementation is littered with remarkable failures . though many contributing factors have been cited in the literature , we argue that the integrated nature of erp systems , which generally requires an organization to adopt standardized business processes reflected in the design of the software , is a key factor contributing to these failures . we submit that the integration and standardization imposed by most erp systems may not be suitable for all types of organizations and thus the fit between the characteristics of the adopting organization and the standardized business process designs embedded in the adopted erp system affects the likelihood of implementation success or failure . in this paper , we use the structural contingency theory to identify a set of dimensions of organizational structure and erp system characteristics that can be used to gauge the degree of fit , thus providing some insights into successful erp implementations . propositions are developed based on analyses regarding the success of erp implementations in different types of organizations . these propositions also provide directions for future research that might lead to prescriptive guidelines for managers of organizations contemplating implementing erp systems .
a neuroscience based design of intelligent tools for the elderly and disabled . <eos> the author has developed one basic research approach for universal accessibility over a period of <digit> years . as reviewed in this paper , he and his co researchers have designed several intelligent tools for universal accessibility as well as obtained many basic findings concerning neuroscience of human information processing . some of the tools have been manufactured in japan and the technologies as well as the basic findings have been applied to construct human centered computer interfaces such as virtual reality , automatic speech recognition and speech syntheses . moreover , these newly developed computer interface technologies have led to the improvement in the design of models for developing universal accessibility devices . lastly , the author has emphasized that a neuroscience based design of intelligent tools for the elderly and disabled may open a large market .
designing for semantic access a video browsing system . <eos> users of browsing applications often have vague information needs which can only be described in conceptual terms . therefore , a video browsing system must accept conceptual queries for preselection and offer mechanisms for interactive inspection of the result set by the user . in this paper , we describe a mm dbms that we extended with the following components our retrieval engine calculates relevance values for the results of a conceptual query by feature aggregation on video shot granularity to offer conceptual , content based access . to reduce startup delays within sessions , our admission control module admits only complete browsing sessions , if required resources , which are heuristically predicted from query results , are available . in addition , our intelligent client buffer strategy employs the retrieval relevance values to enable flexible user interactions during browsing .
a method for analyzing reading comprehension in computer science courses . <eos> reading has traditionally been seen as an essential component in learning , especially at the university level . however , many instructors in higher education , especially in technical courses , do not emphasize reading or try to evaluate it . in this abstract we present an automated system designed to measure and improve reading comprehension and describe preliminary results using the system .
studies on soluble ectodomain proteins of relaxin ( lgr7 ) and insulin <digit> ( lgr8 ) receptors . <eos> abstract the ectodomains of both the relaxin ( lgr7 ) and the insl3 ( lgr8 ) receptors can be expressed on the cell surface using only a single transmembrane domain . these membrane anchored proteins retain the ability to bind relaxin and can be cleaved from the cell surface . the subsequent lgr7 protein , 7bp , binds relaxin and can act as a functional relaxin antagonist . by contrast , the equivalent lgr8 protein 8bp does not bind relaxin or antagonize lgr8 activity . the 7bp protein has been successfully immobilized onto chemically derivatized surfaces for the capture of relaxin peptides and subsequent identification via seldi ms analysis .
on chip delay measurement for silicon debug . <eos> efficient test and debug techniques are indispensable for performance characterization of large complex integrated circuits in deep submicron and nanometer technologies . performance characterization of such chips requires on chip hardware and efficient debug schemes in order to reduce time to market and ensure shipping of chips with lower defect levels . in this paper we present an on chip scheme for delay fault detection and performance characterization . the proposed technique allows for accurate measurement of delays of speed paths for speed binning and facilitates a systematic and efficient test and debug scheme for delay faults . the area overhead associated with the proposed technique is very low .
surface mooring network in the kuroshio extension . <eos> as a contribution to the global earth observation system of systems , the national oceanic and atmospheric administration ( noaa ) is developing surface moorings that carry a suite of field proven and cost effective sensors to monitor air sea heat , moisture , and momentum fluxes , carbon dioxide uptake , and upper ocean temperature , salinity , and currents . in june <digit> , an noaa surface mooring , referred to as the kuroshio extension observatory ( keo ) , was deployed in the kuroshio extension 's ( ke ) southern recirculation gyre , approximately <digit> nautical miles east of japan . in <digit> , a partnership between noaa and the japan agency for marine earth science and technology was formed that deployed a second mooring ( referred to as jkeo ) north of the ke jet in february <digit> . ke is a region of strong currents , typhoons , and winter storms . designing and maintaining moorings in the ke is a challenging engineering task . all data are publicly available . a subset of the data are telemetered and made available in near real time through the global telecommunications system and web based data distribution systems . data from these time series reference sites serve a wide research and operational community and are being used for assessing numerical weather prediction analyses and reanalyses and for quantifying the air sea interaction in this dynamic region .
a parallel fully coupled algebraic multilevel preconditioner applied to multiphysics pde applications drift diffusion , flow transport reaction , resistive mhd . <eos> this study considers the performance of a fully coupled algebraic multilevel preconditioner for newton krylov solution methods . the performance of the preconditioner is demonstrated on a set of challenging multiphysics partial differential equation ( pde ) applications a drift diffusion approximation for semiconductor devices a low mach number formulation for the simulation of coupled flow , transport and non equilibrium chemical reactions and a low mach number formulation for visco resistive magnetohydrodynamics ( mhd ) systems . these systems contain multiple physical mechanisms that are strongly coupled , highly nonlinear , non symmetric and produce solutions with multiple length and time scales . in the context of this study the governing pdes for these systems are discretized in space by a stabilized finite element ( fe ) method that collocates all unknowns at each node of the fe mesh . the algebraic multilevel preconditioner is based on an aggressive coarsening graph partitioning of the non zero block structure of the jacobian matrix . the performance of the algebraic multilevel preconditioner is compared with a standard variable overlap additive schwarz domain decomposition preconditioner . representative performance and parallel scaling results are presented for a set of direct to steady state and fully implicit transient solutions . the performance studies include parallel weak scaling studies on up to <digit> cores and also includes the solution of systems as large as two billion unknowns carried out on <digit> <digit> cores of a cray xt3 <digit> . in general , the results of this study indicate that on this reasonably diverse set of challenging multiphysics applications the algebraic multilevel preconditioner performs very well . copyright ( c ) <digit> john wiley sons , ltd .
an efficient bi objective personnel assignment algorithm based on a hybrid particle swarm optimization model . <eos> a hybrid particle swarm optimization ( hpso ) algorithm which utilizes random key ( rk ) encoding scheme , individual enhancement ( ie ) scheme , and particle swarm optimization ( pso ) for solving a bi objective personnel assignment problem ( bopap ) is presented . the hpso algorithm which was proposed by kuo et al. , 2007andkuo et al. , 2009b is used to solve the flow shop scheduling problem ( fssp ) . in the research of bopap , the main contribution of the work is to improve the f1_f2 heuristic algorithm which was proposed by huang , chiu , yeh , and chang ( <digit> ) . the objective of the f1_f2 heuristic algorithm is to get the satisfaction level ( sl ) value which is satisfied the bi objective values f1 , and f2 for the personnel assignment problem . in this paper , pso is used to search the solution of the input problem in the bopap space . then , with the rk encoding scheme in the virtual space , we can exploit the global search ability of pso thoroughly . based on the ie scheme , we can enhance the local search ability of particles . the experimental results show that the solution quality of bopap based on the proposed hpso algorithm for the first objective f1 ( i.e. , total score ) , the second objective f2 ( i.e. , standard deviation ) , the coefficient of variance ( cv ) , and the time cost is far better than that of the f1_f2 heuristic algorithm . to the best our knowledge , this presented result of the bopap is the best bi objective algorithm known .
micro droplets generated on a rising bubble through an oppositely charged oil water interface . <eos> the mass transfer between immiscible two liquid phases can be greatly enhanced by bubbling gas through a reactor . numerous micro water droplets breaking out from a ruptured water film around a rising bubble through the oil ( upper phase ) water ( lower phase ) interface were demonstrated in the preceding paper ( uemura et al. in europhys lett <digit> <digit> , <digit> ) . in this study , we attempt to oppositely charge the oil and water layers , taking into account the findings of the preliminary study ( uemura et al. in j vis <digit> <digit> , <digit> ) . as a result , this study successfully produces more and finer water droplets than the preceding experiments .
evolving collaboration networks in scientometrics in <digit> <digit> a micro macro analysis . <eos> this paper reports first results on the interplay of different levels of the science system . specifically , we would like to understand if and how collaborations at the author ( micro ) level impact collaboration patterns among institutions ( meso ) and countries ( macro ) . all 2,541 papers ( articles , proceedings papers , and reviews ) published in the international journal scientometrics from <digit> <digit> are analyzed and visualized across the different levels and the evolving collaboration networks are animated over time . studying the three levels in isolation we gain a number of insights ( <digit> ) usa , belgium , and england dominated the publications in scientometrics throughout the <digit> year period , while the netherlands and spain were the subdominant countries ( <digit> ) the number of institutions and authors increased over time , yet the average number of papers per institution grew slowly and the average number of papers per author decreased in recent years ( <digit> ) a few key institutions , including univ sussex , khbo , katholieke univ leuven , hungarian acad sci , and leiden univ , have a high centrality and betweenness , acting as gatekeepers in the collaboration network ( <digit> ) early key authors ( lancaster fw , braun t , courtial jp , narin f , or vanraan afj ) have been replaced by current prolific authors ( such as rousseau r or moed hf ) . comparing results across the three levels reveals that results from one level might propagate to the next level , e.g. , top rankings of a few key single authors can not only have a major impact on the ranking of their institution but also lead to a dominance of their country at the country level movement of prolific authors among institutions can lead to major structural changes in the institution networks . to our knowledge , this is the most comprehensive and the only multi level study of scientometrics conducted to date .
testing a walkthrough methodology for theory based design of walk up and use interfaces . <eos> the value of theoretical analyses in user interface design has been hotly debated . all sides agree that it is difficult to apply current theoretical models within the constraints of real world development projects . we attack this problem in the context of bringing the theoretical ideas within a model of exploratory learning <digit> to bear on the evaluation of alternative interfaces for walk up and use systems . we derived a cognitive walkthrough procedure for systematically evaluating features of an interface in the context of the theory . four people independently applied this procedure to four alternative interfaces for which we have empirical usability data . consideration of the walkthrough sheds light on the consistency with which such a procedure can be applied as well as the accuracy of the results .
fair flow control for atm abr multipoint connections . <eos> multipoint to multipoint communication can be implemented by combining the point to multipoint and multipoint to point connection algorithms . in an atm multipoint to point connection , multiple sources send data to the same destination on a shared tree . traffic from multiple branches is merged into a single stream after every merge point . it is sometimes impossible for the network to determine any source specific characteristics since all sources in the multipoint connection may use the same connection identifiers . the challenge is to develop a fair rate allocation algorithm without per source accounting as this is inequivalent to per connection or per flow accounting in this case . we define fairness objectives for multipoint connections , and we design and simulate an o ( <digit> ) fair atm abr rate allocation scheme for point to point and multipoint connections sharing the same links . simulation results show that the algorithm performs well and exhibits many desirable properties . we list key modifications necessary for any atm abr rate allocation scheme to fairly accommodate multiple sources . ( c ) <digit> elsevier science b.v. all rights reserved .
geometric algorithms for automated design of rotary platen multi shot molds . <eos> this paper describes algorithms for automated design of rotary platen type of multi shot molds for manufacturing multi material objects . the approach behind our algorithms works in the following manner . first , we classify the given multi material object into several basic types based on the relationships among different components in the object . for every basic type , we find a molding sequence based on the precedence constraints resulting due to accessibility and disassembly requirements . then , starting from the last mold stage , we generate the mold pieces for every mold stage . we expect that algorithms described in this paper will provide the necessary foundations for automating the design of rotary platen molds .
a real time responsiveness measurement method of linux based mobile systems for p2p cloud systems . <eos> linux based mobile computing systems such as robots , electronic control devices , and smart phone are the most important types of p2p cloud systems in recent days . to improve the overall performance of networked systems , each mobile computing system requires real time characteristics . for this reason , mobile computing system developers want to know how well real time responsiveness is supported several real time measurement tools have been proposed . however , those previous tools have their own measurement schemes and we think that the results from those models do not show how responsive those systems are . in this paper , we propose elrm , a new real time measurement method that has clear measurement interval definitions and an accurate measurement method for real time responsiveness . we evaluate elrm on various mobile computing systems and compare it with other existing models . as a result , our method can obtain more accurate and intuitive real time responsiveness measurement results .
automatic lung segmentation method for mri based lung perfusion studies of patients with chronic obstructive pulmonary disease . <eos> a novel fully automatic lung segmentation method for magnetic resonance ( mr ) images of patients with chronic obstructive pulmonary disease ( copd ) is presented . the main goal of this work was to ease the tedious and time consuming task of manual lung segmentation , which is required for region based volumetric analysis of four dimensionalmr perfusion studies which goes beyond the analysis of small regions of interest .
headphones with touch control . <eos> the touch headphones are meant for portable music players and aim to present an improvement to the conventional remote control in the headphone wire , and a solution for controls on wireless in ear type headphones . two capacitive touch sensors per earpiece sense when earpieces are being tapped on , and being put in or out .
an adaptive unsupervised approach toward pixel clustering and color image segmentation . <eos> this paper proposes an adaptive unsupervised scheme that could find diverse applications in pattern recognition as well as in computer vision , particularly in color image segmentation the algorithm , named ant colony fuzzy c means hybrid algorithm ( afha ) , adaptively clusters image pixels viewed as three dimensional data pieces in the rgb color space the ant system ( as ) algorithm is applied for intelligent initialization of cluster centroids . which endows clustering with adaptivity . considering algorithmic efficiency , an ant subsampling step is performed to reduce computational complexity while keeping the clustering performance close to original one . experimental results have demonstrated afha clustering 's advantage of smaller distortion and more balanced cluster centroid distribution over fcm with random and uniform initialization quantitative comparisons with the x means algorithm also show that afha makes a better pre segmentation scheme over x means we further extend its application to natural image segmentation . taking into account the spatial information and conducting merging steps in the image space extensive tests were taken to examine the performance of the proposed scheme results indicate that compared with classical segmentation algorithms such as mean shift and normalized cut , our method could generate reasonably good or better image partitioning , which illustrates the method 's practical value ( c ) <digit> elsevier ltd. all rights reserved
an implementation of the acm siggraph proposed graphics standard in a multisystem environment . <eos> los alamos scientific laboratory ( lasl ) has implemented a graphics system designed to support one user interface for all graphics devices in all operating environments at lasl . the common graphics system ( cgs ) will support level one of the graphics standard proposed by the acm siggraph graphic standards planning committee . cgs is available in six operating environments of two different word lengths and supports four types of graphics devices . it can generate a pseudodevice file that may be postprocessed and edited for a particular graphics device , or it can generate device specific graphics output directly . program overlaying and dynamic buffer sharing are also supported . cgs is structured to isolate operating system dependencies and graphics device dependencies . it is written in the ratfor ( rational fortran ) language , which supports control flow statements and macro expansion . cgs is maintained as a single source program from which each version can be extracted automatically .
can social bookmarking enhance search in the web . <eos> social bookmarking is an emerging type of a web service that helps users share , classify , and discover interesting resources . in this paper , we explore the concept of an enhanced search , in which data from social bookmarking systems is exploited for enhancing search in the web . we propose combining the widely used link based ranking metric with the one derived using social bookmarking data . first , this increases the precision of a standard link based search by incorporating popularity estimates from aggregated data of bookmarking users . second , it provides an opportunity for extending the search capabilities of existing search engines . individual contributions of bookmarking users as well as the general statistics of their activities are used here for a new kind of a complex search where contextual , temporal or sentiment related information is used . we investigate the usefulness of social bookmarking systems for the purpose of enhancing web search through a series of experiments done on datasets obtained from social bookmarking systems . next , we show the prototype system that implements the proposed approach and we present some preliminary results .
a survey on approaches to gridification . <eos> the grid shows itself as a globally distributed computing environment , in which hardware and software resources are virtualized to transparently provide applications with vast capabilities . just like the electrical power grid , the grid aims at offering a powerful yet easy to use computing infrastructure to which applications can be easily ' plugged ' and efficiently executed . unfortunately , it is still very difficult to grid enable applications , since current tools force users to take into account many details when adapting applications to run on the grid . in this paper , we survey some of the recent efforts in providing tools for easy gridification of applications and propose several taxonomies to identify approaches followed in the materialization of such tools . we conclude this paper by describing common features among the proposed approaches , and by pointing out open issues and future directions in the research and development of gridification methods . copyright ( c ) <digit> john wiley sons , ltd .
online sequential extreme learning machine in nonstationary environments . <eos> system identification in nonstationary environments represents a challenging problem to solve and lots of efforts have been put by the scientific community in the last decades to provide adequate solutions on purpose . most of them are targeted to work under the system linearity assumption , but also some have been proposed to deal with the nonlinear case study . in particular the authors have recently advanced a neural architecture , namely time varying neural networks ( tv nn ) , which has shown remarkable identification properties in the presence of nonlinear and nonstationary conditions . tv nn training is an issue due to the high number of free parameters and the extreme learning machine ( elm ) approach has been successfully used on purpose . elm is a fast learning algorithm that has recently caught much attention within the neural networks ( nns ) research community . many variants of elm have been appeared in recent literature , specially for the stationary case study . the reference one for tv nn training is named elm tv and is of batch learning type . in this contribution an online sequential version of elm tv is developed , in response to the need of dealing with applications where sequential arrival or large number of training data occurs . this algorithm generalizes the corresponding counterpart working under stationary conditions . its performances have been evaluated in some nonstationary and nonlinear system identification tasks and related results show that the advanced technique produces comparable generalization performances to elm tv , ensuring at the same time all benefits of an online sequential approach .
accelerating mean time to failure computations . <eos> in this paper we consider the problem of numerical computation of the mean time to failure ( mttf ) in markovian dependability and or performance models . the problem can be cast as a system of linear equations which is solved using an iterative method preserving sparsity of the markov chain matrix . for highly dependable systems , system failure is a rare event and the above system solution can take an extremely large number of iterations . we propose to solve the problem by dividing the computation in two parts . first , by making some of the high probability states absorbing , we compute the mttf of the modified markov chain . in a subsequent step , by solving another system of linear equations , we are able to compute the mttf of the original model . we prove that for a class of highly dependable systems , the resulting method can speed up computation of the mttf by orders of magnitude . experimental results supporting this claim are presented . we also obtain bounds on the convergence rate for computing the mean entrance time of a rare set of states in a class of queueing models .
genetic code an alternative model of translation . <eos> abstract our earlier studies of translation have led us to a specific numeric coding of nucleotides ( a <digit> , c <digit> , g <digit> , and u <digit> ) that is , a quaternary numeric system to ordering of digrams and codons ( read right to left . yx and z.yx ) as ordinal numbers from <digit> to <digit> and to seek hypothetic transformation of mrna to <digit> canonic amino acids . in this work , we show that amino acids match the ordinal numberthat is , follow as transforms of their respective digrams and or mrna codons . sixteen digrams and their respective amino acids appear as a parallel ( discrete ) array . a first approximation of translation in this view is demonstrated by a twisted spiral on the side of phantom codons and by ordering amino acids in the form of a cross on the other side , whereby the transformation of digrams and or phantom codons to amino acids appears to be one to one classification of canonical amino acids derived from our dynamic model clarifies physicochemical criteria , such as purinity , pyrimidinity , and particularly codon rules . the system implies both the rules of siemion and siemion and of davidov , as well as balances of atomic and nucleon numbers within groups of amino acids . formalization in this system offers the possibility of extrapolating backward to the initial organization of heredity .
a two step automatic sleep stage classification method with dubious range detection . <eos> a two step classifier for automatic sleep staging is proposed . the system provides two outputs non dubious and dubious classification . the dubious epochs are tagged and re assigned according to a post processing step . the system indicates to an expert physician which results need revision . the accuracy of non dubious classification for wake and rem is around <digit> % .
smartphone based hierarchical crowdsourcing for weed identification . <eos> a novel hierarchical crowdsourcing based system for weed identification . combines image processing with crowdsourcing weed identification . framework for unsupervised determination of crowd hierarchy . prototype that supports low cost and accurate weed identification .
cooperating with free riders in unstructured p2p networks . <eos> free riding is a common phenomenon in peer to peer ( p2p ) file sharing networks . although several mechanisms have been proposed to handle free ridingmostly to exclude free riders , few of them have been adopted in a practical system . this may be attributed to the fact that the mechanisms are often nontrivial , and that completely eliminating free riders could jeopardize the sheer power of the network arising from the huge volume of its participants . rather than excluding free riders , we incorporate and utilize them to provide global index service to the files shared in the network , as well as to relay messages in the search process . the simulation results indicate that our mechanism not only can shift the query processing load from non free riders to free riders , but can also significantly boost the search efficiency of a plain gnutella . moreover , the mechanism is quite resilient to high free riding ratio .
sharing many secrets with computational provable security . <eos> two new multi secret sharing schemes , with computational provable security . the security proofs are in the standard model . the two schemes generalize schemes previously proposed in the literature . we compare the two schemes in terms of security , efficiency and extendability . the schemes work for general access structures .
joint packet scheduling and radio resource assignment for wimax networks . <eos> the ieee 802.16 standard defines the qos signaling framework and various types of service flows , but left the qos based packet scheduling and radio resource assignment undefined . this paper proposes a novel joint packet scheduling and radio resource assignment algorithm for wimax networks . our algorithms can effectively assign the suitable slots to meet the qos requirements of the different service type flows while taking the throughput and fairness into considerations . the effectiveness of our algorithms have been demonstrated through extensive analysis and simulation data . the results show that our algorithms greatly improve the throughput with relatively low complexity .
multi context photo browsing on mobile devices based on tilt dynamics . <eos> this paper presents a photo browsing system on mobile devices to browse and search photos efficiently by tilting action . it employs tilt dynamics and a multi scale photo screen layout for enhancing the browsing and the search capability respectively . the implementation uses continuous inputs from an accelerometer , and a multimodal ( visual , audio and vibrotactile ) display coupled with the states of this model . the model is based on a simple physical model , with its characteristics shaped to enhance controllability . the multi scale layout holds both local and global view for users to both control photos and look at the surrounding context in a single framework . the experiment on samsung mits pda used seven novice users browsing from <digit> photos . we compare a tilt based interaction method with a button based browser and an ipod wheel by a quantitative usability criteria and subjective experience . the proposed tilt dynamics improves the usability over conventional dynamics . the ipod wheel has mixed performance comparing worse on some metrics than button pushing or tilt interaction , despite its commercial popularity .
session based access control in geographically replicated internet services . <eos> performance critical services over internet often rely on geographically distributed architectures of replicated servers . content delivery networks ( cdn ) are a typical example where service is based on a distributed architecture of replica servers to guarantee resource availability and proximity to final users . in such distributed systems , network links are not dedicated , and may be subject to external traffic . this brings up the need to develop access control policies that adapt to network load changing conditions . further , internet services are mainly session based , thus an access control support must take into account a proper differentiation of requests and perform session based decisions while considering the dynamic availability of resources due to external traffic . in this paper we introduce a distributed architecture with access control capabilities at session aware access points . we consider two types of services characterized by different patterns of resource consumption and priorities . we formulate a markov modulated poisson decision process for access control that captures the heterogeneity of multimedia services and the variable availability of resources due to external traffic . the proposed model is optimized by means of stochastic analysis , showing the impact of external traffic on service quality . the structural properties of the optimal solutions are studied and considered as the basis for the formulation of heuristics . the performance of the proposed heuristics is studied by means of simulations , showing that in some typical scenario they perform close to the optimum .
periods in partial words an algorithm . <eos> partial words are finite sequences over a finite alphabet that may contain some holes . a variant of the celebrated finewilf theorem shows the existence of a bound l l ( h , p , q ) l l ( h , p , q ) such that if a partial word of length at least l with h holes has periods p and q , then it also has period gcd ( p , q ) gcd ( p , q ) . in this paper , we associate a graph with each p and q periodic word , and study two types of vertex connectivity on such a graph modified degree connectivity and r set connectivity where r q mod p . as a result , we give an algorithm for computing l ( h , p , q ) l ( h , p , q ) in the general case and show how to use it to derive the closed formulas .
unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming . <eos> this article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words . first , a language bias for word se mentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value . in the second phase , the words segmented by the genetic algorithm are used as an input for the first order decision list learner clog . the result is a set of first order rules which can be used for segmentation of unseen words . when applied on either the training data or unseen data , these rules produce segmentations which are linguistically meaningful , and to a large degree conforming to the annotation provided .
nonexponential evolution equations and operator ordering . <eos> nonexponential evolution equations can be treated using a formalism involving the evolution operator method , which , unlike the ordinary case , is not expressed in terms of exponential operators . the use of this technique requires particular care associated with the operator ordering . in this paper , we will present a first systematic approach to this type of problems . ( c ) <digit> elsevier ltd. all rights reserved .
application of genetic algorithm for unknown parameter estimations in cylindrical fin . <eos> this article deals with the application of the genetic algorithm ( ga ) for optimizing an inverse problem and retrieving unknown parameters in cylindrical fin geometry . parameters such as the thermal conductivity and the heat transfer coefficient are attempted for estimation in order to satisfy a desired temperature field in the medium . the study is done for single parameter and simultaneous two parameter retrievals . the temperature field is calculated from a forward problem using the finite difference method using some known values of the properties . these properties are ultimately retrieved by an inverse approach using the ga. the study is done for different controlling parameters such as the number of generations , measurement errors and number of measurement locations . for two parameter simultaneous estimation , many combination of unknown parameters are observed to satisfy a given temperature field , and their ratio is only found to be successfully estimated . the present work is proposed to be useful for selecting the thermal properties which are required to satisfy a given temperature field .
formulation of pedestrian movement in microscopic models with continuous space representation . <eos> when the microscopic pedestrian models , in which pedestrian space is continuously represented , are used to simulate pedestrian movement in the buildings with internal obstacles , some issues arise and need be dealt with in detail . this paper discusses two of the issues , namely formulating the desired direction of each pedestrian in the buildings and determining the region around each pedestrian , other individuals and obstacles in which affect his or her movement . the methods for computing the desired direction and effect region are proposed , using the algorithms for the potential of pedestrian space . by numerical experiments , the performance results of three proposed formulae for the desired direction are compared , the method for the effect region is tested , and the validity of the method for computing the desired direction as considering the border effect of obstacles is verified . numerical results indicate that the proposed methods can be used to formulate pedestrian movement , especially in the buildings with internal obstacles , in the microscopic models with continuous space representation .
extending clips to support temporal representation and reasoning . <eos> applications using expert systems for monitoring and control problems often require the ability to represent temporal knowledge and to apply reasoning based on that knowledge . incorporating temporal representation and reasoning into expert systems leads to two problems in development dealing with an implied temporal order of events using a non procedural tool and maintaining the large number of temporal relations that can occur among facts in the knowledge base . in this paper we explore these problems by using an expert system shell , clips ( c language integrated production system ) , to create temporal relations using common knowledge based constructs . we also build an extension to clips through a user defined function which generates the temporal relations from those facts . we use the extension to create and maintain temporal relations in a workflow application that monitors and controls an engineering design change review process . we also propose a solution to ensure truth maintenance among temporally related facts that links our temporal extension to the clips facility for truth maintenance .
mpeg <digit> digital items to support integration of heterogeneous multimedia content . <eos> the melisa system is a distributed platform for multi platform sports content broadcasting , providing end users with a wide range of real time interactive services during the sport event , such as statistics , visual aids or enhancements , betting , and user and context specific advertisements . in this paper , we present the revamped design of the complete system and the implementation of a middleware entity utilizing concepts present in the emerging mpeg <digit> framework . more specifically , all multimedia content is packaged in a self contained digital item , containing both the binary information ( video , graphics , etc. ) required for the playback , as well as structured representations of the different entities that can handle this item and the actions they can perform on it . this module essentially stands between the different components of the integrated content preparation system , thereby not disrupting their original functionality at all additional tweaks are performed in the receiver sides , as well , to ensure that the additional information and provisions are respected . the outcome of this design upgrade is that ipr issues are dealt with successfully , both with respect to the content itself and to the functionality of the subscription levels in addition to this , end users can be presented with personalized forms of the final content , e.g. , viewing in play virtual advertisements that match their shopping habits and preferences , thus enhancing the viewing experience and creating more revenue opportunities via targeted advertisements . ( c ) <digit> elsevier b.v. all rights reserved .
linguistic recognition system for identification of some possible genes mediating the development of lung adenocarcinoma . <eos> in the present article , we develop a linguistic recognition system for identification of some possible genes mediating the development of human lung adenocarcinoma . the methodology involves dimensionality reduction , classifying the genes through incorporation of the notion of linguistic fuzzy sets low , medium and high , and finally selection of some possible genes obtained by a rule generation grouping technique . the system has been successfully applied on two microarray gene expression data sets . the results are appropriately validated by some earlier investigations , gene expression profiles and t test . the proposed methodology has been able to find more true positives than an existing one in identifying responsible genes . moreover , we have found some new genes that may have role in mediating the development of lung adenocarcinoma . ( c ) <digit> elsevier b.v. all rights reserved .
the existence and upper bound for two types of restricted connectivity . <eos> in this paper , we study two types of restricted connectivity k ( g ) k ( g ) is the cardinality of a minimum vertex cut s s such that every component of g s g s has at least k k vertices k ( g ) is the cardinality of a minimum vertex cut s s such that there are at least two components in g s g s of order at least k k . in this paper , we give some sufficient conditions for the existence and upper bound of k ( g ) k ( g ) and or k ( g ) , and study some properties of these two parameters .
coloring geometric range spaces . <eos> we study several coloring problems for geometric range spaces . in addition to their theoretical interest , some of these problems arise in sensor networks . given a set of points in r ( <digit> ) or r ( <digit> ) , we want to color them so that every region of a certain family ( e. g. , every disk containing at least a certain number of points ) contains points of many ( say , k ) different colors . in this paper , we think of the number of colors and the number of points as functions of k. obviously , for a fixed k using k colors , it is not always possible to ensure that every region containing k points has all colors present . thus , we introduce two types of relaxations either we allow the number of colors used to increase to c ( k ) , or we require that the number of points in each region increases to p ( k ) . symmetrically , given a finite set of regions in r ( <digit> ) or r ( <digit> ) , we want to color them so that every point covered by a sufficiently large number of regions is contained in regions of k different colors . this requires the number of covering regions or the number of allowed colors to be greater than k. the goal of this paper is to bound these two functions for several types of region families , such as halfplanes , halfspaces , disks , and pseudo disks . this is related to previous results of pach , tardos , and toth on decompositions of coverings .
a methodology for design of scalable architectures in software radio networks a unified device and network perspective . <eos> this paper proposes the tissue methodology as a novel methodology for analysis , design and synthesis of networked embedded systems and subsequent development of distributed architectural frameworks . the proposed method aims at reducing the development time through the use of reconfigurable hw sw components and the application of automatic code generation techniques . we devise the usefulness of the proposed methodology in the context of mobile ad hoc networks ( manet ) which exploit software radio ( sr ) technology for reconfigurability issues . drawbacks of current design and simulation tools and advantages coming from the application of the tm are discussed in the paper .
temporal development methods for agent based systems . <eos> in this paper we overview one specific approach to the formal development of multi agent systems . this approach is based on the use of temporal logics to represent both the behaviour of individual agents , and the macro level behaviour of multi agent systems . we describe how formal specification , veri . cation and refinement can all be developed using this temporal basis , and how implementation can be achieved by directly executing these formal representations . we also show how the basic framework can be extended in various ways to handle the representation and implementation of agents capable of more complex deliberation and reasoning .
lay persons ' and professionals ' nutrition related vocabularies and their matching to a general and a specific thesaurus . <eos> this study examines the differences between expressions used by lay persons and professionals in nutrition related questions and answers , and to what degree general finnish ontology ( gfo ) and a medical thesaurus ( finmesh ) cover these expressions . fifty question answer pairs were collected in an electronic answering service . nutrition related concepts and their expressions with their semantic relations were identified . the vocabularies of lay persons and professionals were found to be quite similar . this hints that a special consumer health vocabulary in the field of nutrition is not needed . gfo covered <digit> % of all expressions in questions and <digit> % of expressions in answers . finmesh covered <digit> % of expressions in both groups . the overlapping match of the thesauri was low , <digit> % in both questions and in answers . gfo and finmesh were found to be poor tools for supporting users in expressing nutrition related information needs . gfo seemed not to form a covering bridge to finmesh .
estimating forest biomass using small footprint lidar data an individual tree based approach that incorporates training data . <eos> a new individual tree based algorithm for determining forest biomass using small footprint lidar data was developed and tested . this algorithm combines computer vision and optimization techniques to become the first training data based algorithm specifically designed for processing forest lidar data . the computer vision portion of the algorithm uses generic properties of trees in small footprint lidar canopy height models ( chms ) to locate trees and find their crown boundaries and heights . the ways in which these generic properties are used for a specific scene and image type is dependent on <digit> parameters , nine of which are set using training data and the neldermead simplex optimization procedure . training data consist of small sections of the lidar data and corresponding ground data . after training , the biomass present in areas without ground measurements is determined by developing a regression equation between properties derived from the lidar data of the training stands and biomass , and then applying the equation to the new areas . a first test of this technique was performed using <digit> plots ( radius <digit> m ) in a loblolly pine plantation in central virginia , usa ( 37.42 n , 78.68 w ) that was not intensively managed , together with corresponding data from a lidar canopy height model ( resolution 0.5 m ) . results show correlations ( r ) between actual and predicted aboveground biomass ranging between 0.59 and 0.82 , and rmses between 13.6 and 140.4 t ha depending on the selection of training and testing plots , and the minimum diameter at breast height ( <digit> or <digit> cm ) of trees included in the biomass estimate . correlations between lidar derived plot density estimates were low ( 0.22 r 0.56 ) but generally significant ( at a <digit> % confidence level in most cases , based on a one tailed test ) , suggesting that the program is able to properly identify trees . based on the results it is concluded that the validation of the first training data based algorithm for determining forest biomass using small footprint lidar data was a success , and future refinement and testing are merited .
the upper layers of the iso osi reference model ( part ii ) ( reprinted from computer standards and interfaces , vol <digit> , pg <digit> <digit> , <digit> ) . <eos> this review is intended as an introduction to the communication concepts and functions associated with the upper layers of the iso osi reference model . it describes , in general terms , the requirements and the benefits of an open communication and defines the fundamental requirements for interworking among distributed computer systems . ( c ) <digit> published by elsevier science b.v. all rights reserved .
adjustable chain trees for proteins . <eos> a chain tree is a data structure for changing protein conformations . it enables very fast detection of clashes and free energy potential calculations . a modified version of chain trees that adjust themselves to the changing conformations of folding proteins is introduced . this results in much tighter bounding volume hierarchies and therefore fewer intersection checks . computational results indicate that the efficiency of the adjustable chain trees is significantly improved compared to the traditional chain trees .
real time point based rendering using visibility map . <eos> because of its simplicity and intuitive approach , point based rendering has been a very popular research area . recent approaches have focused on hardware accelerated techniques . by applying a deferred shading scheme , both high quality images and high performance rendering have been achieved . however , previous methods showed problems related to depth based visibility computation . we propose an extended point based rendering method using a visibility map . in our method we employ a distance based visibility technique ( replacing depth based visibility ) , an averaged position map and an adaptive fragment processing scheme , resulting in more accurate and improved image quality , as well as improved rendering performance .
expression and effect of transforming growth factor and tumor necrosis factor in human pheochromocytoma . <eos> this study observed the expression of transforming growth factor ( tgf ) and tumor necrosis factor ( tnf ) in pheochromocytoma ( pheo ) tissue and examined their effects on the proliferation and apoptosis of human pheo cells . the mrna and protein expressions of tgf and tnf were higher in pheo tissues than in normal adrenal medullary tissues , and their expressions varied with pathological features . tgf and tnf stimulated the proliferation of primary human pheo cells , but had no effect on the cell apoptosis . both tgf and tnf might be involved in the pathogenesis of human pheo . tnf needs to be further investigated before its treatment of pheo can be realized in clinical practice
interactive visualisation of spins and clusters in regular and small world ising models with cuda on gpus . <eos> three dimensional simulation models are hard to visualise for dense lattice systems , even with cutaways and flythrough techniques . we use multiple graphics processing units ( gpus ) , cuda and opengl to increase our understanding of computational simulation models such as the <digit> d and <digit> d ising systems with small world link rewiring by accelerating both the simulation and visualisation into interactive time . we show how interactive model parameter updates , visual overlaying of measurements and graticules , cluster labelling and other visual highlighting cues enhance user intuition of the models meaning and exploit the enhanced simulation speed to handle model systems large enough to explore multi scale phenomena .
efficient and accurate nearest neighbor and closest pair search in high dimensional space . <eos> nearest neighbor ( nn ) search in high dimensional space is an important problem in many applications . from the database perspective , a good solution needs to have two properties ( i ) it can be easily incorporated in a relational database , and ( ii ) its query cost should increase sublinearly with the dataset size , regardless of the data and query distributions . locality sensitive hashing ( lsh ) is a well known methodology fulfilling both requirements , but its current implementations either incur expensive space and query cost , or abandon its theoretical guarantee on the quality of query results . motivated by this , we improve lsh by proposing an access method called the locality sensitive b tree ( lsb tree ) to enable fast , accurate , high dimensional nn search in relational databases . the combination of several lsb trees forms a lsb forest that has strong quality guarantees , but improves dramatically the efficiency of the previous lsh implementation having the same guarantees . in practice , the lsb tree itself is also an effective index which consumes linear space , supports efficient updates , and provides accurate query results . in our experiments , the lsb tree was faster than ( i ) idistance ( a famous technique for exact nn search ) by two orders of magnitude , and ( ii ) medrank ( a recent approximate method with nontrivial quality guarantees ) by one order of magnitude , and meanwhile returned much better results . as a second step , we extend our lsb technique to solve another classic problem , called closest pair ( cp ) search , in high dimensional space . the long term challenge for this problem has been to achieve subquadratic running time at very high dimensionalities , which fails most of the existing solutions . we show that , using a lsb forest , cp search can be accomplished in ( worst case ) time significantly lower than the quadratic complexity , yet still ensuring very good quality . in practice , accurate answers can be found using just two lsb trees , thus giving a substantial reduction in the space and running time . in our experiments , our technique was faster ( i ) than distance browsing ( a well known method for solving the problem exactly ) by several orders of magnitude , and ( ii ) than d shift ( an approximate approach with theoretical guarantees in low dimensional space ) by one order of magnitude , and at the same time , outputs better results .
a performance evaluation of a coverage compensation based algorithm for wireless sensor networks . <eos> recent years , coverage has been widely investigated as one of the fundamental quality measurements of wireless sensor networks . in order to maintaining the coverage while saving energy of networks , algorithms have been developed to keep a minimum cover set of sensors working and turn off the redundant sensors . generally , centralized algorithms can give a better result than distributed algorithms in terms of the number of active sensors . however , the heavy computation requirements and message overhead for collecting geographical location data keep centralized algorithms out of most distributed scenarios . in this article , based on the idea of coverage compensation a distributed node partition algorithm for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as ga. a genetic algorithm for coverage is proposed too to demonstrate how an optimal coverage node distribution created by ga can be used in a distributed scenario . ours works are simulated on jgap and ns2 . the simulation result shows that our partition algorithm based on coverage compensation can achieve the same performance as ocops in terms of coverage and number of active sensors while using less control messages .
