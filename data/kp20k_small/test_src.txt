a feedback vertex set of <digit> degenerate graphs . <eos> a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is <digit> degenerate if each subgraph g g of g has a vertex v such that dg ( v ) <digit> d g ( v ) <digit> . in this paper , we prove that ( g ) 2n <digit> ( g ) <digit> n <digit> for any <digit> degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given <digit> degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n <digit> <digit> n <digit> .
hybrid analytical modeling of pending cache hits , data prefetching , and mshrs . <eos> this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .
autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome . <eos> autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .
numerical solution of a three dimensional solidification problem in aluminium casting . <eos> in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .
definition and recognition of rib features in aircraft structural part . <eos> in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .
an algebraic approach to guarantee harmonic balance method using grobner base . <eos> harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .
a graph coloring based tdma scheduling algorithm for wireless sensor networks . <eos> wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance <digit> coloring algorithm and can get colors near to ( ( updelta <digit> ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .
building model as a service to support geosciences . <eos> model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .
shot change detection using scene based constraint . <eos> a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .
tail asymptotics for hol priority queues handling a large number of independent stationary sources . <eos> in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .
a variant of parallel plane sweep algorithm for multicore systems . <eos> parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .
the antecedents and consequents of user perceptions in information technology adoption . <eos> a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .
improving classification with latent variable models by sequential constraint optimization . <eos> in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .
a framework for a real time intelligent and interactive brain computer interface . <eos> a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .
characterizing output processes of e m e k <digit> queues . <eos> our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k <digit> queues . we show , by numerical examples , that the interdeparture time of e m e k <digit> retains the ifr property if m > k. ( c ) <digit> elsevier ltd. all rights reserved .
a low complexity down mixing structure on quadraphonic headsets for surround audio . <eos> this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately <digit> % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .
security personalization for internet and web services . <eos> the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .
two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures . <eos> this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a <digit> nm cmos fabrication process . ( c ) <digit> elsevier b.v. all rights reserved .
simulation aided design of organizational structures in manufacturing systems using structuring strategies . <eos> this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .
explicit constructions of selectors and related combinatorial structures , with applications . <eos> in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .
selective finite element refinement in torsional problems based on the membrane analogy . <eos> this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .
rns montgomery multiplication algorithm for duplicate processing of base transformations . <eos> this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for <digit> bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .
from quality in use to value in the world . <eos> this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .
comparative study of family <digit> gpcrs in fugu rubripes . <eos> abstract in this study , members of family <digit> gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family <digit> gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .
blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks . <eos> this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .
polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers . <eos> the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of <digit> under ambient conditions can be obtained .
an integration of online and pseudo online information for cursive word recognition . <eos> in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .
generation of quasi gaussian pulses based on correlation techniques . <eos> the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .
learning linear pca with convex semi definite programming . <eos> the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .
the neighborhood auditing tool a hybrid interface for auditing the umls . <eos> the umls 's integration of more than <digit> source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) <digit> elsevier inc. all rights reserved .
exclusion regions for optimization problems . <eos> branch and bound methods for finding all solutions of a global optimization problem in a box frequently have the difficulty that subboxes containing no solution can not be easily eliminated if they are close to the global minimum . this has the effect that near each global minimum , and in the process of solving the problem also near the currently best found local minimum , many small boxes are created by repeated splitting , whose processing often dominates the total work spent on the global search . this paper discusses the reasons for the occurrence of this so called cluster effect , and how to reduce the cluster effect by defining exclusion regions around each local minimum found , that are guaranteed to contain no other local minimum and hence can safely be discarded . in addition , we will introduce a method for verifying the existence of a feasible point close to an approximate local minimum . these exclusion regions are constructed using uniqueness tests based on the krawczyk operator and make use of first , second and third order information on the objective and constraint functions .
learning about meetings . <eos> most people participate in meetings almost every day , multiple times a day . the study of meetings is important , but also challenging , as it requires an understanding of social signals and complex interpersonal dynamics . our aim in this work is to use a data driven approach to the science of meetings . we provide tentative evidence that ( i ) it is possible to automatically detect when during the meeting a key decision is taking place , from analyzing only the local dialogue acts , ( ii ) there are common patterns in the way social dialogue acts are interspersed throughout a meeting , ( iii ) at the time key decisions are made , the amount of time left in the meeting can be predicted from the amount of time that has passed , ( iv ) it is often possible to predict whether a proposal during a meeting will be accepted or rejected based entirely on the language ( the set of persuasive words ) used by the speaker .
design and implementation of an expert interface system for integration of photogrammetric and geographic information systems for intelligent preparation and structuring of spatial data . <eos> preparation of spatial data for geographic information system ( gis ) simultaneously during feature digitizing process from photogrammetric models reduces data editing phases after feature digitizing process . therefore , the problems , caused by separating spatial data production process from preparation of this data , are overcome as far as possible . to achieve this purpose , specialty and expertise required for spatial data structuring and preparation for gis , should be available in an interface system which establishes a direct connection between photogrammetric and gis systems . in this case , when a user digitizes a feature from a photogrammetric model , decision making process about the method of editing , structuring , layering , and storing of the feature in gis database , can be carried out by such an interface system . thus , according to the capabilities of expert systems for modeling the knowledge and deduction methods of experts , generating an expert interface system between photogrammetric and gis systems , offers a suitable solution for this integration . in this paper , the capabilities of expert systems for intelligent spatial data structuring and preparation simultaneously during feature digitizing process from photogrammetric models , have been investigated . also , design , implementation and test of an expert interface system for integration of photogrammetric and gis systems in order to take advantages of capabilities of both systems simultaneously as one integrated system , has been described .
wevan a mechanism for evidence creation and verification in vanets . <eos> there are traffic situations ( e.g. incorrect speeding tickets ) in which a given vehicles driving behavior at some point in time has to be proved to a third party . vehicle mounted sensorial devices are not suitable for this matter since they can be maliciously manipulated . however , surrounding vehicles may give their vision on another ones behavior . furthermore , these data may be shared with the affected vehicle through vanets . in this paper , a vanet enabled data exchange mechanism called wevan is presented . the goal of this mechanism is to build and verify evidences based on surrounding vehicles ( called witnesses ) testimonies . due to the short range nature of vanets , the connectivity to witnesses may be reduced with time the later their testimonies are requested , the lower the amount of witnesses may be . simulation results show that if testimonies are ordered 5s later , an average of <digit> testimonies may be collected in highway scenarios . other intervals and road settings are studied as well .
extensional normalisation and type directed partial evaluation for typed lambda calculus with sums . <eos> we present a notion of eta long beta normal term for the typed lambda calculus with sums and prove , using grothendieck logical relations , that every term is equivalent to one in normal form . based on this development we give the first type directed partial evaluator that constructs normal forms of terms in this calculus .
yet another write optimized dbms layer for flash based solid state storage . <eos> flash based solid state storage ( flashsss ) has write oriented problems such as low write throughput , and limited life time . especially , flashssds have a characteristic vulnerable to random writes , due to its control logic utilizing parallelism between the flash memory chips . in this paper , we present a write optimized layer of dbmss to address the write oriented problems of flashsss in on line transaction processing environments . the layer consists of a write optimized buffer , a corresponding log space , and an in memory mapping table , closely associated with a novel logging scheme called incremental logging ( icl ) . the icl scheme enables dbmss to reduce page writes at the least expense of additional page reads , while replacing random writes into sequential writes . through experiments , our approach demonstrated up to an order of magnitude performance enhancement in i o processing time compared to the original dbms , increasing the longevity of flashsss by approximately a factor of two .
wrinkle development analysis in thin sail like structures using mitc shell finite elements . <eos> we propose a method of modelling sail type structures which captures the wrinkling behaviour of such structures . the method is validated through experimental and analytical test cases , particularly in terms of wrinkling prediction . an enhanced wrinkling index is proposed as a valuable measure characterizing the global wrinkling development on the deformed structure . the method is based on a pseudo dynamic finite element procedure involving non linear mitc shell elements . the major advantage compared to membrane models generally used for this type of analysis is that no ad hoc wrinkling model is required to control the stability of the structure . we demonstrate our approach to analyse the behaviour of various structures with spherical and cylindrical shapes , characteristic of downwind sails over a rather wide range of shape and constitutive parameters . in all cases convergence is reached and the overall flying shape is most adequately represented , which shows that our approach is a most valuable alternative to standard techniques to provide deeper insight into the physical behaviour . limitations appear only in some very special instances in which local wrinkling related instabilities are extremely high and would require specific additional treatments , out of the scope of the present study .
cliques , holes and the vertex coloring polytope . <eos> certain subgraphs of a given graph g restrict the minimum number chi ( g ) of colors that can be assigned to the vertices of g such that the endpoints of all edges receive distinct colors . some of such subgraphs are related to the celebrated strong perfect graph theorem . as it implies that every graph g contains a clique of size chi ( g ) , or an odd hole or an odd anti hole as an induced subgraph . in this paper , we investigate the impact of induced maximal cliques , odd holes and odd anti holes on the polytope associated with a new <digit> <digit> integer programming formulation of the graph coloring problem . we show that they induce classes of facet defining , inequalities . ( c ) <digit> elsevier b.v. all rights reserved .
an experimental validation of a novel clustering approach to pwarx identification . <eos> in this paper , the problem of clustering based procedure for the identification of piecewise auto regressive exogenous ( pwarx ) models is addressed . this problem involves both the estimation of the parameters of the affine sub models and the hyperplanes defining the partitions of the state input regression . in fact , we propose the use of the chiu 's clustering algorithm in order to overcome the main drawbacks of the existing methods which are the poor initialization and the presence of outliers . in addition , our approach is able to generate automatically the number of sub models . simulation results are presented to illustrate the performance of the proposed method . an application of the developed approach to an olive oil esterification reactor is also suggested in order to validate simulation results .
a primal dual approximation algorithm for the asymmetric prize collecting tsp . <eos> we present a primal dual log ( n ) approximation algorithm for the version of the asymmetric prize collecting traveling salesman problem , where the objective is to find a directed tour that visits a subset of vertices such that the length of the tour plus the sum of penalties associated with vertices not in the tour is as small as possible . the previous algorithm for the problem ( v.h. nguyen and t.t nguyen in int . j. math . oper . res . <digit> ( <digit> ) <digit> , <digit> ) which is not combinatorial , is based on the held karp relaxation and heuristic methods such as the frieze et al. s heuristic ( frieze et al. in networks <digit> <digit> , <digit> ) or the recent asadpour et al. s heuristic for the atsp ( asadpour etal . in 21st acm siam symposium on discrete algorithms , <digit> ) . depending on which of the two heuristics is used , it gives respectively <digit> log ( n ) and ( <digit> <digit> frac log ( n ) log ( log ( n ) ) ) as an approximation ratio . our algorithm achieves an approximation ratio of log ( n ) which is weaker than ( <digit> <digit> frac log ( n ) log ( log ( n ) ) ) but represents the first combinatorial approximation algorithm for the asymmetric prize collecting tsp .
second order ambient intelligence . <eos> this text attempts to describe an imagined future of ambient intelligence . it assumes that one day most of the current issues within ambient intelligence will be solved and that a second order ambient intelligence will be formulated , one with new research agendas . it describes several topics and ideas that might be part of this agenda and surmises on the prerequisites for this change .
pdms prism glass optical coupling for surface plasmon resonance sensors based on mems technology . <eos> a miniaturized surface plasmon resonance ( spr ) chip has been developed for biomedical and chemical analysis with low cost and high performance . the techniques of bulk silicon micromachining and polymer replication were used to fabricate the kretschmann spr sensor composed of a polydimethylsiloxane ( pdms ) prism , a coupling glass and microchannels . the plasmon properties of thin metal films were investigated theoretically based on fresnel analysis , with optical boundary conditions pertaining to the surface plasmon resonance at the gold water and gold air interfaces . the theoretical results show that difference in the refractive index ( ri ) between the pdms prism and the coupling glass layer affect the precision of the spr angle and the spr curve . meanwhile , the period of the interference fringe attaching on the spr curve increases with an increase in wavelength and a decrease in the refractive index of the coupling glass layer . the gold thickness of <digit> nm is required while employing a fixed incident wavelength of <digit> nm , to achieve optimum spr excitation conditions and the sensor sensitivity . the characteristics of this spr sensor were evaluated in the angular interrogation mode of employing the incident wavelength of <digit> nm in air and water media , respectively . the obtained spr angles were approximately consistent with the theoretical ones .
isogeometric analysis for strain field measurements . <eos> in this paper , the potential of isogeometric analysis for strain field measurement by digital image correlation is investigated . digital image correlation ( dic ) is a full field kinematics measurement technique based on gray level conservation principle and the formulation we adopt allows for using arbitrary displacement bases . the high continuity properties of non uniform rational b spline ( nurbs ) functions are exploited herein as an additional regularization of the initial ill posed problem . k refinement is analyzed on an artificial test case where the proposed methodology is shown to outperform the usual finite element based dic . finally a fatigue tensile test on a thin aluminum sheet is analyzed . strain localization occurs after a certain number of cycles and combination of nurbs into a dic algorithm clearly shows a great potential to improve the robustness of non linear constitutive law identification .
stable computation of the functional variation of the dirichletneumann operator . <eos> this paper presents an accurate and stable numerical scheme for computation of the first variation of the dirichletneumann operator in the context of eulers equations for ideal free surface fluid flows . the transformed field expansion methodology we use is not only numerically stable , but also employs a spectrally accurate fourier chebyshev collocation method which delivers high fidelity solutions . this implementation follows directly from the authors previous theoretical work on analyticity properties of functional variations of dirichletneumann operators . these variations can be computed in a number of ways , but we establish , via a variety of computational experiments , the superior effectiveness of our new approach as compared with another popular boundary perturbation algorithm ( the method of operator expansions ) .
optimal tool selection for 2.5 d milling , part <digit> a solid modeling approach for construction of the voronoi mountain . <eos> cutter selection is a critical subtask of machining process planning . in this two part series , we develop a robust approach for the selection of an optimal set of milling cutters for a 2.5 d generalized pocket . in the first article ( part <digit> ) , we present a solid modeling approach for the construction of the voronoi mountain for the pocket geometry , which is a 3d extension of the voronoi diagram . the major contributions of this work include ( <digit> ) the development of a robust and systematic procedure for construction of the voronoi mountain for a multiply connected curvilinear polygon and ( b ) an extension of the voronoi mountain concept to handle open edges .
evaluating the novelty of text mined rules using lexical knowledge . <eos> in this paper , we present a new method of estimating the novelty of rules discovered by data mining methods using wordnet , a lexical knowledge base of english words . we assess the novelty of a rule by the average semantic distance in a knowledge hierarchy between the words in the antecedent and the consequent of the rule the more the average distance , more is the novelty of the rule . the novelty of rules extracted by the discotex text mining system on amazon.com book descriptions were evaluated by both human subjects and by our algorithm . by computing correlation coefficients between pairs of human ratings and between human and automatic ratings , we found that the automatic scoring of rules based on our novelty measure correlates with human judgments about as well as human judgments correlate with one another . text mining
visor vast independence system optimization routine . <eos> an algorithm is sketched that generates all k maximal independent sets and all m minimal dependent sets of an arbitrary independence system , based on a set of cardinality n having at most <digit> ( n ) subsets . with access to an oracle that decides if a set is independent or not . because the algorithm generates all those sets , it solves the problems of finding all maximum independent and minimum dependent sets . those problems are known to be impossible to solve in general in time polynomial in n , k , and m , and they are np hard . the algorithm proposed and used is efficient in the sense that it requires only o ( nk m ) or o ( k nm ) visits to the oracle , the nonpolynomial part is only related to bitstring comparisons and the like , which can be performed rather quickly and , to some degree , in parallel on a sequential machine . this complexity compares favorably with another algorithm that is o ( n ( <digit> ) k ( <digit> ) ) . the design of a computer routine that implements the algorithm in a highly optimized way is discussed . the routine behaves as expected , as is shown by numerical experiments on a range of randomly generated independence systems with n up to n <digit> . application on an engineering design problem with n <digit> shows the routine requires almost <digit> ( <digit> ) times less visits to the oracle than an exhaustive search , while the time spent in visiting the oracle is still significantly larger than that spent for all other computations together .
superconvergence in high order galerkin finite element methods . <eos> in this paper , we shall use local estimates to give the superconvergence of high order galerkin finite element method for the elliptic equation of second order with constant coefficients by using the symmetric technique and integral identity . we get improved superconvergence on the inner locally symmetric mesh with respect to a point x0 for rectangular and triangular meshes .
a review of design pattern mining techniques . <eos> the quality of a software system highly depends on its architectural design . high quality software systems typically apply expert design experience which has been captured as design patterns . as demonstrated solutions to recurring problems , design patterns help to reuse expert experience in software system design . they have been extensively applied in the industry . mining the instances of design patterns from the source code of software systems can assist in the understanding of the systems and the process of re engineering them . more importantly , it also helps to trace back to the original design decisions , which are typically missing in legacy systems . this paper presents a review on current techniques and tools for mining design patterns from source code or design of software systems . we classify different approaches and analyze their results in a comparative study . we also examine the disparity of the discovery results of different approaches and analyze possible reasons with some insight .
stabilization of second order nonholonomic systems in canonical chained form . <eos> stabilization of a class of second order nonholonomic systems in canonical chained form is investigated in this paper . first , the models of two typical second order nonholonomic systems , namely , a three link planar manipulator with the third joint unactuated , and a kinematic redundant manipulator with all joints free and driven by forces torques imposing on the end effector , are presented and converted to second order chained form by transformations of coordinate and input . a discontinuous control law is then proposed to stabilize all states of the system to the desired equilibrium point exponentially . computer simulation is given to show the effectiveness of the proposed controller .
truthful mechanisms for two range values variant of unrelated scheduling . <eos> in this paper , we consider a restricted variant of the scheduling problem , where the machines are the strategic players . for this multi parameter mechanism design problem , the only known truthful mechanisms use task independent allocation algorithms and only have approximation ratio o ( m ) n. nisan , ronen . algorithmic mechanism design ( extended abstract ) , in stoc '99 proceedings of the thirty first annual acm symposium on theory of computing , acm , new york , ny , usa , <digit> . pp. <digit> <digit> a. mu'alem , m. schapira , setting lower bounds on truthfulness extended abstract , in soda '07 proceedings of the eighteenth annual acm siam symposium on discrete algorithms , society for industrial and applied mathematics , philadelphia . pa , usa , <digit> , pp. <digit> <digit> p. lu , c. yu , an improved randomized truthful mechanism for scheduling unrelated machines , in 25th international symposium on theoretical aspects of computer science , stacs , <digit> , pp. <digit> <digit> p. lu , c. yu , randomized truthful mechanisms for scheduling unrelated machines , in c.h. papadimitriou , s. zhang ( eds . ) , proceedings of wine , in lecture notes in computer science , vol . <digit> , springer , <digit> , pp. <digit> <digit> . lavi and swamy first use the cycle monotone condition and design a <digit> approximation truthful mechanism for a two value variant in r. lavi , c. swamy , truthful mechanism design for multi dimensional scheduling via cycle monotonicity , in ec '07 proceedings of the 8th acm conference on electronic commerce , acm , new york , ny , usa , <digit> , pp. <digit> <digit> , where the processing time of task j on machine i , say t ( ij ) , can only be either a lower value l ( j ) or a higher value h ( j ) . we consider a generalized variant . where t ( ij ) lies in l ( j ) , l ( j ) ( <digit> epsilon ) boolean or h ( j ) , h ( j ) ( <digit> epsilon ) and epsilon is a parameter satisfying some condition . we consider two special cases , case a when h ( j ) l ( j ) > <digit> , for all ( j ) and case b when h ( j ) l ( j ) < <digit> , for all j , and give randomized truthful mechanisms with approximation ratio <digit> ( <digit> epsilon ) for both cases . based on these two cases ' results , we are also able to deal with the general case of our two range values scheduling problem . we use a combination of two mechanisms , which is also a novel method in mechanism design for scheduling problems , and finally we give a randomized truthful mechanism with approximation ratio <digit> ( <digit> epsilon ) . although the generalization seems a little incremental , we actually use a very novel technique in the key step of proving truthfulness for case a , as well as a new mechanism scheme for case b. besides , the results in this paper are the first truthful mechanisms with constant approximation ratios when a machine ( player ) can report infinitely possible values , which is quite different from the two value variant , in which only finite values are available . furthermore , together with lavi and swamy 's work , our results suggest that such a task dependent approach can really do much better for the scheduling unrelated machines problem . ( c ) <digit> elsevier b.v. all rights reserved .
a note on the relationships among certified discrete log cryptosystems . <eos> the certified discrete logarithm problem modulo p prime is a discrete logarithm problem under the conditions that the complete factorization of p <digit> is given and by which the base g is certified to be a primitive root mod p. for the cryptosystems based on the intractability of certified discrete logarithm problem , sakurai shizuya showed that breaking the diffie hellman key exchange scheme reduces to breaking the shamir <digit> pass key transmission scheme with respect to the expected polynomial time turing reducibility . in this paper , we show that we can remove randomness from the reduction above , and replace the reducibility with the polynomial time many one . since the converse reduction is known to hold with respect to the polynomial time many one reducibility , our result gives a stronger evidence for that the two schemes are completely equivalent as certified discrete log cryptosystems .
hydraulic performance of a large slanted axial flow pump . <eos> purpose the pump of the taipuhe pump station , larger flow discharge , lower head , is one of the largest <digit> slanted axial flow pumps in the world . however , few studies have been done for the larger slanted axial flow pump on safe operation . the purpose of this paper is to analyze the impeller elevation , unsteady flow , hydraulic thrust and the zero head flow characteristics of the pump . design methodology approach the flow field in and through the pump was analyzed numerically during the initial stages of the pump design process , then the entire flow passage through the pump was analyzed to calculate the hydraulic thrust to prevent damage to the bearings and improve the operating stability the zero head pump flow characteristics were analyzed to ensure that the pump will work reliably at much lower heads . findings the calculated results are in good agreement with experimental data for the pump elevation effects , the performance curve , pressure oscillations , hydraulic thrust and zero head performance . research limitations implications since it is assumed that there is no gap between blades and shroud , gap cavitations are beyond the scope of the paper . originality value the paper indicates the slanted axial flow pump characteristics including the characteristic curves , pressure fluctuations , hydraulic thrust and radial force for normal operating conditions and zero head conditions . it shows how to guarantee the pump safety operating by computational fluid dynamics .
high radix montgomery modular exponentiation on reconfigurable hardware . <eos> it is widely recognized that security issues will play a crucial role in the majority of future computer and communication systems . central tools for achieving system security are cryptographic algorithms . this contribution proposes arithmetic architectures which are optimized for modern field programmable gate arrays ( fpgas ) . the proposed architectures perform modular exponentiation with very long integers . this operation is at the heart of many practical public key algorithms such as rsa and discrete logarithm schemes . we combine a high radix montgomery modular multiplication algorithm with a new systolic array design . the designs are flexible , allowing any choice of operand and modulus . the new architecture also allows the use of high radices . unlike previous approaches , we systematically implement and compare several variants of our new architecture for different bit lengths . we provide absolute area and timing measures for each architecture . the results allow conclusions about the feasibility and time space trade offs of our architecture for implementation on commercially available fpgas . we found that 1,024 bit rsa decryption can be done in 3.1 ms with our fastest architecture .
combined use of supervised and unsupervised learning for power system dynamic security mapping . <eos> this paper proposes a new methodology which combines supervised and unsupervised learning for evaluating power system dynamic security . based on the concept of stability margin , pre fault power system conditions are assigned to the output neurons on the two dimensional grid with the growing hierarchical self organizing map technique ( ghsom ) via supervised artificial neural networks ( anns ) which perform an estimation of post fault power system state . the technique estimates the dynamic stability index that corresponds to the most critical value of synchronizing and damping torques of multimachine power systems . ann based pattern recognition is carried out with the growing hierarchical self organizing feature mapping in order to provide adaptive neural network architecture during its unsupervised training process . numerical tests , carried out on a ieee <digit> bus power system are presented and discussed . the analysis using such method provides accurate results and improves the effectiveness of system security evaluation .
0.35 mu m cmos t r switch for 2.4 ghz short range wireless applications . <eos> this paper describes the design and implementation of a transmit receive switch for 2.4 ghz ism band applications . the t r switch is implemented in a 0.35 mum bulk cmos process and it occupies <digit> mum . <digit> mum of die area . a parasitic mosfet model including bulk resistance is used to optimize the physical dimensions of the transistors with regard to insertion loss and isolation . the measured insertion loss is 1.3 db without port matching . simulations using measured s parameters indicate that an insertion loss of 0.8 db can be obtained with a conjugate match . the measured isolation is <digit> db and the maximum transmit power is <digit> dbm .
probabilistic equivalence checking of multiple valued functions . <eos> this paper describes a probabilistic method for verifying the equivalence of two multiple valued functions . each function is hashed to an integer code by transforming it to a integer valued polynomial and evaluating it for values of variables taken independently and uniformly at random from a finite field . since the polynomial is unique for a given function , if two hash codes are different , then the functions are not equivalent . however , if two hash codes are the same , the functions may or may not be equivalent , because different polynomials may happen to hash to the same code . thus , the method presented in this paper determines the equivalence of two functions with a known ( small ) probability of error , arising from collisions between inequivalent functions . such a method seems to be an attractive alternative for verifying functions that are too large to be handled by deterministic equivalence checking methods .
a model and environment for improving multimedia scholarly reading practices . <eos> the evolution of multimedia document production and diffusion technologies has lead to a significant spread of knowledge in form of pictures and recordings . however , scholarly reading tasks are still principally performed on textual contents . we argue that this is due to a lack of critical and structured tools ( <digit> ) to handle the wide spectrum of interpretive operations involved by the polymorphous scholarly reading process ( <digit> ) to perform these operations on a heterogeneous multimedia corpus . this firstly calls for identifying fundamental document requirements for such reading practices . then , we present a flexible model and a software environment which enable the reader to structure , annotate , link , fragment , compare , freely organise and spatially lay out documents , and to prepare the writing of their critical comment . we eventually discuss experiments with humanities scholars , and explore new academic reading practices which take advantage of document engineering principles such as multimedia document structuring , publication or sharing .
sensor selection for energy efficient ambulatory medical monitoring . <eos> epilepsy affects over three million americans of all ages . despite recent advances , more than <digit> % of individuals with epilepsy never achieve adequate control of their seizures . the use of a small , portable , non invasive seizure monitor could benefit these individuals tremendously . however , in order for such a device to be suitable for long term wear , it must be both comfortable and lightweight . typical state of the art non invasive seizure onset detection algorithms require <digit> scalp electrodes to be placed on the head . these electrodes are used to generate <digit> data streams , called channels . the large number of electrodes is inconvenient for the patient and processing <digit> channels can consume a considerable amount of energy , a problem for a battery powered device . in this paper , we describe an automated way to construct detectors that use fewer channels , and thus fewer electrodes . starting from an existing technique for constructing <digit> channel patient specific detectors , we use machine learning to automatically construct reduced channel detectors . we evaluate our algorithm on data from <digit> patients used in an earlier study . on average , our algorithm reduced the number of channels from <digit> to 4.6 while decreasing the mean fraction of seizure onsets detected from <digit> % to <digit> % . for <digit> out of the <digit> patients , there was no degradation in the detection rate . while the average detection latency increased from 7.8 s to 11.2 s , the average rate of false alarms per hour decreased from 0.35 to 0.19 . we also describe a prototype implementation of a single channel eeg monitoring device built using off the shelf components , and use this implementation to derive an energy consumption model . using fewer channels reduced the average energy consumption by <digit> % , which amounts to a 3.3 x increase in battery lifetime . finally , we show how additional energy savings can be realized by using a low power screening detector to rule out segments of data that are obviously not seizures . though this technique does not reduce the number of electrodes needed , it does reduce the energy consumption by an additional <digit> % .
process synchronization without long term interlock . <eos> a technique is presented for replacing long term interlocking of shared data by the possible repetition of unprivileged code in case a version number ( associated with the shared data ) has been changed by another process . four principles of operating system architecture ( which have desirable effects on the intrinsic reliability of a system ) are presented implementation of a system adhering to these principles requires that long term lockout be avoided .
application of an artificial immune system based fuzzy neural network to a rfid based positioning system . <eos> due to the rapid development of globalization , which makes supply chain management more complicated , more companies are applying radio frequency identification ( rfid ) , in warehouse management . the obvious advantages of rfid are its ability to scan at high speed , its penetration and memory . in addition to recycling , use of a rfid system can also reduce business costs , by indentifying the position of goods and picking carts . this study proposes an artificial immune system ( ais ) based fuzzy neural network ( fnn ) , to learn the relationship between the rfid signals and the picking cart 's position . since the proposed network has the merits of both ais and fnn . it is able to avoid falling into the local optimum and possesses a learning capability . the results of the evaluation of the model show that the proposed ais based fnn really can predict the picking cart position more precisely than conventional fnn and , unlike an artificial neural network , it is much easier to interpret the training results , since they are in the form of fuzzy if then rules . ( c ) <digit> elsevier ltd. all rights reserved .
single dimension multidimensional software pipelining for loops . <eos> traditionally , software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops . this paper proposes a three step approach , called single dimension software pipelining ( ssp ) , to software pipeline a loop nest at an arbitrary loop level that has a rectangular iteration space and contains no sibling inner loops in it . the first step identifies the most profitable loop level for software pipelining in terms of initiation rate , data reuse potential , or any other optimization criteria . the second step simplifies the multidimensional data dependence graph ( ddg ) of the selected loop level into a one dimensional ddg and constructs a one dimensional ( 1d ) schedule . based on the one dimensional schedule , the third step derives a simple mapping function that specifies the schedule time for the operation instances in the multidimensional loop . the classical modulo scheduling is subsumed by ssp as a special case . ssp is also closely related to hyperplane scheduling , and , in fact , extends it to be resource constrained . we prove that ssp schedules are correct and at least as efficient as those schedules generated by traditional modulo scheduling methods . we extend ssp to schedule imperfect loop nests , which are most common at the instruction level . multiple initiation intervals are naturally allowed to improve execution efficiency . feasibility and correctness of our approach are verified by a prototype implementation in the orc compiler for the ia <digit> architecture , tested with loop nests from livermore and spec2000 floating point benchmarks . preliminary experimental results reveal that , compared to modulo scheduling , software pipelining at an appropriate loop level results in significant performance improvement . software pipelining is beneficial even with prior loop transformations .
a geometric based method for recognizing overlapping polygonal shaped and semi transparent particles in gray tone images . <eos> a geometric based method is proposed to recognize the overlapping particles of different polygonal shapes such as rectangular , regular and or irregular prismatic particles in a gray tone image . the first step consists in extracting the salient corners , identified by their locations and orientations , of the overlapping particles . although there are certain difficulties like the perspective geometric projection , out of focus , transparency and superposition of the studied particles . then , a new clustering technique is applied to detect the shape by grouping its correspondent salient corners according to the geometric properties of each shape . a simulation process is carried out for evaluating the performance of the proposed method . then , it is particularly applied on a real application of batch cooling crystallization of the ammonium oxalate in pure water . the experimental results show that the method is efficient to recognize the overlapping particles of different shapes and sizes .
explicit dimension reduction and its applications . <eos> we construct a small set of explicit linear transformations mapping r n to r t , where t o ( log ( gamma ( <digit> ) ) epsilon ( <digit> ) ) , such that the l <digit> norm of any vector in r n is distorted by at most <digit> epsilon in at least a fraction of <digit> gamma of the transformations in the set . albeit the tradeoff between the size of the set and the success probability is suboptimal compared with probabilistic arguments , we nevertheless are able to apply our construction to a number of problems . in particular , we use it to construct an epsilon sample ( or pseudorandom generator ) for linear threshold functions on sn <digit> for epsilon o ( <digit> ) . we also use it to construct an epsilon sample for spherical digons in sn <digit> for epsilon o ( <digit> ) . this construction leads to an efficient oblivious derandomization of the goemans williamson max cut algorithm and similar approximation algorithms ( i.e. , we construct a small set of hyperplanes such that for any instance we can choose one of them to generate a good solution ) . our technique for constructing an epsilon sample for linear threshold functions on the sphere is considerably different than previous techniques that rely on k wise independent sample spaces .
capital one financial and a decade of experience with newly vulnerable markets some propositions concerning the competitive advantage of new entrants . <eos> market share and brand recognition have historically provided advantage to established players in mature industries . the success of capital one , an attacker in the mature credit card industry is therefore interesting , both to researchers and to executives developing strategies . a partial explanation is offered by the theory of newly vulnerable markets . the success of capital one can be partially attributed to its application of information based strategies to several newly vulnerable markets , allowing it to target and retain the most profitable customers . these strategies sustained double digit return on equity and double digit increase in sales volume and profits every year of our study .
scene analysis and geometric homology . <eos> during the last <digit> <digit> years there has been a dramatic revival of interest in applied geometric problems . geometers have reconsidered a number of questions in infinitesimal mechanics , questions treated by j.c. maxwell and l. cremona <digit> , <digit> , <digit> in <digit> <digit> , further developed under the banner of graphical statics <digit> , <digit> , but left largely untouched since the end of the nineteenth century . at the same time , computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis . a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal . the work of this group , reported in the pages of the journal structural topology <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> ( and elsewhere ) , was a biproduct of research on infinitesimal mechanics , using methods derived from graphical statics , as well as from exterior algebra and its modern offspring , the doubilet rota stein double algebra <digit> , <digit> . independently , huffman <digit> , duda and hart <digit> , and others recognized that maxwell 's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene . more recently , sugihara <digit> and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry . they wrote a software package capable of modifying a rough plane sketch , so as to make it a true projection of a 3d scene . the starting point of the during the last <digit> <digit> years there has been a dramatic revival of interest in applied geometric problems . geometers have reconsidered a number of questions in infinitesimal mechanics , questions treated by j.c. maxwell and l. cremona <digit> , <digit> , <digit> in <digit> <digit> , further developed under the banner of graphical statics <digit> , <digit> , but left largely untouched since the end of the nineteenth century . at the same time , computer scientists have come to recognize that the tools of graphical statics and of applied projective geometry are fundamental to research in scene analysis . a good deal of the recent revival of interest is due to the efforts of the structural topology research group at the university of montreal . the work of this group , reported in the pages of the journal structural topology <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> ( and elsewhere ) , was a biproduct of research on infinitesimal mechanics , using methods derived from graphical statics , as well as from exterior a <digit> , <digit> . independently , huffman <digit> , duda and hart <digit> , and others recognized that maxwell 's reciprocal figures could help in deciding whether a given plane image is the projection of a 3d polyhedral scene . more recently , sugihara <digit> and his colleagues in nagoya created what may be considered a pilot project for automated descriptive geometry . they wrote a software package capable of modifying a rough plane sketch , so as to make it a true projection of a 3d scene . the starting point of the projective geometric analysis of scenes is the observation that the set of all three dimensional realizations ( scenes ) having a given two dimensional projection ( a drawing , or image ) form a linear space . much information about an image , and about its possible spatial interpretations , can be obtained simply by calculating ( either locally or globally ) the linear dimension ( or rank ) of its linear space of scenes . in practice , the image is a pattern on a cathode ray tube , an aerial photograph , an engineer 's or architect 's drawing , or an x ray or nmr scan . the rank of its space of scenes will reveal whether there is ambiguity or uniqueness in the construction of its spatial interpretation , or whether such a construction is in fact impossible , as would be the case for a poorly conceived engineering drawing , or even in an otherwise correctly conceived drawing , if too many hypotheses are made concerning the 3d structure of the scene . calculation of the rank of the space of scenes having a given image should , in principle , be accomplished using simple combinatorial algorithms based on easily remembered rules of thumb . this is the goal , and it shows every sign of being achievable . the problem has , however , a certain degree of unavoidable difficulty . the requirement that a given image be an accurate projection of a non trivial ( non planar ) 3d scene imposes conditions on the image , conditions which are perhaps best described in terms of not always elementary constructions with straight edge and pencil . in this paper , we begin to sort out the interplay of these projective conditions by creating a new homology theory for geometric configurations . the new homology theory applies to geometric objects which are more rigid , less pliable , than the rubber sheets studied by the topology of henri poincar and his school . the passage to this higher degree of invariance is made possible by the creation of a homology theory with ( restricted ) vector , rather than ( unrestricted ) scalar , coefficients , or equivalently , by the use of a cohomology theory based on locally linear , rather than on locally constant , functions . we have verified that the new theory agrees with the cohomology theory for the sheaf of locally linear functions on a certain ( combinatorially defined ) topological space . the basic objects about which this new homology theory has something non trivial to say are extremely general . from the geometric point of view , they are simply finite sets of points in a projective space or finite sets of vectors in a vector space . in order to emphasize the departure we take from linear algebra as it is usually practiced , we should say that we study vector spaces with a selected basis , that is , concrete vector spaces , in their usual representation as spaces f p of functions from a set p into a field f. finally , we might say we are simply studying rectangular matrices . since such objects are found throughout applied mathematics , the resulting homology theory has a very broad range of potential application . indeed , potential applications of this new homology theory are to any domain where one is interested in the global behavior of systems determined locally by linear constraints .
imagesense towards contextual image advertising . <eos> the daunting volumes of community contributed media contents on the internet have become one of the primary sources for online advertising . however , conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the web page , without considering the inherent characteristics of visual contents . this article presents a contextual advertising system driven by images , which automatically associates relevant ads with an image rather than the entire text in a web page and seamlessly inserts the ads in the nonintrusive areas within each individual image . the proposed system , called imagesense , supports scalable advertising of , from root to node , web sites , pages , and images . in imagesense , the ads are selected based on not only textual relevance but also visual similarity , so that the ads yield contextual relevance to both the text in the web page and the image content . the ad insertion positions are detected based on image salience , as well as face and text detection , to minimize intrusiveness to the user . we evaluate imagesense on a large scale real world images and web pages , and demonstrate the effectiveness of imagesense for online image advertising .
construction and blind estimation of phase sequences for subcarrier phase control based papr reduction in ldpc coded ofdm systems . <eos> as described in this paper construction and blind estimation methods of phase sequences are proposed for subcarrier phase control based peak to average power ratio ( papr ) reduction in low density parity check ( ldpc ) coded orthogonal frequency division multiplexing ( ofdm ) systems . on the transmitter side phase sequence patterns are constructed based on a given parity check matrix . the papr of the ofdm signal is reduced by multiplying the constructed phase sequence selected from the same number of candidates as the number of weighting factor ( wf ) combinations in a partial transmit sequence ( pts ) method . on the receiver side the phase sequence is estimated blindly using the decoding function i e the most likely phase sequence among a limited number of possible phase sequence candidates is inferred by comparing the sum product calculation results of each candidate . computer simulation results show that papr of qpsk ofdm and 16qam ofdm signals can be reduced respectively by about <digit> <digit> db and <digit> <digit> db without marked degradation of the block error rate ( bler ) performance as compared to perfect estimation in an attenuated <digit> path rayleigh fading condition .
mathsat tight integration of sat and mathematical decision procedures . <eos> recent improvements in propositional satisfiability techniques ( sat ) made it possible to tackle successfully some hard real world problems ( e.g. , model checking , circuit testing , propositional planning ) by encoding into sat . however , a purely boolean representation is not expressive enough for many other real world applications , including the verification of timed and hybrid systems , of proof obligations in software , and of circuit design at rtl level . these problems can be naturally modeled as satisfiability in linear arithmetic logic ( lal ) , that is , the boolean combination of propositional variables and linear constraints over numerical variables . in this paper we present mathsat , a new , sat based decision procedure for lal , based on the ( known approach ) of integrating a state of the art sat solver with a dedicated mathematical solver for lal . we improve mathsat in two different directions . first , the top level line procedure is enhanced and now features a tighter integration between the boolean search and the mathematical solver . in particular , we allow for theory driven backjumping and learning , and theory driven deduction we use static learning in order to reduce the number of boolean models that are mathematically inconsistent we exploit problem clustering in order to partition mathematical reasoning and we define a stack based interface that allows us to implement mathematical reasoning in an incremental and backtrackable way . second , the mathematical solver is based on layering that is , the consistency of ( partial ) assignments is checked in theories of increasing strength ( equality and uninterpreted functions , linear arithmetic over the reals , linear arithmetic over the integers ) . for each of these layers , a dedicated ( sub ) solver is used . cheaper solvers are called first , and detection of inconsistency makes call of the subsequent solvers superfluous . we provide a through experimental evaluation of our approach , by taking into account a large set of previously proposed benchmarks . we first investigate the relative benefits and drawbacks of each proposed technique by comparison with respect to a reference option setting . we then demonstrate the global effectiveness of our approach by a comparison with several state of the art decision procedures . we show that the behavior of mathsat is often superior to its competitors , both on lal and in the subclass of difference logic .
strongly regular graphs with the ( <digit> ) vertex condition . <eos> the ( t ) vertex condition , for an integer ( t ge <digit> ) , was introduced by hestenes and higman ( siam am math soc proc <digit> <digit> , <digit> ) providing a combinatorial invariant defined on edges and non edges of a graph . finite rank <digit> graphs satisfy the condition for all values of ( t ) . moreover , a long standing conjecture of klin asserts the existence of an integer ( t_0 ) such that a graph satisfies the ( t_0 ) vertex condition if and only if it is a rank <digit> graph . we present the first infinite family of non rank <digit> strongly regular graphs satisfying the ( <digit> ) vertex condition . this implies that the klin parameter ( t_0 ) is at least <digit> . the examples are the point graphs of a certain family of generalized quadrangles .
( ( epsilon ) ) efficiency in difference vector optimization . <eos> the paper deals with the problem of characterizing pareto optima ( efficient solutions ) for the difference of two mappings vector valued in a finite or infinite dimensional preordered space . closely related to the well known optimality criterion of scalar dc optimization , a mixed vectorial condition is obtained in terms of both strong ( fenchel ) and weak ( pareto ) ( epsilon ) subdifferentials that completely characterizes the exact or approximate weak efficiency . this condition also allows to deal with some special restricted mappings . moreover , the condition established in the literature in terms of strong ( epsilon ) subdifferentials for characterizing the strongly efficient solutions ( usual optima ) , is shown here to remain valid without assuming that the objective space is order complete .
iterative visual clustering for unstructured text mining . <eos> this paper proposes the iterative visual clustering ( ivc ) on unstructured text sequences to form and evaluate keyword clusters , based on which users can use visual analysis , domain knowledge to discover knowledge in the text . the text sequence data are broken down into a list representative keywords after textual evaluation , and the keywords are then grouped to form keyword clusters via an iterative stochastic process and are visualized as distributions over the time lines . the visual evaluation model provides shape evaluations as quantitative tools and users ' interactions as qualitative tools to visually investigate the trends , patterns represented by the keyword clusters ' distributions . the keyword clustering model , guided by the feedback of visual evaluations , step wisely enumerates newer generations of keyword clusters and their patterns , therefore narrows down the search space . then the proposed ivc is applied onto nursing narratives and is able to identify interesting keyword clusters implying hidden knowledge regarding to the working patterns and environment of registered nurses . the loop of producing next generation of keyword clusters in ivc is driven and controlled by users ' perception , domain knowledge and interactions , and it is also guided by a stochastic search model . so both semantic and distribution features enable ivc to have significant applications as a text mining tool , on many other data sets , such as biomedical literatures .
economic growth , telecommunications development and productivity growth of the telecommunications sector evidence around the world . <eos> this paper studies the relationships between economic growth , telecommunications development and productivity growth of the telecommunications sector in different countries and regions of the world . in particular , this study assesses the impact of mobile telecommunications on economic growth and telecommunications productivity . the results indicate that there is a bidirectional relationship between real gross domestic product ( gdp ) and telecommunications development ( as measured by teledensity ) for european and high income countries . however , when the impact of mobile telecommunications development on economic growth is measured separately , the bi directional relationship is no longer restricted to european and high income countries . this study also finds that countries in the upper middle income group have achieved a higher average total factor productivity ( tfp ) growth than other countries . countries with competition and privatization in telecommunications have achieved a higher tfp growth than those without competition and privatization . the diffusion of mobile telecommunications services is found to be a significant factor that has improved the tfp growth of the telecommunications sector in central and eastern europe ( cee ) .
examining learning from text and pictures for different task types does the multimedia effect differ for conceptual , causal , and procedural tasks . <eos> the multimedia effect ( me ) is a well researched effect in the field of learning and instruction . in this article , two views that explain the me are compared . the outcome oriented view focuses on the beneficial effect of text and pictures on mental representations , whereas the process oriented view focuses on the beneficial effect of text and pictures for information processing . to contrast these views , the me sizes for different task types were compared ( i.e. , conceptual , causal , procedural tasks ) . whereas the outcome oriented view predicts no differences in me size , the process oriented view predicts that the me is largest in causal tasks , smaller in procedural tasks , and smallest in conceptual tasks . sixty five students learnt with text only or with text and pictures . task type and information source ( i.e. , whether the text , picture , or text and picture provided the answer to a post test question ) were varied within subjects . the results showed that , in line with the process oriented view , the me was smaller for conceptual tasks than for procedural tasks . contrary to the expectations , the me was larger in procedural tasks than in causal tasks . moreover , the pattern of results varied with information source . research and practical implications are described , so that pictures can be deployed optimally .
sufficient conditions for lambda ' optimality of graphs with small conditional diameter . <eos> a restricted edge cut s of a connected graph g is an edge cut such that g s has no isolated vertex . the restricted edge connectivity lambda ' ( g ) is the minimum cardinality over all restricted edge cuts . a graph is said to lambda ' optimal if lambda ' ( g ) xi ( g ) , where xi ( g ) denotes the minimum edge degree of g defined as xi ( g ) min d ( u ) d ( nu ) <digit> u nu is an element of e ( g ) . the p diameter of g measures how far apart a pair of subgraphs satisfying a given property p can be , and hence it generalizes the standard concept of diameter . in this paper we prove two kind of results , according to which property p is chosen . first , let d <digit> ( resp . d <digit> ) be the p diameter where p is the property that the corresponding subgraphs have minimum degree at least one ( resp . two ) . we prove that a graph with odd girth g is lambda ' optimal if d <digit> <digit> , being the minimum degree of g. using the property q of being vertices of g f we prove that a graph with girth g is not an element of <digit> , <digit> , <digit> is lambda ' optimal if this q diameter is at most <digit> ( g <digit> ) <digit> . ( c ) <digit> elsevier b.v. all rights reserved .
comparative analysis of clicks and judgments for ir evaluation . <eos> queries and click through data taken from search engine transaction logs is an attractive alternative to traditional test collections , due to its volume and the direct relation to end user querying . the overall aim of this paper is to answer the question how does click through data differ from explicit human relevance judgments in information retrieval evaluation we compare a traditional test collection with manual judgments to transaction log based test collections by using queries as topics and subsequent clicks as pseudo relevance judgments for the clicked results . specifically , we investigate the following two research questions firstly , are there significant differences between clicks and relevance judgments . earlier research suggests that although clicks and explicit judgments show reasonable agreement , clicks are different from static absolute relevance judgments . secondly , are there significant differences between system ranking based on clicks and based on relevance judgments this is an open question , but earlier research suggests that comparative evaluation in terms of system ranking is remarkably robust .
splitting the difference . <eos> so , nat ' ralists observe , a flea hath smaller fleas that on him prey and these have smaller still to bite 'em and so proceed ad infinitum . jonathan swift , on poetry a rhapsody , <digit>
s2 s <digit> quasicontinuous posets . <eos> in this paper , we consider a common generalization of both s2 s <digit> continuous posets and quasicontinuous domains , and we introduce new concepts of way below relations and s2 s <digit> quasicontinuous posets . the main results are ( <digit> ) the way below relation on an s2 s <digit> quasicontinuous poset has the interpolation property ( <digit> ) the <digit> <digit> topology on an s2 s <digit> quasicontinuous poset is completely regular ( <digit> ) a poset is s2 s <digit> continuous iff it is meet s2 s <digit> continuous and s2 s <digit> quasicontinuous .
does the polynomial hierarchy collapse if onto functions are invertible . <eos> the class tfnp , defined by megiddo and papadimitriou , consists of multivalued functions with values that are polynomially verifiable and guaranteed to exist . do we have evidence that such functions are hard , for example , if tfnp is computable in polynomial time does this imply the polynomial time hierarchy collapses by computing a multivalued function in deterministic polynomial time we mean on every input producing one of the possible values of the function on that input . we give a relativized negative answer to this question by exhibiting an oracle under which tfnp functions are easy to compute but the polynomial time hierarchy is infinite . we also show that relative to this same oracle , p not equal up and tfnp ( np ) functions are not computable in polynomial time with an np oracle .
unsupervised object segmentation with a hybrid graph model ( hgm ) . <eos> in this work , we address the problem of performing class specific unsupervised object segmentation , i.e. , automatic segmentation without annotated training images . object segmentation can be regarded as a special data clustering problem where both class specific information and local texture color similarities have to be considered . to this end , we propose a hybrid graph model ( hgm ) that can make effective use of both symmetric and asymmetric relationship among samples . the vertices of a hybrid graph represent the samples and are connected by directed edges and or undirected ones , which represent the asymmetric and or symmetric relationship between them , respectively . when applied to object segmentation , vertices are superpixels , the asymmetric relationship is the conditional dependence of occurrence , and the symmetric relationship is the color texture similarity . by combining the markov chain formed by the directed subgraph and the minimal cut of the undirected subgraph , the object boundaries can be determined for each image . using the hgm , we can conveniently achieve simultaneous segmentation and recognition by integrating both top down and bottom up information into a unified process . experiments on <digit> object classes ( 9,415 images in total ) show promising results .
dispersion free wave splittings for structural elements . <eos> wave splittings are derived for three types of structural elements membranes , timoshenko beams , and mindlin plates . the timoshenko beam equation and the mindlin plate equation are inherently dispersive , as is each fourier component of the membrane equation in an angular decomposition of the field . the distinctive feature of the wave splittings derived in the present paper is that , in homogeneous regions , they transform the dispersive wave equations into simple one way wave equations without dispersion . such splittings have uses both for radial scattering problems in the 2d cases and for scattering problems in dispersive media . as an example of how the splittings may be applied , a direct scattering problem is solved for a membrane with radially varying density . the imbedding method is utilized , and agreement is obtained with an fe simulation .
designing a practical data filter cache to improve both energy efficiency and performance . <eos> conventional data filter cache ( dfc ) designs improve processor energy efficiency , but degrade performance . furthermore , the single cycle line transfer suggested in prior studies adversely affects level <digit> data cache ( l1 dc ) area and energy efficiency . we propose a practical dfc that is accessed early in the pipeline and transfers a line over multiple cycles . our dfc design improves performance and eliminates a substantial fraction of l1 dc accesses for loads , l1 dc tag checks on stores , and data translation lookaside buffer accesses for both loads and stores . our evaluation shows that the proposed dfc can reduce the data access energy by 42.5 % and improve execution time by 4.2 % .
analytical model for anomalous positive bias temperature instability in la based hfo2 nfets based on independent characterization of charging components . <eos> pbti improvement in hfo2 nfets achieved by a controlled insertion of la . anomalous negative vth due to charge exchange between high k and metal gate . anomalous and conventional pbti components are decoupled and studied separately . analytical model including both components for lifetime extrapolation is presented .
a social behaviour evolution approach for evolutionary optimisation . <eos> evolutionary algorithms were originally designed to locate basins of optimum solutions in a stationary environment . therefore , additional techniques and modifications have been introduced to deal with further requirements such as handling dynamic fitness functions or finding multiple optima . in this paper , we present a new approach for building evolutionary algorithms that is based on concepts borrowed from social behaviour evolution . algorithms built with the proposed paradigm operate on a population of individuals that move in the search space as they interact and form groups . the interaction follows a set of social behaviours evolved by each group to enhance its adaptation to the environment ( and other groups ) and to achieve different desirable goals such as finding multiple optima , maintaining diversity , or tracking a moving peak in a changing environment . each group has two sets of behaviours one for intra group interactions and one for inter group interactions . these behaviours are evolved using mathematical models from the field of evolutionary game theory . this paper describes the proposed paradigm and starts studying it characteristics by building a new evolutionary algorithm and studying its behavior . the algorithm has been tested using a benchmark problem generator with promising initial results , which are also reported .
planar c1 c <digit> hermite interpolation with ph cuts of degree ( 1,3 ) ( <digit> , <digit> ) of laurent series . <eos> we introduce a new class of ph curves , ph cuts of degree ( 1,3 ) ( <digit> , <digit> ) of laurent series . we show how to find ph skew cut interpolants to a c1 c <digit> hermite data set . we show that two of these interpolants are short , simple curves with stable shape . our curves are fair with different shapes to those of other interpolants . we can obtain regular ph interpolants for collinear c1 c <digit> hermite data sets .
bicepstrum based blind identification of the acoustic emission ( ae ) signal in precision turning . <eos> it is believed that the acoustic emissions ( ae ) signal contains potentially valuable information for monitoring precision cutting processes , as well as to be employed as a control feedback signal . however , ae stress waves produced in the cutting zone are distorted by the transmission path and the measurement systems . in this article , a bicepstrum based blind system identification technique is proposed as a valid tool for estimating both , transmission path and sensor impulse response . assumptions under which application of bicepstrum is valid are discussed and diamond turning experiments are presented , which demonstrate the feasibility of employing bicepstrum for ae blind identification .
on solving hierarchical problems with top down control . <eos> we review recent work on the hierarchical if and only if problem and present a new hierarchical problem , hiff m that does not fit with previous explanations for evolutionary difficulty on hierarchical problems decomposed by levels for rmhc2 . rmhc2 is a hill climbing algorithm augmented with a multi level selection scheme . when used with the ideal sieve for a problem , as is done in this paper , rmhc2 exerts top down control on the evolutionary dynamics , in the sense that adaptation of higher levels are given priority over adaptation of lower levels , and creates stabilizing selection pressure with potential to increase evolvability . through hiff m , we discovered that the summary statistic , fitness distance correlation by level , is not a reliable indicator of when a hierarchical problem is solvable by rmhc2 , and that the two properties proposed to explain search easiness for rmhc2 are inadequate . our investigation of this anomaly led us to propose an additional property for hierarchical evolution difficulty under rmhc2 inter level conflict . we also discuss how hierarchical control can be subverted through the information transfer capacity of the transposition operation .
extracting semantic frames from thai medical symptom unstructured text with unknown target phrase boundaries . <eos> due to the limitations of language processing tools for the thai language , pattern based information extraction from thai documents requires supplementary techniques . based on sliding window rule application and extraction filtering , we present a framework for extracting semantic information from medical symptom phrases with unknown boundaries in thai unstructured text information entries . a supervised rule learning algorithm is employed for automatic construction of information extraction rules from hand tagged training symptom phrases . two filtering components are introduced one uses a classification model to predict rule application across a symptom phrase boundary based on instantiation features of rule internal wildcards , the other uses weighted classification confidence to resolve conflicts arising from overlapping extractions . in our experimental study , we focus our attention on two basic types of symptom phrasal descriptions one is concerned with abnormal characteristics of some observable entities and the other with human body locations at which primitive symptoms appear . the experimental results show that the filtering components improve precision while preserving recall satisfactorily .
robust doa estimation for uncorrelated and coherent signals . <eos> a new direction of arrival ( doa ) estimation method is introduced with arbitrary array geometry when uncorrelated and coherent signals coexist . the doas of uncorrelated signals are first estimated via subspace based high resolution doa estimation technique . then a matrix that only contains the information of coherent signals can be formulated by eliminating the contribution of uncorrelated signals . finally a subspace block sparse reconstruction approach is taken for doa estimations of the coherent signals .
checkpoint allocation and release . <eos> out of order speculative processors need a bookkeeping method to recover from incorrect speculation . in recent years , several microarchitectures that employ checkpoints have been proposed , either extending the reorder buffer or entirely replacing it . this work presents an in dept study of checkpointing in checkpoint based microarchitectures , from the desired content of a checkpoint , via implementation trade offs , and to checkpoint allocation and release policies . a major contribution of the article is a novel adaptive checkpoint allocation policy that outperforms known policies . the adaptive policy controls checkpoint allocation according to dynamic events , such as second level cache misses and rollback history . it achieves 6.8 % and 2.2 % speedup for the integer and floating point benchmarks , respectively , and does not require a branch confidence estimator . the results show that the proposed adaptive policy achieves most of the potential of an oracle policy whose performance improvement is 9.8 % and 3.9 % for the integer and floating point benchmarks , respectively . we exploit known techniques for saving leakage power by adapting and applying them to checkpoint based microarchitectures . the proposed applications combine to reduce the leakage power of the register file to about one half of its original value .
a quadratic spline approximation using detail multi layer for soft shadow generation in augmented reality . <eos> implementation of shadows is crucial to enhancement of images in ar environments . without shadows , virtual objects would look floating over the scene resulting in unrealistic rendering of ar environments . casting hard shadows would provide only spatial information while soft shadows help improve realism of ar environments . several algorithms have been proposed to render realistic shadows which often incurred high computational costs . little attention has been directed towards the balanced trade off between shadow quality and computational costs . in this study , two approaches are proposed quadratic spline interpolation ( qsi ) to soften the outline of the shadow and detail multi layer ( dml ) technique to optimize the volume of computations for the generation of soft shadows based on real light sources . qsi estimates boarder hard shadow samples while dml involves three main phases real light sources estimation , soft shadow production and reduction of the complexity of <digit> dimensional objects shadows . to be more precise , a reflective hemisphere is used to capture real light and to create an environment map . the median cut algorithm is implemented to locate the direction of real light sources on the environment map . subsequently , the original hard shadows are retrieved and a sample of multilayer hard shadows is produced where each layer has its unique size and colour . these layers overlap to produce soft shadows based on the real light sources directions . finally , the level of details ( lod ) algorithm is implemented to increase the efficiency of soft shadows by decreasing the complexity of vertex transformations . the proposed technique is tested using three samples of multilayer hard shadows with varying numbers of light sources generated from the median cut algorithm . the experimental results show that the proposed technique successfully produces realistic soft shadows at low computational costs .
the enhanced optical coupling in a quantum well infrared photodetector based on a resonant mode of an airdielectricmetal waveguide . <eos> the hybrid structure consisting of periodic gold stripes and an overlaying gold film is proposed to enhance the optical coupling of a quantum well infrared photodetector . an airdielectricmetal waveguide is formed when the hybrid structure is integrated on the top of the quantum well detector with the substrate being removed . finite difference time domain method is used to numerically obtain the reflection spectrum and the field distribution of the waveguide . the results show that a strong electric field component is induced in parallel to the growth direction of quantum well when the waveguide resonant mode occurs at the detective wavelength of the quantum well infrared photodetector . the relationship between the structural parameters and the resonant wavelength is derived by using the effective refractive index method of the airdielectricmetal waveguide . a high coupling efficiency can be obtained and the performance of the qwip can be greatly improved .
redirection based recovery for mpls network systems . <eos> to provide a reliable backbone network , fault tolerance should be considered in the network design . for a multiprotocol label switching ( mpls ) based backbone network , the fault tolerant issue focuses on how to protect the traffic of a label switched paths ( lsp ) against node and link failures . in ietf , two well known recovery mechanisms ( protection switching and rerouting ) have been proposed . to further enhance the fault tolerant performance of the two recovery mechanisms , the proposed approach utilizes the failure free lsps to transmit the traffic of the failed lsp ( the affected traffic ) . to avoid affecting the original traffic of each failure free lsp , the proposed approach applies the solution of the minimum cost flow to determine the amount of affected traffic to be transmitted by each failure free lsp . for transmitting the affected traffic along a failure free working lsp , ip tunneling technique is used . we also propose a permission token scheme to solve the packet disorder problem . finally , simulation experiments are performed to show the effectiveness of the proposed approach .
designing a cross language comparison shopping agent . <eos> this research pertains to the design and development of a shopbot called webshopper . this shopbot is intended to help customers find and compare e tailers that market their wares using different languages . webshopper is built with a multilingual ontology to overcome the language barriers that arise with global e commerce . this research proposes a semi automatic method of constructing a multilingual ontology by using the formal concept analysis and association analysis . it also proposes an automatic method for the categorization of product data into predefined classes , with the aim of alleviating administrators ' task load . additionally , a semantic search mechanism based on concept similarity is designed to assist customers in finding more desirable products . the experimental results show that these methods perform well and the shopbot can help customers find real bargains on the web and to find products that can not be bought locally .
medical informaticsthe state of the art in the hospital authority . <eos> since its inception in <digit> , the hospital authority ( ha ) has strongly supported the development and implementation of information systems both to improve the delivery of care and to make better information available to managers . this paper summarizes the progress to date and discusses current and future developments . following the first two phases of the ha information technology strategy the basic infrastructural elements were laid in place . these included the foundation administrative and financial systems and databases establishment of a wide area network linking all hospitals and clinics together laboratory , radiology and pharmacy systems with access to results in the ward . a major push into clinical systems began in <digit> with the clinical management system ( cms ) , which established a clinical workstation for use in both ward and ambulatory settings . the cms is now running at all major hospitals , and provides single logon access to almost all the electronically collected clinical data in the ha . the next phase of development is focussed on further support for clinical activities in the cms . key elements include the longitudinal electronic patient record ( epr ) , clinical order entry , generic support for clinical reports , broadening the scope to include allied health and the rehabilitative phase , clinical decision support , an improved clinical documentation framework , sharing of clinical information with other health care providers and a comprehensive data repository for analysis and reporting purposes .
prioritization of potential candidate disease genes by topological similarity of proteinprotein interaction network and phenotype data . <eos> we construct a reliable heterogeneous network by fusing multiple networks . we devise a random walk based algorithm on the reliable heterogeneous network . combining topological similarity with phenotype data helps to predict causal genes . the algorithm is still in good performance at low parameter values .
extended beta regression in r shaken , stirred , mixed , and partitioned . <eos> beta regression an increasingly popular approach for modeling rates and proportions is extended in various directions ( a ) bias correction reduction of the maximum likelihood estimator , ( b ) beta regression tree models by means of recursive partitioning , ( c ) latent class beta regression by means of finite mixture models . all three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved latent heterogeneity in the data . using the analogy of smithson and verkuilen ( <digit> ) , these extensions make beta regression not only a better lemon squeezer ( compared to classical least squares regression ) but a full fledged modern juicer offering lemon based drinks shaken and stirred ( bias correction and reduction ) , mixed ( finite mixture model ) , or partitioned ( tree model ) . all three extensions are provided in the r package betareg ( at least 2.4 <digit> ) , building on generic algorithms and implementations for bias correction reduction , model based recursive partioning , and finite mixture models , respectively . specifically , the new functions betatree ( ) and betamix ( ) reuse the object oriented flexible implementation from the r packages party and flexmix , respectively .
four layer framework for combinatorial optimization problems domain . <eos> four layer framework for combinatorial optimization problems models domain is suggested for applied problems structuring and solving ( <digit> ) basic combinatorial models and multicriteria decision making problems ( e.g. , clustering , knapsack problem , multiple choice problem , multicriteria ranking , assignment allocation ) ( <digit> ) composite models procedures ( e.g. , multicriteria combinatorial problems , morphological clique problem ) ( <digit> ) basic ( standard ) solving frameworks , e.g. ( i ) hierarchical morphological multicriteria design ( hmmd ) ( ranking , combinatorial synthesis based on morphological clique problem ) , ( ii ) multi stage design ( two level hmmd ) , ( iii ) special multi stage composite framework ( clustering , assignment location , multiple choice problem ) and ( <digit> ) domain oriented solving frameworks , e.g. ( a ) design of modular software , ( b ) design of test inputs for multi function system testing , ( c ) combinatorial planning of medical treatment , ( d ) design and improvement of communication network topology , ( e ) multi stage framework for information retrieval , ( f ) combinatorial evolution and forecasting of software , devices . the multi layer approach covers decision cycle , i.e. , problem statement , models , algorithms procedures , solving schemes , decisions , decision analysis and improvement .
computing monodromy via continuation methods on random riemann surfaces . <eos> we consider a riemann surface x defined by a polynomial f ( x , y ) of degree d , whose coefficients are chosen randomly . hence , we can suppose that x is smooth , that the discriminant delta ( x ) of f has d ( d <digit> ) simple roots , delta , and that delta ( <digit> ) not equal <digit> , i.e. the corresponding fiber has d distinct points y ( <digit> ) , ... , y ( d ) . when we lift a loop <digit> is an element of gamma subset of c delta by a continuation method , we get d paths in x connecting y ( <digit> ) , ... , y ( d ) , hence defining a permutation of that set . this is called monodromy . here we present experimentations in maple to get statistics on the distribution of transpositions corresponding to loops around each point of delta . multiplying families of neighbor transpositions , we construct permutations and the subgroups of the symmetric group they generate . this allows us to establish and study experimentally two conjectures on the distribution of these transpositions and on transitivity of the generated subgroups . assuming that these two conjectures are true , we develop tools allowing fast probabilistic algorithms for absolute multivariate polynomial factorization , under the hypothesis that the factors behave like random polynomials whose coefficients follow uniform distributions . ( c ) <digit> elsevier b.v. all rights reserved .
feature based decision aggregation in modular neural network classifiers . <eos> in several modular neural network ( mnn ) architectures , the individual decisions at the module level have to be integrated together using a voting scheme . all these voting schemes use the outputs of the individual modules to produce a global output without inferring explicit information from the problem feature space . this makes the choice of the aggregation procedure very subjective . in this work , a new mnn architecture will be presented . this architecture integrates learning into the voting scheme . we will be focusing on making the decision fusion a more dynamic process . in this context , dynamic means the aggregation procedure which has the flexibility to adapt to changes in the input . this approach requires the aggregation procedure to gather information about the input to help better understand how to dynamically aggregate decisions .
efficient bootstrap with weakly dependent processes . <eos> the efficient bootstrap methodology is developed for overidentified moment conditions models with weakly dependent observation . the resulting bootstrap procedure is shown to be asymptotically valid and can be used to approximate the distributions of t t statistics , the j j statistic for overidentifying restrictions , and wald , lagrange multiplier and distance statistics for nonlinear hypotheses . the asymptotic validity of the efficient bootstrap based on a computationally less demanding approximate k k step estimator is also shown . the finite sample performance of the proposed bootstrap is assessed using simulations in an intertemporal consumption based asset pricing model .
multi relay cooperative diversity protocol with improved spectral efficiency . <eos> cooperative diversity protocols have attracted a great deal of attention since they are thought to be capable of providing diversity multiplexing tradeoff among single antenna wireless devices . in the high signal to noise ratio ( snr ) region , cooperation is rarely required hence , the spectral efficiency of the cooperative protocol can be improved by applying a proper cooperation selection technique . in this paper , we present a simple cooperation selection technique based on instantaneous channel measurement to improve the spectral efficiency of cooperative protocols . we show that the same instantaneous channel measurement can also be used for relay selection . in this paper two protocols are proposed proactive and reactive the selection of one of these protocols depends on whether the decision of cooperation selection is made before or after the transmission of the source . these protocols can successfully select cooperation along with the best relay from a set of available m relays . if the instantaneous source to destination channel is strong enough to support the system requirements , then the source simply transmits to the destination as a noncooperative direct transmission otherwise , a cooperative transmission with the help of the selected best relay is chosen by the system . analysis and simulation results show that these protocols can achieve higher order diversity with improved spectral efficiency , i.e. , a higher diversity multiplexing tradeoff in a slow fading environment .
a motion tolerant dissolve detection algorithm . <eos> gradual shot change detection is one of the most important research issues in the field of video indexing retrieval . among the numerous types of gradual transitions , the dissolve type gradual transition is considered the most common one , but it is also the most difficult one to detect . in most of the existing dissolve detection algorithms , the false miss detection problem caused by motion is very serious . in this paper , we present a novel dissolve type transition detection algorithm that can correctly distinguish dissolves from disturbance caused by motion . we carefully model a dissolve based on its nature and then use the model to filter out possible confusion caused by the effect of motion . experimental results show that the proposed algorithm is indeed powerful .
gmm based evaluation of emotional style transformation in czech and slovak . <eos> in the development of the voice conversion and the emotional speech style transformation in the text to speech systems , it is very important to obtain feedback information about the users opinion on the resulting synthetic speech quality . for this reason , the evaluations of the quality of the produced synthetic speech must often be performed for comparison . the main aim of the experiments described in this paper was to find out whether the classifier based on gaussian mixture models ( gmms ) could be applied for evaluation of male and female resynthesized speech that had been transformed from neutral to four emotional states ( joy , surprise , sadness , and anger ) spoken in czech and slovak languages . we suppose that it is possible to combine this gmm based statistical evaluation with the classical one in the form of listening tests or it can replace them . for verification of our working hypothesis , a simple gmm emotional speech classifier with a one level structure was realized . the next task of the performed experiment was to investigate the influence of different types and values ( mean , median , standard deviation , relative maximum , etc. ) of the used speech features ( spectral and or supra segmental ) on the gmm classification accuracy . the obtained gmm evaluation scores are compared with the results of the conventional listening tests based on the mean opinion scores . in addition , correctness of the gmm classification is analyzed with respect to the influence of the setting of the parameters during the gmm trainingthe number of mixture components and the types of speech features . the paper also describes the comparison experiment with the reference speech corpus taken from the berlin database of emotional speech in german language as the benchmark for the evaluation of the performance of our one level gmm classifier . the obtained results confirm practical usability of the developed gmm classifier , so we will continue in this research with the aim to increase the classification accuracy and compare it with other approaches like the support vector machines .
collage of two dimensional words . <eos> we consider a new operation on one dimensional ( resp . two dimensional ) word languages , obtained by piling up , one on top of the other , words of a given recognizable language ( resp . two dimensional recognizable language ) on a previously empty one dimensional ( resp . two dimensional ) array . the resulting language is the set of words seen from above a position in the array is labeled by the topmost letter . we show that in the one dimensional case , the language is always recognizable . this is no longer true in the two dimensional case which is shown by a counter example , and we investigate in which particular cases the result may still hold . ( c ) <digit> published by elsevier b.v.
inference management , trust and obfuscation principles for quality of information in emerging pervasive environments . <eos> the emergence of large scale , distributed , sensor enabled , machine to machine pervasive applications necessitates engaging with providers of information on demand to collect the information , of varying quality levels , to be used to infer about the state of the world and decide actions in response . in these highly fluid operational environments , involving information providers and consumers of various degrees of trust and intentions , information transformation , such as obfuscation , is used to manage the inferences that could be made to protect providers from misuses of the information they share , while still providing benefits to their information consumers . in this paper , we develop the initial principles for relating to inference management and the role that trust and obfuscation plays in it within the context of this emerging breed of applications . we start by extending the definitions of trust and obfuscation into this emerging application space . we , then , highlight their role as we move from the tightly coupled to loosely coupled sensory inference systems and describe how quality , value and risk of information relate in collaborative and adversarial systems . next , we discuss quality distortion illustrated through a human activity recognition sensory system . we then present a system architecture to support an inference firewall capability in a publish subscribe system for sensory information and conclude with a discussion and closing remarks .
digging in the digg social news website . <eos> the rise of social media aggregating websites provides platforms where users can actively publish , evaluate , and disseminate content in a collaborative way . in this paper , we present a large scale empirical study about digg.com , one of the biggest social media aggregating websites . our analysis is based on crawls of 1.5 million users and <digit> million published stories on digg . we study the distinct network structure , the collaborative user characteristics , and the content dissemination process on digg . we empirically illustrate that friendship relations are used effectively in disseminating half of the content , although there exists a high overlap between the interests of friends . a successful content dissemination process can also be performed by random users who are browsing and digging stories . since <digit> % of the published content on digg is defined as news , it is important for the content to obtain sufficient votes in a short period of time before becoming obsolete . finally , we show that the synchronization of users ' activities in time is the key to a successful content dissemination process . the dynamics between users ' voting activities consequently decrease the efficiency of friendship relations during content dissemination . the results presented in this paper define basic observations and measurements to understand the underlying mechanism of disseminating content in current online social news aggregators . these findings are helpful to understand the influence of service interfaces and user behaviors on content dissemination .
usage of agents in document management . <eos> extensible java based agent framework ( xjaf ) is a pluggable architecture of the hierarchical intelligent agents system with communication based on kqml . workers , inc. is a workflow management system implemented using mobile agents . it is especially suited for highly distributed and heterogeneous environments . the application of the above mentioned systems will be considered in the area of document management systems .
ezpal environment for composing constraint axioms by instantiating templates . <eos> many ontology development tools allow users to supplement frame based representations with arbitrary logical sentences . however , few users actually take advantage of this opportunity . for example , in the ontolingua ontology library , only <digit> % of the ontologies have any user defined axioms . we believe the difficulty of composing axioms primarily accounts for the lack of axioms in these knowledge bases many domain experts can not translate their thoughts into abstract and symbolic representations . we attempt to remedy the difficulties by identifying groups of axioms that manifest common patterns , creating templates that allow users to compose axioms by filling in the blanks . we studied axioms in two public ontology libraries , and derived <digit> templates that cover <digit> % of all the user defined axioms . we describe our methodology for identifying the templates and present examples . we constructed an interface that allows users to create constraints on knowledge bases by filling in blanks our usability testing shows that users could use templates to encode axioms with a success rate similar to that of experts writing directly in an axiom language . our approach should foster the introduction of axioms and constraints that are currently missing in many ontologies .
critical infrastructure dependencies a holistic , dynamic and quantitative approach . <eos> the proper functioning of critical infrastructures is crucial to societal well being . however , critical infrastructures are not isolated , but instead are tightly coupled , creating a complex system of interconnected infrastructures . dependencies between critical infrastructures can cause a failure to propagate from one critical infrastructure to other critical infrastructures , aggravating and prolonging the societal impact . for this reason , critical infrastructure operators must understand the complexity of critical infrastructures and the effects of critical infrastructure dependencies . however , a major problem is posed by the fact that detailed information about critical infrastructure dependencies is highly sensitive and is usually not publicly available . moreover , except for a small number of holistic and dynamic research efforts , studies are limited to a few critical infrastructures and generally do not consider time dependent behavior . this paper analyzes how a failed critical infrastructure that can not deliver products and services impacts other critical infrastructures , and how a critical infrastructure is affected when another critical infrastructure fails . the approach involves a holistic analysis involving multiple critical infrastructures while incorporating a dynamic perspective based on the time period that a critical infrastructure is non operational and how the impacts evolve over time . this holistic approach , which draws on the results of a survey of critical infrastructure experts from several countries , is intended to assist critical infrastructure operators in preparing for future crises .
traffic distribution for end to end qos routing with multicast multichannel services . <eos> with the development of multimedia group applications and multicasting demands , the construction of multicast routing tree satisfying quality of service ( qos ) is more important . a multicast tree , which is constructed by existing multicast algorithms , suffers three major weaknesses ( <digit> ) it can not be constructed by multichannel routing , transmitting a message using all available links , thus the data traffic can not be preferably distributed ( <digit> ) it does not formulate duplication capacity consequently , duplication capacity in each node can not be optimally distributed ( <digit> ) it can not change the number of links and nodes used optimally . in fact , it can not employ and cover unused backup multichannel paths optimally . to overcome these weaknesses , this paper presents a polynomial time algorithm for distributed optimal multicast routing and quality of service ( qos ) guarantees in networks with multichannel paths which is called distributed optimal multicast multichannel routing algorithm ( dommr ) . the aim of this algorithm is ( <digit> ) to minimize end to end delay across the multichannel paths , ( <digit> ) to minimize consumption of bandwidth by using all available links , and ( <digit> ) to maximize data rate by formulating network resources . dommr is based on the linear programming formulation ( lpf ) and presents an iterative optimal solution to obtain the best distributed routes for traffic demands between all edge nodes . computational experiments and numerical simulation results will show that the proposed algorithm is more efficient than the existing methods . the simulation results are obtained by applying network simulation tools such as qsb , opnet and matlb to some samples of network . we then introduce a generalized problem , called the delay constrained multicast multichannel routing problem , and show that this generalized problem can be solved in polynomial time .
improved error exponent for time invariant and periodically time variant convolutional codes . <eos> an improved upper bound on the error probability ( first error event ) of time invariant convolutional codes , and the resulting error exponent , is derived ill this paper . the improved error bound depends on both the delay of the code k and its width ( the number of symbols that enter the delay line in parallel ) b. determining the error exponent of time invariant convolutional codes is an open problem . while the previously known bounds on the error probability of time invariant codes led to the block coding exponent , obtain a better error exponent ( strictly better for b > <digit> ) . in the limit b > infinity our error exponent equals the yudkin viterbi exponent derived for time variant convolutional codes . these results are also used to derive an improved error exponent for periodically time variant codes .
online reputation management for improving marketing by using a hybrid mcdm model . <eos> online reputation management ( orm ) has been considered as a significant tool of internet marketing . the purpose of this paper is to construct a decision model for evaluating performances and improving professional services of marketing . to investigate the interrelationship and influential weights among criteria , this study uses a hybrid mcdm model including decision making trial and evaluation laboratory ( dematel ) , dematel based analytic network process ( called danp ) . the empirical findings reveal that criteria have self effect relationships based on dematel technique . according to the network relation map ( nrm ) , the dimension that professional services of marketing should improve first when carrying out orm is online reputation . in the five criteria for evaluation , distributed reputation systems is the most important criterion impacting orm , followed by employees and social responsibility .
feasibility of a primarily digital research library . <eos> this position paper explores the issues related to the feasibility of having a primarily digital research library support the teaching and research needs of a university . the asian university for women ( auw ) , a new university in chittagong , bangladesh , will open in september <digit> . it must make a decision regarding the investment to be made in research resources to support the university . mass digitization efforts now make it possible to consider establishing a research library that consists primarily of digital resources rather than print . there are , however , many issues that make this consideration quite complex and far from certain . in this paper we explore the issues at a preliminary level . we focus on four broad perspectives in order to begin addressing the complex interactions that must be considered in transitioning to a primarily digital research environment technical , economic , policy and social issues . the purpose of this paper is to begin to explore a research agenda for transitioning from a model for libraries where resources are primarily print to one that is predominantly digital . our research in this area is just beginning , so our purpose is to raise the issues rather than offer firm conclusions .
targeting multiple myeloma cells and their bone marrow microenvironment . <eos> although multiple myeloma ( mm ) is sensitive to chemotherapy and radiation therapy , long term disease free survival is rare , and mm remains incurable despite conventional and high dose therapies . direct ( cell cell contact ) and soluble ( via cytokines ) forms of interactions between mm cells and bone marrow stroma regulate growth , survival , and homing of mm cells . these interactions also play a critical role in angiogenesis and in myeloma bone disease . in recent years , several studies have established the biologic significance of cytokines in mm pathogenesis and delineated signaling cascades mediating their effects , providing the framework for related novel therapies targeting not only the mm cell , but also the bone marrow microenvironment .
a network service curve approach for the stochastic analysis of networks . <eos> the stochastic network calculus is an evolving new methodology for backlog and delay analysis of networks that can account for statistical multiplexing gain . this paper advances the stochastic network calculus by deriving a network service curve , which expresses the service given to a flow by the network as a whole in terms of a probabilistic bound . the presented network service curve permits the calculation of statistical end to end delay and backlog bounds for broad classes of arrival and service distributions . the benefits of the derived service curve are illustrated for the exponentially bounded burstiness ( ebb ) traffic model . it is shown that end to end performance measures computed with a network service curve are bounded by o ( h log h ) , where h is the number of nodes traversed by a flow . using currently available techniques that compute end to end bounds by adding single node results , the corresponding performance measures are bounded by o ( h <digit> ) .
mitigating kinematic locking in the material point method . <eos> the material point method exhibits kinematic locking when traditional linear shape functions are used with a rectangular grid . the locking affects both the strain and the stress fields , which can lead to inaccurate results and nonphysical behavior . this paper presents a new anti locking approach that mitigates the accumulation of fictitious strains and stresses , significantly improving the kinematic response and the quality of all field variables . the technique relies on the huwashizu multi field variational principle , with separate approximations for the volumetric and the deviatoric portions of the strain and stress fields . the proposed approach is validated using a series of benchmark examples from both solid and fluid mechanics , demonstrating the broad range of modeling possibilities within the mpm framework when combined with appropriate anti locking techniques and algorithms .
computational dialectic and rhetorical invention . <eos> this paper has three dimensions , historical , theoretical and social . the historical dimension is to show how the ciceronian system of dialectical argumentation served as a precursor to computational models of argumentation schemes such as araucaria and carneades . the theoretical dimension is to show concretely how these argumentation schemes reveal the interdependency of rhetoric and logic , and so the interdependency of the normative with the empirical . it does this by identifying points of disagreement in a dialectical format through using argumentation schemes and critical questions . the social dimension is to show how the ciceronian dialectical viewpoint integrates with the use of computational tools that can be used to support the principle of reason based deliberation and facilitate deliberative democracy .
the roadmaker 's algorithm for the discrete pulse transform . <eos> the discrete pulse transform ( dpt ) is a decomposition of an observed signal into a sum of pulses , i.e. , signals that are constant on a connected set and zero elsewhere . originally developed for <digit> d signal processing , the dpt has recently been generalized to more dimensions . applications in image processing are currently being investigated . the time required to compute the dpt as originally defined via the successive application of lulu operators ( members of a class of minimax filters studied by rohwer ) has been a severe drawback to its applicability . this paper introduces a fast method for obtaining such a decomposition , called the roadmaker 's algorithm because it involves filling pits and razing bumps . it acts selectively only on those features actually present in the signal , flattening them in order of increasing size by sub tracing an appropriate positive or negative pulse , which is then appended to the decomposition . the implementation described here covers <digit> d signal as well as two and <digit> d image processing in a single framework . this is achieved by considering the signal or image as a function defined on a graph , with the geometry specified by the edges of the graph . whenever a feature is flattened , nodes in the graph are merged , until eventually only one node remains . at that stage , a new set of edges for the same nodes as the graph , forming a tree structure , defines the obtained decomposition . the roadmaker 's algorithm is shown to be equivalent to the dpt in the sense of obtaining the same decomposition . however , its simpler operators are not in general equivalent to the lulu operators in situations where those operators are not applied successively . a by product of the roadmaker 's algorithm is that it yields a proof of the so called highlight conjecture , stated as an open problem in <digit> . we pay particular attention to algorithmic details and complexity , including a demonstration that in the <digit> d case , and also in the case of a complete graph , the roadmaker 's algorithm has optimal complexity it runs in time o ( m ) , where m is the number of arcs in the graph .
implementation relations and test generation for systems with distributed interfaces . <eos> some systems interact with their environment at physically distributed interfaces called ports and we separately observe sequences of inputs and outputs at each port . as a result we can not reconstruct the global sequence that occurred and this reduces our ability to distinguish different systems in testing or in use . in this paper we explore notions of conformance for an input output transition system that has multiple ports , adapting the widely used ioco implementation relation to this situation . we consider two different scenarios . in the first scenario the agents at the different ports are entirely independent . alternatively , it may be feasible for some external agent to receive information from more than one of the agents at the ports of the system , these local behaviours potentially being brought together and here we require a stronger implementation relation . we define implementation relations for these scenarios and prove that in the case of a single port system the new implementation relations are equivalent to ioco . in addition , we define what it means for a test case to be controllable and give an algorithm that decides whether this condition holds . we give a test generation algorithm to produce sound and complete test suites . finally , we study two implementation relations to deal with partially specified systems .
dual centers type <digit> fuzzy clustering framework and its verification and validation indices . <eos> the clustering model considers dual centers rather than single centers . the dual centers type <digit> clustering model and algorithm are proposed . the relations among parameters of the proposed model are explained . the degrees of belonging to the clusters are defined by type <digit> fuzzy numbers . the verification and verification indices are developed for model evaluation .
generalized sharing in survivable optical networks . <eos> shared path protection has been demonstrated to be a very efficient survivability scheme for optical networking . in this scheme , multiple backup paths can share a given optical channel if their corresponding primary routes are not expected to fail simultaneously . the focus in this area has been the optimization of the total channels ( i.e. , bandwidth ) provisioned in the network through the intelligent routing of primary and backup routes . in this work , we extend the current path protection sharing scheme and introduce the generalized sharing concept . in this concept , we allow for additional sharing of important node devices . these node devices ( e.g. , optical electronic optical regenerators ( oeos ) , pure all optical converters , etc. ) constitute the dominant cost factor in an optical backbone network and the reduction of their number is of paramount importance . for demonstration purposes , we extend the concept of <digit> n shared path protection to allow for the sharing of electronic regenerators needed for coping with optical transmission impairments . both design and control plane issues are discussed through numerical examples . considerable cost reductions in electronic budget are demonstrated .
on the usefulness of knowledge of error variances in the consistent estimation of an unreplicated ultrastructural model . <eos> this article considers an unreplicated ultrastructural model and discusses the asymptotic properties of three consistent estimators of slope parameter arising from the knowledge of measurement error variances . conditions are deduced when knowing the error variances associated with both the study and the explanatory variables is more less beneficial than using a single error variance in the formulation of slope estimators .
exact solution of the heat equation with boundary condition of the fourth kind by hes variational iteration method . <eos> in this paper , solutions of the heat equation with the boundary condition of the fourth kind are presented . the proposed solution is based on hes variational iteration method , after the application of which the exact solution of the problem is obtained .
efficient neighborhood search for the one machine earlinesstardiness scheduling problem . <eos> this paper addresses the one machine scheduling problem where the objective is to minimize a sum of costs such as earlinesstardiness costs . since the sequencing problem is np hard , local search is very useful for finding good solutions . unlike scheduling problems with regular cost functions , the scheduling ( or timing ) problem is not trivial when the sequence is fixed . therefore , the local search approaches must deal with both job interchanges in the sequence and the timing of the sequenced jobs . we present a new approach that efficiently searches in a large neighborhood and always returns a solution for which the timing is optimal .
power characteristics of inductive interconnect . <eos> the width of an interconnect line affects the total power consumed by a circuit . the effect of wire sizing on the power characteristics of an inductive interconnect line is presented in this paper . the matching condition between the driver and the load affects the power consumption since the short circuit power dissipation may decrease and the dynamic power will increase with wider lines . a tradeoff , therefore , exists between short circuit and dynamic power in inductive interconnects . the short circuit power increases with wider linewidths only if the line is underdriven . the power characteristics of inductive interconnects therefore may have a great influence on wire sizing optimization techniques . an analytic solution of the transition time of a signal propagating along an inductive interconnect with an error of less than <digit> % is presented . the solution is useful in wire sizing synthesis techniques to decrease the overall power dissipation . the optimum linewidth that minimizes the total transient power dissipation is determined . an analytic solution for the optimum width with an error of less than <digit> % is presented . for a specific set of line parameters and resistivities , a reduction in power approaching <digit> % is achieved as compared to the minimum wire width . considering the driver size in the design process , the optimum wire and driver size that minimizes the total transient power is also determined .
convergence acceleration of rungekutta schemes for solving the navierstokes equations . <eos> the convergence of a rungekutta ( rk ) scheme with multigrid is accelerated by preconditioning with a fully implicit operator . with the extended stability of the rungekutta scheme , cfl numbers as high as <digit> can be used . the implicit preconditioner addresses the stiffness in the discrete equations associated with stretched meshes . this rk implicit scheme is used as a smoother for multigrid . fourier analysis is applied to determine damping properties . numerical dissipation operators based on the roe scheme , a matrix dissipation , and the cusp scheme are considered in evaluating the rk implicit scheme . in addition , the effect of the number of rk stages is examined . both the numerical and computational efficiency of the scheme with the different dissipation operators are discussed . the rk implicit scheme is used to solve the two dimensional ( <digit> d ) and three dimensional ( <digit> d ) compressible , reynolds averaged navierstokes equations . turbulent flows over an airfoil and wing at subsonic and transonic conditions are computed . the effects of the cell aspect ratio on convergence are investigated for reynolds numbers between 5.7106 5.7 <digit> <digit> and <digit> <digit> <digit> <digit> . it is demonstrated that the implicit preconditioner can reduce the computational time of a well tuned standard rk scheme by a factor between <digit> and <digit> .
realizations of the game domination number . <eos> domination game is a game on a finite graph which includes two players . first player , dominator , tries to dominate a graph in as few moves as possible meanwhile the second player , staller , tries to hold him back and delay the end of the game as long as she can . in each move at least one additional vertex has to be dominated . the number of all moves in the game in which dominator makes the first move and both players play optimally is called the game domination number and is denoted by ( gamma _ g ) . the total number of moves in a staller start game is denoted by ( gamma _ g prime ) . it is known that ( gamma _ g ( g ) gamma _ g prime ( g ) le <digit> ) for any graph ( g ) . graph ( g ) realizes a pair ( ( k , l ) ) if ( gamma _ g ( g ) k ) and ( gamma _ g prime ( g ) l ) . it is shown that pairs ( ( 2k ,2 k <digit> ) ) for all ( k ge <digit> ) can be realized by a family of <digit> connected graphs . we also present <digit> connected classes which realize pairs ( ( k , k ) ) and ( ( k , k <digit> ) ) . exact game domination number for combs and <digit> connected realization of the pair ( ( 2k 1,2 k ) ) are also given .
lumiproxy a hybrid representation of image based models . <eos> in this paper , we present a hybrid representation of image based models combining the textured planes and the hierarchical points . taking a set of depth images as input , our method starts from classifying input pixels into two categories , indicating the planar and non planar surfaces respectively . for the planar surfaces , the geometric coefficients are reconstructed to form the uniformly sampled textures . for nearly planar surfaces , some textured planes , called lumiproxies , are constructed to represent the equivalent visual appearance . the hough transform is used to find the positions of these textured planes , and optic flow measures are used to determine their textures . for remaining pixels corresponding to the non planar geometries , the point primitive is applied , reorganized as the obb tree structure . then , texture mapping and point splatting are employed together to render the novel views , with the hardware acceleration .
constraint based methods for biological sequence analysis . <eos> the need for processing biological information is rapidly growing , owing to the masses of new information in digital form being produced at this time . old methodologies for processing it can no longer keep up with this rate of growth . the methods of artificial intelligence ( ai ) in general and of language processing in particular can offer much towards solving this problem . however , interdisciplinary research between language processing and molecular biology is not yet widespread , partly because of the effort needed for each specialist to understand the other one 's jargon . we argue that by looking at the problems of molecular biology from a language processing perspective , and using constraint based logic methodologies we can shorten the gap and make interdisciplinary collaborations more effective . we shall discuss several sequence analysis problems in terms of constraint based formalisms such concept formation rules , constraint handling rules ( chr ) and their grammatical counterpart , chrg . we postulate that genetic structure analysis can also benefit from these methods , for instance to reconstruct from a given rna secondary structure , a nucleotide sequence that folds into it . our proposed methodologies lend direct executability to high level descriptions of the problems at hand and thus contribute to rapid while efficient prototyping .
adaptive load balancing algorithm for multiple homing mobile nodes . <eos> in places where mobile users can access multiple wireless networks simultaneously , a multipath scheduling algorithm can benefit the performance of wireless networks and improve the experience of mobile users . however , existing literature shows that it may not be the case , especially for tcp flows . according to early investigations , there are mainly two reasons that result in bad performance of tcp flows in wireless networks . one is the occurrence of out of order packets due to different delays in multiple paths . the other is the packet loss which is resulted from the limited bandwidth of wireless networks . to better exploit multipath scheduling for tcp flows , this paper presents a new scheduling algorithm named adaptive load balancing algorithm ( albam ) to split traffic across multiple wireless links within the isp infrastructure . targeting at solving the two adverse impacts on tcp flows , albam develops two techniques . firstly , albam takes advantage of the bursty nature of tcp flows and performs scheduling at the flowlet granularity where the packet interval is large enough to compensate for the different path delays . secondly , albam develops a packet number estimation algorithm ( pnea ) to predict the buffer usage in each path . with pnea , albam can prevent buffer overflow and schedule the tcp flow to a less congested path before it suffers packet loss . simulations show that albam can provide better performance to tcp connections than its other counterparts .
query by output . <eos> it has recently been asserted that the usability of a database is as important as its capability . understanding the database schema , the hidden relationships among attributes in the data all play an important role in this context . subscribing to this viewpoint , in this paper , we present a novel data driven approach , called query by output ( qbo ) , which can enhance the usability of database systems . the central goal of qbo is as follows given the output of some query q on a database d , denoted by q ( d ) , we wish to construct an alternative query q such that q ( d ) and q ( d ) are instance equivalent . to generate instance equivalent queries from q ( d ) , we devise a novel data classification based technique that can handle the at least one semantics that is inherent in the query derivation . in addition to the basic framework , we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility . our framework is evaluated comprehensively on three real data sets and the results show that the instance equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities .
three dimensional quantitative structureactivity relationships study on hiv <digit> reverse transcriptase inhibitors in the class of dipyridodiazepinone derivatives , using comparative molecular field analysis1 . <eos> a three dimensional quantitative structureactivity relationships ( 3d qsar ) method , comparative molecular field analysis ( comfa ) , was applied to a set of dipyridodiazepinone ( nevirapine ) derivatives active against wild type ( wt ) and mutant type ( y181c ) hiv <digit> reverse transcriptase . the starting geometry of dipyridodiazepinone was taken from x ray crystallographic data . all <digit> derivatives , divided into a training set of <digit> compounds and a test set of <digit> molecules , were then constructed and full geometrical optimizations were performed , based on a semiempirical molecular orbital method ( am1 ) . comfa was used to discriminate between structural requirements for wt and y181c inhibitory activities . the resulting comfa models yield satisfactory predictive ability regarding wt and y181c inhibitions , with r2cv 0.624 and 0.726 , respectively . comfa contour maps reveal that steric and electrostatic interactions corresponding to the wt inhibition amount to 58.5 % and 41.5 % , respectively , while steric and electrostatic effects have approximately equal contributions for the explanation of inhibitory activities against y181c . the contour maps highlight different characteristics for different types of wild type and mutant type hiv <digit> rt. in addition , these contour maps agree with experimental data for the binding topology . consequently , the results obtained provide information for a better understanding of the inhibitorreceptor interactions of dipyridodiazepinone analogs . <digit> elsevier science inc .
a shared memory implementation of the hierarchical radiosity method . <eos> the radiosity method is a simulation method from computer graphics to visualize the global illumination in scenes containing diffuse objects within an enclosure . a variety of realizations ( including parallel approaches ) were proposed to achieve a high efficiency while guaranteeing the same accuracy of the graphical representation . the hierarchical radiosity method reduces the computational costs considerably but results in a highly irregular algorithm which makes a parallel implementation more difficult . we investigate a task oriented shared memory implementation and present optimizations with different behavior concerning locality and granularity . to be able to concentrate on load balancing and scalability issues , we use a shared memory machine with uniform memory access time , the sb pram . ( c ) <digit> elsevier science b.v. all rights reserved .
a transaction mapping algorithm for frequent itemsets mining . <eos> in this paper , we present a novel algorithm for mining complete frequent itemsets . this algorithm is referred to as the tm ( transaction mapping ) algorithm from hereon . in this algorithm , transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space and the counting of itemsets is performed by intersecting these interval lists in a depth first order along the lexicographic tree . when the compression coefficient becomes smaller than the average number of comparisons for intervals intersection at a certain level , the algorithm switches to transaction id intersection . we have evaluated the algorithm against two popular frequent itemset mining algorithms , fp growth and declat , using a variety of data sets with short and long frequent patterns . experimental data show that the tm algorithm outperforms these two algorithms .
semi continuous network flow problems . <eos> we consider semi continuous network flow problems , that is , a class of network flow problems where some of the variables are restricted to be semi continuous . we introduce the semi continuous inflow set with variable upper bounds as a relaxation of general semi continuous network flow problems . two particular cases of this set are considered , for which we present complete descriptions of the convex hull in terms of linear inequalities and extended formulations . we consider a class of semi continuous transportation problems where inflow systems arise as substructures , for which we investigate complexity questions . finally , we study the computational efficacy of the developed polyhedral results in solving randomly generated instances of semi continuous transportation problems .
construct message authentication code with one way hash functions and block ciphers . <eos> we suggest an mac scheme which combines a hash function and an block cipher in order . we strengthen this scheme to prevent the problem of leaking the intermediate hash value between the hash function and the block cipher by additional random bits . the requirements to the used hash function are loosely . security of the proposed scheme is heavily dependent on the underlying block cipher . this scheme is efficient on software implementation for processing long messages and has clear security properties .
estimation of uncertainty in dynamic simulation results . <eos> this paper presents a new approach for calculation of uncertainty in dynamic simulation results . the statistical moments ( mean , variance , skewness etc. ) of the simulation results are calculated using gaussian quadrature with '' customized '' weight function . based on these moments , an approximating probability density function ( pdf ) is created by expansion into orthogonal polynomial series . the percentiles of the distribution can then be calculated . the method is computationally less demanding than monte carlo simulation when the number of uncertain parameters are limited . a number of examples are used to illustrate the applicability of the proposed framework .
a neural implementation of the jade algorithm ( njade ) using higher order neurons . <eos> a neural implementation of the jade algorithm , called njade , is developed which adaptively determines the mixing matrices to be jointly diagonalized with the jade algorithm . this alleviates the problem of algebraically determining these mixing matrices which becomes a very tedious if not impossible undertaking with high dimensional data . the new learning rule uses higher order neurons and generalizes oja 's pca learning rule . as a test case the new njade algorithm is applied to high dimensional natural image ensembles to learn appropriate edge filter structures . quantitative comparison concerning various filter characteristics is made with results obtained with a probabilistic ica algorithm with kernel based source density estimation .
jpeg <digit> encoding method for reducing tiling artifacts . <eos> this paper proposes an effective jpeg <digit> encoding method for reducing tiling artifacts , which cause one of the biggest problems in jpeg <digit> encoders . symmetric pixel extension is generally thought to be the main factor in causing artifacts . however this paper shows that differences in quantization accuracy between tiles are a more significant reason for tiling artifacts at middle or low bit rates . this paper also proposes an algorithm that predicts whether tiling artifacts will occur at a tile boundary in the rate control process and that locally improves quantization accuracy by the original post quantization control . this paper further proposes a method for reducing processing time which is yet another serious problem in the jpeg <digit> encoder . the method works by predicting truncation points using the entropy of wavelet transform coefficients prior to the arithmetic coding . these encoding methods require no additional processing in the decoder . the experiments confirmed that tiling artifacts were greatly reduced and that the coding process was considerably accelerated .
laparoscopic myomectomy . <eos> the appearance of uterine myomas has been linked to infertility . it has been suggested that surgical management of myomas by laparoscopic myomectomy improves fertility rates in these group of patients . in this paper we initially describe specific aspects of the surgical technique of laparoscopic myomectomy including the set up , precise technique for hysteroromy , enucleation of the myoma , suturing of the uterus , and extraction of the myoma . we detail recent findings that demonstrate improved fertility rates in women undergoing laparoscopic myomectomy . we recommend that , when criteria for selection of patients is strictly adhered to and patients present with no other associated infertility , laparoscopic myomectomy be used to increase the implantation rate .
theoretical study on the antioxidant properties of <digit> ' hydroxychalcones h atom vs. electron transfer mechanism . <eos> the free radical scavenging activity of six <digit> ' hydroxychalcones has been studied in gas phase and solvents using the density functional theory ( dft ) method . the three main working mechanisms , hydrogen atom transfer ( hat ) , stepwise electron transfer proton transfer ( et pt ) and sequential proton loss electron transfer ( splet ) have been considered . the o h bond dissociation enthalpy ( bde ) , ionization potential ( ip ) , proton affinity ( pa ) and electron transfer energy ( ete ) parameters have been computed in gas phase and solvents . the theoretical results confirmed the important role of the b ring in the antioxidant properties of hydroxychalcones . in addition , the calculated results matched well with experimental values . the results suggested that hat would be the most favorable mechanism for explaining the radical scavenging activity of hydroxychalcone in gas phase , whereas splet mechanism is thermodynamically preferred pathway in aqueous solution .
program analysis for event based distributed systems . <eos> designing distributed applications around the idiom of events has several benefits including extensibility and scalability . to improve conciseness , safety , and efficiency of corresponding programs , several authors have recently proposed programming languages or language extensions with support for event based programming . the presence of a dedicated programming language and compilation process offers avenues for program analyses to further improve simplicity , safety , and expressiveness of distributed event based software . this paper presents three program analyses specifically designed for event based programs immutability analysis avoids costly cloning of events in the presence of co located handlers for same events guard analysis allows for simple yet expressive subscriptions which can be further simplified and handled efficiently causality analysis determines causal dependencies among events which are related , allowing unrelated events to be transferred independently for efficiency . we convey the benefits of our approach by empirically evaluating their performance benefits .
automatic determination of envelopes and other derived curves within a graphic environment . <eos> dynamic geometry programs provide environments where accurate construction of geometric configurations can be done . nevertheless , intrinsic limitations in their standard development technology mostly produce objects that are equationally unknown and so can not be further used in constructions . in this paper , we pursue the development of a geometric system that uses in the background the symbolic capabilities of two computer algebra systems , cocoa and mathematica . the cooperation between the geometric and symbolic modules of the software is illustrated by the computation of plane envelopes and other derived curves . these curves are described both graphically and analytically . since the equations of these curves are known , the system allows the construction of new elements depending on them . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
a platform for okapi based contextual information retrieval . <eos> we present an extensible java based platform for contextual retrieval based on the probabilistic information retrieval model . modules for dual indexes , relevance feedback with blind or machine learning approaches and query expansion with context are integrated into the okapi system to deal with the contextual information . this platform allows easy extension to include other types of contextual information .
the ternary description language as a formalism for the parametric general systems theory part iii . <eos> this part is a continuation of the first and second parts of my article that were published in the international journal of general systems , vol . <digit> ( <digit> <digit> ) , pp. <digit> <digit> vol . <digit> ( <digit> ) , pp. <digit> <digit> . in part iii , we deal with the construction of the axiomatic system of the ternary description language ( tdl ) . axioms and rules of inference are formulated . on the basis of these axioms and rules some theorems of tdl are proved . several system theoretical laws , which concern the values of systems parameters , are proved as theorems of tdl . thus the deductive construction of general systems theory is made .
definitions and approaches to model quality in model based software development a review of literature . <eos> more attention is paid to the quality of models along with the growing importance of modelling in software development . we performed a systematic review of studies discussing model quality published since <digit> to identify what model quality means and how it can be improved . from forty studies covered in the review , six model quality goals were identified i.e. , correctness , completeness , consistency , comprehensibility , confinement and changeability . we further present six practices proposed for developing high quality models together with examples of empirical evidence . the contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research . ( c ) <digit> elsevier b.v. all rights reserved .
animal identification introduction and history . <eos> in the beginning of the <digit> research institutes in different countries developed the first electronic animal identification systems . these systems were tested on experimental farms . the first systems were all built with the conventional components and attached to a collar around the cows neck . in the 1980s however special integrated circuits were developed minimising the size of the transponders . now in the 1990s , official organisations are testing systems for identification and registration of all animals to control movements from birth to slaughterhouse . this will enable farm livestock to be traced at the outbreak of diseases and residues in slaughter animals to be followed up . injectable transponders , electronic eartags and rumenal bolusses are being used .
a fractional variational iteration method for solving fractional nonlinear differential equations . <eos> recently , fractional differential equations have been investigated by employing the famous variational iteration method . however , all the previous works avoid the fractional order term and only handle it as a restricted variation . a fractional variational iteration method was first proposed in g.c. wu , e.w.m. lee , fractional variational iteration method and its application , phys . lett . a <digit> ( <digit> ) <digit> and gave a generalized lagrange multiplier . in this paper , two fractional differential equations are approximately solved with the fractional variational iteration method .
three level privacy control for sensing based real world content digital diorama . <eos> digital diorama , the sensing based real world content , can be constructed by integrating real time information obtained from sensors monitoring the real world . in order to increase the benefits of viewers without violating the privacy of monitored persons , this paper proposes three level privacy control over the monitored persons based on their agreement to the usage of their information obtained from sensors privacy control with i ) no agreement , ii ) partial agreement . and iii ) mutual agreement . i ) presents only their positions to show where persons are . ii ) additionally presents the information which can be automatically obtained from sensors such as age and gender to show what kinds of persons are where without disclosing the visual appearances . iii ) presents their visual appearances based on their mutual agreement with specific viewers . our evaluation indicated that the representation simulating each privacy control presented information from sensors with acceptable privacy protection .
enhancing wireless video streaming using lightweight approximate authentication . <eos> in this paper we propose a novel lightweight approximate authentication algorithm that provides efficient protection for wireless video streaming where bit errors are frequent . the benefits of the proposed algorithm over other algorithms are fast execution , due to its simplicity , and small message authentication code size . the algorithm is capable of detecting even a small number of bit errors in relatively small packets that are used in video streaming . these features have never previously been available at the same time . another benefit of the approximate authentication is that it supports error resilient video decoding by dropping packets with too many bit errors , thus improving the perceived quality of the video stream . the performance of the algorithm is demonstrated via simulations and measurements
distributed federative qos resource management . <eos> in a distributed multimedia system qos resources have to be managed carefully to utilize the resource pool in a way that bottlenecks can be avoided . our key idea is to let the applications participate on the resource management . we propose a distributed architecture with a fine granulated , balanced resource management with explicit qos characteristics . the architecture is based on a distributed cooperative resource manager which combines both the adaption and reservation principle for guaranteeing qos . we have designed and implemented a prototype of our federative qos resource manager ( fqrm ) in the java environment .
algorithms for on line order batching in an order picking warehouse . <eos> in manual order picking systems , order pickers walk or ride through a distribution warehouse in order to collect items required by ( internal or external ) customers . order batching consists of combining these indivisible customer orders into picking orders . with respect to order batching , two problem types can be distinguished in off line ( static ) batching , all customer orders are known in advance in on line ( dynamic ) batching , customer orders become available dynamically over time . this paper considers an on line order batching problem in which the maximum completion time of the customer orders arriving within a certain time period has to be minimized . the author shows how heuristic approaches for off line order batching can be modified in order to deal with the on line situation . in a competitive analysis , lower and upper bounds for the competitive ratios of the proposed algorithms are presented . the proposed algorithms are evaluated in a series of extensive numerical experiments . it is demonstrated that the choice of an appropriate batching method can lead to a substantial reduction of the maximum completion time .
passive and active reduction techniques for on chip high frequency digital power supply noise . <eos> signal integrity has become a major problem in digital ic design . one cause is device scaling that results in a sharp reduction of supply voltage , creating stringent noise margin requirements to ensure functionality . this paper introduces both a novel on chip decoupling capacitance methodology and active noise cancellation ( anc ) structure . the decoupling methodology focuses on quantification and location . the anc structure , with an area of <digit> mu m x <digit> mu m , uses decoupling capacitance to sense noise and inject a proportional current into v ( ss ) as a method of reduction . a chip has been designed and fabricated using tsmc 's <digit> nm technology . measurements show that the decoupling methodology improved the average voltage headroom loss by <digit> % while the anc structure improved the average voltage headroom loss by <digit> % .
on modal mu calculus over finite graphs with small components or small tree width . <eos> this paper is a continuation and correction of a paper presented by the same authors at the conference gandalf <digit> . we consider the modal mu calculus and some fragments of it . for every positive integer k we consider the class scck of all finite graphs whose strongly connected components have size at most k , and the class twk of all finite graphs of tree width at most k. as upper bounds , we show that for every k , the temporal logic ctl collapses to alternation free mu calculus in scck and in tw1 , the winning condition for parity games of any index n belongs to the level delta ( <digit> ) of modal mu calculus . as lower bounds , we show that buchi automata are not closed under complement in tw2 and cobuchi nondeterministic and alternating automata differ in tw1 .
product line selection and pricing analysis impact of genetic relaxations . <eos> a model for the product line selection and pricing problem ( plsp ) is presented and three solution procedures based on a genetic algorithm are developed to analyze the results based on consumer preference patterns . since the plsp model is nonlinear and integer , two of the solution procedures use genetic encoding to relax the np hard model . the relaxations result in linear integer and shortest path models for the fitness evaluation which are solved using branch and bound and labeling algorithms , respectively . performance of the quality of solutions generated by the procedures is evaluated for various problem sizes and customer preference structures . the results show that the genetic relaxations provide efficient and effective solution methodologies for the problem , when compared to the pure artificial intelligence technique of genetic search . the impact of the preference structure on the product line and the managerial implications of the solution characteristics generated by the genetic relaxations are also discussed . the models can be used to explicitly consider tradeoffs between marketing and operations concerns in designing a product line . ( c ) <digit> elsevier ltd. all rights reserved .
a decomposed model predictive functional control approach to air vehicle pitch angle control . <eos> the requirements for the pitch angle control of an air vehicle are a very fast response with as few vibrations as possible . the vibrations can damage the equipment that is carried within the body of the vehicle . the main problem to deal with is the relatively fast and under damped dynamics of the vehicle and the slow actuators and sensors . we have solved the problem by using a predictive approach . the main idea of this approach is a process output prediction based on a decomposed process model . the decomposition enables the extension of the model based approach to processes with integrative behavior such as in the case of a rocket 's pitch angle control . the proposed approach is not only useful in this case but it gives us a framework to design the control for a wide range of processes . we compared the predictive design methodology with the classical compensator control approach , known from aerospace system control . the advantage of the new approach is the reduced vibrations during the transient response .
the impact of electronic medical record systems on outpatient workflows a longitudinal evaluation of its workflow effects . <eos> the promise of the electronic medical record ( emr ) <digit> lies in its ability to reduce the costs of health care delivery and improve the overall quality of care a promise that is realized through major changes in workflows within the health care organization . yet little systematic information exists about the workflow effects of emrs . moreover , some of the research to date points to reduced satisfaction among physicians after implementation of the emr and increased time , i.e. , negative workflow effects . a better understanding of the impact of the emr on workflows is , hence , vital to understanding what the technology really does offer that is new and unique . ( i ) to empirically develop a physician centric conceptual model of the workflow effects of emrs ( ii ) to use the model to understand the antecedents to the physicians workflow expectation from the new emr ( iii ) to track physicians satisfaction overtime , <digit> months and <digit> months after implementation of the emr ( iv ) to explore the impact of technology learning curves on physicians reported satisfaction levels . the current research uses the mixed method technique of concept mapping to empirically develop the conceptual model of an emr 's workflow effects . the model is then used within a controlled study to track physician expectations from a new emr system as well as their assessments of the emr 's performance <digit> months and <digit> months after implementation . the research tracks the actual implementation of a new emr within the outpatient clinics of a large northeastern research hospital . the pre implementation survey netted <digit> physician responses post implementation time <digit> survey netted <digit> responses , and time <digit> survey netted <digit> physician responses . the implementation of the actual emr served as the intervention . since the study was conducted within the same setting and tracked a homogenous group of respondents , the overall study design ensured against extraneous influences on the results . outcome measures were derived empirically from the conceptual model . they included <digit> items that measured physician perceptions of the emr 's workflow effect on the following eight issues ( <digit> ) administration , ( <digit> ) efficiency in patient processing , ( <digit> ) basic clinical processes , ( <digit> ) documentation of patient encounter , ( <digit> ) economic challenges and reimbursement , ( <digit> ) technical issues , ( <digit> ) patient safety and care , and ( <digit> ) communication and confidentiality . the items were used to track expectations prior to implementation and they served as retrospective measures of satisfaction with the emr in post implementation time <digit> and time <digit> . the findings suggest that physicians conceptualize emrs as an incremental extension of older computerized provider order entries ( cpoes ) rather than as a new innovation . the emrs major functional advantages are seen to be very similar to , if not the same as , those of cpoes . technology learning curves play a statistically significant though minor role in shaping physician perceptions . the physicians expectations from the emr are based on their prior beliefs rather than on a rational evaluation of the emr 's fit , functionality , or performance . their decision regarding the usefulness of the emr is made very early , within the first few months of use of the emr . these early perceptions then remain stable and become the lens through which subsequent experience with the emr is interpreted . the findings suggest a need for communication based interventions aimed at explaining the value , fit , and usefulness of emrs to physicians early in the pre and immediate post emr implementation stages .
application of the lattice boltzmann method to flow in aneurysm with ring shaped stent obstacles . <eos> to resolve the characteristics of a highly complex flow , a lattice boltzmann method with an extrapolation boundary technique was used in aneurysms with and without transverse objects oil the upper wall , and results were compared with the non stented aneurysm . the extrapolation boundary concept allows the use of cartesian grids even when the boundaries do not conform to cartesian coordinates . to case the code development and facilitate the incorporation of new physics , a new scientific programming strategy based on object oriented concepts was developed . the reduced flow , smaller vorticity magnitude and wall shear stress , and smaller du dy near the dome of the aneurysm were observed when the proposed stent obstacles were used . the height of the stent obstacles was more effective to reduce the vorticity near the dome of the aneurysm than the width of the stent . the rectangular stent with <digit> % height of vessel radius was observed to be optimal and decreased the magnitude of the vorticity by <digit> % near the dome of the aneurysm . copyright ( c ) <digit> john wiley sons , ltd .
simplification rules for the coherent probability assessment problem . <eos> in this paper we develop a procedure for checking the consistency ( coherence ) of a partial probability assessment . the general problem ( called cpa ) is np complete , hence , to have a reasonable application some heuristic is needed . our proposal differs from others because it is based on a skilful use of the logical relations present among the events . in other approaches the consistency problem is reduced directly to the satisfiability of a system of linear constraints . here , thanks to the characterization of particular configurations and to the elimination of variables , an instance of the problem is reduced to smaller instances . to obtain such results , we introduce a procedure based on rules resembling those given by davis putnam for the satisfiability of boolean formulas . at the end a particularized description of an actual implementation is given .
constructing special k dominating sets using variations on the greedy algorithm . <eos> this paper focuses on the efficient selection of a special type of subset of network nodes , which we call a k k spr set , for the purpose of coordinating the routing of messages through a network . such a set is a special k k hop connected k k dominating set that has an additional property that promotes the regular occurrence of routers in all directions . the distributed algorithms introduced here for obtaining a k k spr set require that each node broadcast at most three messages to its k k hop neighbors . these transmissions can be made asynchronously . the time required to send these messages and the sizes of the resulting sets are compared by means of data collected from simulations . the main contribution is the adaptation of some variations of the distributed greedy algorithms to the problem of generating a small k k spr set . these variations are much faster than the standard distributed greedy algorithm . yet , when used with a sensible choice for a certain parameter , our empirical evidence strongly suggests that the resulting set size will generally be very close to the set size for the standard greedy algorithms .
the convergence test of transformation performance of resource cities in china considering undesirable output . <eos> the main challenge for sustainable development of resource cities is to work out a feasible strategy for transformation processes . this paper introduces a new approach for analysis of transformation performance . using the environmental production technology and a malmquist resource performance index ( mrpi ) , we conduct sigma , absolute beta and conditional beta convergence tests for the transformation performance of <digit> resource cities in china . the results show that mrpi does not follow the same trend as economic strength of three chinese regions . in addition , the transformation performance results exhibit a convergence trend for the <digit> resource cities . crown copyright ( c ) <digit> published by elsevier ltd. all rights reserved .
ant colony optimization based clustering methodology . <eos> a novel aco based methodology ( aco c ) is proposed for spatial clustering . it works in data sets with no a priori information . it includes solution evaluation , neighborhood construction and data set reduction . it has a multi objective framework , and yields a set of non dominated solutions . experimental results show that aco c outperforms other competing approaches .
breaching euclidean distance preserving data perturbation using few known inputs . <eos> we examine euclidean distance preserving data perturbation as a tool for privacy preserving data mining . such perturbations allow many important data mining algorithms ( e.g. hierarchical and k means clustering ) , with only minor modification , to be applied to the perturbed data and produce exactly the same results as if applied to the original data . however , the issue of how well the privacy of the original data is preserved needs careful study . we engage in this study by assuming the role of an attacker armed with a small set of known original data tuples ( inputs ) . little work has been done examining this kind of attack when the number of known original tuples is less than the number of data dimensions . we focus on this important case , develop and rigorously analyze an attack that utilizes any number of known original tuples . the approach allows the attacker to estimate the original data tuple associated with each perturbed tuple and calculate the probability that the estimation results in a privacy breach . on a real <digit> dimensional dataset , we show that the attacker , with <digit> known original tuples , can estimate an original unknown tuple with less than <digit> % error with probability exceeding 0.8 .
consistent interactive augmentation of live camera images with correct near field illumination . <eos> inserting virtual objects in real camera images with correct lighting is an active area of research . current methods use a high dynamic range camera with a fish eye lens to capture the incoming illumination . the main problem with this approach is the limitation to distant illumination . therefore , the focus of our work is a real time description of both near and far field illumination for interactive movement of virtual objects in the camera image of a real room . the daylight , which is coming in through the windows , produces a spatially varying distribution of indirect light in the room therefore a near field description of incoming light is necessary . our approach is to measure the daylight from outside and to simulate the resulting indirect light in the room . to accomplish this , we develop a special dynamic form of the irradiance volume for real time updates of indirect light in the room and combine this with importance sampling and shadow maps for light from outside . this separation allows object movements with interactive frame rates ( <digit> <digit> fps ) . to verify the correctness of our approach , we compare images of synthetic objects with real objects .
dynamic programming based approximation algorithms for sequence alignment with constraints . <eos> given two sequences x and y , the classical dynamic programming solution to the local alignment problem searches for two subsequences i subset of or equal to x and j subset of or equal to y with maximum similarity score under a given scoring scheme . in several applications , variants of this problem arise with different objectives and with length constraints on the subsequences i and j. this constraint can be explicit , such as requiring i j greater than or equal to t , or j < t , or may be implicit such as in cyclic sequence comparison , or as in the maximization of length normalized scores , and driven by practical considerations . we present a survey of approximation algorithms for various alignment problems with constraints , and several new approximation algorithms . these approximations are in two distinct senses in one the constraints are satisfied but the score computed is within a prescribed tolerance of the optimum instead of the exact optimum . in another , the alignment returned is assured to have at least the optimum score with respect to the given constraints , but the length constraints are satisfied to within a prescribed tolerance from the required values . the algorithms proposed involve applications of techniques from fractional programming and dynamic programming .
a model of multisecond timing behaviour under peak interval procedures . <eos> in this study , the authors developed a fundamental theory of interval timing behaviour , inspired by the learning to time ( let ) model and the scalar expectancy theory ( set ) model , and based on quantitative analyses of such timing behaviour . our experiments used the peak interval procedure with rats . the proposed model of timing behaviour comprises clocks , a regulator , a mixer , a response , and memory . using our model , we calculated the basic clock speeds indicated by the subjects behaviour under such peak procedures . in this model , the scalar property can be defined as a kind of transposition , which can then be measured quantitatively . the akaike information criterion ( aic ) values indicated that the current model fit the data slightly better than did the set model . our model may therefore provide a useful addition to set for the analysis of timing behaviour .
the support vector machine under test . <eos> support vector machines ( svms ) are rarely benchmarked against other classification or regression methods . we compare a popular svm implementation ( libsvm ) to <digit> classification methods and <digit> regression methodsall accessible through the software rby the means of standard performance measures ( classification error and mean squared error ) which are also analyzed by the means of bias variance decompositions . svms showed mostly good performances both on classification and regression tasks , but other methods proved to be very competitive .
using cascade method for table access on small devices . <eos> users increasingly expect access to web data from a wide range of devices , both wired and wireless . the goal of our research is to inform the design of applications that support data access by providing reasonably seamless migration of web data among internet compatible devices with minimal loss of effectiveness and efficiency . this study focuses on the tables of data on small mobile devices . in this paper we report on the results of a user study that compare effectiveness , efficiency and preference of two methods for the display and use of tables on small screens column row expansion and cascade , a cell based expansion method .
practical use of polynomials over the reals in proofs of termination . <eos> nowadays , polynomial interpretations are an essential ingredient in the development of tools for proving termination . we have recently proven that polynomial interpretations over the reals are strictly better for proving polynomial termination of rewriting than those which only use integer coefficients . some essential aspects of their practical use , though , remain unexplored or underdeveloped . in this paper , we compare the two current frameworks for using polynomial intepretations over the reals and show that one of them is strictly better than the other , thus making a suitable choice for implementations . we also prove that the use of algebraic real co efficients in the interpretations suffice for termination proofs . we also discuss the use of algorithms and techniques from tarski 's first order logic of the real closed fields for implementing their use in proofs of termination . we argue that more standard constraint solving techniques are better suited for this . we propose an algorithm to solve the polynomial constraints which arise when specific finite subsets of rational ( or even algebraic real ) numbers are considered for giving value to the coefficients . we provide a preliminary experimental evaluation of the algorithm which has been implemented as part of the termination tool mu term .
detecting coherent energy . <eos> we apply the mathematics of cognitive radio to a single receiver to obtain a new coherent energy metric . this allows us to derive the time correlation law separating gaussian colored noise from coherent signal energy .
optimal , quality aware scheduling of data consumption in mobile ad hoc networks . <eos> in this paper we study the delivery of quality contextual information in mobile ad hoc networks . we consider that information has a certain quality level that fades over time . mobile context aware applications receive and process disseminated information given that the corresponding quality is above the lowest level . the necessity for optimally scheduling information delivery arises from the dynamic nature of the network , e.g. , probabilistic spreading , caching , deferred delivery , and mobility of nodes . we propose two policies for optimal scheduling information delivery consumption based on the optimal stopping theory . the mobile nodes delay the reporting of information to mobile context aware applications in search for better quality . the proposed policies efficiently deal with the delivery of quality information in mobile ad hoc networks .
partnering enhanced nlp with semantic analysis in support of information extraction . <eos> information extraction using natural language processing ( nlp ) tools focuses on extracting explicitly stated information from textual material . this includes named entity recognition ( ner ) , which produces entities and some of the relationships that may exist among them . intelligent analysis requires examining the entities in the context of the entire document . while some of the relationships among the recognized entities may be preserved during extraction , the overall context of a document may not be preserved . in order to perform intelligent analysis on the extracted information , we provide an ontology , which describes the domain of the extracted information , in addition to rules that govern the classification and interpretation of added elements . the ontology is at the core of an interactive system that assists analysts with the collection , extraction , organization , analysis and retrieval of information , with the topic of terrorism financing as a case study . user interaction provides valuable assistance in assigning meaning to extracted information . the system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference . this case study demonstrates the information extraction features as well as the inference power that is supported by the ontology .
on stabilization of gradient based training strategies for computationally intelligent systems . <eos> this paper develops a novel training methodology for computationally intelligent systems utilizing gradient information in parameter updating . the devised scheme uses the first order dynamic model of the training procedure and applies the variable structure systems approach to control the training dynamics . this results in an optimal selection of the learning rate , which is continually updated as prescribed by the adopted strategy . the parameter update rule is then mixed with the conventional error backpropagation method in a weighted average . the paper presents an analysis of the imposed dynamics , which is the response of the training dynamics driven solely by the inputs designed by variable structure control approach . the analysis continues with the global stability proof of the mixed training methodology and the restrictions on the design parameters . the simulation studies presented are focused on the advantages of the proposed scheme with regards to the compensation of the adverse effects of the environmental disturbances and its capability to alleviate the inherently nonlinear behavior of the system under investigation . the performance of the scheme is compared with that of a conventional backpropagation , it is observed that the method presented is robust under noisy observations and time varying parameters due to the integration of gradient descent technique with variable structure systems methodology , in the application example studied , control of a two degrees of freedom direct drive robotic manipulator is considered . a standard fuzzy system is chosen as the controller in which the adaptation is carried out only on the defuzzifier parameters .
improving recruit distribution decisions in the us marine corps . <eos> the united states marine corps ( usmc ) accomplishes its mission to put the right marine in the right place at the right time with the right skills and quality of life in various ways . one of these is a recruit distribution modeling ( rdm ) and information system that assigns new recruits to entry level schools , thereby determining the entire career paths . this article proposes improvements to the existing marine corps decision processes and information systems for recruit distribution . the proposed system , recruit distribution decision support system ( rddss ) , provides intuitive navigation through a hierarchy of switchboards , and promotes data integrity by eliminating manual data entry for data already available in the system . it incorporates four objective measures for understanding the quality of proposed distributions , and allows the user to generate and compare multiple solutions based on the trade off between these objectives . it is a fully functional working prototype system that was installed into the usmc manpower environment , and demonstrated to provide several improvements over the current technology .
boundary effects on the soil water characteristic curves obtained from lattice boltzmann simulations . <eos> pore scale simulations using a lattice boltzmann method ( lbm ) based numerical model were conducted to examine how the capillary pressure ( pc ) ( p c ) and saturation ( s ) evolve within a virtual porous medium subjected to drainage and imbibition cycles . the results show the presence of a sharp front ( interface separating the wetting and non wetting fluids ) across the cell during the test , which expectably moves up and down as the controlling non wetting fluid pressure at the upper boundary varies to simulate different pc p c levels over the drainage and imbibition cycle . this phenomenon , representing inhomogeneity at the simulated scale , is in conflict with the homogenization applied to the pressure cell for deriving the constitutive pcs p c s relationship . different boundary conditions , adopted to achieve more homogeneous states in the virtual soil , resulted in different pcs p c s curves . no unique relationship between pc p c and s , even with the interfacial area ( anw ) ( a nw ) included , could be found . this study shows dependence of the lbm predicted pcs p c s relation on the chosen boundary conditions . this effect should be taken into account in future numerical studies of multiphase flow within porous media .
a generic sampling framework for improving anomaly detection in the next generation network . <eos> the heterogeneous nature of network traffic in next generation networks ( ngns ) may impose scalability issue to traffic monitoring applications . while this issue can be well addressed by existing sampling approaches , owing to their inherent ' lossy ' characteristic and data reduction principle , traditional sampling techniques suffer from incomplete traffic statistics , which can lead to inaccurate inferences of the network traffic . by focusing on two distinct traffic monitoring applications , namely , anomaly detection and traffic measurement , we highlight the possibility of addressing the accuracy of both applications without having to sacrifice one for the sake of the other . in light of this , we propose a generic sampling framework , which is capable of providing creditable network traffic statistics for accurate anomaly detection in the non , while at the same time preserves the principal purpose of sampling ( i.e. , to sample dominant traffic flows for accurate traffic measurement ) , and thus addressing the accuracy of both applications concurrently . with the emphasize on the accuracy of anomaly detection and the scalability of monitoring devices , the performance evaluation over real network traces demonstrates the superiority of the proposed framework over traditional sampling techniques . copyright ( c ) <digit> john wiley sons , ltd .
the ( k ) separator problem polyhedra , complexity and approximation results . <eos> given a vertex weighted undirected graph ( g ( v , e , w ) ) and a positive integer ( k ) , we consider the ( k ) separator problem it consists in finding a minimum weight subset of vertices whose removal leads to a graph where the size of each connected component is less than or equal to ( k ) . we show that this problem can be solved in polynomial time for some graph classes including bounded treewidth , ( m k_2 ) free , ( ( g_1 , g_2 , g_3 , p_6 ) ) free , interval filament , asteroidal triple free , weakly chordal , interval and circular arc graphs . polyhedral results with respect to the convex hull of the incidence vectors of ( k ) separators are reported . approximation algorithms are also presented .
structural and electronic properties of z isomers of ( <digit> alpha > <digit> '' ,2 alpha > o > <digit> '' ) phenylflavans substituted with r h , oh and och3 calculated in aqueous solution with pcm solvation model . <eos> in the search for new antioxidants , flavan structures called our attention , as substructures of many important natural compounds , including catechins ( flavan <digit> ols ) , simple and dimeric proanthocyanidins , and condensed tannins . in this work the conformational space of the z isomers of ( <digit> alpha > <digit> '' , <digit> alpha > o > <digit> '' ) phenylflavans substituted with r h , oh and och3 was scanned in aqueous solution , simulating the solvent by the polarizable continuum model ( pcm ) . geometry optimizations were performed at b3lyp <digit> <digit> g level . electronic distributions were analyzed at a better calculation level , thus improving the basis set ( <digit> <digit> g ) . a topological study based on bader 's theory ( atoms in molecules ) and natural bond orbital ( nbo ) framework was performed . furthermore , molecular electrostatic potential maps ( meps ) were obtained and thoroughly analyzed . the stereochemistry was discussed , and the effect of the solvent was addressed . moreover , intrinsic properties were identified , focusing on factors that may be related to their antioxidant properties . hyperconjugative and inductive effects were described . the coordinated nbo aim analysis allowed us to rationalize the changes of meps in a polar solvent . to investigate the molecular and structural properties of these compounds in biological media , the polarizabilities and dipolar moments were predicted which were further used to enlighten stability and reactivity properties . all conformers were taken into account . relevant stereoelectronic aspects were described for understanding the stabilization and antioxidant function of these structures .
re thinking metaphor , experience and aesthetic awareness . <eos> purpose the purpose of this paper is to explore current questions about metaphor , experience and aesthetic awareness that persist through the variations of critical approaches and projective research in architectural theory and practice . design methodology approach further considerations focus on the advanced technological possibilities which re invest the relations between principles of cybernetics and architecture . findings the current between art and architecture is more than ever manifested in fields related to the computer sciences and its conceptual background cybernetic sciences . originality value the paper re thinks the aesthetic value of architecture and architectural experience in this time of digital productivity .
agent technologies for sensor networks . <eos> the development of agent technologies for sensor networks has received increasing research attention within both the sensor network and multi agent systems research communities . the international workshops on agent technologies for sensor networks ( atsn ) held in <digit> , <digit> and <digit> sought to bring these communities together , and this special issue of the computer journal presents extended versions of some of the papers that appeared at these workshops , along with new submissions specifically for this journal .
optimal consensus of fuzzy opinions under group decision making environment . <eos> the gist of this paper is to propose a new method for aggregating individual fuzzy opinions into an optimal group consensus . by optimality , we mean the sum of weighted dissimilarity among aggregated consensus and individual opinions is minimized . we propose an iterative procedure for approximating the optimal consensus of expert opinions . finally , the importance of each expert is taken into consideration in the process of aggregation . ( c ) <digit> elsevier science b.v. all rights reserved .
multiscale bagging and its applications . <eos> we propose multiscale bagging as a modification of the bagging procedure . in ordinary bagging , the bootstrap resampling is used for generating bootstrap samples . we replace it with the multiscale bootstrap algorithm . in multiscale bagging , the sample size in of bootstrap samples may be altered from the sample size n of learning dataset . for assessing the output of a classifier , we compute bootstrap probability of class label the frequency of observing a specified class label in the outputs of classifiers learned from bootstrap samples . a scaling law of bootstrap probability with respect to sigma ( <digit> ) n m has been developed in connection with the geometrical theory . we consider two different ways for using multiscale bagging of classifiers . the first usage is to construct a confidence set of class labels , instead of a single label . the second usage is to find . inputs close to decision boundaries in the context of query by bagging for active learning . it turned out , interestingly , that an appropriate choice of m is m n , i.e. , sigma ( <digit> ) <digit> , for the first usage , and m infinity , i.e. , sigma ( <digit> ) <digit> , for the second usage .
estimation of lower and upper bounds on the power consumption from scheduled data flow graphs . <eos> in this paper , we present an approach for the calculation of lower and upper bounds on the power consumption of data path resources like functional units , registers , i o ports , and busses from scheduled data flow graphs executing a specified input data stream . the low power allocation and binding problem is formulated , first , it is shown that this problem without constraining the number of resources can be relaxed to the bipartite weighted matching problem which is solvable in o ( n ) ( <digit> ) . n is the number of arithmetic operations , variables , i o access or bus access operations which have to be bound to data path resources , in a second step we demonstrate that the relaxation can be efficiently extended by including lagrange multipliers in the problem formulation to handle a resource constraint , the estimated bounds take into account the effects of resource sharing . the technique can be used , for example , to prune the design space in high level synthesis for low power before the allocation and binding of the resources . the application of the technique on benchmarks with real application input data shows the tightness of the bounds .
the effects of perceived risk and technology type on users acceptance of technologies . <eos> previous studies on technology adoption disagree regarding the relative magnitude of the effects of perceived usefulness and perceived ease of use . however these studies did not consider moderating variables . we investigated four potential moderating variables perceived risk , technology type , user experience , and gender in users technology adoption . their moderating effects were tested in an empirical study of <digit> subjects . results showed that perceived risk , technology type , and gender were significant moderating variables . however the effects of user experience were marginal after the variance of errors was removed .
estimation of process parameter variations in a pre defined process window using a latin hypercube method . <eos> the aim of this paper is to present a methodology that provides an analytical tool for estimation of robustness and response variation within a pre defined process window . to exemplify the developed methodology , the stochastic simulation technique is used for a sheet metal forming application . a sampling plan based on the latin hypercube sampling method for variation of design parameters is utilized , and the thickness reduction is specified as the response . moreover , the response surface methodology is applied for understanding the quantitative relationship between design parameters and response value . the conclusions of this study are that the applied method gives a possibility to illustrate and interpret the variation of the response versus a design parameter variation . consequently , it gives significant insights into the usefulness of individual design parameters . it has been shown that the method enables us to estimate the admissible design parameter variations and to predict the actual safe margin for given process parameters . furthermore , the dominating design parameters can be predicated using sensitivity analysis , and this in its turn clarifies how the reliability criteria are met . finally , the developed software can be used as an additional module for set up of stochastic finite element simulations and to collect the numerical results from different solvers within different applications .
architecture and applications of the fingermouse a smart stereo camera for wearable computing hci . <eos> in this paper we present a visual input hci system for wearable computers , the fingermouse . it is a fully integrated stereo camera and vision processing system , with a specifically designed asic performing stereo block matching at 5mpixel s ( e.g. qvga 320240at <digit> fps ) and a disparity range of <digit> , consuming 187mw ( 78mw in the asic ) . it is button sized ( 43mm18mm ) and can be worn on the body , capturing the users hand and processing in real time its coordinates as well as a <digit> bit image of the hand segmented from the background . alternatively , the system serves as a smart depth camera , delivering foreground segmentation and tracking , depth maps and standard images , with a processing latency smaller than 1ms . this paper describes the fingermouse functionality and its applications , and how the specific architecture outperforms other systems in size , latency and power consumption .
wireless sensor networking for rain fed farming decision support . <eos> wireless sensor networks ( wsns ) can be a valuable decision support tool for farmers . this motivated our deployment of a wsn system to support rain fed agriculture in india . we defined promising use cases and resolved technical challenges throughout a two year deployment of our common sense net system , which provided farmers with environment data . however , the direct use of this technology in the field did not foster the expected participation of the population . this made it difficult to develop the intended decision support system . based on this experience , we take the following position in this paper currently , the deployment of wsn technology in developing regions is more likely to be effective if it targets scientists and technical personnel as users , rather than the farmers themselves . we base this claim on the lessons learned from the common sense system deployment and the results of an extensive user experiment with agriculture scientists , which we describe in this paper .
on stable parametric finite element methods for the stefan problem and the mullinssekerka problem with applications to dendritic growth . <eos> we introduce a parametric finite element approximation for the stefan problem with the gibbsthomson law and kinetic undercooling , which mimics the underlying energy structure of the problem . the proposed method is also applicable to certain quasi stationary variants , such as the mullinssekerka problem . in addition , fully anisotropic energies are easily handled . the approximation has good mesh properties , leading to a well conditioned discretization , even in three space dimensions . several numerical computations , including for dendritic growth and for snow crystal growth , are presented .
binomial moments of the distance distribution bounds and applications . <eos> we study a combinatorial invariant of codes which counts the number of ordered pairs of codewords in all subcodes of restricted support in a code . this invariant can be expressed as a linear form of the components of the distance distribution of the code with binomial numbers as coefficients . for this reason we call it a binomial moment of the distance distribution . binomial moments appear in the proof of the macwilliams identities and in many other problems of combinatorial coding theory . we introduce a linear programming problem for bounding these linear forms from below . it turns out that some known codes ( <digit> error correcting perfect codes , golay codes , nordstrom robinson code , etc. ) yield optimal solutions of this problem , i.e. , have minimal possible binomial moments of the distance distribution . we derive several general feasible solutions of this problem , which give lower bounds on the binomial moments of codes with given parameters , and derive the corresponding asymptotic bounds . applications of these bounds include new lower bounds on the probability of undetected error for binary codes used over the binary symmetric channel with crossover probability p and optimality of many codes for error detection . asymptotic analysis of the bounds enables us to extend the range of code rates in which the upper bound on the undetected error exponent is tight .
characteristics of wap traffic . <eos> this paper considers the characteristics of wireless application protocol ( wap ) traffic . we start by constructing a wap traffic model by analysing the behaviour of users accessing public wap sites via a monitoring system . a wide range of different traffic scenarios were considered , but most of these scenarios resolve to one of two basic types . the paper then uses this traffic model to consider the effects of large quantities of wap traffic on the core network . one traffic characteristic which is of particular interest in network dimensioning is the degree of self similarity , so the paper looks at the characteristics of aggregated traffic with wap , web and packet speech components to estimate its self similarity . the results indicate that , while wap traffic alone does not exhibit a significant degree of self similarity , a combined load from various traffic sources retains almost the same degree of self similarity as the most self similar individual source .
multimodal interactions in typically and atypically developing children natural versus artificial environments . <eos> this review addresses the central role played by multimodal interactions in neurocognitive development . we first analyzed our studies of multimodal verbal and nonverbal cognition and emotional interactions within neuronal , that is , natural environments in typically developing children . we then tried to relate them to the topic of creating artificial environments using mobile toy robots to neurorehabilitate severely autistic children . by doing so , both neural natural and artificial environments are considered as the basis of neuronal organization and reorganization . the common thread underlying the thinking behind this approach revolves around the brains intrinsic properties neuroplasticity and the fact that the brain is neurodynamic . in our approach , neural organization and reorganization using natural or artificial environments aspires to bring computational perspectives into cognitive developmental neuroscience .
theoretical demonstration of symmetric iv i v curves in asymmetric molecular junction of monothiolate alkane . <eos> a molecular junction of an asymmetric molecule generally demonstrates an asymmetric currentvoltage ( iv i v ) curve , due to the unequal voltage drops at the two moleculeelectrode contacts . however , for asymmetric s ( ch2 ) nch3 s ( ch <digit> ) n ch <digit> molecules , symmetric iv i v curves are always obtained in the experimental measurements . here , we investigate the electronic transport of the aus ( ch2 ) 7ch3au au s ( ch <digit> ) <digit> ch <digit> au molecular junction in order to reveal the mechanism of the symmetric iv i v curve with atk package , in which the density functional theory is combined with keldysh nonequilibrium green 's function method to calculate the electronic and transport properties of nanoscale systems . and the symmetric iv i v curve can be interpreted by the curved surface model , which reproduces curved surface of the top electrode in the experiment .
fuzzy reliability analysis of repairable industrial systems using soft computing based hybridized techniques . <eos> the present study analyzes the fuzzy reliability of a repairable industrial system utilizing uncertain data . one traditional ( flt ) and two soft computing based hybridized techniques ( gablt and ngablt ) are used . some very important fuzzy reliability indices of a washing system in a paper plant have been computed . it is observed that gablt performs consistently well in comparison to other two techniques . the analysis may be helpful for improving the performance of the considered system .
demagnetization properties of ipm and spm motors used in the high demanding automotive application . <eos> purpose in order to reduce co2 emissions of new cars many hydraulic and mechanical systems like e.g. water pump , oil pump , power steering , clime compressor have been exchanged with pure electromechanical systems , which are driven only on request . this helps to reduce fuel consumption . this trend requires of utilization of modern brushless electric motors , which are controlled from power electronic control unit ecu . in today 's car can be found between <digit> to <digit> electric motors . many of them are still simple brush type with ferrite magnets . also in this area , drift in the direction of brushless motors can bee seen , because of higher efficiency , longer lifetime , lower noise , better emc and more controllable torque vs speed characteristic . there are different technological solutions , which can been used in the area of brushless motors in order to reduce size and cost of single component . one major factor of bldc ac motor is rear earth permanent magnet material used during production . a magnet material cost could be in the range from <digit> percent ( basis price <digit> ) up to <digit> percent ( basis price <digit> ) of total material motor cost , depends on actual rear earth material price level . in order to reduce magnet cost , the aim of this paper is to find the most robust motor design , which can be resistant against maximum temperature and phase current amplitude for the same magnet material properties , coercive force hcj . this behaviour is called demagnetization property . design methodology approach analysis was performed based on review of literature , own theoretical and practical research and experience in the area of electromechanical systems for automotive application . during motor analysis computer numerical simulation method , cad and experiment were used . findings as a result , comparison of different motors ' topologies with different properties of magnet materials is presented . the worked out methodology shows very good correlation between simulations and measurements . this work can be used in order to reduce test effort and reduce cost of design . practical implications the presented methodology reduces for new designs test effort and development cost and gives an implication of robust motor topology for demagnetization effects . originality value it is the first paper where demagnetization effects have been studied theoretically and in laboratory in order to find the most robust design , reduce magnet cost by reduction of dysprosium content and develop simulation procedure for analysis of demagnetizations behaviours of interior and surface permanent magnet .
a novel direct search approach for combined heat and power dispatch . <eos> a novel approach based on the direct search method ( dsm ) is proposed for the solution of combined heat and power ( chp ) dispatch problem . to deal with the mutual dependency of multiple demand and heatpower capacity of cogeneration units , the penalty functions should be considered in dsm to enforce the corresponding violated constraints from the infeasible region into the feasible region . many nonlinear characteristics of the generator can be handled properly in the direct search procedure . to increase the possibility of exploring the search space where the global optimal solution exists , another effective strategy based on a successive refinement search technique is also proposed to guarantee a possibly complete examination of the solution space . numerical experiments are included to demonstrate the proposed direct search approach can obtain a higher quality solution than many existing techniques .
fourth and tenth order compact finite difference solutions of perturbed circular vortex flows . <eos> in this study , high order compact finite difference calculations are reported for 2d unsteady incompressible circular vortex flow in primitive variable formulation . the fourth order runge kutta temporal discretization is used together with fourth or tenth order compact spatial discretization . dependent on the perturbation initially imposed , the solutions display a tripole , triangular or square vortex . the comparison of the predictions with the detailed spectral calculations of kloosterziel and carnevale ( j. fluid mech . 1999 388 217 257 ) shows that the vorticity fields are very well captured . the spectral resolution of the present method was quantified from the decomposition of the vorticity distribution in its azimuthal components and compared with reported spectral results . using identical grid resolution to the reference results yields negligible differences in the main features of the flow . the perturbation amplitude and its first harmonic are virtually identical to the reference results for both fourth or tenth order spatial discretization , as theoretically expected but seldom a posteriori verified . the differences between the two spatial discretizations appear only for coarser grids , favouring the tenth order discretization . copyright ( c ) <digit> john wiley sons , ltd .
simulation based study of wireless rf interconnects for practical cmos implementation . <eos> an electromagnetic analysis for the practical implementation of on chip antennas to be used as wireless ic interconnects is presented . the undesired electromagnetic signal coupling between the on chip antennas and the metal interconnects is characterized under varying geometries and placement of the metal interconnects . the variations in the transmission gain between the antenna pair due to the typical complementary metal oxide semiconductor ( cmos ) manufacturing requirements are presented . using a <digit> d finite element method ( fem ) based full wave electromagnetic solver , it is shown that the antenna characteristics are significantly impacted by the presence of the essential epitaxial layer and the required minimum metal utilization . it is also shown in a 250nm cmos technology that there can be a significant electromagnetic signal coupling between the on chip transmitting antenna and the metal interconnects on a die ( 12.09 db for a 1.6 mm long , <digit> m wide interconnect at a distance of <digit> m from the antenna ) . design considerations are presented for the metal interconnects in the presence of on chip antennas in order to minimize the undesired electromagnetic signal coupling .
identification of tumor immune system via recurrent neural network . <eos> cancer immunotherapy is an emerging therapy for cancer disease treatment which stimulates immune systems to fight against tumor cells . in this paper , a back propagation neural network with some feedbacks from hidden layer is used as a method of identification for one validated mathematical model . since it is not possible to model complex system due to void of information and knowledge to model all complexity of complex system , identification methods are effective tools for modeling ill defined system . afterward , it is possible to perform control methods on the estimated model to reach the clinical goals . the simulation results have shown the correctness of the identification process .
an efficient iterative algorithm for the approximation of the fast and slow dynamics of stiff systems . <eos> the relation between the iterative algorithms based on the computational singular perturbation ( csp ) and the invariance equation ( ie ) methods is examined . the success of the two methods is based on the appearance of fast and slow time scales in the dynamics of stiff systems . both methods can identify the low dimensional surface in the phase space ( slow invariant manifold , sim ) , where the state vector is attracted under the action of fast dynamics . it is shown that this equivalence of the two methods can be expressed by simple algebraic relations . csp can also construct the simplified non stiff system that models the slow dynamics of the state vector on the sim . an extended version of ie is presented which can also perform this task . this new ie version is shown to be exactly similar to a modified version of csp , which results in a very efficient algorithm , especially in cases where the sim dimension is small , so that significant model simplifications are possible .
an adaptive comb filter with flexible notch gain . <eos> this paper proposes an adaptive comb filter with flexible notch gain . it can appropriately remove a periodic noise from an observed signal . the proposed adaptive comb filter uses a simple lms algorithm to update the notch gain coefficient for removing the noise and preserving a desired signal , simultaneously . simulation results show the effectiveness of the proposed comb filter .
does the use of structured reporting improve usability a comparative evaluation of the usability of two approaches for findings reporting in a large scale telecardiology context . <eos> poor usability leads to a low adoption rate of telemedicine systems . mode of input , free text or structured report , influences usability . usability and user satisfaction are higher for structured report interfaces in telecardiology .
fixed points of correspondences defined on cone metric spaces . <eos> in the present note , we investigate the fixed points of correspondences defined on cone metric spaces satisfying a conditionally contractive condition .
personal content management system a semantic approach . <eos> the amount of multimedia resources that is created and needs to be managed is increasing considerably . additionally , a significant increase of metadata , either structured ( metadata fields of standardized metadata formats ) or unstructured ( free tagging or annotations ) is noticed . this increasing amount of data and metadata , combined with the substantial diversity in terms of used metadata fields and constructs , results in severe problems to manage and retrieve these multimedia resources . standardized metadata schemes can be used but the plethora of these schemes results in interoperability issues . in this paper , we propose a metadata model suited for personal content management systems . we create a layered metadata service that implements the presented model as an upper layer and combines different metadata schemes in the lower layers . semantic web technologies are used to define and link formal representations of these schemes . specifically , we create an ontology for the dig35 metadata standard and elaborate on how it is used within this metadata service . to illustrate the service , we present a representative use case scenario consisting of the upload , annotation , and retrieval of multimedia content within a personal content management system .
a next generation multimedia call center for internet commerce imc . <eos> human assistance , as well as automated service , is necessary for providing more convenient services to customers on the internet based commerce system . call centers have been typically human based service systems . however , the services of existing public switched telephone network based call centers are not enough to meet the needs of customers on the internet . most of them have been designed without considering the interaction involved in shopping on the internet in our research , we design a call center named imc ( internet based multimedia call center ) that can be integrated with an internet shopping mall . it contains <digit> parts an internet multimedia dialogue system and a human agent assisting system . the internet multimedia dialogue system is an internet and multimedia version of the interactive voice response service of computer telephony integration based call centers because it provides access to the multimedia web page along with the recorded voice explanation through the internet . the human agent assisting system aims to select the most appropriate human agents in the call center and support them in providing high quality individualized information for each customer . imc is a real time , human embedded system that can provide high quality services cost effectively for internet commerce .
wind tunnel experiments of tracer dispersion downwind from a small scale physical model of a landfill . <eos> wind tunnel experiments have been carried out on a small scale physical model of a municipal waste landfill ( mwl ) in the criaciv ( research centre of building aerodynamics and wind engineering ) environmental wind tunnel in prato ( italy ) . the mwl model simulates a landfill whose surface is higher than the surrounding surface , applying a <digit> <digit> scaling factor . modelling an area source such as landfill is a difficult task for numerical models due to turbulence phenomena that modifies the flow near the source increasing ground level concentration ( glc ) . for the specific task , a new set up of the wind tunnel has been developed , with respect to previous studies carried out on line and point sources physical models . the tracer used in the experiments was ethylene , suitable for non buoyant plume conditions , typical for mwl emissions . a detailed result database has been obtained in terms of glc and concentration profiles as well as flow turbulence and velocity field characterisation .
exclusively your 's dynamic individuate search by extending user profile . <eos> a universal search engine is unable to provide a personal touch to a user query . to overcome the deficiency of a universal search engine , vertical search engines are used , which return search results from a specific domain . an alternate option is to use a personalized search system . in our endeavor to provide personalized search results , the proposed system , exclusively your 's , observes a user browsing behavior and his actions . based on the observed user behavior , it dynamically constructs user profile which consists of some terms that are related to user 's interest . the constructed profile is later used for query expansion . the goal of research work in this paper is not to provide all the relevant results , but a few high quality personalized search results at the top of ranked list , which in other words means high precision . we performed experiments by personalizing google , yahoo , and naver ( widely used search engine in korea ) . the results show that using exclusively your 's , a search engine yields significant improvement . we also compared the user profile constructed by the proposed approach with other similar personalization approaches the results show a marginal increase in precision .
constrained diffusion limited aggregation in <digit> dimensions . <eos> diffusion limited aggregation ( dla ) has usually been studied in <digit> dimensions as a model of fractal growth processes such as river networks , plant branching , frost on glass , electro deposition , lightning , mineral deposits , and coral . here , the basic principles are extended into <digit> dimensions and used to create , among other things , believable models of root systems . an additional innovation is a means of constraining the growth of the 3d dla by a surface or containing it within a vessel .
skill specific spoken dialogs in a reading tutor that listens . <eos> project listen 's reading tutor listens to children read aloud . a controlled study indicates that the reading tutor helps children 's reading comprehension . however , the results for word attack ( decoding ) skills and word identification skills were not statistically better than in the control condition . our thesis therefore proposes to develop skill specific dialogs based on cognitive skill models and successful tutoring strategies . these dialogs will be dynamically assembled by the reading tutor and include text , speech , illustrations , and dialog parameters . we hypothesize that such dialogs will improve elementary students ' reading abilities .
a difference expansion oriented data hiding scheme for restoring the original host images . <eos> this paper proposes a lossless data embedding scheme that exploits the difference expansion of the pixels to conceal large amount of message data in a digital image . the proposed scheme takes into consideration the correlation between the pixel and its surrounding pixels to determine the degree of the difference expansion for message data embedding . the performance has been evaluated in terms of image distortion , payload capacity , as well as embedding rate . the experimental results show that the scheme is capable of providing a great payload capacity , and the image quality of the embedded image is better than that of tians and celiks schemes for a gray level image . what is more , for a color image , the proposed scheme outperforms alattars scheme at low psnr . in addition , the proposed scheme can completely restore the original image after data extraction .
bit parallel random number generation for discrete uniform distributions . <eos> when a die is cast , the outcome is one of the six sides , i.e. the outcome is discrete and uniformly distributed over the range r <digit> , <digit> , <digit> , <digit> , <digit> , <digit> . generating random numbers with such a distribution is very easy obtain a random number w epsilon w , the domain of the random numbers , and take ( w mod r ) <digit> . however , many uniform discrete distributions have a rather short range , e.g. , r <digit> in a dice game , and r <digit> for the walking directions of a <digit> dimensional nonreversal random walk . the number w is typically a machine word , i.e. log ( <digit> ) ( w ) approximate to <digit> in a <digit> bit computer , so generating a log ( <digit> ) ( r ) bit random number has consumed about <digit> random bits . when w much greater than r , it is wasteful and hence inefficient . this paper presents an efficient algorithm for generating random numbers for the distributions with r discrete uniform outcomes . the algorithm uses parallel bit wise operations on machine words . the performance results of the algorithm are presented . the statistical quality of the random numbers generated from this algorithm is also discussed . ( c ) <digit> elsevier science b.v. all rights reserved .
a metaobject protocol for clforjava . <eos> clforjava is a new implementation of common lisp that intertwines its architecture and operation with java . the authors describe a new architecture for a clos mop that supports transparent , bi directional access between lisp and java . the access requires no special techniques nor syntactic mechanisms on the part of the programmer being either java or lisp . the core of the new mop is a data structure that melds the fundamental structures of java instances ( n tuples ) and clos instances ( <digit> tuples ) in such a way that the respective object systems can interact without cumbersome translations . methods from their respective object systems can interact freely . we discuss certain aspects of the respective mops that prevent a complete integration and replacement of one system by the other .
a systematic evaluation of disk imaging in encase 6.8 and linen 6.1 . <eos> tools for disk imaging ( or more generally speaking , digital acquisition ) are a foundation for forensic examination of digital evidence . therefore it is crucial that such tools work as expected . the only way to determine whether this is the case or not is through systematic testing of each tool . in this paper we present such an evaluation of the disk imaging functions of encase 6.8 and linen 6.1 , conducted on behalf of the swedish national laboratory of forensic science . although both tools performed as expected under most circumstances , we identified cases where flaws that can lead to inaccurate and incomplete acquisition results in linen 6.1 were exposed . we have also identified limitations in the tool that were not evident from its documentation . in addition summarizing the test results , we present our testing methodology , which has novel elements that we think can benefit other evaluation projects .
parallel simulation of devs and cell devs models on windows based pc cluster systems . <eos> the growing popularity of networks of workstations ( now ) in scientific computation has drawn increasing interest from the m s community . this paper addresses the issue of parallel discrete event simulation of devs and cell devs models on a microsoft windows based cluster system comprising interconnected general purpose personal computers . we present the architecture and features of pcd win , a parallel simulator that takes advantage of the multi purpose graphical user interface of the deinompi middleware for construction of ad hoc pc clusters and configuration of simulation environment . this environment significantly reduces the learning curve for general users and the cost of the simulation platform . pcd win has been developed using a modular approach that promotes code reuse and allows for easy switching to other middleware technologies . the portability of the simulator is enhanced with multi platform programming and compilation techniques . moreover , it leaves open the possibility of further extensions such as web based distributed simulation and database based model construction by leveraging the native support of microsoft visual studio . the experiments demonstrate the capability of the new simulator , making it an ideal m s toolkit for tapping the computational power of general purpose desktop computers .
a computable version of the daniell stone theorem on integration and linear functionals . <eos> for every measure mu , the integral i f bar right arrow integral f d mu is a linear functional on the set of real measurable functions . by the daniell stone theorem , for every abstract integral lambda f > r on a stone vector lattice f of real functions f omega > r there is a measure mu such that integral f d mu lambda ( f ) for all f is an element of f. in this paper we prove a computable version of this theorem . ( c ) <digit> elsevier b.v. all rights reserved .
factors influencing intention to use e government services among citizens in malaysia . <eos> this study is an exploratory study on the e government in malaysia . with the liberalization and globalization , internet has been used as a medium of transaction in almost all aspects of human living . this study investigates the factors that influencing the intention to use e government service among malaysians . this study integrates constructs from the models of technology acceptance model ( tam ) , diffusion of innovation ( doi ) which been moderated by culture factor and trust model with five dimensions . the study was conducted by surveying a broad diversity of citizens in malaysia community . a structured questionnaire was used to collect data from <digit> respondents but only <digit> of the respondents with complete answers participating in the study . the result of the analysis showed that trust , perceived usefulness , perceived relative advantage and perceived image , respectively , has a direct positive significant relationship towards intention to use e government service and perceived complexity has a significant negative relationship towards intention to use e government service . while perceived strength of online privacy and perceived strength of non repudiation have a positive impact on a citizen 's trust to use e government service . however , the uncertainty avoidance ( moderating factor ) used in the study has no significant effect on the relationship between the innovation factors ( complexity , relative advantage and image ) and intention to use e government service . finally in comparing the explanatory power of the entire intention based model ( tam , doi and trust ) with the studied model , it has been found that the doi model has a better explanatory power .
reducing the energy dissipation of the issue queue by exploiting narrow immediate operands . <eos> in contemporary superscalar microprocessors , issue queue is a considerable energy dissipating component due its complex scheduling logic . in addition to the energy dissipated for scheduling activities , read and write lines of the issue queue entries are also high energy consuming pieces of the issue queue . when these lines are used for reading and writing unnecessary information bits , such as the immediate operand part of an instruction that does not use the immediate field or the insignificant higher order bits of an immediate operand that are in fact not needed , significant amount of energy is wasted . in this paper , we propose two techniques to reduce the energy dissipation of the issue queue by exploiting the immediate operand files of the stored instructions firstly by storing immediate operands in separate immediate operand files rather than storing them inside the issue queue entries and secondly by issue queue partitioning based on widths of immediate operands of instructions . we present our performance results and energy savings using a cycle accurate simulator and testing the design with spec2k benchmarks and <digit> nm cmos ( umc ) technology .
study of speed dependent packet error rate for wireless sensor on rotating mechanical structures . <eos> wireless sensors on rotating mechanical structures have rich and fast changing multipath that can not be easily predicted by conventional regression approaches in time for effective transmission coding or power control , resulting in deteriorated transmission quality . this study aims to study the speed dependent packet error rate ( per ) of wireless sensor radios on rotating mechanical structures . a series of rotating ieee 802.15.4 sensor radio transmission experiments and vector network analyzer measurements have been conducted to derive and validate a predictive per model for a fast rotating sensor radio channel based on channel impulse response measurements . the proposed predictive per model , including power attenuation , bit error rate ( ber ) and per sub models , captures the channel property of rotating sensors based on the received signal strength and the radio receiving sensitivity . the per model has accurately predicted the per profile of sensors on a rotating machine tool spindle as well as a rotating plate of a prototype rotation system . the analysis provides an in depth understanding of how multipath propagation causes the fast power variation and the resulting speed dependent per for wireless sensors on rotating mechanical structures .
crosstalk in vlsi interconnections . <eos> we address the problem of crosstalk computation and reduction using circuit and layout techniques in this paper , we provide easily computable expressions for crosstalk amplitude and pulse width in resistive , capacitively coupled lines , the expressions hold for nets with arbitrary number of pins and of arbitrary topology under any specified input excitation . experimental results show that the average error is about <digit> % and the maximum error is less than <digit> % . the expressions are used to motivate circuit techniques , such as transistor sizing , and layout techniques , such as wire ordering and wire width optimization to reduce crosstalk .
a hybrid genetic algorithm for the energy efficient virtual machine placement problem in data centers . <eos> server consolidation using virtualization technology has become an important technology to improve the energy efficiency of data centers . virtual machine placement is the key in the server consolidation technology . in the past few years , many approaches to the virtual machine placement have been proposed . however , existing virtual machine placement approaches consider the energy consumption by physical machines only , but do not consider the energy consumption in communication network , in a data center . however , the energy consumption in the communication network in a data center is not trivial , and therefore should be considered in the virtual machine placement . in our preliminary research , we have proposed a genetic algorithm for a new virtual machine placement problem that considers the energy consumption in both physical machines and the communication network in a data center . aiming at improving the performance and efficiency of the genetic algorithm , this paper presents a hybrid genetic algorithm for the energy efficient virtual machine placement problem . experimental results show that the hybrid genetic algorithm significantly outperforms the original genetic algorithm , and that the hybrid genetic algorithm is scalable .
sensitivity of tapered optical fiber surface plasmon resonance sensors . <eos> the effect of tapered profiles on the sensitivity of spr sensor is studied . it is observed that as the taper ratio decreases the sensitivity of proposed sensor for each profile increases up to certain taper ratio where the plasmonic condition is satisfied . in all considered cases , the maximum sensitivity is obtained for sinusoidal tapered profile .
an image contrast enhancement method based on genetic algorithm . <eos> contrast enhancement plays a fundamental role in image video processing . histogram equalization ( he ) is one of the most commonly used methods for image contrast enhancement . however , he and most other contrast enhancement methods may produce un natural looking images and the images obtained by these methods are not desirable in applications such as consumer electronic products where brightness preservation is necessary to avoid annoying artifacts . to solve such problems , we proposed an efficient contrast enhancement method based on genetic algorithm in this paper . the proposed method uses a simple and novel chromosome representation together with corresponding operators . experimental results showed that this method makes natural looking images especially when the dynamic range of input image is high . also , it has been shown by simulation results that the proposed genetic method had better results than related ones in terms of contrast and detail enhancement and the resulted images were suitable for consumer electronic products .
limits of a conjecture on a leakage resilient cryptosystem . <eos> we introduce the hidden shares number problem , a variant of the hidden number problem . we give a leakage resilience bound for elgamal cryptosystem with stateful decryption . we have implemented our attack and give some details about our implementation .
robust reconstruction of low resolution document images by exploiting repetitive character behaviour . <eos> in this paper , we present a new approach for reconstructing low resolution document images . unlike other conventional reconstruction methods , the unknown pixel values are not estimated based on their local surrounding neighbourhood , but on the whole image . in particular , we exploit the multiple occurrence of characters in the scanned document . in order to take advantage of this repetitive behaviour , we divide the image into character segments and match similar character segments to filter relevant information before the reconstruction . a great advantage of our proposed approach over conventional approaches is that we have more information at our disposal , which leads to a better reconstruction of the high resolution ( hr ) image . experimental results confirm the effectiveness of our proposed method , which is expressed in a better optical character recognition ( ocr ) accuracy and visual superiority to other traditional interpolation and restoration methods .
complexity of inflammatory responses in endothelial cells and vascular smooth muscle cells determined by microarray analysis . <eos> to better understand the molecular basis of vascular cell system behavior in inflammation , we used gene expression microarrays to analyze the expression of 7,075 genes and their response to il <digit> and tnf in cultures of coronary artery endothelium and smooth muscle derived from a single coronary artery . the most noticeable difference between the cell types was the considerably greater magnitude and complexity of the transcriptional response in the endothelial cells . two hundred and nine genes were regulated in the endothelium and only <digit> in vascular smooth muscle . among the <digit> regulated genes in the endothelium , <digit> have not been previously associated with endothelial cell activation and many implicate the endothelium in unconventional roles . for example , the induced genes include several that have only been associated with leukocyte function ( e.g. , il <digit> receptor , ebi <digit> receptor ) and others related to antiviral and antibacterial defense ( e.g. , oligoadenylate synthetase , lmp7 , toll like receptor <digit> , complement component <digit> ) . in addition , <digit> genes likely to participate in signal transduction ( eg . il <digit> receptor , stk2 kinase , staf50 , anp receptor , vip receptor , rac3 , ifp35 ) were regulated providing evidence that a major effect of tnf and il <digit> is to alter the potential of the endothelial cell to respond to various other external stimuli .
partial x ray photoelectron spectroscopy to constructing neural network model of plasma etching surface . <eos> a new model to control plasma processes was constructed by combining a backpropagation neural network ( bpnn ) with x ray photoelectron spectroscopy ( xps ) . this technique was evaluated with the data collected during the etching of silicon carbide films at nf3 inductively coupled plasma . the etching characteristics modeled were the etch rate and surface roughness measured by scanning electron microscope and atomic force microscopy , respectively . for systematic modeling , the etching was characterized by means of <digit> full factorial experiment plus one center point . the bpnn was trained by the training data composed of xps spectra corresponding to five major peaks . prediction performance of trained bpnn model was tested with a test data set , not belonging to the training data . in modeling surface roughness , pure xps model yielded an improvement of about <digit> % over pca xps ( <digit> % data variance ) model . for the etch rate data , the improvement was more than <digit> % irrespective of the data variances . these results indicate that non reduced xps spectra are more effective in constructing a prediction model . xps models can be utilized to diagnose or control plasma processes .
aggregate profit based caching replacement algorithms for streaming media transcoding proxy systems . <eos> this work derives a generalized video object profit function from the extended weighted transcoding graph to calculate the individual cache profit of certain versions of a video object , and the aggregate profit from caching multiple versions of the same video object . this proposed function takes into account the popularity of certain versions of an object , the transcoding delay among versions , and the average duration of access of each version . based on the profit function , cache replacement algorithms are proposed to reduce the startup delay and network traffic by efficiently caching video objects with the most profits . two kinds of simulations were conducted to evaluate the performance of the proposed algorithms . these simulations exploit partial viewing traces and complete viewing traces , separately . the results demonstrate that the proposed algorithms outperform the competing algorithms by <digit> % <digit> % in delay saving ratio and <digit> % <digit> % in byte hit ratio .
refining and reasoning about nonfunctional requirements . <eos> nonfunctional requirements ( nfr ) must be addressed early in the software development cycle to avoid the cost of revisiting those requirements or re factoring at the later stages of the development cycle . methods and frameworks that identify and incorporate nfr at each stage of development cycle reduce this cost . the methodology used in this work for refining and reasoning about nfr is based on the nfr framework . this work identifies four nfr types and provides the methodology for developing domain specific nfr by using techniques for converting the requirements into design artifacts per nfr type . the contribution is four nfr types functionally restrictive , additive restrictive , policy restrictive , and architecture restrictive and the software engineering process that provides specific refinements that result in unique architectural and design artifacts . by applying the same functional requirement focus to the different nfr domains it enhances the development process and promotes software quality attributes such as composability , maintainability , evolvability , and traceability .
model updated image guidance initial clinical experiences with gravity induced brain deformation . <eos> image guided neurosurgery relies on accurate registration of the patient , the preoperative image series , and the surgical instruments in the same coordinate space . recent clinical reports have documented the magnitude of gravity induced brain deformation in the operating room and suggest these levels of tissue motion may compromise the integrity of such systems , we are investigating a model based strategy which exploits the wealth of readily available preoperative information in conjunction with intraoperatively acquired data to construct and drive a three dimensional ( <digit> d ) computational model which estimates volumetric displacements in order to update the neuronavigational image set . using model calculations , the preoperative image database can be deformed to generate a more accurate representation of the surgical focus during an operation , in this paper , we present a preliminary study of four patients that experienced substantial brain deformation from gravity and correlate cortical shift measurements with model predictions , additionally , me illustrate our image deforming algorithm and demonstrate that preoperative image resolution is maintained . results over the four cases show that the brain shifted , on average , 5.7 mm in the direction of gravity and that model predictions could reduce this misregistration error to an average of 1.2 mm .
communication structure and collective actions in social media . <eos> in this paper i present results a study of different types of social media communication and networking channels that allow for collective action ( ca ) twitter , jaiku qaiku , ning and facebook . my preliminary findings indicate , that the visual outlining and the structure of communication create different kinds of collectivity and collective actions . a status stream is effective for simple and fast repetitive mass actions and for individual mass broadcasting , while channels and threads are needed as a backchannel for more complicated , coordinated and iterative tasks and support a sense of community . when planning collective action on the internet , ranging from citizen participation to marketing campaigns , it is essential to note that different social media tools support different forms of collective action and feelings of collectiveness .
better reporting of randomized trials in biomedical journal and conference abstracts . <eos> well reported research published in conference and journal abstracts is important as individuals reading these reports often base their initial assessment of a study based on information reported in the abstract . however , there is growing concern about the reliability and quality of information published in these reports . this article provides an overview of research evidence underpinning the need for better reporting of abstracts reported in conference proceedings and abstracts of journal articles with a particular focus in the area of health care . where available we highlight evidence which refers specifically to abstracts reporting randomized trials . we seek to identify current initiatives aimed at improving the reporting of these reports and recommend that an extension of the consort statement ( consolidated standards of reporting trials ) , consort for abstracts , be developed . this checklist would include a list of essential items to be reported in any conference or journal abstract reporting the results of a randomized trial .
continuous testing in eclipse . <eos> continuous testing uses excess cycles on a developer 's workstation to continuously run regression tests in the background , providing rapid feedback about test failures as code is edited . it reduces the time and energy required to keep code well tested , and it prevents regression errors from persisting uncaught for long periods of time .
bayesian analysis of the logit model and comparison of two metropolishastings strategies . <eos> we examine some markov chain monte carlo ( mcmc ) methods for a generalized non linear regression model , the logit model . it is first shown that mcmc algorithms may be used since the posterior is proper under the choice of non informative priors . then two non standard mcmc methods are compared a metropolishastings algorithm with a bivariate normal proposal resulting from an approximation , and a metropolishastings algorithm with an adaptive proposal . the results presented here are illustrated by simulations , and show the good behavior of both methods , and superior performances of the method with an adaptive proposal in terms of convergence to the stationary distribution and exploration of the posterior distribution surface .
aggregate features and adaboost for music classification . <eos> we present an algorithm that predicts musical genre and artist from an audio waveform . our method uses the ensemble learner adaboost to select from a set of audio features that have been extracted from segmented audio and then aggregated . our classifier proved to be the most effective method for genre classification at the recent mirex <digit> international contests in music information extraction , and the second best method for recognizing artists . this paper describes our method in detail , from feature extraction to song classification , and presents an evaluation of our method on three genre databases and two artist recognition databases . furthermore , we present evidence collected from a variety of popular features and classifiers that the technique of classifying features aggregated over segments of audio is better than classifying either entire songs or individual short timescale features .
weak limits and their calculation in analog signal theory . <eos> purpose this paper aims to improve the mathematical justification of certain analog signal theory concepts and offer a rigorous framework for it . design methodology approach the framework relies on functional analysis , namely theory of distributions and the concept of weak limit . its notation is adjusted to resemble the notation usually used in engineering signal theory . it can be used to prove in a rigorous manner already established results in signal theory , but also to establish new ones . findings examples have shown the lack of rigour caused by using ordinary calculus in proving fundamental signal theoretic results . on that basis , concepts of limit , fourier transform and derivative are revisited in the spirit of functional analysis . a new useful formula for weak limit computation is proved . originality value functional analysis is efficiently used in signal theory in a manner approachable by engineers . an original and efficient formula for weak limit computation is presented and proved .
the social information infrastructure . <eos> the division of social , behavioral , and economic research of the national science foundation has explored aggressively the potential involvement of the social sciences in the national information infrastructure . we invision the nii as a global network of computer communications , which will evolve out of the internet , linking all social scientists to massive digital libraries and to myriad smaller distributed data sources containing information of every imaginable sort . five workshops have charted applications of high performance computing in the social and behavioral sciences cognitive science , computational geography computational economics , artificial social intelligence , and electronic networks . a survey of seer programs revealed that many are helping to create the information infrastructure , and substantial investment in six '' flagship '' digital library projects will develop the systems necessary for the nii of the 21st century .
web based public participation geographical information systems an aid to local environmental decision making . <eos> current research examining the potential of the world wide web as a means of increasing public participation in local environmental decision making in the uk is discussed . the paper considers traditional methods of public participation and argues that new internet based technologies have the potential to widen participation in the uk planning system . evidence is provided of the potential and actual benefits of online spatial decision support systems in the uk through a real environmental decision support problem in a village in northern england . the paper identifies key themes developing in this area of web based geographical information systems ( gis ) and provides a case study example of an online public participation gis from inception to the final phase in a public participation process . it is shown that in certain uk planning problems and policy formulation processes , participatory online systems are a useful means of informing and engaging the public and can potentially bring the public closer to a participatory planning system .
medical image analysis for cancer management in natural computing framework . <eos> natural computing , through its repertoire of nature inspired strategies , is playing a major role in the development of intelligent decision making systems . the objective is to provide flexible , application oriented solutions to current medical image analysis problems . it encompasses fuzzy sets , neural networks , genetic algorithms , rough sets , swarm intelligence , and a host of other paradigms , mimicking biological and physical processes from nature . radiographic imaging modalities , like computed tomography ( ct ) , positron emission tomography ( pet ) , and magnetic resonance imaging ( mri ) , help in providing improved diagnosis , prognosis and treatment planning for cancer . this survey highlights the role of natural computing , in efficiently analyzing radiographic medical images , for improved tumor management . we also provide a categorization of the segmentation , feature extraction and selection methods , based on different natural computing technologies , with reference to the application involving malignancy of the brain , breast , prostate , skin , lung , and liver .
parallelisation of the lagrangian model in a mixed eulerian lagrangian cfd algorithm . <eos> this manuscript presents an algorithm implemented in a commercial computational fluid dynamics ( cfd ) code for parallelisation of the lagrangian particle tracking model in a mixed eulerian lagrangian cfd algorithm . the algorithm is based on the domain decomposition parallelisation strategy and asynchronous message passing protocol . the methodology is tested on two industrial cfd test cases and the parallelisation results are presented . further , it is discussed how the parallel efficiency of the runs can be improved by adopting the domain decomposition scattering technique . ( c ) <digit> elsevier inc. all rights reserved .
sequential and parallel triangulating algorithms for elimination game and new insights on minimum degree . <eos> elimination game is a well known algorithm that simulates gaussian elimination of matrices on graphs , and it computes a triangulation of the input graph . the number of fill edges in the computed triangulation is highly dependent on the order in which elimination game processes the vertices , and in general the produced triangulations are neither minimum nor minimal . in order to obtain a triangulation which is close to minimum , the minimum degree heuristic is widely used in practice , but until now little was known on the theoretical mechanisms involved . in this paper we show some interesting properties of elimination came in particular that it is able to compute a partial minimal triangulation of the input graph regardless of the order in which the vertices are processed . this results in a new algorithm to compute minimal triangulations that are sandwiched between the input graph and the triangulation resulting from elimination came . one of the strengths of the new approach is that it is easily parallelizable , and thus we are able to present the first parallel algorithm to compute such sandwiched minimal triangulations . in addition , the insight that we gain through elimination game is used to partly explain the good behavior of the minimum degree algorithm . we also give a new algorithm for producing minimal triangulations that is able to use the minimum degree idea to a wider extent . ( c ) <digit> elsevier b.v. all rights reserved .
quality evaluation of e government digital services . <eos> in this paper we present a quality estimation model for digital e government services suitable for quality evaluation , monitoring , discovery , selection and composition .
throughput improvement of incremental redundancy ldpc coded mimo v blast system . <eos> in this paper , we present ensembles of incremental redundancy low density parity check ( ir ldpc ) codes to improve the throughput performance of hybrid forward error correction ( fec ) automatic repeat request ( arq ) schemes in a vertical bell lab layered space time ( v blast ) system . these ensembles are designed to have good error rate performance at short block lengths , which result in higher throughput performance . the throughput simulations in various fading conditions show that these ensembles outperform a conventional random punctured ensemble by <digit> db eb n0 at a throughput region of 0.8 . to reduce the traffic of feedback channels , we consider using an adaptive code selection algorithm . in these adaptive hybrid fec arq schemes , the number of negative acknowledgement signals for retransmission is greatly reduced at operating snr ranges without any significant throughput loss .
benefits of averaging lateration estimates obtained using overlapped subgroups of sensor data . <eos> in this paper , we suggest averaging lateration estimates obtained using overlapped subgroups of distance measurements as opposed to obtaining a single lateration estimate from all of the measurements directly if a redundant number of measurements are available . least squares based closed form equations are used in the lateration . in the case of gaussian measurement noise the performances are similar in general and for some subgroup sizes marginal gains are attained . averaging laterations method becomes especially beneficial if the lateration estimates are classified as useful or not in the presence of outlier measurements whose distributions are modeled by a mixture of gaussians ( mog ) pdf . a new modified trimmed mean robust averager helps to regain the performance loss caused by the outliers . if the measurement noise is gaussian , large subgroup sizes are preferable . on the contrary , in robust averaging small subgroup sizes are more effective for eliminating measurements highly contaminated with mog noise . the effect of high variance noise was almost totally eliminated when robust averaging of estimates is applied to qr decomposition based location estimator . the performance of this estimator is just <digit> cm worse in root mean square error compared to the cramrrao lower bound ( crlb ) on the variance both for gaussian and mog noise cases . theoretical crlbs in the case of mog noise are derived both for time of arrival and time difference of arrival measurement data .
em based iterative receiver for coded mimo systems in unknown spatially correlated noise . <eos> we present iterative channel estimation and decoding schemes for multi input multi output ( mimo ) rayleigh block fading channels in spatially correlated noise . an expectation maximization ( em ) algorithm is utilized to find the maximum likelihood ( ml ) estimates of the channel and spatial noise covariance matrices , and to compute soft information of coded symbols which is sent to an error control decoder . the extrinsic information produced by the decoder is then used to refine channel estimation . several iterations are performed between the above channel estimation and decoding steps . we derive modified cramer rao bound ( mcrb ) for the unknown channel and noise parameters , and show that the proposed em based channel estimation scheme achieves the mcrb at medium and high snrs . for a bit error rate of <digit> ( <digit> ) and long frame length , there is negligible performance difference between the proposed scheme and the ideal coherent detector that utilizes the true channel and noise covariance matrices . copyright ( c ) <digit> john wiley sons , ltd .
parameter exploration in science and engineering using many task computing . <eos> robust scientific methods require the exploration of the parameter space of a system ( some of which can be run in parallel on distributed resources ) , and may involve complete state space exploration , experimental design , or numerical optimization techniques . many task computing ( mtc ) provides a framework for performing robust design , because it supports the execution of a large number of otherwise independent processes . further , scientific workflow engines facilitate the specification and execution of complex software pipelines , such as those found in real science and engineering design problems . however , most existing workflow engines do not support a wide range of experimentation techniques , nor do they support a large number of independent tasks . in this paper , we discuss nimrod k a set of add in components and a new run time machine for a general workflow engine , kepler . nimrod k provides an execution architecture based on the tagged dataflow concepts , developed in 1980s for highly parallel machines . this is embodied in a new kepler director that supports many task computing by orchestrating execution of tasks on on clusters , grids , and clouds . further , nimrod k provides a set of actors that facilitate the various modes of parameter exploration discussed above . we demonstrate the power of nimrod k to solve real problems in cardiac science .
group context based adaptations for recommendation . <eos> in groupware or community based applications the user interface is usually static or tailored to the individual user 's needs . newer developments try to adapt the user interface automatically in regard to user contexts . even though these techniques are proven useful , there exists no contextadaptive system taking the current context of a group or community in regard . in this paper , we briefly discuss the problems of defining context and present our understanding of context as a subset of the current information state . we provide an exemplary scenario to present different approaches how to compute group contexts based on semantic models and user contexts , and the consequences for the adaptation goals in the interface or through changes at system functionalities or tools . we additionally discuss the problems occurring at evaluating adaptations and the value of group context for collaborative work .
issues of trust and control on agent autonomy . <eos> the relationship between trust and control is quite relevant both for the very notion of trust and for modelling and implementing trust control relations with autonomous systems . we claim that control is antagonistic of the strict form of trust ' trust in y ' but also that it completes and complements it for arriving to a global trust . in other words , putting control and guaranties is trust building it produces a sufficient trust , when trust in y 's autonomous willingness and competence would not be enough . we also argue that control requires new forms of trust trust in the control itself or in the controller , trust in y as for being monitored and controlled , trust in possible authorities , etc. finally , we show that paradoxically control could not be antagonistic of strict trust in y , but it can even create , increase it by making y more willing or more effective . in conclusion , depending on the circumstances , control makes y more reliable or less reliable control can either decrease or increase trust . a good theory of trust can not be complete without a theory of control .
youubi open software for ubiquitous learning . <eos> we propose a reference architecture for u learning environments . we propose a method development and validation of u learning environments . we present a u learning environment that combines playful aspects with learning strategies .
single symbol ml decodable distributed stbcs for cooperative networks . <eos> in this correspondence , the distributed orthogonal space time block codes ( dostbcs ) , which achieve the single symbol maximum likelihood ( ml ) decodability and full diversity order , are first considered . however , systematic construction of the dostbcs is very hard , since the noise covariance matrix is not diagonal in general . thus , some special dostbcs , which have diagonal noise covariance matrices at the destination terminal , are investigated . these codes are referred to as the row monomial dostbcs . an upper bound of the data rate of the row monomial dostbc is derived and it is approximately twice higher than that of the repetition based cooperative strategy . furthermore , systematic construction methods of the row monomial dostbcs achieving the upper bound of the data rate are developed when the number of relays and or the number of information bearing symbols are even .
ontology based data mining approach implemented for sport marketing . <eos> since sport marketing is a commercial activity , precise customer and marketing segmentation must be investigated frequently and it would help to know the sport market after a specific customer profile , segmentation , or pattern come with marketing activities has found . such knowledge would not only help sport firms , but would also contribute to the broader field of sport customer behavior and marketing . this paper proposes using the apriori algorithm of association rules , and clustering analysis based on an ontology based data mining approach , for mining customer knowledge from the database . knowledge extracted from data mining results is illustrated as knowledge patterns , rules , and maps in order to propose suggestions and solutions to the case firm , taiwan adidas , for possible product promotion and sport marketing .
layered acting for character animation . <eos> we introduce an acting based animation system for creating and editing character animation at interactive speeds . our system requires minimal training , typically under an hour , and is well suited for rapidly prototyping and creating expressive motion . a real time motion capture framework records the user 's motions for simultaneous analysis and playback on a large screen . the animator 's real world , expressive motions are mapped into the character 's virtual world . visual feedback maintains a tight coupling between the animator and character . complex motion is created by layering multiple passes of acting . we also introduce a novel motion editing technique , which derives implicit relationships between the animator and character . the animator mimics some aspect of the character motion , and the system infers the association between features of the animator 's motion and those of the character . the animator modifies the mimic by acting again , and the system maps the changes onto the character . we demonstrate our system with several examples and present the results from informal user studies with expert and novice animators .
an extensible architecture based framework for coordination languages . <eos> the dynamic and heterogeneous nature of distributed systems makes the development of distributed applications a difficult task . various tools , such as middleware systems , component systems , and coordination languages , offer support the application developer at different levels . there are several coordination systems that integrate such tools into a complete environment to build applications from heterogeneous components . to achieve extensibility they usually have a layered architecture an application is first mapped to a middle layer and then to a target system . but this approach hides the specific features of a target system from the developer , as they are not represented in the middle layer , and often induces additional run time overhead . in this paper , we introduce the extensible coordination framework ecf that allows developers to build efficient distributed applications which exploit the specific features of the target systems . support for target systems and application domains are encapsulated by extension modules . modules can be built on top of other modules to support refined functionality .
novel approaches to the measurement of arterial blood flow from dynamic digital x ray images . <eos> we have developed two new algorithms for the measurement of blood flow from dynamic x ray angiographic images . both algorithms aim to improve on existing techniques . first , a model based ( mb ) algorithm is used to constrain the concentration distance curve matching approach . second , a weighted optical flow algorithm ( op ) is used to improve on point based optical flow methods by averaging velocity estimates along a vessel with weighting based on the magnitude of the spatial derivative . the op algorithm was validated using a computer simulation of pulsatile blood flow . both the op and the mb algorithms were validated using a physiological blood flow circuit . dynamic biplane digital x ray images were acquired following injection of iodine contrast medium into a variety of simulated arterial vessels . the image data were analyzed using our integrated angiographic analysis software sara to give blood how waveforms using the nib and op algorithms . these waveforms were compared to flow measured using an electromagnetic flow meter ( emf ) . in total <digit> instantaneous measurements of flow were made and compared to the emf recordings . it was found that the new algorithms showed low measurement bias and narrow limits of agreement and also outperformed the concentration distance curve matching algorithm ( org ) and a modification of this algorithm ( pa ) in all studies .
his monitor an approach to assess the quality of information processing in hospitals . <eos> hospital information systems ( his ) are a substantial quality and cost factor for hospitals . systematic monitoring of his quality is an important task however , this task is often seen to be insufficiently supported . to support systematic his monitoring , we developed his monitor , comprising about <digit> questions , focusing on how a hospital information system does efficiently support clinical and administrative tasks . the structure of his monitor consists of a matrix , crossing his quality criteria on one axis with a list of process steps within patient care on the other axis . his monitor was developed based on several pretests and was now tested in a larger feasibility study with <digit> participants . his monitor intends to describe strengths and weaknesses of information processing in a hospital . results of the feasibility study show that his monitor was able to highlight certain his problems such as insufficiently supported cross departmental communication , legibility of drug orders and other paper based documents , and overall time needed for documentation . we discuss feasibility of his monitor and the reliability and validity of the results . further refinement and more formal validation of his monitor are planned .
recursive channel estimation based on finite parameter model using reduced complexity maximum likelihood equalizer for ofdm over doubly selective channels . <eos> to take intercarrier interference ( ici ) attributed to time variations of the channel into consideration , the time and frequency selective ( doubly selective ) channel is parameterized by a finite parameter model . by capitalizing on the finite parameter model to approximate the doubly selective channel , a kalman filter is developed for channel estimation . the ici suppressing , reduced complexity viterbi type maximum likelihood ( rml ) equalizer is incorporated into the kalman filter for recursive channel tracking and equalization to improve the system performance . an enhancement in the channel tracking ability is validated by theoretical analysis , and a significant improvement in ber performance using the channel estimates obtained by the recursive channel estimation method is verified by monte carlo simulations .
a web service agent based decision support system for securities exception management . <eos> with rising trading volumes and increasing risks in securities transactions , the securities industry is making an effort to shorten the trade lifecycle and minimize transaction risks . while attempting to achieve this , exception management is crucial to pass trade information within the trade lifecycle in a timely and accurate fashion . for a competitive solution to exception management , a web service agent based decision support system is developed in this paper . agent technology is applied to deal with the dynamic , complex , and distributed processes in exception management web services techniques are proposed for more scalability and interoperability in network based business environment . by integrating agent technology with web services to make use of the advantages from both , this approach leads to more intelligence , flexibility and collaboration in business exception management . the effectiveness of this approach is evaluated through a use case and demonstration feedback .
measuring energy consumption using eml ( energy measurement library ) . <eos> energy consumption and efficiency is a main issue in high performance computing systems in order to reach exascale computing . researchers in the field are focusing their effort in reducing the first and increasing the latter while there is no current standard for energy measurement . current energy measurement tools are specific and architectural dependent and this has to be addressed . by creating a standard tool , it is possible to generate independence between the experiments and the hardware , and thus , researchers effort can be focused in energy , by maximizing the portability of the code used for experimentation with the multiple architectures we have access nowadays . we present the energy measurement library ( eml ) library , a software library that eases the access to the energy measurement tools and can be easily extended to add new measurement systems . using eml , it is viable to obtain architectural and algorithmic parameters that affect energy consumption and efficiency . the use of this library is tested in the field of the analytic modeling of the energy consumed by parallel programs .
methylation sensitive representational difference analysis and its application to cancer research . <eos> methylation sensitive representational difference analysis ( ms rda ) was previously established to detect differences in the methylation status of two genomes . this method uses the digestion of genomic dna with a methylation sensitive restriction enzyme , hpaii , and pcr to prepare hpaii amplicons , followed by rda . an hpaii amplicon prepared using betaine and reverse electrophoresis was enriched 3.6 fold ( compared with the hpaii amplicon prepared by the original method ) with dna fragments originating from cpg islands ( cgis ) . as for the specificity of ms rda , it was shown that dna fragments that are unmethylated in the tester and almost completely methylated in the driver are efficiently isolated . this indicated that genes that are in biallelic methylation or in monoallelic methylation with loss of the other allele are efficiently isolated . further , by use of two additional methylation sensitive six base recognition restriction enzymes , sacii and nari , more dna fragments were isolated from cgis in the <digit> regions of genes . after analysis of human lung , gastric , and breast cancers , <digit> genes were seen to be silenced and additional genes seen to show decreased expression in association with methylation of genomic regions outside cgis in the <digit> regions of genes . ms rda is effective in identifying silenced genes in various cancers .
simple ptass for families of graphs excluding a minor . <eos> we show that very simple algorithms based on local search are polynomial time approximation schemes for maximum independent set , minimum vertex cover and minimum dominating set , when the input graphs have a fixed forbidden minor .
a modification of the index of liou and wang for ranking fuzzy number . <eos> different methods have been proposed for ranking fuzzy numbers . these include methods based on distances , centroid point , coefficient of variation , and weighted mean value . however , there is still no method that can always give a satisfactory result to every situation some are counterintuitive and not discriminating . this paper presents an approach for ranking fuzzy numbers with integral value that is an extension of the index of liou and wang . this method , that is independent of the type of membership function used , can rank more than two fuzzy numbers simultaneously . this ranking method use an index of optimism to reflect the decision maker 's optimistic attitude , but rather it also contains an index of modality that represents the neutrality of the decision maker . the approach is illustrated with numerical examples .
a mobility based load control scheme in hierarchical mobile ipv6 networks . <eos> by introducing a mobility anchor point ( map ) , hierarchical mobile ipv6 ( hmipv6 ) reduces the signaling overhead and handoff latency associated with mobile ipv6 . in this paper , we propose a mobility based load control ( mlc ) scheme , which mitigates the burden of the map in fully distributed and adaptive manners . the mlc scheme combines two algorithms a threshold based admission control algorithm and a session to mobility ratio ( smr ) based replacement algorithm . the threshold based admission control algorithm gives higher priority to ongoing mobile nodes ( mns ) than new mns , by blocking new mns when the number of mns being serviced by the map is greater than a predetermined threshold . on the other hand , the smr based replacement algorithm achieves efficient map load distribution by considering mns traffic and mobility patterns . we analyze the mlc scheme using the continuous time markov chain in terms of the new mn blocking probability , ongoing mn dropping probability , and binding update cost . also , the map processing latency is evaluated based on the m g <digit> queueing model . analytical and simulation results demonstrate that the mlc scheme outperforms other schemes and thus it is a viable solution for scalable hmipv6 networks .
randomized diffusion for indivisible loads . <eos> presentation of new algorithm for balancing discrete loads . algorithm is very natural and avoids nodes having negative loads . quality measure is the gap between maximum and minimum load , called discrepancy . upper bounds on discrepancy after algorithm has run as long as its continuous counterpart .
automatic image segmentation by dynamic region merging . <eos> this paper addresses the automatic image segmentation problem in a region merging style . with an initially oversegmented image , in which many regions ( or superpixels ) with homogeneous color are detected , an image segmentation is performed by iteratively merging the regions according to a statistical test . there are two essential issues in a region merging algorithm order of merging and the stopping criterion . in the proposed algorithm , these two issues are solved by a novel predicate , which is defined by the sequential probability ratio test and the minimal cost criterion . starting from an oversegmented image , neighboring regions are progressively merged if there is an evidence for merging according to this predicate . we show that the merging order follows the principle of dynamic programming . this formulates the image segmentation as an inference problem , where the final segmentation is established based on the observed image . we also prove that the produced segmentation satisfies certain global properties . in addition , a faster algorithm is developed to accelerate the region merging process , which maintains a nearest neighbor graph in each iteration . experiments on real natural images are conducted to demonstrate the performance of the proposed dynamic region merging algorithm .
boundary properties of the inconsistency of pairwise comparisons in group decisions . <eos> we study the inconsistency of pairwise comparisons in group decision making . we study the effect of the aggregation of pairwise comparisons on their consistency . we derive general results and provide a complete study for well known inconsistency indices . we start a discussion on the meaning of inconsistency of aggregated preferences .
sensitivity of optimal prices to system parameters in a steady state service facility . <eos> we consider the problem of maximizing the long run average reward in a service facility with dynamic pricing . we investigate sensitivity of optimal pricing policies to the parameters of the service facility which is modelled as an m m s k m m s k queueing system . arrival process to the facility is poisson with arrival rate a decreasing function of the price currently being charged by the facility . we prove structural results on the optimal pricing policies when the parameters in the facility change . namely , we show that optimal prices decrease when the capacity of the facility or the number of servers in the facility increase . under a reasonable assumption , we also show that optimal prices increase as the overall demand for the service provided by the facility increases or when the service rate of the facility decreases . we illustrate how these structural results simplify the required computational effort while finding the optimal policy .
a self organizing p2p system with multi dimensional structure . <eos> this paper presents and analyzes self can , a self organizing p2p system that , while relying on the multi dimensional structured organization of peers provided by can , exploits the operations of ant based mobile agents to sort the resource keys and distribute them to peers . the benefits of the self organization approach are remarkable , starting from increased flexibility and robustness , to better load balancing characteristics . most notably , peer indexes and resource keys can be defined on different and independent spaces , which overcomes the main limitation of standard structured p2p systems , i.e. , the necessity of assigning each key to a peer having a specified index . this decoupling opens the possibility of giving a semantic meaning to resource keys and enables the efficient execution of multi dimensional range queries , which are essential in some types of distributed systems , for example in grids .
self organized traffic control . <eos> in this paper we propose and present preliminary results on the migration of traffic lights as roadside based infrastructures to in vehicle virtual signs supported only by vehicle to vehicle communications . we design a virtual traffic light protocol that can dynamically optimize the flow of traffic in road intersections without requiring any roadside infrastructure . elected vehicles act as temporary road junction infrastructures and broadcast traffic light messages that are shown to drivers through in vehicle displays . this approach renders signalized control of intersections truly ubiquitous , which significantly increases the overall traffic flow . we pro vide compelling evidence that our proposal is a scalable and cost effective solution to urban traffic control .
search strategies on a new health information retrieval system . <eos> purpose the goals of this study are to evaluate the merits of a newly developed health information retrieval system to investigate users ' search strategies when using the new search system and to study the relationships between users ' search strategies and their prior topic knowledge . design methodology approach the paper developed a new health information retrieval system called meshmed . a term browser and a tree browser are included in the new system in addition to the traditional search box . the term browser allows a user to search medical subject heading ( mesh ) terms using natural language . the tree browser presents a hierarchical tree structure of related mesh terms . a user study with <digit> participants was conducted to evaluate the benefits of meshmed . findings the paper found that meshmed provides a user with more choices to select an appropriate searching component and form more effective search strategies . based on the time a participant spent using different meshmed components , the paper identified three different search styles the traditional style , the novel style , and the balanced style , which falls in between . meshmed was particularly helpful for users with low topic knowledge . originality value a new health information retrieval system ( meshmed ) was designed and developed ( and is currently available at http 129.89.43.129 meshmed ) . this is the first study to explore users ' search strategies on such a system . the study results can inform the design of future clinical oriented health information retrieval systems .
anfis approach to the scour depth prediction at a bridge abutment . <eos> an accurate estimation of the maximum possible scour depth at bridge abutments is of paramount importance in decision making for the safe abutment foundation depth and also for the degree of scour counter measure to be implemented against excessive scouring . despite analysis of innumerable prototype and hydraulic model studies in the past , the scour depth prediction at the bridge abutments has remained inconclusive . this paper presents an alternative to the conventional regression model ( rm ) in the form of an adaptive network based fuzzy inference system ( anfis ) modelling . the performance of anfis over rm and artificial neural networks ( anns ) is assessed here . it was found that the anfis model performed best among of these methods . the causative variables in raw form result in a more accurate prediction of the scour depth than that of their grouped form .
an improved strength pareto evolutionary algorithm <digit> with application to the optimization of distributed generations . <eos> this paper presents an improved strength pareto evolutionary algorithm <digit> ( ispea2 ) , which introduces a penalty factor in objective function constraints , uses adaptive crossover and a mutation operator in the evolutionary process , and combines simulated annealing iterative process over spea2 . the testing result of ispea2 by authoritative testing functions meets the requirement of petro optimum fronts . the case study result shows that the proposed algorithm provides a rapid convergence in obtaining pareto optimal solutions during the calculation process of evolution . based on the fuzzy set theory , ispea2 is able to solve the multi objective problems in the ieee <digit> bus system , and its validity and practicality are demonstrated by the utilization on dgs economic dispatch and optimal operation in the field of power industry .
a distributed instrument for performance analysis of real time ethernet networks . <eos> ethernet technology is widely used in real time industrial automation . thanks to real time ethernet ( rte ) protocols , defined in iec61784 <digit> standard , new top performance automation solutions can be created . such systems may have communication cycle time down to tens of mu s and cycle jitter less than <digit> mu s , making network testing and debugging very critical . existing network and protocol analyzers can perform detailed local analysis , but characterization of high performance rte systems requires measurement of transmission delays and these instruments can not be adequately synchronized among them to realize a distributed measurement network . this paper introduces a new low cost distributed measurement instrument to measure timing characteristics of rte nodes ( end to end delays , synchronization , etc. ) . the proposed instrument has multiple fpga based probes that allow for simultaneous synchronized logging on different place of the target rte network . a pc based monitor station stores all the data , ready for further elaboration . architecture details are discussed , a prototype has been realized , and some experimental results are presented . for instance , synchronization accuracy between probes is below <digit> ns .
documentation standards for beginning students . <eos> the importance of writing programs that are readable has finally gained preeminence in the struggle with such competing and contradictory goals as cuteness and optimization of code . as a result , a much greater stress on documentation standards is found in computer science education these days . industry and government standards for documentation are being more widely adhered to and certain points of agreement have emerged . some excellent books have been written that cover the subject ( van tassel , <digit> ledgard , <digit> kernighan plauger , <digit> ) however it is safe to say that both the exhaustive treatment of the subject in such publications and the extremely high standards proposed probably preclude wholesale adoption by instructors of beginning level programming courses . what is proposed here is a set of common sense , scaled down documentation standards for the student in a first programming course in , say , fortran , pl i , algol , or basic . the following represents an amalgam of documentation requirements achieved as a result of teaching introductory programming to college students for nine years . the actual sources have been the literature , colleagues , and last but not least , experience . they are not intended to represent an only or best approach the author has recently encountered other efforts in this direction that must surely be as reasonable and effective . it does represent one educator 's approach it is sufficiently scaled down so that one might reasonably expect to use it as a standard for beginning students and it may be most useful as a contributor of components to be integrated into a more effective set of standards . the basics of documentation and readable programming include comments , meaningful variable names , labelled output , flowcharts , and clear program flow . the major components of and basic rules for each of these categories will be presented in the context of the needs and limitations of the beginning student .
differential space time modulation for ds cdma systems . <eos> differential space time modulation ( dstm ) schemes were recently proposed to fully exploit the transmit and receive antenna diversities without the need for channel state information . dstm is attractive in fast flat fading channels since accurate channel estimation is difficult to achieve . in this paper . we propose a new modulation scheme to improve the performance of ds cdma systems in fast time dispersive fading channels . this scheme is referred to as the differential space time modulation for ds cdma ( dst cdma ) systems . the new modulation and demodulation schemes are especially studied for the fast fading down link transmission in ds cdma systems employing multiple transmit antennas and one receive antenna . we present three demodulation schemes . referred to as the differential space time rake ( dstr ) receiver , differential space time deterministic ( dstd ) receiver , and differential space time deterministic de prefix ( dstdd ) receiver , respectively . the dstd receiver exploits the known information of the spreading sequences and their delayed paths deterministically besides the rake type combination consequently , it can outperform the dstr receiver . which employs the rake type combination only , especially for moderate to high snr . the dstdd receiver avoids the effect of intersymbol interference and hence can offer better performance than the dstd receiver . copyright ( c ) <digit> john wiley sons , ltd .
on the normalization of interval and fuzzy weights . <eos> the normalization of interval and fuzzy weights is often necessary in multiple criteria decision analysis ( mcda ) under uncertainty , especially in analytic hierarchy process ( ahp ) with interval or fuzzy judgements . the existing normalization methods based on interval arithmetic and fuzzy arithmetic are found flawed and need to be revised . this paper presents the correct normalization methods for interval and fuzzy weights and offers relevant theorems in support of them . numerical examples are examined to show the correctness of the proposed normalization methods and their differences from those existing normalization methods . ( c ) <digit> elsevier b.v. all rights reserved .
a queue with semi markovian batch plus poisson arrivals with application to the mpeg frame sequence . <eos> we consider a queueing system with a single server having a mixture of a semi markov process ( smp ) and a poisson process as the arrival process , where each smp arrival contains a batch of customers . the service times are exponentially distributed . we derive the distributions of the queue length of both smp and poisson customers when the sojourn time distributions of the smp have rational laplace stieltjes transforms . we prove that the number of unknown constants contained in the generating function for the queue length distribution equals the number of zeros of the denominator of this generating function in the case where the sojourn times of the smp follow exponential distributions . the linear independence of the equations generated by those zeros is discussed for the same case with additional assumption . the necessary and sufficient condition for the stability of the system is also analyzed . the distributions of the waiting times of both smp and poisson customers are derived . the results are applied to the case in which the smp arrivals correspond to the exact sequence of motion picture experts group ( mpeg ) frames . poisson arrivals are regarded as interfering traffic . in the numerical examples , the mean and variance of the waiting time of the atm cells generated from the mpeg frames of real video data are evaluated .
fractal dimension as a descriptor of urban growth dynamics . <eos> the objective of this paper is to examine the development of the urban form of the city of olomouc since the 1920s in terms of fractal dimension , and to link the observation with two other descriptors of shape area and perimeter . the fractal dimension of built up areas and fractal dimension of the boundary of the city are calculated employing the box counting method the possibilities of their interpretation and usage in urban planning are discussed . the process of urban growth is observed with respect to its fractality and perspectives of this approach are discussed . an interesting dependence between area and its fractal dimension is derived .
adaptive critics for dynamic optimization . <eos> a novel action dependent adaptive critic design ( acd ) is developed for dynamic optimization . the proposed combination of a particle swarm optimization based actor and a neural network critic is demonstrated through dynamic sleep scheduling of wireless sensor motes for wildlife monitoring . the objective of the sleep scheduler is to dynamically adapt the sleep duration to nodes battery capacity and movement pattern of animals in its environment in order to obtain snapshots of the animal on its trajectory uniformly . simulation results show that the sleep time of the node determined by the actor critic yields superior quality of sensory data acquisition and enhanced node longevity .
linearity and recursion in a typed lambda calculus . <eos> we show that the full pcf language can be encoded in l _ rec , a syntactically linear calculus extended with numbers , pairs , and an unbounded recursor that preserves the syntactic linearity of the calculus . we give call by name and call by value evaluation strategies and discuss implementation techniques for l _ rec , exploiting its linearity .
power aware page allocation . <eos> one of the major challenges of post pc computing is the need to reduce energy consumption , thereby extending the lifetime of the batteries that power these mobile devices . memory is a particularly important target for efforts to improve energy efficiency . memory technology is becoming available that offers power management features such as the ability to put individual chips in any one of several different power modes . in this paper we explore the interaction of page placement with static and dynamic hardware policies to exploit these emerging hardware features . in particular , we consider page allocation policies that can be employed by an informed operating system to complement the hardware power management strategies . we perform experiments using two complementary simulation environments a trace driven simulator with workload traces that are representative of mobile computing and an execution driven simulator with a detailed processor memory model and a more memory intensive set of benchmarks ( spec2000 ) . our results make a compelling case for a cooperative hardware software approach for exploiting power aware memory , with down to as little as <digit> % of the energy delay for the best static policy and <digit> % to <digit> % of the energy delay for a traditional full power memory .
change rules for hierarchical beliefs . <eos> the paper builds a belief hierarchy as a framework common to all uncertainty measures expressing that an actor is ambiguous about his uncertain beliefs . the belief hierarchy is further interpreted by distinguishing physical and psychical worlds , associated to objective and subjective probabilities . various rules of transformation of a belief hierarchy are introduced , especially changing subjective beliefs into objective ones . these principles are applied in order to relate different contexts of belief change , revising , updating and even focusing . the numerous belief change rules already proposed in the literature receive epistemic justifications by associating them to specific belief hierarchies and change contexts . as a result , it is shown that the resiliency of probability judgments may have some limits and be reconciled with the possibility of learning from factual messages . ( c ) <digit> published by elsevier inc .
a characterization of the two commodity network design problem . <eos> we study the uncapacitated version of the two commodity network design problem . we characterize optimal solutions and show that when flow costs are zero there is an optimal solution with at most one shared path . using this characterization , we solve the problem on a transformed graph with o ( n ) nodes and o ( m ) arcs based on a shortest path algorithm . next , we describe a linear programming reformulation of the problem using o ( m ) variables and o ( n ) constraints and show that it always has an integer optimal solution . we also interpret the dual constraints and variables as generalizations of the are constraints and node potentials for the shortest path problem . we show that the polyhedron described by the constraints of the reformulation always has an integer optimal solution for a more general two commodity problem with flow costs and an additional condition on the cost function . ( c ) <digit> john wiley sons , inc .
on the minimum number of negations leading to super polynomial savings . <eos> we show that an explicit sequence of monotone functions f ( n ) <digit> , <digit> ( n ) > <digit> , <digit> ( m ) ( m less than or equal to n ) can be computed by boolean circuits with polynomial ( in n ) number of and , or and not gates , but every such circuit must use at least log n o ( log log n ) not gates . this is almost optimal because results of markov j. acm <digit> ( <digit> ) <digit> and fisher lecture notes in comput . sci. , vol . <digit> , springer , <digit> , p. <digit> imply that , with only small increase of the total number of gates , any circuit in n variables can be simulated by a circuit with at most log ( n <digit> ) not gates . ( c ) <digit> elsevier b.v. all rights reserved .
differential log domain wave active filters . <eos> in this paper , the design of differential log domain wave filters is outlined which is based on the log domain wave technique . the differential configuration is achieved by introducing the differential log domain wave equivalent . the resulting filters inherit the good characteristics of the wave active filters , are very modular and easy to design . the method is demonstrated by a design example and its validity is verified through simulations results .
a hybrid method based on linear programming and tabu search for routing of logging trucks . <eos> in this paper , we consider an operational routing problem to decide the daily routes of logging trucks in forestry . this industrial problem is difficult and includes aspects such as pickup and delivery with split pickups , multiple products , time windows , several time periods , multiple depots , driver changes and a heterogeneous truck fleet . in addition , the problem size is large and the solution time limited . we describe a two phase solution approach which transforms the problem into a standard vehicle routing problem with time windows . in the first phase , we solve an lp problem in order to find a destination of flow from supply points to demand points . based on this solution , we create transport nodes which each defines the origin ( s ) and destination for a full truckload . in phase two , we make use of a standard tabu search method to combine these transport nodes , which can be considered to be customers in vehicle routing problems , into actual routes . the tabu search method is extended to consider some new features . the solution approach is tested on a set of industrial cases from major forest companies in sweden .
the definition of assembly line balancing difficulty and evaluation of balance solution quality . <eos> assembly line balancing is a classic ill structured problem where total enumeration is infeasible and optimal solutions uncertain for industrial problems . a quantitative approach to classifying problem difficulty and solution quality is therefore important . two existing measures of difficulty , order strength and west ratio are compared to a new compound expression of difficulty , project index . project index is based on individual assessment of precedence ( precedence index ) and task time ( task time index ) . the current working definition of project index is given . early criteria for judging assembly lines use balance delay and smoothness index , both are flawed as criteria . line and balance efficiency are developed as more appropriate . project index , line and balance efficiency will be illustrated for a published test case examined by the a line balancing package . the potential for a learning approach , selecting models to suit problems using the measures of difficulty , will form part of the conclusions within this paper .
a class oriented feature selection approach for multi class imbalanced network traffic datasets based on local and global metrics fusion . <eos> feature selection is often used as a pre processing step for machine learning based network traffic classification . many feature selection techniques have been developed to find an optimal subset of relevant features and to improve overall classification accuracy . but such techniques ignore the class imbalance problem encountered in network traffic classification . the selected feature subset may bias towards the traffic class that occupies the majority of traffic flows on the internet . to address this issue , this paper proposes a new approach , called class oriented feature selection ( cofs ) , to identify a relevant feature subset for every class . it combines the proposed local metric and the existing global metric to yield a potentially optimal feature subset for each class , and then removes the redundant features in each feature subset based on the weighted symmetric uncertainty . additionally , to enhance the generalization on network traffic data , an ensemble learning based scheme is presented with cofs to overcome the negative impacts of the data drift on a traffic classifier . experiments on real world network traffic data show that cofs outperforms existing feature selection techniques in most cases . moreover , our approach achieves > <digit> % flow accuracy and > <digit> % byte accuracy on average .
identifying and quantifying structural nonlinearities in engineering applications from measured frequency response functions . <eos> engineering structures seldom behave linearly and , as a result , linearity checks are common practice in the testing of critical structures exposed to dynamic loading to define the boundary of validity of the linear regime . however , in large scale industrial applications , there is no general methodology for dynamicists to extract nonlinear parameters from measured vibration data so that these can be then included in the associated numerical models . in this paper , a simple method based on the information contained in the frequency response function ( frf ) properties of a structure is studied . this technique falls within the category of single degree of freedom ( sdof ) modal analysis methods . the principle upon which it is based is effectively a linearisation whereby it is assumed that at given amplitude of displacement response the system responds at the same frequency as the excitation and that stiffness and damping are constants . in so doing , by extracting this information at different amplitudes of vibration response , it is possible to estimate the amplitude dependent natural frequency and modal loss factor . because of its mathematical simplicity and practical implementation during standard vibration testing , this method is particularly suitable for practical applications . in this paper , the method is illustrated and new analyses are carried out to validate its performance on numerical simulations before applying it to data measured on a complex aerospace test structure as well as a full scale helicopter .
on the formal specification and verification of multi agent systems . <eos> this article describes first steps towards the formal specification and verification of multiagent systems , through the use of temporal belief logics . the article first describes concurrent metatem , a multi agent programming language , and then develops a logic that may be used to reason about concurrent metatem systems . the utility of this logic for specifying and verifying concurrent metatem systems is demonstrated through a number of examples . the article concludes with a brief discussion on the wider implications of the work , and in particular on the use of similar logics for reasoning about multi agent systems in general .
a contribution to multimedia document modeling and querying . <eos> metadata on multimedia documents may help to describe their content and make their processing easier , for example by identifying events in temporal media , as well as carrying descriptive information for the overall resource . metadata is essentially static and may be associated with , or embedded in , the multimedia contents . the aim of this paper is to present a proposal for multimedia documents annotation , based on modeling and unifying features elicited from content and structure mining . our approach relies on the availability of annotated metadata representing segment content and structure as well as segment transcripts . temporal and spatial operators are also taken into account when annotating documents . any feature is identified into a descriptor called meta document . these meta documents are the basis of querying by adapted query languages .
a comparison of pressure and tilt input techniques for cursor control . <eos> three experiments were conducted in this study to investigate the human ability to control pen pressure and pen tilt input , by coupling this control with cursor position , angle and scale . comparisons between pen pressure input and pen tilt input have been made in the three experiments . experimental results show that decreasing pressure input resulted in very poor performance and was not a good input technique for any of the three experiments . in experiment <digit> coupling to cursor position , the tilt input technique performed relatively better than the increasing pressure input technique in terms of time . even though the tilt technique had a slightly higher error rate . in experiment <digit> coupling to cursor angle , the tilt input performed a little better than the increasing pressure input in terms of time , but the gap between them is not so apparent as experiment <digit> . in experiment <digit> coupling to cursor scale , tilt input performed a little better than increasing pressure input in terms of adjustment time . based on the results of our experiments , we have inferred several design implications and guidelines .
a framework for analyzing the cognitive complexity of computer assisted clinical ordering . <eos> computer assisted provider order entry is a technology that is designed to expedite medical ordering and to reduce the frequency of preventable errors . this paper presents a multifaceted cognitive methodology for the characterization of cognitive demands of a medical information system . our investigation was informed by the distributed resources ( dr ) model , a novel approach designed to describe the dimensions of user interfaces that introduce unnecessary cognitive complexity . this method evaluates the relative distribution of external ( system ) and internal ( user ) representations embodied in system interaction . we conducted an expert walkthrough evaluation of a commercial order entry system , followed by a simulated clinical ordering task performed by seven clinicians . the dr model was employed to explain variation in user performance and to characterize the relationship of resource distribution and ordering errors . the analysis revealed that the configuration of resources in this ordering application placed unnecessarily heavy cognitive demands on the user , especially on those who lacked a robust conceptual model of the system . the resources model also provided some insight into clinicians interactive strategies and patterns of associated errors . implications for user training and interface design based on the principles of humancomputer interaction in the medical domain are discussed .
crack propagation analysis in composite materials by using moving mesh and multiscale techniques . <eos> a novel multiscale method for crack propagation analysis in composites is proposed . an adaptive model refinement is used during crack propagation to improve efficiency . competition between different damage mechanisms is handled during crack simulation . matrix cracking is modeled by a novel optimization strategy based on moving meshes . the proposed approach is validated by original comparisons with existing methods .
motivations in virtual health communities and their relationship to community , connectedness and stress . <eos> this study explores the relationships between motivations for joining virtual health communities , online behaviors , and psycho social outcomes . a sample of <digit> women from two virtual health communities focusing on infertility completed survey measures assessing motivations , posting and receiving support , connectedness , community , and stress . our results indicate that socio emotional support motivations for joining the community were associated with posting support within the virtual community , while informational motivations were related to receiving support . further , receiving support was associated with greater sense of virtual community as well as more general feelings of connectedness , which was related to less stress . implications for virtual health community research are discussed .
enterprise resource planning implementation procedures and critical success factors . <eos> enterprise resource planning ( erp ) systems are highly complex information systems . the implementation of these systems is a difficult and high cost proposition that places tremendous demands on corporate time and resources . many erp implementations have been classified as failures because they did not achieve predetermined corporate goals . this article identifies success factors , software selection steps , and implementation procedures critical to a successful implementation . a case study of a largely successful erp implementation is presented and discussed in terms of these key factors .
dflowz a free program to evaluate the area potentially inundated by a debris flow . <eos> debris flow inundated area can be estimated using scaling relationships . we provide a free , open source program to evaluate debris flow hazard . the model considers the uncertainties in scaling relationships and input data . a graphical user interface facilitate the process of susceptibility mapping .
a novel approach to identify optimal access point and capacity of multiple dgs in a small , medium and large scale radial distribution systems . <eos> distributed generation ( dg ) sources are predicated to play major role in distribution systems due to the demand growth for electrical energy . location and sizing of dg sources found to be important on the system losses and voltage stability in a distribution network . in this paper an efficient technique is presented for optimal placement and sizing of dgs in a large scale radial distribution system . the main objective is to minimize network power losses and to improve the voltage stability . a detailed performance analysis is carried out on <digit> bus , <digit> bus and <digit> bus large scale radial distribution systems to demonstrate the effectiveness of the proposed technique . performing multiple power flow analysis on <digit> bus system , the effect of dg sources on the most sensitive buses to voltage collapse is also carried out .
robust discrete control of nonlinear processes application to chemical reactors . <eos> trajectory tracking or rejecting persistent disturbances with digital controllers in nonlinear processes is a class of problems where classical control methods breakdown since it is very difficult to describe the dynamic behavior over the entire trajectory . in this paper , a model based robust control scheme is proposed as a potential solution approach for these systems . the proposed control algorithm is a robust error feedback controller that allows us to track predetermined operation profiles while attenuating the disturbances and maintaining the stability conditions of the nonlinear processes . various numerical simulation examples demonstrate the effectiveness of this robust scheme . two examples deal with effective trajectory tracking in chemical reactors over a wide range of operating conditions . the third example analyses the attenuation of periodic load in a biological reactor . all examples illustrate the ability of the robust control scheme to provide good control in the face of parameter uncertainties and load disturbances . ( c ) <digit> elsevier ltd. all rights reserved .
wavelet based statistical approach for speckle reduction in medical ultrasound images . <eos> a novel speckle reduction method is introduced , based on soft thresholding of the wavelet coefficients of a logarithmically transformed medical ultrasound image . the method is based on the generalised gaussian distributed ( ggd ) modelling of sub band coefficients . the method used was a variant of the recently published bayesshrink method by chang and vetterli , derived in the bayesian framework for denoising natural images . it was scale adaptive , because the parameters required for estimating the threshold depend on scale and sub band data . the threshold was computed by ksigma ( <digit> ) sigma ( x ) , where sigma and sigma ( x ) were the standard deviation of the noise and the sub band data of the noise free image , respectively , and k was a scale parameter . experimental results showed that the proposed method outperformed the median filter and the homomorphic wiener filter by <digit> % in terms of the coefficient of correlation and <digit> % in terms of the edge preservation parameter . the numerical values of these quantitative parameters indicated the good feature preservation performance of the algorithm , as desired for better diagnosis in medical image processing .
computer based imaging and interventional mri applications for neurosurgery . <eos> advances in computer technology and the development of open mri systems definitely enhanced intraoperative image guidance in neurosurgery . based upon the integration of previously acquired and processed 3d information and the corresponding anatomy of the patient , this requires computerized image processing methods ( segmentation , registration , and display ) and fast image integration techniques . open mr systems equipped with instrument tracking systems , provide an interactive environment in which biopsies and minimally invasive interventions or open surgeries can be performed . enhanced by the integration of multimodal imaging these techniques significantly improve the available treatment options and can change the prognosis for patients with surgically treatable diseases .
dynamic response of a pile embedded in a porous medium subjected to plane sh waves . <eos> in this paper , the frequency domain dynamic response of a pile embedded in a porous medium subjected to sh seismic waves is investigated . the surrounding porous medium of the pile is described by biots poro elastic theory , while the pile embedded in the porous medium is treated as a beam and described by a beam vibration theory . using the hankel transformation method , the fundamental solution for a half space porous medium subjected to a horizontal circular patch load is established . according to the fictitious pile methodology , the second kind of fredholm integral equation for the pile is established in terms of the obtained fundamental solution and free wave field . the solution of the integral equation yields the dynamic response of the pile to plane sh waves . numerical results indicate that the parameters of the porous medium , the pile and incident waves have considerable influences on the dynamic response of the pile and the porous medium .
towards computational models of animal cognition , an introduction for computer scientists . <eos> the last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then . the study of animal behavior or behavior biology has been one of the major contributors for this convergence . behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves , the sensory and the motor systems . to some extent , behavior is similar to the output ( or response ) of a computer system or a network node if we consider an animal brain as a computer node . this paper is the first in a two part series in which i review the state of the art research in behavior biology inspired computing and communication , with the first part focusing on animal cognition and the second part on animal communication ( ma , <digit> ) . the present article also assumes the task of presenting a general introduction on behavior biology literature , which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series . i sets three objectives in this cognition part ( i ) to present a brief overview on the literature of behavior biology for computer scientists ( ii ) to summarize the state of the art studies in several cognitive aspects of animal behavior focusing on emerging research in cognitive ecology , social learning and innovation , as well as animal logics ( iii ) to review some important existing studies inspired by animal behavior and further present a perspective on the future research . these cognition related topics offer insights for research fields such as machine learning , human computer interactions ( hci ) , brain computer interfaces ( bcis ) , evolutionary computing , pervasive computing , etc. in perspective , i suggest that the interaction between behavioral biology and computer science should be bidirectional , and a new subject , behavioral informatics , or more general computational behavior biology , should be developed by the cooperative efforts between biologists and computer scientists .
crossing boundaries in facebook students framing of language learning activities as extended spaces . <eos> young peoples interaction online is rapidly increasing , which enables new spaces for communication the impact on learning , however , is not yet acknowledged in education . the aim of this exploratory case study is to scrutinize how students frame their interaction in social networking sites ( sns ) in school practices and what that implies for educational language teaching and learning practices . analytically , the study departs from a sociocultural perspective on learning , and adopts conceptual distinctions of frame analysis . the results based on ethnographic data from a facebook group in english learning classes , with <digit> students aged between <digit> and <digit> from colombia , finland , sweden and taiwan indicate that there is a possibility for boundary crossing , which could generate extended spaces for collaborative language learning activities in educational contexts where students combine their school subject of learning language and their communicative use of language in their everyday life . such extended spaces are , however , difficult to maintain and have to be recurrently negotiated . to take advantage of young peoples various dynamic communicative uses of language in their everyday life in social media , the implementation of such media for educational purposes has to be deliberately , collaboratively and dynamically negotiated by educators and students to form a new language learning space with its own potentials and constraints .
a study of the stabilizing process of unstable structures by dynamic relaxation method . <eos> in this paper , the stabilizing process of unstable structures is studied by dynamic relaxation method . the process of applying the internal force to unstable structures is called as stabilizing process of unstable structure . the initial behavior of unstable structures such as cables , pneumatic structures or cable domes , are unstable state because of no initial internal stiffness . the dynamic relaxation method is the energy minimization technique that searches the static equilibrium state by simple vector iteration method . because the dynamic relaxation method does not need to assemble the tangential stiffness matrix of structure during each iteration of the stabilizing process , the computational effort and cpu run time can be reduced . the finite difference integration technique is used to integrate the dynamic equilibrium equation for static equilibrium state . some numerical examples are presented to confirm the efficiency and applicability of dynamic relaxation method .
a novel parallelization approach for hierarchical clustering . <eos> identification of groups of genes that manifest similar expression patters is a key step in the analysis of gene expression data . hierarchical clustering is developed for that purpose . a fundamental problem with the previous implementations of this clustering method is its limitation to handle large data sets within a reasonable time and memory resources . in this paper , we present a parallel approach for solving this problem . implementation of the parallel algorithm is illustrated on data from high dimensional microarray experiments related to the gene expression in cancerous disease and arabidopsis seedling growth . they show considerable reduction in computational time and inter node communication overhead , especially for large data sets .
surjective multidimensional cellular automata are non wandering a combinatorial proof . <eos> a combinatorial proof that surjective d dimensional ca are non wandering is given . this answers an old open question stated in blanchard and tisseur ( <digit> ) <digit> . moreover , an explicit upper bound for the return time is given . ( c ) <digit> elsevier b.v. all rights reserved .
event based application of ws security policy on soap messages . <eos> ws security and ws security policy are the common standards for ensuring integrity and confidentiality for web service messages . on the one hand they allow very flexible definition of security requirements . on the other hand they lead to complex security administration and low performance message processing . in this paper , we present our solution for a security gateway , which uses complete event based xml and ws security processing to create policy conforming soap messages . the evaluation of our implementation shows that the event based approach leads to a much better performance than tree based ws security implementations . further , we discuss some problematical issues of ws security policy processing , such as determination of digital identities .
a scheduling problem with three competing agents . <eos> scheduling with multiple agents has become a popular topic in recent years . however , most of the research focused on problems with two competing agents . in this article , we consider a single machine scheduling problem with three agents . the objective is to minimize the total weighted completion time of jobs from the first agent given that the maximum completion time of jobs from the second agent does not exceed an upper bound and the maintenance activity from the third agent must be performed within a specified period of time . a lower bound based on job division and several propositions are developed for the branch and bound algorithm , and a genetic algorithm with a local search is constructed to obtain near optimal solutions . in addition , computational experiments are conducted to test the performance of the algorithms .
tsa tree seed algorithm for continuous optimization . <eos> this paper presents a new optimizer to solve continuous optimization problems . new optimizer is proposed by considering relations between trees and their seeds . the new optimizer is applied to solve <digit> benchmark functions . the results of the proposed method are compared with state of arts methods . the proposed method is a useful optimizer for continuous optimization .
solving fuzzy multidimensional multiple choice knapsack problems the multi start partial bound enumeration method versus the efficient epsilon constraint method . <eos> in this paper a new fuzzy multidimensional multiple choice knapsack problem ( mmkp ) is proposed . in the proposed fuzzy mmkp , each item may belong to several groups according to a predefined fuzzy membership value . the total profit and the total cost of the knapsack problem are considered as two conflicting objectives . a mathematical approach and a heuristic algorithm are proposed to solve the fuzzy mmkp . one method is an improved version of a well known exact multi objective mathematical programming technique , called the efficient constraint method . the second method is a heuristic algorithm called multi start partial bound enumeration ( pbe ) . both methods are used to comparatively generate a set of non dominated solutions for the fuzzy mmkp . the performance of the two methods is statistically compared with respect to a set of simulated benchmark cases using different diversity and accuracy metrics .
a trace transformation technique for communication refinement . <eos> models of computation like kahn and dataflow process networks provide convenient means for modeling signal processing applications . this is partly due to the abstract primitives that these models offer for communication between concurrent processes . however , when mapping an application model onto an architecture , these primitives need to be mapped onto architecture level communication primitives . we present a trace transformation technique that supports a system architect in performing this communication refinement . we discuss the implementation of this technique in a tool for architecture exploration named spade and present examples .
a regularity condition of the information matrix of a multilayer perceptron network . <eos> the fisher information matrix of a multi layer perceptron network can be singular at certain parameters , and in such cases many statistical techniques based on asymptotic theory can not be applied properly . in this paper , we prove rigorously that the fisher information matrix of a three layer perceptron network is positive definite if and only if the network is irreducible that is , if there is no hidden unit that makes no contribution to the output and there is no pair of hidden units that could be collapsed to a single unit without altering the input output map . this implies that a network that has a singular fisher information matrix can be reduced to a network with a positive definite fisher information matrix by eliminating redundant hidden units . copyright <digit> elsevier science ltd
how hci design influences web security decisions . <eos> even though security protocols are designed to make computer communication secure , it is widely known that there is potential for security breakdowns at the human machine interface . this paper reports on a diary study conducted in order to investigate what people identify as security decisions that they make while using the web . the study aimed to uncover how security is perceived in the individual 's context of use . from this data , themes were drawn , with a focus on addressing security goals such as confidentiality and authentication . this study is the first study investigating users ' web usage focusing on their self documented perceptions of security and the security choices they made in their own environment .
the complexity of finding arc disjoint branching flows . <eos> the concept of arc disjoint flows in networks was recently introduced in bang jensen and bessy ( <digit> ) . this is a very general framework within which many well known and important problems can be formulated . in particular , the existence of arc disjoint branching flows , that is , flows which send one unit of flow from a given source s s to all other vertices , generalizes the concept of arc disjoint out branchings ( spanning out trees ) in a digraph . a pair of out branchings b s , <digit> , b s , <digit> from a root s s in a digraph d ( v , a ) d ( v , a ) on n n vertices corresponds to arc disjoint branching flows x1 , x2 x <digit> , x <digit> ( the arcs carrying flow in xi x i are those used in b s , i , i 1,2 i <digit> , <digit> ) in the network that we obtain from d d by giving all arcs capacity n <digit> n <digit> . it is then a natural question to ask how much we can lower the capacities on the arcs and still have , say , two arc disjoint branching flows from the given root s s . we prove that for every fixed integer k <digit> k <digit> it is an np complete problem to decide whether a network n ( v , a , u ) n ( v , a , u ) where uij k u i j k for every arc ij i j has two arc disjoint branching flows rooted at s s . a polynomial problem to decide whether a network n ( v , a , u ) n ( v , a , u ) on n n vertices and uij n k u i j n k for every arc ij i j has two arc disjoint branching flows rooted at s s .
double ended queues with impatience . <eos> the effect of impatient behaviour is studied primarily in the context of double ended queues where each demands service from the other , typically taxis and passengers . related models , single queue , and double , with a variety of mechanisms are considered . impatience is to be understood in a wider context than simply becoming tired of waiting it can arise because the customer , for some reason , runs out of time ( inventory and organ transplantation ) , or because an alternative service becomes available ( communication applications ) . the emphasis in this paper is theoretical but a brief numerical assessment of operational consequences is given . the double ended ( or synchronization ) queue is a model for a variety of service demanding providing systems . in an orderly taxi rank at a railway station or airport , on one side a queue is formed by the arrival of stream of passengers who wait for taxis to their destinations while on the other side a queue of taxis waiting for passengers . obviously , the two queues can never coexist . the concept of impatience enters when a taxi or passenger leaves the queue before receiving service . this concept of reneging is widely applicable . in health care , for example , organs are stored for transplantation for needful patients . both the organs and the demands for them have limited lifetime . a similar scenario applies to perishable inventory systems . in a similar manner , the real time communication networks admit impatient behaviour . a typical example is a processor shared queue in data networks with random time out periods or deadlines . the paper sets out the basics in a variety of theoretical model settings with the common feature of exponential arrival , service and impatience mechanisms . a brief discussion based on numerical calculation is given of some operational features of the models but the thrust is on the theoretical techniques needed to make meaningful operational assessments .
su <digit> ridge waveguide with holographic grating embedded in nanoimprinted groove . <eos> fabrication of su <digit> slab and ridge waveguides with holographic grating for dfb laser , effectively utilizing nanoimprint technology ( nil ) , is presented . rhodamine 6g doped su <digit> slab and ridge waveguides were embedded in grooves defined by nil in uv curable resin . utilization of nil made it easier to form such a three dimensional micro structure consisting of ridge stripe and fine corrugation grating . te polarized 587nm laser and te polarized 594nm light emissions were observed from the slab and ridge waveguides , respectively , when the waveguides were irradiated by 532nm pulsed nd yag laser .
optical method for characterization of nanoplates in lyosol . <eos> in the recent years , two dimensional nanoparticles ( the nanoplates ) have become a subject of intense scientific researches and industrial applications . the liquid exfoliation of layered crystals becomes the most simple and efficient manufacturing method of graphene and layered compounds nanosheets . the primary product of the liquid exfoliation is a lyosol a colloidal suspension of desired nanoparticles in a fluid . the shape of the produced nanoparticles may be determined with the use of an electron microscopy ( sem ) . in the case of spherical particles the concentration and the particles size can be assessed by the light absorption and the dynamic laser light scattering ( dls ) measurements respectively . we propose an optical method for fast and comprehensive characterization of shape and size of nanoplates in a lyosol . the system makes use of an optical goniometer . it allows to measure angular distribution of intensity of the light scattered on the lyosol sample . the distribution of the intensity ( indicatrix ) depends on the shape and size of the nanoparticles . we developed a method of theoretical indicatrix calculation of the nanoplates with the use of amsterdam discrete dipole approximation method . the experimentally measured indicatrix is compared with the theoretical calculations ( standardized indicatrices ) . we observed that the nanoplate has a dual character of light scattering . it can scatter light as a small or big particle according to the direction of the incident light beam . we have applied the proposed method of characterization for a water suspension of graphene nanosheets . results of the measurements are in good agreement with particles dimensions assessed with the sem images and do not fit with the graphene dimensions estimated with the dls method . it leads to a conclusion that the two dimensional particles size estimation with the dls is highly inaccurate .
what you see is what you code a live algorithm development and visualization environment for novice learners . <eos> pedagogical algorithm visualization ( av ) systems produce graphical representations that aim to assist learners in understanding the dynamic behavior of computer algorithms . in order to foster active learning , computer science educators have developed av systems that empower learners to construct their own visualizations of algorithms under study . notably , these systems support a similar development model in which coding an algorithm is temporally distinct from viewing and interacting with the resulting visualization . given that they are known to have problems both with formulating syntactically correct code , and with understanding how code executes , novice learners would appear likely to benefit from a more live development model that narrows the gap between coding an algorithm and viewing its visualization . in order to explore this possibility , we have implemented what you see is what you code , an algorithm development and visualization model geared toward novices first learning to program under the imperative paradigm . in the model , the line of algorithm code currently being edited is reevaluated on every edit , leading to immediate syntactic feedback , along with immediate semantic feedback in the form of an av . analysis of usability and field studies involving introductory computer science students suggests that the immediacy of the model 's feedback can help novices to quickly identify and correct programming errors , and ultimately to develop semantically correct code .
inver2dbasea program to compute basement depths of density interfaces above which the density contrast varies with depth . <eos> a computer program to invert the gravity anomalies of density interfaces above which the density contrast varies with depth , is presented . the sedimentbasement interface is approximated by an n sided polygon . a function subprogram gr2dpol and two subroutine subprograms zor2dpol and simeq support the main program . subroutine zor2dpol calculates the initial depth estimates of a density interface at all anomaly points on the principal profile using the infinite slab approximation . function subprogram gr2dpol computes the theoretical gravity anomaly of a density interface at each anomaly point on the profile and returns to the main program . subroutine simeq is used to solve n incremental depth parts of the vertices of a density interface . partial derivatives are calculated by a simple finite difference method . normal equations are constructed and solved by marquardt 's algorithm . the proposed inversion scheme is independent on the equal station spacing criteria . the validity of the algorithm is demonstrated by calculating the basement depths of the tucson basin , southern arizona .
graph based construction of textured large field of view mosaics for bladder cancer diagnosis . <eos> large field of view panoramic images greatly facilitate bladder cancer diagnosis and follow up . such 2d mosaics can be obtained by registering the images of a video sequence acquired during cystoscopic examinations . the scientific challenge in the registration process lies in the strong inter and intra patient texture variability of the images , from which primitives can not be robustly extracted . state of the art registration methods are not at the same time robust and accurate , especially for image pairs with a small amount of overlap ( less than <digit> % ) or strong perspective transformations . moreover , no previous contribution to cystoscopy mosaicing presents panoramic images created from multiple overlapping sequences ( e.g. zigzags or loop trajectories ) . we show how such overlapping sections can be automatically detected and present a novel registration algorithm that robustly superimposes non consecutive image pairs , which are related by stronger perspective transformations and share less overlap than consecutive images ( less than <digit> % ) . globally coherent panoramic images are constructed using a non linear optimization and a novel contrast enhancing stitching method . results on both phantom and patient data are obtained using constant algorithm parameters , which demonstrate the robustness of the proposed method . while the methods presented in this contribution are specifically designed for cystoscopy mosaicing , they can also be applied to more general mosaicing problems . we demonstrate this on a traditional stitching application , where a set of pictures of a building are stitched into a seamless , globally coherent panoramic image .
an heuristic set for evaluation in information visualization . <eos> evaluation is a key research challenge within the international information visualization ( infovis ) community , and heuristic evaluation is one recognized method . various sets of heuristics have been proposed but there remains no consensus as to which heuristics are most useful for addressing aspects specific to the complex interactive visual displays used in modern infovis systems . this paper presents a first effort to empirically determine a new set of such general heuristics tailored for heuristic evaluation of common and important usability problems in infovis techniques . participants in the study rated how well a total of <digit> heuristics from <digit> earlier published heuristic sets could explain a collection of <digit> usability problems derived from earlier infovis evaluations . the results were used to synthesize <digit> heuristics that , as a set , provided the highest explanatory coverage . the paper also stresses the challenges for future research to validate and further improve upon this set .
a dependable infrastructure for cooperative web services coordination . <eos> a current trend in the web services community is to define coordination mechanisms to execute collaborative tasks involving multiple organizations . following this tendency , in this paper the authors present a dependable ( i.e. , intrusion tolerant ) infrastructure for cooperative web services coordination that is based on the tuple space coordination model . this infrastructure provides decoupled communication and implements several security mechanisms that allow dependable coordination even in presence of malicious components . this work also investigates the costs related to the use of this infrastructure and possible web service applications that can benefit from it .
performance evaluation of an ieee 802.15.4 sensor network with a star topology . <eos> one class of applications envisaged for the ieee 802.15.4 lr wpan ( low data rate wireless personal area network ) standard is wireless sensor networks for monitoring and control applications . in this paper we provide an analytical performance model for a network in which the sensors are at the tips of a star topology , and the sensors need to transmit their measurements to the hub node so that certain objectives for packet delay and packet discard are met . we first carry out a saturation throughput analysis of the system i.e. , it is assumed that each sensor has an infinite backlog of packets and the throughput of the system is sought . after a careful analysis of the csma ca mac that is employed in the standard , and after making a certain decoupling approximation , we identify an embedded markov renewal process , whose analysis yields a fixed point equation , from whose solution the saturation throughput can be calculated . we validate our model against ns2 simulations ( using an ieee 802.15.4 module developed by zheng <digit> ) . we find that with the default back off parameters the saturation throughput decreases sharply with increasing number of nodes . we use our analytical model to study the problem and we propose alternative back off parameters that prevent the drop in throughput . we then show how the saturation analysis can be used to obtain an analytical model for the finite arrival rate case . this finite load model captures very well the qualitative behavior of the system , and also provides a good approximation to the packet discard probability , and the throughput . for the default parameters , the finite load throughput is found to first increase and then decrease with increasing load . we find that for typical performance objectives ( mean delay and packet discard ) the packet discard probability would constrain the system capacity . finally , we show how to derive a node lifetime analysis using various rates and probabilities obtained from our performance analysis model .
navigation models for a flexible , multi mode vr navigation framework . <eos> navigation is a key issue for virtual reality ( vr ) applications because it forms an integral part of the feeling of presence , which should be conveyed by vr applications . this paper presents several vr navigation modes which are useful for orientation and interaction in virtual environments ( ves ) . due to the existence of different kinds of applications several navigation modes are required .
multiscale simulations application to the heat transfer simulation of sliding solids . <eos> molecular dynamics is a powerful tool allowing the simulation of matter behaviour at the atomic scale . due to computation time , it is clearly not possible to use molecular dynamics to simulate a forming process . however , atomistic simulations can be used to study and understand the physical phenomena that occur during matter deformation . as an example , heat transfer between the contacting solids in forming processes is one of the important physics phenomena that have to be taken into account in order to do realistic simulations . a multiscale analysis of heat transfer is presented . this analysis leads to two kinds of models a macroscopic model which can be used for the simulation of the process itself and a microscopic model that is used to determine the parameters of the macroscopic model . in this microscopic model , the friction heat generation phenomena has to be described quite accurately . friction heat is mainly due to plastic and elastic deformation and adhesion . thus , to understand the underlying friction heat generation phenomena , atomistic simulations using molecular dynamics are carried out . it is shown that friction heat is the transformation of mechanical work given to the system at the macroscopic scale into potential energy during elastic deformation . this potential energy which is stored in the system is finally transformed into atomic kinetic energy ( friction heat ) during plastic transformation .
l p nested symmetric distributions . <eos> in this paper , we introduce a new family of probability densities called l p nested symmetric distributions . the common property , shared by all members of the new class , is the same functional form rho ( x ) ( rho ) over tilde ( f ( x ) ) , where f is a nested cascade of l p norms parallel to x parallel to ( p ) ( sigma vertical bar x ( i ) vertical bar ( p ) ) ( <digit> p ) . l p nested symmetric distributions thereby are a special case of nu spherical distributions for which f is only required to be positively homogeneous of degree one . while both , nu spherical and l p nested symmetric distributions , contain many widely used families of probability models such as the gaussian , spherically and elliptically symmetric distributions , l p spherically symmetric distributions , and certain types of independent component analysis ( ica ) and independent subspace analysis ( isa ) models , nu spherical distributions are usually computationally intractable . here we demonstrate that l p nested symmetric distributions are still computationally feasible by deriving an analytic expression for its normalization constant , gradients for maximum likelihood estimation , analytic expressions for certain types of marginals , as well as an exact and efficient sampling algorithm . we discuss the tight links of l p nested symmetric distributions to well known machine learning methods such as ica , isa and mixed norm regularizers , and introduce the nested radial factorization algorithm ( nrf ) , which is a form of non linear ica that transforms any linearly mixed , non factorial l p nested symmetric source into statistically independent signals . as a corollary , we also introduce the uniform distribution on the l p nested unit sphere .
data delivery in fragmented wireless sensor networks using mobile agents . <eos> due to the wide range of applications in sensors and wireless sensor networks ( wsn ) , research in this area has recently received increasing attention . wsns rely on network connectivity to deliver data to a base station through multihop communication . however , connectivity may not be always achievable for a number of reasons . in this paper , we study the problem of data delivery in disconnected wsns . a special class of disconnected sensor networks called fragmented wireless sensor networks ( fwsn ) is considered . a fwsn consists of several groups of connected sensors that we call fragments . to achieve connectivity between these fragments , mobile agents move in the network and act as data relays between fragments , in order to eventually deliver data to the base station . the main contribution of this paper is the modeling of the movement of these mobile relay nodes as a closed queueing network to obtain steady state results of the distribution of the mobile relays in the network . building on these results , we derive the distributions of the fragment to fragment , and fragment to sink delays . comparing these analytical results to results from the tossim simulator , it is shown that this model accurately captures the system behavior , and can be used to predict data delivery delays .
implementation of conditional simulation by successive residuals . <eos> conditional simulation of ergodic and stationary gaussian random fields using successive residuals is a new approach used to overcome the size limitations of the lu decomposition algorithm as well as provide fast updating of existing simulated realizations with new data . this paper discusses two different implementations of this approach . the implementations differ in the use of the new information available in the first implementation new information is partially used to generate updated realizations however , in the second implementation , the realizations are updated using all the new information available . the implementations are validated using the walker lake data set , and compared through a case study at a stockwork gold deposit .
completion of overdetermined parabolic pdes . <eos> in this paper we apply methods of commutative algebra to analysis of systems of pdes . more precisely , we show that systems which are parabolic in a generalized sense are equivalent to certain completed systems which are parabolic in the standard sense . we also propose a constructive method for getting this completion , and grobner basis methods , via symbol modules of the systems , play a central role in practical computations . moreover , we can easily construct systems which are not parabolic in the generalized sense but nevertheless become parabolic when completed . ( c ) <digit> elsevier ltd. all rights reserved .
steady state analysis of a discrete time batch arrival queue with working vacations . <eos> this paper analyzes a discrete time batch arrival queue with working vacations . in a geo x g <digit> system , the server works at a lower speed during the vacation period which becomes a lower speed operation period . this model is more appropriate for the communication systems with the transmit units arrived in batches . we formulate the system as an embedded markov chain at the departure epoch and by the m g <digit> type matrix analytic approach , we derive the probability generating function ( pgf ) of the stationary queue length . then , we obtain the distribution for the number of the customers at the busy period initiation epoch , and use the stochastic decomposition technique to present another equivalent pgf of the queue length . we also develop a variety of stationary performance measures for this system . some special models and numerical results are presented . finally , a real world example in an ethernet passive optical network ( epon ) is provided .
bimql an open query language for building information models . <eos> in this paper we present the on going development of a framework for a domain specific , open query language for building information models . the proposed query language is intended for selecting , updating and deleting of data stored in industry foundation classes models . even though some partial solutions already have been suggested , none of them are open source , domain specific , platform independent and implemented at the same time . this paper provides an overview of existing approaches , conceptual sketches of the language in development and documents the current state of implementation as a prototype plugin developed for the open source model server platform bimserver.org . we report on the execution of example test cases to show the general feasibility of the approach chosen .
a prototype for an agent based secure electronic marketplace including reputation tracking mechanisms . <eos> software agents will play a crucial role in the coming digital economy , but their reliability and honesty can not be guaranteed by technical security mechanisms , such as encryption of messages and digital signing of documents , and entities that can sanction fraudulent behavior in open networks are still in a rudimentary stage . this article describes a reputation mechanism that records previous cooperation behavior of participants in agent based markets and conveys this information to other software agents , thereby influencing the future behavior of participants . the mechanism has been prototypically implemented in the avalanche multiagent system . the deployment of this reputation mechanism will help to exclude fraudulent software agents from market participation .
an advanced hydro mechanical constitutive model for unsaturated soils with different initial densities . <eos> this paper presents an advanced constitutive model for unsaturated soils , using bishops effective stress ( ) and the effective degree of saturation ( se ) as two fundamental constitutive variables in the proposed constitutive model . a sub loading surface and a unified hardening parameter ( h ) are introduced into the se modelling framework to interpret the effects of initial density on coupled hydro mechanical behaviour of compacted soils . compared with existing models in the literature , the main advantage of the proposed model that it is capable of modelling hydro mechanical behaviour of unsaturated soils compacted to different initial densities , such as the dependence of loadingcollapse volume on initial void ratio and density effect on the shearing induced saturation change . the proposed model requires <digit> material parameters , all of which can be calibrated through conventional laboratory tests . numerical studies are conducted to assess the performance of the model for a hypothetical soil under two typical hydro mechanical loading scenarios . the proposed advanced unsaturated soil model is then validated against a number of experimental results for both isotropic and triaxial conditions reported in the literature .
understanding the value of software engineering technologies . <eos> when ai search methods are applied to software process models , then appropriate technologies can be discovered for a software project . we show that those recommendations are greatly affected by the business context of its use . for example , the automatic defect reduction tools explored by the ase community are only relevant to a subset of software projects , and only according to certain value criteria . therefore , when arguing for the value of a particular technology , that argument should include a description of the value function of the target user community .
the unfinished history of usage rights for spectrum . <eos> the key task in the next stage of spectrum management is to adapt regulation to the prospect of widespread sharing , on a much more sophisticated basis than sharing is used today . there is a role for the regulator to take steps to expand the area of choice within which public and private sector users can operate . this is best done in general by enhancing the flexibility of usage rights , which itself is best achieved by enhancing the freedom to trade them in the dimensions of time , space , level of interference and priority of access , by subdividing , re aggregating , etc. however , there are considerable transactions cost impediments to trading where unlicensed users are involved . this creates a role for the regulator pro actively to investigate different allocations , to make provisions for the most promising to occur and to incorporate both in refarming exercises and in primary assignments based on auctions configurations of usage rights , which might favour promising avenues of shared spectrum use .
a discrete scheme of laplacebeltrami operator and its convergence over quadrilateral meshes . <eos> laplacebeltrami operator and its discretization play a central role in the fields of image processing , computer graphics , computer aided geometric design and so on . in this paper , a discrete scheme for laplacebeltrami operator over quadrilateral meshes is constructed based on a bilinear interpolation of the quadrilateral . convergence results for the proposed discrete scheme are established under some conditions . numerical results which justify the theoretical analysis are also given .
energy and time efficient algorithm for cloud offloading using dynamic profiling . <eos> with the advent of computationally intensive application for mobile devices there is need of time and energy efficient component offloading algorithm which involves execution of resource intensive components of an application on remote machine . traditional solution includes offloading of entire application ( no partition ) , offloading predetermined components ( static partition ) or making offloading decision at runtime for each component ( <digit> ilp ) . our proposed solution of dynamic profiling uses depth first search ( topological sorting ) to calculate the offloading point at runtime . the subsequent nodes are offloaded with high probability . experimental result demonstrates that proposed algorithm is better than <digit> ilp in time domain while outperforming no partitioning and static partitioning in energy domain .
adaptive critic design based robust neural network control for nonlinear distributed parameter systems with unknown dynamics . <eos> in this paper , an adaptive critic design ( acd ) based robust on line neural network control design is developed for a class of parabolic partial differential equation ( pde ) systems with unknown nonlinear dynamics . first , the galerkin method is applied to the parabolic pde system to derive a finite dimensional slow one and an infinite dimensional stable fast subsystem . the obtained slow system is an ordinary differential equation ( ode ) system with unknown nonlinearities , which accurately describes the dynamics of the slow modes of the pde system . then , a novel acd based robust optimal control scheme is proposed for the resulting nonlinear slow system with unknown dynamics . an action neural network ( nn ) is employed to approximate all the derived unknown nonlinear terms and a robust control term is further developed to attenuate the nn reconstruction errors and disturbances . especially , by developing novel critic signals and lyapunov function candidate , together with the adaptive bounding technique , no a prior knowledge for the bounds of the disturbance term , the nn ideal weights of action nn and critic nn and the nn reconstruction errors is required . finally , simulation results demonstrate the effectiveness of the proposed robust optimal control scheme .
web services with generic simulation models for discrete event simulation . <eos> today the internet and the world wide web ( www ) are on the cusp of a paradigm shift . up to now most actions in the www are sorts of human computer interaction , but the introduction of the extensible markup language ( xml ) changed the perception . the internet will be seen as a great space of information and with the use of xml and following technologies like web services , grid computing and semantic web the difference between human machine interaction and machine machine interaction vanishes . this work investigates the usefulness of xml in the simulation domain and uses web service technology to build the simasp framework for discrete event simulation ( des ) . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
the impact of sample reduction on pca based feature extraction for supervised learning . <eos> the curse of dimensionality is pertinent to many learning algorithms , and it denotes the drastic raise of computational complexity and classification error in high dimensions . in this paper , different feature extraction ( fe ) techniques are analyzed as means of dimensionality reduction , and constructive induction with respect to the performance of nave bayes classifier . when a data set contains a large number of instances , some sampling approach is applied to address the computational complexity of fe and classification processes . the main goal of this paper is to show the impact of sample reduction on the process of fe for supervised learning . in our study we analyzed the conventional pca and two eigenvector based approaches that take into account class information . the first class conditional approach is parametric and optimizes the ratio of between class variance to the within class variance of the transformed data . the second approach is a nonparametric modification of the first one based on the local calculation of the between class covariance matrix . the experiments are conducted on ten uci data sets , using four different strategies to select samples ( <digit> ) random sampling , ( <digit> ) stratified random sampling , ( <digit> ) kd tree based selective sampling , and ( <digit> ) stratified sampling with kd tree based selection . our experiments show that if the sample size for fe model construction is small then it is important to take into account both class information and data distribution . further , for supervised learning the nonparametric fe approach needs much less instances to produce a new representation space that result in the same or higher classification accuracy than the other fe approaches .
a topology preserving level set method for geometric deformable models . <eos> active contour and surface models , also known as deformable models , are powerful image segmentation techniques . geometric deformable models implemented using level set methods have advantages over parametric models due to their intrinsic behavior , parameterization independence , and ease of implementation . however , a long claimed advantage of geometric deformable models the ability to automatically handle topology changes turns out to be a liability in applications where the object to be segmented has a known topology that must be preserved . in this paper , we present a new class of geometric deformable models designed using a novel topology preserving level set method , which achieves topology preservation by applying the simple point concept from digital topology . these new models maintain the other advantages of standard geometric deformable models including subpixel accuracy and production of nonintersecting curves or surfaces . moreover , since the topology preserving constraint is enforced efficiently through local computations , the resulting algorithm incurs only nominal computational overhead over standard geometric deformable models . several experiments on simulated and real data are provided to demonstrate the performance of this new deformable model algorithm .
elastic geodesic paths in shape space of parameterized surfaces . <eos> this paper presents a novel riemannian framework for shape analysis of parameterized surfaces . in particular , it provides efficient algorithms for computing geodesic paths which , in turn , are important for comparing , matching , and deforming surfaces . the novelty of this framework is that geodesics are invariant to the parameterizations of surfaces and other shape preserving transformations of surfaces . the basic idea is to formulate a space of embedded surfaces ( surfaces seen as embeddings of a unit sphere in r <digit> ) and impose a riemannian metric on it in such a way that the reparameterization group acts on this space by isometries . under this framework , we solve two optimization problems . one , given any two surfaces at arbitrary rotations and parameterizations , we use a path straightening approach to find a geodesic path between them under the chosen metric . second , by modifying a technique presented in <digit> , we solve for the optimal rotation and parameterization ( registration ) between surfaces . their combined solution provides an efficient mechanism for computing geodesic paths in shape spaces of parameterized surfaces . we illustrate these ideas using examples from shape analysis of anatomical structures and other general surfaces .
on the coupling of the homotopy perturbation method and laplace transformation . <eos> in this paper , a laplace homotopy perturbation method is employed for solving one dimensional non homogeneous partial differential equations with a variable coefficient . this method is a combination of the laplace transform and the homotopy perturbation method ( lhpm ) . lhpm presents an accurate methodology to solve non homogeneous partial differential equations with a variable coefficient . the aim of using the laplace transform is to overcome the deficiency that is mainly caused by unsatisfied conditions in other semi analytical methods such as hpm , vim , and adm. the approximate solutions obtained by means of lhpm in a wide range of the problem 's domain were compared with those results obtained from the actual solutions , the homotopy perturbation method ( hpm ) and the finite element method . the comparison shows a precise agreement between the results , and introduces this new method as an applicable one which it needs fewer computations and is much easier and more convenient than others , so it can be widely used in engineering too . ( c ) <digit> elsevier ltd. all rights reserved .
central limit theorems for super ornstein uhlenbeck processes . <eos> suppose that x x t t <digit> is a supercritical super ornstein uhlenbeck process , that is , a superprocess with an ornstein uhlenbeck process on ( mathbb r d ) corresponding to ( l frac <digit> <digit> sigma <digit> delta b x cdot nabla ) as its underlying spatial motion and with branching mechanism ( ) <digit> ( <digit> , ) ( e x <digit> x ) n ( dx ) , where ( <digit> ) > <digit> , <digit> , and n is a measure on ( <digit> , ) such that ( <digit> , ) x <digit> n ( dx ) < . let ( mathbb p _ mu ) be the law of x with initial measure . then the process w t e t x t is a positive ( mathbb p _ mu ) martingale . therefore there is w such that w t w , ( mathbb p _ mu ) a.s. as t . in this paper we establish some spatial central limit theorems for x.
e learning recommender system for a group of learners based on the unified learner profile approach . <eos> in the age of information explosion , e learning recommender systems ( el_rss ) have emerged as effective information filtering techniques that attempt to provide the most appropriate learning resources for learners while using e learning systems . these learners are differentiated on the basis of their learning styles , goals , knowledge levels and others . several attempts have been made in the past to design el_rss to recommend resources to individuals however , an investigation of recommendations to a group of learners in e learning is still in its infancy . in this paper , we focus on the problem of recommending resources to a group of learners rather than to an individual . the major challenge in group recommendation is how to merge the individual preferences of different learners that form a group and extract a pseudo unified learner profile ( ulp ) that closely reflects the preferences of all learners . firstly , we propose a profile merging scheme for the ulp by utilizing learning styles , knowledge levels and ratings of learners in a group . thereafter , a collaborative approach is proposed based on the ulp for effective group recommendations . experimental results are presented to demonstrate the effectiveness of the proposed group recommendation strategy for e learning .
sms based human hosted interactive tv in finland . <eos> interactive tv entertainment has brought to life a new kind of tv game show host culture in finland . a qualitative study of sms to tv human hosted interactive tv games , specifically , tv mobile games and call quizzes , was conducted by recording sample interactive tv programs and corresponding discussion forums on the internet and analyzing the content . the role of the human host in these programs was analyzed and discussed to answer these questions why is this interactive entertainment popular what different dimensions can be found how could this field be used more effectively and what are the aspects developers should pay attention to while designing itv entertainment this research is important beyond finland since finland tends to pioneer interactive entertainment that later spreads out to other countries
artificial channel aided lmmse estimation for timefrequency selective channels in ofdm context . <eos> this paper proposes a linear minimum mean square error based ( lmmse ) channel estimation method , which allows avoiding the necessary knowledge of the channel covariance matrix or its estimation . to do so , a perfectly tunable filter acting like an artificial channel is added at the receiver side . we show that an lmmse estimation of the sum of this artificial channel and the physical channel only needs the covariance matrix of the artificial channel , and the channel estimation is finally obtained by subtracting the frequency coefficients of the added filter . we call this method artificial channel aided lmmse ( aca lmmse ) . theoretical developments and simulations prove that its performance is close to theoretical lmmse , and we show that this method reduces the computational complexity , compared to usual lmmse , due to the covariance matrix used for aca lmmse is computed only once throughout the transmission duration . we put the conditions on the artificial channel parameters to get the expected mask effect . simulations display the performance of the proposed method , in terms of mmse and bit error rate ( ber ) . indeed , the difference of ber between our method and the theoretical lmmse is less than 2db .
prediction of automobile tire cornering force characteristics by finite element modeling and analysis . <eos> in this study , a detailed finite element model of a radial automobile tire is constructed for the prediction of cornering force characteristics during the design stage . the nonlinear stressstrain relationship of rubber as well as a linear elastic approximation , reinforcement , large displacements , and frictional ground contact are modeled . validity of various simplifications is checked . the cornering force characteristics obtained by the finite element tire model are verified on the experimental setup constructed for this purpose .
design and implementation of a mppt circuit for a solar uav . <eos> this paper presents a maximum power point tracking ( mppt ) circuit for an unmanned air vehicle . the design of the mppt is proposed utilizing a boost converter topology . the power of the photovoltaic cells is monitored by a closed loop microcontroller based control system , and the pwm signal of the boost converter continuously adjusted to extract maximum power . the mppt is used to charge the lithium ion polymer battery and feed the electrical load of the unmanned aircraft .
technical assessment and evaluation of environmental models and software letter to the editor . <eos> this letter details the collective views of a number of independent researchers on the technical assessment and evaluation of environmental models and software . the purpose is to stimulate debate and initiate action that leads to an improved quality of model development and evaluation , so increasing the capacity for models to have positive outcomes from their use . as such , we emphasize the relationship between the model evaluation process and credibility with stakeholders ( including funding agencies ) with a view to ensure continued support for modelling efforts . many journals , including em s , publish the results of environmental modelling studies and must judge the work and the submitted papers based solely on the material that the authors have chosen to present and on how they present it . there is considerable variation in how this is done with the consequent risk of considerable variation in the quality and usefulness of the resulting publication . part of the problem is that the review process is reactive , responding to the submitted manuscript . in this letter , we attempt to be proactive and give guidelines for researchers , authors and reviewers as to what constitutes best practice in presenting environmental modelling results . this is a unique contribution to the organisation and practice of model based research and the communication of its results that will benefit the entire environmental modelling community . for a start , our view is that the community of environmental modellers should have a common vision of minimum standards that an environmental model must meet . a common vision of what a good model should be is expressed in various guidelines on good modelling practice . the guidelines prompt modellers to codify their practice and to be more rigorous in their model testing . our statement within this letter deals with another aspect of the issue it prompts professional journals to codify the peer review process . introducing a more formalized approach to peer review may discourage reviewers from accepting invitations to review given the additional time and labour requirements . the burden of proving model credibility is thus shifted to the authors . here we discuss how to reduce this burden by selecting realistic evaluation criteria and conclude by advocating the use of standardized evaluation tools as this is a key issue that needs to be tackled .
on the wadge reducibility of k partitions . <eos> we establish some results on the wadge degrees and on the boolean hierarchy of k partitions of some spaces , where k is a natural number . the main attention is paid to the baire space , baire domain and their close relatives . for the case of delta ( <digit> ) ( <digit> ) measurable k partitions the structures of wadge degrees are characterized completely . for many degree structures , undecidability of the first order theories is shown , for any k > <digit> . ( c ) <digit> elsevier inc. all rights reserved .
simulation of axonal excitability using a spreadsheet template created in microsoft excel . <eos> the objective of this present study was to implement an established simulation protocol ( a.m. brown , a methodology for simulating biological systems using microsoft excel , comp . methods prog . biomed . <digit> ( <digit> ) <digit> ) to model axonal excitability . the simulation protocol involves the use of in cell formulas directly typed into a spreadsheet and does not require any programming skills or use of the macro language . once the initial spreadsheet template has been set up the simulations described in this paper can be executed with a few simple keystrokes . the model axon contained voltage gated ion channels that were modeled using hodgkin huxley style kinetics . the basic properties of axonal excitability modeled were ( <digit> ) threshold of action potential firing , demonstrating that not only are the stimulus amplitude and duration critical in the generation of an action potential , but also the resting membrane potential ( <digit> ) refractoriness , the phenomenon of reduced excitability immediately following an action potential . the difference between the absolute refractory period , when no amount of stimulus will elicit an action potential , and relative refractory period , when an action potential may be generated by applying increased stimulus , was demonstrated with regard to the underlying state of the na and k channels ( <digit> ) temporal summation , a process by which two sub threshold stimuli can unite to elicit an action potential was shown to be due to conductance changes outlasting the first stimulus and summing with the second stimulus induced conductance changes to drive the membrane potential past threshold ( <digit> ) anode break excitation , where membrane hyperpolarization was shown to produce an action potential by removing na channel inactivation that is present at resting membrane potential . the simulations described in this paper provide insights into mechanisms of axonal excitation that can be carried out by following an easily understood protocol .
trabecular bone remodelling under pathological conditions based on biochemical and mechanical processes involved in bmu activity . <eos> in adulthood , bone tissue is continuously renewed by processes governed by basic multicellular units composed of osteocytes , osteoclasts and osteoblasts , which are subjected to local mechanical loads . osteocytes are known to be integrated mechanosensors that regulate the activation of the osteoclasts and osteoblasts involved in bone resorption and apposition processes , respectively . after collagen tissue apposition , a process of collagen mineralisation takes place , gradually increasing the effective stiffness of bone . this study presents a new model based on physicochemical parameters involved in spongy bone remodelling under pathological conditions . our model simulates the transient evolution of both geometry and effective young 's modulus of the trabeculae , also taking turnover into account . various loads were applied on a trabecula in order to determine the evolution of bone volume fraction under pathological conditions . a parametric study performed on the model showed that one key parameter here is the kinetic constant of hydroxyapatite crystallisation . we subsequently tested our model on a pathological case approaching osteoporosis , involving a decrease in the number of viable osteocytes present in bone . the model converges to a lower value ( <digit> % ) for bone volume fraction than with a normal quantity of osteocytes . this useful tool offers new perspectives for predicting bone remodelling deficits on a local scale in patients with pathological conditions such as osteoporosis and in bedridden patients , as well as for astronauts subjected to weightlessness in space .
robust and imperceptible dual watermarking for telemedicine applications . <eos> in this paper , the effects of different error correction codes on the robustness and imperceptibility of discrete wavelet transform and singular value decomposition based dual watermarking scheme is investigated . text and image watermarks are embedded into cover radiological image for their potential application in secure and compact medical data transmission . four different error correcting codes such as hamming , the bose , ray chaudhuri , hocquenghem ( bch ) , the reedsolomon and hybrid error correcting ( bch and repetition code ) codes are considered for encoding of text watermark in order to achieve additional robustness for sensitive text data such as patient identification code . performance of the proposed algorithm is evaluated against number of signal processing attacks by varying the strength of watermarking and covers image modalities . the experimental results demonstrate that this algorithm provides better robustness without affecting the quality of watermarked image.this algorithm combines the advantages and removes the disadvantages of the two transform techniques . out of the three error correcting codes tested , it has been found that reedsolomon shows the best performance . further , a hybrid model of two of the error correcting codes ( bch and repetition code ) is concatenated and implemented . it is found that the hybrid code achieves better results in terms of robustness . this paper provides a detailed analysis of the obtained experimental results .
robust estimation of dimension reduction space . <eos> most dimension reduction methods based on nonparametric smoothing are highly sensitive to outliers and to data coming from heavy tailed distributions . two recently proposed methods , minimum average variance estimation and outer product of gradients , can be and are made robust in such a way that preserves all advantages of the original approach . their extension based on the local one step m estimators is sufficiently robust to outliers and data from heavy tailed distributions , it is relatively easy to implement , and surprisingly , it performs as well as the original methods when applied to normally distributed data .
complexity of deciding sense of direction . <eos> in this paper we prove that deciding whether a distributed system ( represented as a colored digraph with n nodes ) has weak sense of direction is in ac ( <digit> ) ( using n ( <digit> ) processors ) . moreover , we show that deciding sense of direction is in p. our algorithms can also be used to decide in ac ( <digit> ) whether a colored graph is a cayley color graph .
probabilistic fuzzy image fusion approach for radar through wall sensing . <eos> this paper addresses the problem of combining multiple radar images of the same scene to produce a more informative composite image . the proposed approach for probabilistic fuzzy logic based image fusion automatically forms fuzzy membership functions using the gaussian rayleigh mixture distribution . it fuses the input pixel values directly without requiring fuzzification and defuzzification , thereby removing the subjective nature of the existing fuzzy logic methods . in this paper , the proposed approach is applied to through the wall radar imaging in urban sensing and evaluated on real multi view and polarimetric data . experimental results show that the proposed approach yields improved image contrast and enhances target detection .
inductive time space lower bounds for sat and related problems . <eos> we improve upon indirect diagonalization arguments for lower bounds on explicit problems within the polynomial hierarchy . our contributions are summarized as follows . <digit> . we present a technique that uniformly improves upon most known nonlinear time lower bounds for nondeterminism and alternating computation , on both subpolynomial ( n ( o ( <digit> ) ) ) space rams and sequential one tape machines with random access to the input . we obtain improved lower bounds for boolean satisfiability ( sat ) , as well as all np complete problems that have efficient reductions from sat , and sigma ( k ) sat , for constant k > <digit> . for example , sat can not be solved by random access machines using n ( root <digit> ) time and subpolynomial space . <digit> . we show how indirect diagonalization leads to time space lower bounds for computation with bounded nondeterminism . for both the random access and multitape turing machine models , we prove that for all k > <digit> , there is a constant c ( k ) > <digit> such that linear time with n ( <digit> k ) nondeterministic bits is not contained in deterministic n ( ck ) time with subpolynomial space . this is used to prove that satisfiability of boolean circuits with n inputs and n ( k ) size can not be solved by deterministic multitape turing machines running in n ( k.ck ) time and subpolynomial space .
general form of lattice valued fuzzy sets under the cutworthy approach . <eos> in this note a new solution of problem of synthesis of fuzzy sets is presented . in other words , necessary and sufficient conditions are formulated , under which for a given family of subsets f of a set x and a fixed complete lattice l there is a fuzzy set it x > l , such that the collection of cuts of it coincides with f. moreover , it is proved that the general form of lattice valued fuzzy sets ( considering families of cuts ) are the type of fuzzy sets having the codomain ( <digit> , <digit> ( c ) for a suitable chosen cardinal c. ( c ) <digit> elsevier b.v. all rights reserved .
mechanotransduction in cardiac myocytes . <eos> cardiac myocytes react to diverse mechanical demands with a multitude of transient and long term responses to normalize the cellular mechanical environment . several stretch activated signaling pathways have been identified , most prominently guanine nucleotide binding proteins ( g proteins ) , mitogen activated protein kinases ( mapk ) , janus associated kinase signal transducers and activators of transcription ( jak stat ) , protein kinase c ( pkc ) , calcineurin , intracellular calcium regulation , and several autocrine and paracrine factors . multiple levels of crosstalk exist between pathways . the cellular response to changes in the mechanical environment can lead to cardiac myocyte hypertrophy , cellular growth that can be accompanied by pathological myocyte dysfunction , and tissue fibrosis . several candidates for the primary mechanosensor in cardiac myocytes have been identified , ranging from stretch activated ion channels in the membrane to yet unknown mechanosensitive mechanisms in the nucleus . new and refined experimental techniques will exploit advances in molecular biology and biological imaging to study mechanotransduction in isolated cells and genetically engineered mice to explore the function of individual proteins .
design of a sliding window scheme for detecting high packet rate flows via random packet sampling . <eos> we discuss the design of a sliding window scheme for detecting high packet rate flows via random packet sampling . we determine the values of control parameters , such as the sampling rate and window length , to minimize the false positive ratio , while keeping the false negative ratio sufficiently low and making the on line processing possible . under mild assumptions , we formulate this problem as a nonlinear program and provide its numerically feasible global optimal solution . we then conduct sampling experiments with public trace data and discuss the fundamental characteristics of the sliding window scheme with random packet sampling .
the fuzzy metric truth reasoning approach to decision making in soft computing milieux . <eos> this article considers fuzzy approximate reasoning utilizing the metric ruth approach . this approach assesses the truth of a sentence on the basis of its distance from the respective true one . the author has previously examined this subject matter from the logico methodological point of view . this article focuses on the aspects typical of fuzzy if then rules within control and decision making .
querying a summary of database . <eos> for some years , data summarization techniques have been developed to handle the growth of databases . however these techniques are usually not provided with tools for end users to efficiently use the produced summaries . this paper presents a first attempt to develop a querying tool for the saintetiq summarization model . the proposed search algorithm takes advantage of the hierarchical structure of the saintetiq summaries to efficiently answer questions such as how are , on some attributes , the tuples which have specific characteristics moreover , this algorithm can be seen both as a boolean querying mechanism over a hierarchy of summaries , and as a flexible querying mechanism over the underlying relational tuples .
subband domain coding of binary textual images for document archiving . <eos> in this work , a subband domain textual image compression method is developed . the document image is first decomposed into subimages using binary subband decompositions . next , the character locations in the subbands and the symbol library consisting of the character images are encoded , the method is suitable for keyword search in the compressed data . it is observed that very high compression ratios are obtained with this method . simulation studies are presented .
on site volume rendering with gpu enabled devices . <eos> now that high performance computing systems can rely more on a cloud based infrastructure , it becomes much more important to have ubiquitous data processing and visualization capability . this will allow data sharing among numerous clients using shared data repositories through a secure web server . thanks to the wide availability of gpu support in todays mobile devices such as smart phones and tablets , as well as the recently published webgl standard , pervasive computing for high quality and real time volume rendering may be realized on such high performance platforms . we have invented two high performance volume renderers , namely , single pass gpu ray caster and fast 3d texture slicer , for both mobile and desktop platforms . rigorous experiments and performance assessments reveal that the proposed mobile 3d image rendering system outperforms the existing approaches in the literature .
multipartite priority queues . <eos> we introduce a framework for reducing the number of element comparisons performed in priority queue operations . in particular , we give a priority queue which guarantees the worst case cost of o ( <digit> ) per minimum finding and insertion , and the worst case cost of o ( log n ) with at most log n o ( <digit> ) element comparisons per deletion , improving the bound of <digit> log n o ( <digit> ) known for binomial queues . here , n denotes the number of elements stored in the data structure prior to the operation in question , and log n equals log ( <digit> ) ( max <digit> , n ) . as an immediate application of the priority queue developed , we obtain a sorting algorithm that is optimally adaptive with respect to the inversion measure of disorder , and that sorts a sequence having n elements and i inversions with at most n log ( i n ) o ( n ) element comparisons .
bounds on codes derived by counting components in varshamov graphs . <eos> we are interested in improving the varshamov bound for finite values of length n and minimum distance d. we employ a counting lemma to this end which we find particularly useful in relation to varshamov graphs . since a varshamov graph consists of components corresponding to low weight vectors in the cosets of a code it is a useful tool when trying to improve the estimates involved in the varshamov bound . we consider how the graph can be iteratively constructed and using our observations are able to achieve a reduction in the over counting which occurs . this tightens the lower bound for any choice of parameters n , k , d or q and is not dependent on information such as the weight distribution of a code .
forecasting of circuit breaker behaviour in high voltage electrical power systems necessity for future maintenance management . <eos> two research projects were started in order to investigate new methods of maintenance management . the first project was finished in april <digit> , dealing with the problem of relating the information of individual devices with their importance in the complete system . the combination of both sets of information is the aim of reliability centred maintenance ( rcm ) . more important devices can be maintained more frequently than those of less importance , leading to reduced maintenance costs but retaining a high level of reliability of the system . the question of how new methods of maintenance influence behaviour in future , can not be answered right now . the forecasting of the behaviour of circuit breakers will now be investigated in a further project in the field of maintenance management . a software model will be developed in strong co ordination with partners of some electrical power system utilities . the model shall include the simulation of the behaviour of circuit breakers in the future regarding maintenance activities as well as the operational stresses of the present . this paper will give an overview of the actual activities and aims of the project . main activities have been the definition of investigated circuit breaker types and the methodology for the starting steps of the project .
quality as empowerment going around in circles . <eos> the article introduces a new international educational community based on students quality circles , in which industry and education have learned to collaborate for mutual benefit . in each country represented in this special issue , there have been distinctive bottom up initiatives , informed by the experience of collaboration . we emphasise quality as empowerment .
a computer vision based precision seed drill guidance assistance . <eos> this paper presents a control mechanism aiming to position seed drills relative to the previous lines , while sowing . the position was measured by a machine vision system and used in a feedback control loop . an articulated mechanism was used to ensure the lateral displacement of the drill relative to the tractor . the behaviour of the whole outfit was studied during several field tests . the standard deviation of the error , measured as the difference between the observed inter row distance and its set value , was <digit> mm and its range was less than <digit> mm , which was sufficient to fulfil the requirements of the application . sources of systematic errors were also identified as linked to the geometric considerations . their correction requires an accurate mounting of the camera , which may be possible for a serial montage .
rulebased regulatory and metabolic model for quorum sensing in p. aeruginosa . <eos> in the pathogen p. aeruginosa , the formation of virulence factors is regulated via quorum sensing signaling pathways . due to the increasing number of strains that are resistant to antibiotics , there is a high interest to develop novel antiinfectives . in the combat of resistant bacteria , selective blockade of the bacterial celltocell communication ( quorum sensing ) has gained special interest as antivirulence strategy . here , we modeled the las , rhl , and pqs quorum sensing systems by a multilevel logical approach to analyze how enzyme inhibitors and receptor antagonists effect the formation of autoinducers and virulence factors .
strong and ultra separation axioms on fuzzy bitopological spaces . <eos> given a fuzzy bitopological space ( x , tau ( <digit> ) , tau ( <digit> ) ) , we introduce a new notion of fuzzy pairwise separation axioms by using the family of its level bitopologies l alpha ( tau ( <digit> ) ) , l alpha ( tau ( <digit> ) ) , alpha is an element of 0,1 ) . we prove that these concepts are good extension and we compare them with its corresponding fpti ( kandil and el shafee , <digit> ) and fpt ( i ) ( abu safiya et al. , <digit> ) ( i <digit> , 1,2 , <digit> , <digit> ) , respectively . we show that these notions are not equivalent and we give a number of examples which illustrate this fact . ( c ) <digit> elsevier science b.v. all rights reserved .
secure and scalable mobility management scheme for the internet of things integration in the future internet architecture . <eos> internet of things is becoming a reality with the rapid development of communication technologies . this evolution presents an enrichment of the users ' experiences , but also challenges regarding network scalability , security , privacy vulnerabilities , and mobility support . mobility support for the future internet is focused on id locator split architectures since the limitations of the current internet . this work analyses the security challenges for the himalis ( heterogeneity inclusion and mobility adaptation through locator id separation ) architecture for the particularities from the internet of things and the id locator management messages vulnerable to attacks . this work proposes a secure and scalable mobility management scheme that considers the constraints from the internet of things , solving the possible security and privacy vulnerabilities of the himalis architecture . the proposed scheme supports scalable inter domain authentication and secure location update and binding transfer for the mobility process . the proposed scheme has been verified and evaluated successfully with the avispa framework .
influence of different shoulder elbow configurations on steering precision and steering velocity in automotive context . <eos> influence of posture on driving precision and steering velocity was investigated . arm posture influences steering precision and steering velocity . steering precision and velocity are significantly increased in mid positions . driver safety can be enhanced by implementing these data in the design process . subjective comfort rating confirmed experimental results .
modelling procedures for directed network of data blocks . <eos> here are presented procedures for modelling data in a network . the methods are extensions of pca or pls regression to a forward network of data blocks . it is assumed that the data blocks are organised in a network such that one data block leads to one or more other data blocks . the procedures are stepwise ones . at each step a passage through the network is carried out . from the input weight vectors of the input or starting blocks , the score and loading vectors of all data blocks are computed . it is investigated if some score loading vectors are not significant . if some are , they are deleted and revised estimation of the input weights are carried out . when one step is finished , all data matrices are adjusted for score and loading vectors found . a new passage through the network is carried out on the reduced matrices . if no significant loading score vectors are found for a given set of input weights , the modelling stops . in case of one data block , the algorithm reduces to pca . in case of two data blocks it reduces to pls regression . most methods used in pca or pls regression can be applied to this procedure , e.g. , cross validation and re sampling procedures . it is pointed out , how these methods can be used to extend other regression methods than pca and pls regression to a network regression . ( c ) <digit> elsevier b.v. all rights reserved .
a novel joint processing adaptive nonlinear equalizer using a modular recurrent neural network for chaotic communication systems . <eos> to eliminate nonlinear channel distortion in chaotic communication systems , a novel joint processing adaptive nonlinear equalizer based on a pipelined recurrent neural network ( jprnn ) is proposed , using a modified real time recurrent learning ( rtrl ) algorithm . furthermore , an adaptive amplitude rtrl algorithm is adopted to overcome the deteriorating effect introduced by the nesting process . computer simulations illustrate that the proposed equalizer outperforms the pipelined recurrent neural network ( prnn ) and recurrent neural network ( rnn ) equalizers . ( c ) <digit> elsevier ltd. all rights reserved .
dynamics of connected vehicle systems with delayed acceleration feedback . <eos> acceleration based connected cruise control ( ccc ) is implemented for heterogeneous platoons . the ad hoc nature of wireless vehicle to vehicle ( v2v ) communication is exploited . the design is robust against variation of human parameters and is scalable for large systems . delays are used as design parameters in order to ensure string stability . it is demonstrated that acceleration feedback shall be used in a selective manner .
exact solution for nonlinear schrodinger equation by he 's frequency formulation . <eos> in this work , we apply he 's frequency formulation to search for the solution to nonlinear schrodinger equation . three examples are given and the solutions obtained are in good accordance with wazwaz 's solution abdul majid wazwaz , a study on linear and nonlinear schrodinger equations by the variational iteration method , chaos solitons fractals <digit> ( <digit> ) ( <digit> ) <digit> <digit> . it is shown that he 's frequency formulation is of utter straightforward and effective . ( c ) <digit> elsevier ltd. all rights reserved .
on the role of trust in collaborative web search . <eos> recommender systems combine ideas from information retrieval , user modelling , and artificial intelligence to focus on the provision of more intelligent and proactive information services . as such , recommender systems play an important role when it comes to assisting the user during both routine and specialised information retrieval tasks . like any good assistant it is important that users can trust in the ability of a recommender system to respond with timely and relevant suggestions . in this paper , we will look at a collaborative recommendation system operating in the domain of web search . we will show how explicit models of trust can help to inform more reliable recommendations that translate into more relevant search results . moreover , we demonstrate how the availability of this trust model facilitates important interface enhancements that provide a means to declare the provenance of result recommendations in a way that will allow searchers to evaluate their likely relevance based on the reputation and trustworthiness of the recommendation partners behind these suggestions .
an intelligent learning diagnosis system for web based thematic learning platform . <eos> this work proposes an intelligent learning diagnosis system that supports a web based thematic learning model , which aims to cultivate learners ' ability of knowledge integration by giving the learners the opportunities to select the learning topics that they are interested , and gain knowledge on the specific topics by surfing on the internet to search related learning courseware and discussing what they have learned with their colleagues . based on the log files that record the learners ' past online learning behavior , an intelligent diagnosis system is used to give appropriate learning guidance to assist the learners in improving their study behaviors and grade online class participation for the instructor . the achievement of the learners ' final reports can also be predicted by the diagnosis system accurately . our experimental results reveal that the proposed learning diagnosis system can efficiently help learners to expand their knowledge while surfing in cyberspace web based theme based learning model . ( c ) <digit> elsevier ltd. all rights reserved .
the longest common extension problem revisited and applications to approximate string searching . <eos> the longest common extension ( lce ) problem considers a string s and computes , for each pair ( i , j ) ( i , j ) , the longest substring of s that starts at both i and j. it appears as a subproblem in many fundamental string problems and can be solved by linear time preprocessing of the string that allows ( worst case ) constant time computation for each pair . the two known approaches use powerful algorithms either constant time computation of the lowest common ancestor in trees or constant time computation of range minimum queries in arrays . we show here that , from practical point of view , such complicated approaches are not needed . we give two very simple algorithms for this problem that require no preprocessing . the first is <digit> times faster than the best previous algorithms on the average whereas the second is faster on virtually all inputs . as an application , we modify the landauvishkin algorithm for approximate matching to use our simplest lce algorithm . the obtained algorithm is <digit> to <digit> times faster than the original . we compare it with the more widely used ukkonen 's cutoff algorithm and show that it behaves better for a significant range of error thresholds .
lumbar spine segmentation using a statistical multi vertebrae anatomical shape plus pose model . <eos> segmentation of the spinal column from computed tomography ( ct ) images is a preprocessing step for a range of image guided interventions . one intervention that would benefit from accurate segmentation is spinal needle injection . previous spinal segmentation techniques have primarily focused on identification and separate segmentation of each vertebra . recently , statistical multi object shape models have been introduced to extract common statistical characteristics between several anatomies . these models can be used for segmentation purposes because they are robust , accurate , and computationally tractable . in this paper , we develop a statistical multi vertebrae shape pose model and propose a novel registration based technique to segment the ct images of spine . the multi vertebrae statistical model captures the variations in shape and pose simultaneously , which reduces the number of registration parameters . we validate our technique in terms of accuracy and robustness of multi vertebrae segmentation of ct images acquired from lumbar vertebrae of <digit> subjects . the mean error of the proposed technique is below <digit> mm , which is sufficient for many spinal needle injection procedures , such as facet joint injections .
relative blocking in posets . <eos> poset theoretic generalizations of set theoretic committee constructions are presented . the structure of the corresponding subposets is described . sequences of irreducible fractions associated to the principal order ideals of finite bounded posets are considered and those related to the boolean lattices are explored it is shown that such sequences inherit all the familiar properties of the farey sequences .
mixed finite elements for numerical weather prediction . <eos> we show how mixed finite element methods that satisfy the conditions of finite element exterior calculus can be used for the horizontal discretisation of dynamical cores for numerical weather prediction on pseudo uniform grids . this family of mixed finite element methods can be thought of in the numerical weather prediction context as a generalisation of the popular polygonal c grid finite difference methods . there are a few major advantages the mixed finite element methods do not require an orthogonal grid , and they allow a degree of flexibility that can be exploited to ensure an appropriate ratio between the velocity and pressure degrees of freedom so as to avoid spurious mode branches in the numerical dispersion relation . these methods preserve several properties of the c grid method when applied to linear barotropic wave propagation , namely ( a ) energy conservation , ( b ) mass conservation , ( c ) no spurious pressure modes , and ( d ) steady geostrophic modes on the f plane . we explain how these properties are preserved , and describe two examples that can be used on pseudo uniform grids the recently developed modified rtk q ( k <digit> ) element pairs on quadrilaterals and the bdfm1 p1dg p <digit> dg element pair on triangles . all of these mixed finite element methods have an exact <digit> <digit> ratio of velocity degrees of freedom to pressure degrees of freedom . finally we illustrate the properties with some numerical examples .
dc offset control with application in a zero if 0.18 mu m cmos bluetooth receiver chain . <eos> a compact dc offset correction circuit based on the intrinsic properties of quasi floating gate ( qfg ) transistors is presented . the proposed scheme uses a tuning mechanism to make its initial response faster improving the traditional large settling time of these circuits . a zero if baseband receiver chain suitable for bluetooth that includes the proposed dc offset correction has been designed in a 0.18 mu m cmos technology at 1.2 v supply voltage .
combining accuracy and success rate to improve the performance of extended classifier system ( xcs ) for data mining and control applications . <eos> the emergence of extended classifier systems ( xcs ) raised the bar for learning classifier systems by incorporating the accuracies of the rules in the lcs 's traditional reinforcement mechanism . however , neither xcs nor its extensions take into account the nature of a classifier 's experience of attending the action set . we introduce an experienceevaluation mechanism that , once added to the traditional xcs , would assigns to each member of the action set a success rate indicating how effectively the classifier has contributed to the correct responding of the system to the environment 's queries . application of the augmented system ( called srxcs ) to several benchmark problems shows that the proposed mechanism enhances xcs ' classification capability and its rate of convergence at the same time . application results indicate that srxcs performs notably better on both pattern association and pattern recognition tasks . the applicability and efficiency of the proposed mechanism is further demonstrated through solving a fairly complex path planning problem for an autonomous mobile robot in a dynamic environment .
an experimental study of field dependency in altered gz environments . <eos> failure to address extreme environments constraints at the human computer interaction level may lead to the commission of critical and potentially fatal errors . this experimental study addresses gaps in our current theoretical understanding of the impact of gz accelerations and field dependency independency on task performance in human computer interaction . it investigates the effects of gz accelerations and field dependency independency on human performance in the completion of perceptual motor tasks on a personal digital assistant ( pda ) . we report the results of a controlled experiment , conducted in an aerobatic aircraft under multiple gz conditions , showing that cognitive style significantly impacts latency and accuracy in target acquisition for perceptual motor tasks in altered gz environments and propose design guidelines as countermeasures . based on the results , we argue that developing design requirements taking into account cognitive differences in extreme environments will allow users to execute perceptual motor tasks efficiently without unnecessarily increasing cognitive load and the probability of critical errors .
locality discriminating indexing for document classification . <eos> this paper introduces a locality discriminating indexing ( ldi ) algorithm for document classification . based on the hypothesis that samples from different classes reside in class specific manifold structures , ldi seeks for a projection which best preserves the within class local structures while suppresses the between class overlap . comparative experiments show that the proposed method isable to derives compact discriminating document representations for classification .
accelespell , a gestural interactive game to learn and practice finger spelling . <eos> in this paper , an interactive computer game for learning and practicing continuous fingerspelling is described . the game is controlled by an instrumented glove known as acceleglove and a recognition algorithm based on decision trees . the graphical user interface is designed to allow beginners to remember the correct hand shapes and start finger spelling words sooner than traditional methods of learning .
a systematic optimization approach for assembly sequence planning using taguchi method , doe , and bpnn . <eos> research in assembly planning can be categorised into three types of approach graph based , knowledge based and artificial intelligence approaches . the main drawbacks of the above approaches are as follows the first is time consuming in the second approach it is difficult to find the optimal solution and the third approach requires a high computing efficiency . to tackle these problems , this study develops a novel approach integrated with some graph based heuristic working rules , robust back propagation neural network ( bpnn ) engines via taguchi method and design of experiment ( doe ) , and a knowledge based engineering ( kbe ) system to assist the assembly engineers in promptly predicting a near optimal assembly sequence . three real world examples are dedicated to evaluating the feasibility of the proposed model in terms of the differences in assembly sequences . the results show that the proposed model can efficiently generate bpnn engines , facilitate assembly sequence optimisation and allow the designers to recognise the contact relationships , assembly difficulties and assembly constraints of three dimensional ( 3d ) components in a virtual environment type .
a fuzzy neural network controller with adaptive learning rates for nonlinear slider crank mechanism . <eos> a fuzzy neural network ( fnn ) controller with adaptive learning rates is proposed to control a nonlinear mechanism system in this study . first , the network structure and the on line learning algorithm of the fnn is described . to guarantee the convergence of the tracking error , analytical methods based on a discrete type lyapunov function are proposed to determine the adaptive learning rates of the fnn . next , a slider crank mechanism , which is driven by a permanent magnet ( pm ) synchronous motor , is studied as an example to demonstrate the effectiveness of the proposed control technique the fnn controller is implemented to control the slider position of the motor slider crank nonlinear mechanism . the robust control performance and learning ability of the proposed fnn controller with adaptive learning rates is demonstrated by simulation and experimental results .
simulated analysis for ingap gaas heterostructure emitter bipolar transistor with ingaas gaas superlattice base structure . <eos> a novel ingap gaas heterostructure emitter bipolar transistor ( hebt ) with ingaas gaas superlattice base structure is proposed and demonstrated by two dimensional analysis . as compared with the traditional hebt , the studied superlattice base device exhibits a higher collector current , a higher current gain of <digit> , and a lower baseemitter ( be ) turn on voltage of 0.966 v at a current level of <digit> a , attributed to the increased charge storage of minority carriers in the ingaas gaas superlattice base region by tunneling behavior . the low turn on voltage can reduce the operating voltage and collectoremitter offset voltage for low power consumption in circuit applications .
why it has become more difficult to predict nobel prize winners a bibliometric analysis of nominees and winners of the chemistry and physics prizes ( <digit> ) . <eos> we propose a comprehensive bibliometric study of the profile of nobel prize winners in chemistry and physics from <digit> to <digit> , based on citation data available over the same period . the data allows us to observe the evolution of the profiles of winners in the years leading up toand followingnominations and awarding of the nobel prize . the degree centrality and citation rankings in these fields confirm that the prize is awarded at the peak of the winners citation history , despite a brief halo effect observable in the years following the attribution of the prize . changes in the size and organization of the two fields result in a rapid decline of predictive power of bibliometric data over the century . this can be explained not only by the growing size and fragmentation of the two disciplines , but also , at least in the case of physics , by an implicit hierarchy in the most legitimate topics within the discipline , as well as among the scientists selected for the nobel prize . furthermore , the lack of readily identifiable dominant contemporary physicists suggests that there are few new paradigm shifts within the field , as perceived by the scientific community as a whole .
an interactive documentation system . <eos> most chronic users of time sharing computer systems are familiar with programs that allow the creation and manipulation of text files . less often they have at their disposal programs that will format the document described by a text file , generating output such as a typist might produce . rarely is there any mechanism by which graphics can be integrated with text . lawrence livermore laboratory has a powerful , flexible and interactive computer based documentation system that will format a source document description according to user specifications and incorporate illustrations to produce online documents , offset reproduction masters , <digit> mm color slides , movie titles , or viewgraphs . the flexibility of the system is greatly enhanced by the use of a device independent graphics library . text may be plotted using the hardware characters specific to a device ( when possible ) , or may be drawn as hershey characters or polygonally outlined symbols . illustrations may be defined in a simple 2d graphics language , and graphical output from application programs may also be incorporated directly into a document .
robust time varying filtering and separation of some nonstationary signals in low snr environments . <eos> the proposed algorithm improves filtering performance for monocomponent signals . the proposed algorithm separates multicomponent signals into individual components . the requirement of high sampling rates is significantly relaxed . the proposed algorithm is implemented with low complexity .
towards a general neural controller for quadrupedal locomotion . <eos> our study aims at the design and implementation of a general controller for quadruped locomotion , allowing the robot to use the whole range of quadrupedal gaits ( i.e.from low speed walking to fast running ) . a general legged locomotion controller must integrate both posture control and rhythmic motion control and have the ability to shift continuously from one control method to the other according to locomotion speed . we are developing such a general quadrupedal locomotion controller by using a neural model involving a cpg ( central pattern generator ) utilizing ground reaction force sensory feedback . we used a biologically faithful musculoskeletal model with a spine and hind legs , and computationally simulated stable stepping motion at various speeds using the neuro mechanical system combining the neural controller and the musculoskeletal model . we compared the changes of the most important locomotion characteristics ( stepping period , duty ratio and support length ) according to speed in our simulations with the data on real cat walking . we found similar tendencies for all of them . in particular , the swing period was approximately constant while the stance period decreased with speed , resulting in a decreasing stepping period and duty ratio . moreover , the support length increased with speed due to the posterior extreme position that shifted progressively caudally , while the anterior extreme position was approximately constant . this indicates that we succeeded in reproducing to some extent the motion of a cat from the kinematical point of view , even though we used a 2d bipedal model . we expect that such computational models will become essential tools for legged locomotion neuroscience in the future .
on the superlinear local convergence of a filter sqp method . <eos> transition to superlinear local convergence is shown for a modified version of the trust region filter sqp method for nonlinear programming introduced by fletcher , leyffer , and toint <digit> . hereby , the original trust region sqp steps can be used without an additional second order correction . the main modification consists in using the lagrangian function value instead of the objective function value in the filter together with an appropriate infeasibility measure . moreover , it is shown that the modified trust region filter sqp method has the same global convergence properties as the original algorithm in <digit> .
multiway covariates regression models . <eos> an abundance of methods exist to regress a y variable on a set of x variables collected in a matrix x. in the chemical sciences a growing number of problems translate into arrays of measurements x and y , where x and y are three way arrays or multiway arrays . in this paper a general model is described for regressing such a multiway y on a multiway x , while taking into account three way structures in x and y. a global least squares optimization problem is formulated to estimate the parameters of the model . the model is described and illustrated with a real industrial example from batch process operation . an algorithm is given in an appendix . copyright ( c ) <digit> john wiley sons , ltd .
a note on the iterative object symmetry transform . <eos> this paper introduces a new operator named the iterated object transform that is computed by combining the object symmetry transform with the morphological operator erosion . this new operator has been applied on both binary and gray levels images showing the ability to grasp the internal structure of a digital object . we present also some experiments on artificial and real images and potential applications .
an anelastic allspeed projection method for gravitationally stratified flows . <eos> this paper looks at gravitationally stratified atmospheric flows at low mach and fronde numbers and proposes a new algorithm to solve the compressible euler equations , in which the asymptotic limits are recovered numerically and the boundary conditions for block structured local refinement methods are well posed . the model is non hydrostatic and the numerical algorithm uses a splitting to separate the fast acoustic dynamics from the slower anelastic dynamics . the acoustic waves are treated implicitly while the anelastic dynamics is treated semi implicitly and an embedded boundary method is used to represent orography . we present an example that verifies our asymptotic analysis and a set of results that compares very well with the classical gravity wave results presented by durran . ( c ) <digit> elsevier inc. all rights reserved .
improvement of cardiac ct reconstruction using local motion vector fields . <eos> the motion of the heart is a major challenge for cardiac imaging using ct. a novel approach to decrease motion blur and to improve the signal to noise ratio is motion compensated reconstruction which takes motion vector fields into account in order to correct motion . the presented work deals with the determination of local motion vector fields from high contrast objects and their utilization within motion compensated filtered back projection reconstruction . image registration is applied during the quiescent cardiac phases . temporal interpolation in parameter space is used in order to estimate motion during strong motion phases . the resulting motion vector fields are during image reconstruction . the method is assessed using a software phantom and several clinical cases for calcium scoring . as a criterion for reconstruction quality , calcium volume scores were derived from both , gated cardiac reconstruction and motion compensated reconstruction throughout the cardiac phases using low pitch helical cone beam ct acquisitions . the presented technique is a robust method to determine and utilize local motion vector fields . motion compensated reconstruction using the derived motion vector fields leads to superior image quality compared to gated reconstruction . as a result , the gating window can be enlarged significantly , resulting in increased snr , while reliable hounsfield units are achieved due to the reduced level of motion artefacts . the enlargement of the gating window can be translated into reduced dose requirements .
sharing the costs of maintaining environmental resources a comparison of different programmes . <eos> suppose state a controls some resource such as a rainforest and there are some other agents in the international system that wish to see this resource preserved . these agents are prepared to make a contribution towards sharing the costs of maintaining the resource . what would be the long term trajectory of the resource level under different programmes what type of cost sharing programme would maintain the highest level of the resource which programme would give the best value for money for the contributing player this paper attempts to answer these questions . this is done by examining a dynamic model with an infinite time horizon .
regression based d optimality experimental design for sparse kernel density estimation . <eos> this paper derives an efficient algorithm for constructing sparse kernel density ( skd ) estimates . the algorithm first selects a very small subset of significant kernels using an orthogonal forward regression ( ofr ) procedure based on the d optimality experimental design criterion . the weights of the resulting sparse kernel model are then calculated using a modified multiplicative nonnegative quadratic programming algorithm . unlike most of the skd estimators , the proposed d optimality regression approach is an unsupervised construction algorithm and it does not require an empirical desired response for the kernel selection task . the strength of the d optimality ofr is owing to the fact that the algorithm automatically selects a small subset of the most significant kernels related to the largest eigenvalues of the kernel design matrix , which counts for the most energy of the kernel training data , and this also guarantees the most accurate kernel weight estimate . the proposed method is also computationally attractive , in comparison with many existing skd construction algorithms . extensive numerical investigation demonstrates the ability of this regression based approach to efficiently construct a very sparse kernel density estimate with excellent test accuracy , and our results show that the proposed method compares favourably with other existing sparse methods , in terms of test accuracy , model sparsity and complexity , for constructing kernel density estimates .
pulmonary nodule registration in serial ct scans using global rib matching and nodule template matching . <eos> we propose an automatic nodule registration method between baseline and follow up chest ct scans . initial alignment using the center of the lung volume corrects the gross translational mismatch , and rigid registration using coronal and sagittal maximum intensity projection images effectively refines the rigid motion of the lungs . nodule correspondences are established by finding the most similar region in terms of density as well as the geometrical constraint . the proposed nodule registration method increased the nodule hit rate ( the ratio of the number of successfully matched nodules to total nodule number ) from <digit> % to <digit> % .
robotics in special needs education . <eos> the purpose of this study is to explore the potential of robotics as an educational tool in special needs education . qualitative case studies are used to increase knowledge about programmable lego nxt and topobo robotics constructions kits in special needs education , and about the social robot and topobo that are used in early childhood education when possible learning disabilities have not yet been diagnosed . this study aims to provide suggestions about how robotics might be used to recognize disabilities at an early stage of education and to compensate for them in learning .
applying fisher 's filter to select kdd connections ' features and using neural networks to classify and detect attacks . <eos> most of the neural networks based intrusion detection systems ( ids ) examine all data features to detect intrusion or misuse patterns . some of the features may be redundant or contribute little ( if anything ) to the detection process . that is why the purpose of this study is to identify important kdd features which will be used to train a neural network ( nn ) , in order to best classify and detect attacks . four nns were studied modular , recurrent , principal component analysis ( pca ) , and time lag recurrent ( tlr ) nns . we investigated the performance of combining the fisher 's filter used as a feature selection technique , with one of the previously cited nns . our simulations show that using fisher 's filter improves largely the performance of the four considered nns in terms of detection rate , attack classification , and computational time .
tight upper bounds on the minimum precision required of the divisor and the partial remainder in high radix division . <eos> digit recurrence binary dividers are sped up via two complementary methods keeping the partial remainder in redundant form and selecting the quotient digits in a radix higher than <digit> . use of a redundant partial remainder replaces the standard addition in each cycle by a carry free addition , thus making the cycles shorter . deriving the quotient in high radix reduces the number of cycles ( by a factor of about h for radix <digit> ( h ) ) . to make the redundant partial remainder scheme work , quotient digits must be chosen from a redundant set , such as <digit> , <digit> in radix <digit> . the redundancy provides some tolerance to imprecision so that the quotient digits can be selected based on examining truncated versions of the partial remainder and divisor . no closed form formula for the required precision in the partial remainder and divisor , as a function of the quotient digit set and the range of the partial remainders is known . in this paper , we establish tight upper bounds on the required precision for the partial remainder and divisor . the bounds are tight in the sense that each is only one bit over a well known simple lower bound . we also discuss the implications of these bounds for the quotient digit selection process .
the effect of viewing angle on wrist posture estimation from photographic images using novice raters . <eos> observational assessment of wrist posture using photographic methods is theoretically affected by camera view angle . a study was conducted to investigate whether wrist flexion extension and radial ulnar deviation postures were estimated differently by raters depending on the viewing angle and compared to predictions using a quantitative 2d model of parallax . novice raters ( n <digit> ) estimated joint angles from images of wrist postures photographed from ten different viewing angles . results indicated that ideal views , orthogonal to the plane of motion , produced more accurate estimates of posture compared to non ideal views . the neutral ( <digit> ) posture was estimated the most accurately even at different viewing angles . raters were more accurate than model predictions . findings demonstrate a need for more systematic methods for collecting and analyzing photographic data for observational studies of posture . renewed caution in interpreting existing studies of wrist posture where viewing angle was not controlled is advised .
new proposals for the design of steel beam columns in case of fire , including a new approach for the lateraltorsional buckling . <eos> the possibility of having , in parts <digit> <digit> and <digit> <digit> of eurocode <digit> , the same approach for the design of beam columns and for lateraltorsional buckling , was investigated by the authors in previous papers using a numerical approach , where it was concluded that those assumptions could be made . in the present paper , a new approach for lateraltorsional buckling has been used with the formulae for the design of beam columns at elevated temperature based on pren <digit> <digit> <digit> combined with the formulae from pren <digit> <digit> <digit> . in both cases the results obtained are much better than the current design expressions , when compared with those obtained in the numerical calculations .
a note on the article fuzzy less strongly semiopen sets and fuzzy less strong semicontinuity . <eos> in this note we show that some results in the article by fang jing ming are incorrect . ( c ) <digit> elsevier science b.v. all rights reserved .
differential fault analysis on camellia . <eos> camellia is a <digit> bit block cipher published by ntt and mitsubishi in <digit> . on the basis of the byte oriented model and the differential analysis principle , we propose a differential fault attack on the camellia algorithm . mathematical analysis and simulating experiments show that our attack can recover its <digit> bit , <digit> bit or <digit> bit secret key by introducing <digit> faulty ciphertexts . thus our result in this study describes that camellia is vulnerable to differential fault analysis . this work provides a new reference to the fault analysis of other block ciphers .
the l ( 2,1 ) l ( <digit> , <digit> ) labeling of unigraphs . <eos> the l ( 2,1 ) l ( <digit> , <digit> ) labeling problem consists of assigning colors from the integer set <digit> , , <digit> , , to the nodes of a graph g g in such a way that nodes at a distance of at most two get different colors , while adjacent nodes get colors which are at least two apart . the aim of this problem is to minimize and it is in general np complete . in this paper the problem of l ( 2,1 ) l ( <digit> , <digit> ) labeling unigraphs , i.e. graphs uniquely determined by their own degree sequence up to isomorphism , is addressed and a <digit> <digit> <digit> <digit> approximate algorithm for l ( 2,1 ) l ( <digit> , <digit> ) labeling unigraphs is designed . this algorithm runs in o ( n ) o ( n ) time , improving the time of the algorithm based on the greedy technique , requiring o ( m ) o ( m ) time , that may be near to ( n2 ) ( n <digit> ) for unigraphs .
ground control station embedded mission planning for uas . <eos> as the unmanned aerial system ( uas ) level of automation increases , mission planning relevance raises . a mission plan can be defined as all the information needed to reach the assigned goals , and it is composed by several sub plans . in particular , the mission plan core is represented by the routes . since the route creation process is very complex , the introduction of route creation and verification algorithms is required . these algorithms enhance the crew replan performances during the mission execution , and permit to implement autonomous on board replanning . furthermore , planning replanning processes could also have a key role in the integration of uas in the civil airspace . according to these considerations , a mission planner embedded in the alenia aermacchi uas ground control station ( gcs ) has been developed , comprised of advanced planning algorithms .
automatic frechet differentiation for the numerical solution of boundary value problems . <eos> a new solver for nonlinear boundary value problems ( bvps ) in matlab is presented , based on the chebfun software system for representing functions and operators automatically as numerical objects . the solver implements newton 's method in function space , where instead of the usual jacobian matrices , the derivatives involved are frechet derivatives . a major novelty of this approach is the application of automatic differentiation ( ad ) techniques to compute the operator valued frechet derivatives in the continuous context . other novelties include the use of anonymous functions and numbering of each variable to enable a recursive , delayed evaluation of derivatives with forward mode ad . the ad techniques are applied within a new chebfun class called chebop which allows users to set up and solve nonlinear bvps , both scalar and systems of coupled equations , in a few lines of code , using the nonlinear backslash operator ( ) . this framework enables one to study the behaviour of newton 's method in function space .
asynchronous parallel finite automaton a new mechanism for deep packet inspection in cloud computing . <eos> security is quite an important issue in cloud computing . the general security mechanisms applied in the cloud are always passive defense methods such as encryption . besides these , it 's necessary to utilize real time active monitoring , detection and defense technologies . according to the published researches , deep packets inspection ( dpi ) is the most effective technology to realize active inspection and defense . however , most of the works on dpi focus on its performance in general application scenarios and make improvement for space reduction , which could not meet the demands of high speed and stability in the cloud . therefore it is meaningful to improve the common mechanisms of dpi , making it more suitable for cloud computing . in this paper , an asynchronous parallel finite automaton ( fa ) is proposed . the applying of asynchronous parallelization and heuristic forecast mechanism decreases the time consumed in matching significantly , while still reduces the memory required . moreover , it is immune to overlapping problem , also enhancing the stability . the final evaluation results show that asynchronous parallel fa has higher stability , better performance on both time and memory , and is more suitable for cloud computing .
dynamic of a non autonomous predatorprey system with infinite delay and diffusion . <eos> in the present paper , a nonlinear non autonomous predatorprey dispersion model with continuous delay is studied . sufficient conditions which guarantee the existence of a periodic positive solution are obtained by using gaines and mawhins continuation theorem of coincidence degree theory . moreover , globally asymptotically stability of the system is also obtained by means of a suitable lyapunov functional . the applications show that these criteria are easily verified .
secure and lightweight network admission and transmission protocol for body sensor networks . <eos> a body sensor network ( bsn ) is a wireless network of biosensors and a local processing unit , which is commonly referred to as the personal wireless hub ( pwh ) . personal health information ( phi ) is collected by biosensors and delivered to the pwh before it is forwarded to the remote healthcare center for further processing . in a bsn , it is critical to only admit eligible biosensors and pwh into the network . also , securing the transmission from each biosensor to pwh is essential not only for ensuring safety of phi delivery , but also for preserving the privacy of phi . in this paper , we present the design , implementation , and evaluation of a secure network admission and transmission subsystem based on a polynomial based authentication scheme . the procedures in this subsystem to establish keys for each biosensor are communication efficient and energy efficient . moreover , based on the observation that an adversary eavesdropping in a bsn faces inevitable channel errors , we propose to exploit the adversary 's uncertainty regarding the phi transmission to update the individual key dynamically and improve key secrecy . in addition to the theoretical analysis that demonstrates the security properties of our system , this paper also reports the experimental results of the proposed protocol on resource limited sensor platforms , which show the efficiency of our system in practice .
time domain orthogonal finite element reduction recovery method for electromagnetics based analysis of large scale integrated circuit and package problems . <eos> a time domain orthogonal finite element reduction recovery method is developed to overcome the large problem sizes encountered in the simulation of large scale integrated circuit and package problems . in this method , a set of orthogonal prism vector basis functions is developed . based on this set of bases , an arbitrary <digit> d multilayered system such as a combined package and die is reduced to a single layer system with negligible computational cost . more importantly , the reduced single layer system is diagonal and , hence , can be solved readily . from the solution of the reduced system , the solution of the other unknowns is recovered in linear complexity . the method entails no theoretical approximation . it applies to any arbitrarily shaped multilayer structure involving inhomogeneous materials or any structure that can be geometrically modeled by triangular prism elements . in addition , it permits nonlinear device modeling and broadband simulation within one run . numerical and experimental results have demonstrated its accuracy and high capacity in simulating on chip , package , and die package interface problems .
the role of social network sites in romantic relationships effects on jealousy and relationship happiness . <eos> on social network sites ( sns ) , information about one 's romantic partner is readily available and public for friends . the paper focuses on the negative ( sns jealousy ) and positive ( sns relationship happiness ) consequences of sns use for romantic relationships . we examined whether relationship satisfaction , trait jealousy , sns use and need for popularity predicted these emotional consequences of sns use and tested the moderating role of self esteem . for low self esteem individuals , need for popularity predicted jealousy and relationship happiness . for high self esteem individuals , sns use for grooming was the main predictor . low self esteem individuals try to compensate their low self esteem by creating an idealized picture . undesirable information threatens this picture , and especially individuals with a high need for popularity react with sns jealousy .
nonlinear data projection on non euclidean manifolds with controlled trade off between trustworthiness and continuity . <eos> this paper presents a framework for nonlinear dimensionality reduction methods aimed at projecting data on a non euclidean manifold , when their structure is too complex to be embedded in an euclidean space . the methodology proposes an optimization procedure on manifolds to minimize a pairwise distance criterion that implements a control of the trade off between trustworthiness and continuity , two criteria that , respectively , represent the risks of flattening and tearing the projection . the methodology is presented as general as possible and is illustrated in the specific case of the sphere .
quantum computation for action selection using reinforcement learning . <eos> this paper proposes a novel action selection method based on quantum computation and reinforcement learning ( rl ) . inspired by the advantages of quantum computation , the state action in a rl system is represented with quantum superposition state . the probability of action eigenvalue is denoted by probability amplitude , which is updated according to rewards . and the action selection is carried out by observing quantum state according to collapse postulate of quantum measurement . the results of simulated experiments show that quantum computation can be effectively used to action selection and decision making through speeding up learning . this method also makes a good tradeoff between exploration and exploitation for rl using probability characteristics of quantum theory .
principal agent theory and its application to analyze outsourcing of software development . <eos> much has been written on process models , project management or tool support to increase the return on investment in software through higher quality of the development process and the resulting software or system . yet , we lack understanding in the underlying economic principles e.g. , an external firm paid to develop software for someone else tries to maximize their own profit instead of the contractor 's . these divergences of interests result in projects that consume more time and money and meet fewer requirements than expected . in this paper , we try to fill the gap by providing an insight into the theory and presenting applicable suggestions how to diminish or avoid the problems that arise when selecting the ' best ' contractor and during the project . basic advises on the formulation of contracts can be derived .
saccular projections in the human cerebral cortex . <eos> abstract the cerebral cortical areas processing saccular information were investigated in human subjects using the fmri method and loud clicks , which selectively activate the saccule . the results were compared with previous vestibular evoked potential ( vep ) studies in anesthetized patients following vestibular nerve stimulation . nine normal subjects participated in fmri studies . by comparing the cortical areas activated by a click at <digit> db ( auditory activation ) with those activated by <digit> db ( auditory plus saccular activation ) , the following cortical areas were selectively activated by saccular stimulation intraparietal sulcus , frontal eye fields , prefrontal cortex , and postcentral gyrus , in addition to insula , supplementary motor area , and anterior and posterior cingulate cortex . previous vep studies also revealed similar activation areas by vestibular nerve stimulation with latencies at <digit> ms , suggesting that the shortest pathways for activation of cerebral cortical neurons from the labyrinth are trisynaptic , with a relay in the thalamus . the activated areas are also consistent with results in previous studies using caloric stimulation , which primarily activates horizontal semicircular canals . these results suggest that canal and otolith information is processed largely by similar cortical areas in humans . multiple cortical areas activated by these studies suggest that these areas are involved in different aspects of processing vestibular information . the saccular projections to the prefrontal and frontal cortex suggest that these areas are involved in planning motor synergies to counteract loss of equilibrium .
state density functions over dbm domains in the analysis of non markovian models . <eos> quantitative evaluation of models with generally distributed transitions requires the analysis of non markovian processes that may be not isomorphic to their underlying untimed models and may include any number of concurrent nonexponential timers . the analysis of stochastic time petri nets ( stpns ) copes with the problem by covering the state space with stochastic classes , which extend the theory of difference bounds matrix ( dbm ) with a state probability density function . as a core step , the analysis process requires symbolic manipulation of density functions supported over dbm domains . we characterize and engineer the critical steps of this derivation . we first show that the state density function accepts a continuous piecewise representation over a partition in dbm shaped subdomains . we then develop a closed form symbolic calculus of state density functions under the assumption that transitions in the stpn model have expolynomial distributions over possibly bounded intervals . the calculus shows that within each subdomain , the state density function is a multivariate expolynomial function , and it makes explicit the way in which this form evolves and grows in complexity as the state accumulates memory through subsequent transitions . this enables an efficient implementation of the analysis process and provides the formal basis that supports the introduction of an imprecise analysis based on the approximation of state density functions through bernstein polynomials . the approximation attacks practical and theoretical limits in the applicability of stochastic state classes and devises a new approach to the analysis of non markovian models , relying on approximations in the state space rather than in the structure of the model .
reanalysis and sensitivity reanalysis by combined approximations . <eos> one of the main obstacles in the solution of structural optimization problems is the need to repeat solutions of the analysis and sensitivity analysis equations . in large scale structures , having complex analysis models , the computational effort may become prohibitive . to alleviate this difficulty a general approach for repeated analysis and repeated sensitivity analysis , called combined approximations , was developed during the last <digit> years . the solution is based on the integration of several algorithms and methods . as a result , accurate results can be achieved efficiently . in previous studies , solution procedures for various particular problems were developed . this article summarizes the various formulations and solution procedures for reanalysis and sensitivity reanalysis of linear , nonlinear , static and dynamic systems . it is shown that the various solution procedures are based on applications of similar basic algorithms . numerical examples demonstrate the efficiency of the calculations and the accuracy of the results .
making on line logistics training sustainable through e learning . <eos> the purpose of this study is to investigate the possibility of using an online logistics certification learning environment as a training tool to equip future logisticians with required logistics skills . this study incorporates an online logistics certification website that was constructed for college students to familiarize themselves with the certification . in addition , this study also performed comparison tests on students before and after their interaction with the web based learning environment system to ascertain the systems effectiveness . our findings suggest that such a system might motivate students to familiarize themselves with logistics related certification information and can enhance students professional capabilities . in addition , the web based learning environment might possibly motivate students to join logistics related industries in the future .
panorama weaving fast and flexible seam processing . <eos> a fundamental step in stitching several pictures to form a larger mosaic is the computation of boundary seams that minimize the visual artifacts in the transition between images . current seam computation algorithms use optimization methods that may be slow , sequential , memory intensive , and prone to finding suboptimal solutions related to local minima of the chosen energy function . moreover , even when these techniques perform well , their solution may not be perceptually ideal ( or even good ) . such an inflexible approach does not allow the possibility of user based improvement . this paper introduces the panorama weaving technique for seam creation and editing in an image mosaic . first , panorama weaving provides a procedure to create boundaries for panoramas that is fast , has low memory requirements and is easy to parallelize . this technique often produces seams with lower energy than the competing global technique . second , it provides the first interactive technique for the exploration of the seam solution space . this powerful editing capability allows the user to automatically extract energy minimizing seams given a sparse set of constraints . with a variety of empirical results , we show how panorama weaving allows the computation and editing of a wide range of digital panoramas including unstructured configurations .
mining service abstractions ( nier track ) . <eos> several lines of research rely on the concept of service abstractions to enable the organization , the composition and the adaptation of services . however , what is still missing , is a systematic approach for extracting service abstractions out of the vast amount of services that are available all over the web . to deal with this issue , we propose an approach for mining service abstractions , based on an agglomerative clustering algorithm . our experimental findings suggest that the approach is promising and can serve as a basis for future research .
average voice based speech synthesis using hsmm based speaker adaptation and adaptive training . <eos> in speaker adaptation for speech synthesis , it is desirable to convert both voice characteristics and prosodic features such as f0 and phone duration . for simultaneous adaptation of spectrum , f0 and phone duration within the hmm framework , we need to transform not only the state output distributions corresponding to spectrum and f0 but also the duration distributions corresponding to phone duration . however , it is not straightforward to adapt the state duration because the original hmm does not have explicit duration distributions . therefore , we utilize the framework of the hidden semi markov model ( hsmm ) , which is an hmm having explicit state duration distributions , and we apply an hsmm based model adaptation algorithm to simultaneously transform both the state output and state duration distributions . furthermore , we propose an hsmm based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model . we incorporate these techniques into our hsmm based speech synthesis system , and show their effectiveness from the results of subjective and objective evaluation tests .
a two parameter continuation algorithm for vortex pinning in rotating boseeinstein condensates . <eos> we describe an efficient two parameter continuation algorithm combined with spectral collocation methods for computing the ground state and central vortex state solutions of rotating boseeinstein condensates in optical lattices , where the first kind and second kind chebyshev polynomials are used as the basis functions for the trial function space . by treating the chemical potential and angular velocity as the continuation parameters simultaneously under the additional constraint of normalization condition , the proposed algorithm can effectively compute numerical solutions for a rich variety of physical phenomena observed in physical experiments with very little cost . comparisons with various numerical methods on some sample test problems are reported .
complexity and endogeneity in economic modeling . <eos> purpose the concepts of complexity , endogeneity and circular causation myrdal 's term was cumulative causation are shown to be interrelated ones in configuring an economic model in the framework of systemic embedding and its empirical application . design methodology approach the ensuing framework of economic modeling with complexity provides a controllable and predictable overarching worldview . anomie in the economic universe and its embedded world system are analytically rejected . this consequence is due to the epistemic nature of modeling that combines complexity , endogeneity , and circular causation for attaining predictability and controllability , even in the face of complex systemic perturbations . the epistemology of unity of knowledge contrasted with rationalism is treated as the foundational worldview . an illustrative empirical work is given to convey the conceptual model and its applied viability . findings both the theoretical and empirical results point out how the induced effects of knowledge flows in reference to the epistemology of unity of knowledge continuously improves the complementary relationships of the evolutionary learning fields , and rejects marginalism as being logically non sequiter in such epistemic systems . research limitations implications more variables and data would increase the explanation of the continuous simulation in the evolutionary learning world system model . practical implications more data would increase the versatility of the empirical exercise . social implications the study is based on the idea of social and economic interface in extending the scope of economic modeling . originality value the paper is very original in the area of heterodox economics that questions orthodox economic postulates and presents the complex methodology by circular causation method instead .
dyfram dynamic fragmentation and replica management in distributed database systems . <eos> in distributed database systems , tables are frequently fragmented and replicated over a number of sites in order to reduce network communication costs . how to fragment , when to replicate and how to allocate the fragments to the sites are challenging problems that has previously been solved either by static fragmentation , replication and allocation , or based on a priori query analysis . many emerging applications of distributed database systems generate very dynamic workloads with frequent changes in access patterns from different sites . in such contexts , continuous refragmentation and reallocation can significantly improve performance . in this paper we present dyfram , a decentralized approach for dynamic table fragmentation and allocation in distributed database systems based on observation of the access patterns of sites to tables . the approach performs fragmentation , replication , and reallocation based on recent access history , aiming at maximizing the number of local accesses compared to accesses from remote sites . we show through simulations and experiments on the dascosa distributed database system that the approach significantly reduces communication costs for typical access patterns , thus demonstrating the feasibility of our approach .
the anatomy of decision support during inpatient care provider order entry ( cpoe ) empirical observations from a decade of cpoe experience at vanderbilt . <eos> the authors describe a pragmatic approach to the introduction of clinical decision support at the point of care , based on a decade of experience in developing and evolving vanderbilts inpatient wizorder care provider order entry ( cpoe ) system . the inpatient care setting provides a unique opportunity to interject cpoe based decision support features that restructure clinical workflows , deliver focused relevant educational materials , and influence how care is delivered to patients . from their empirical observations , the authors have developed a generic model for decision support within inpatient cpoe systems . they believe that the models utility extends beyond vanderbilt , because it is based on characteristics of end user workflows and on decision support considerations that are common to a variety of inpatient settings and cpoe systems . the specific approach to implementing a given clinical decision support feature within a cpoe system should involve evaluation along three axes what type of intervention to create ( for which the authors describe <digit> general categories ) when to introduce the intervention into the users workflow ( for which the authors present <digit> categories ) , and how disruptive , during use of the system , the intervention might be to end users workflows ( for which the authors describe <digit> categories ) . framing decision support in this manner may help both developers and clinical end users plan future alterations to their systems when needs for new decision support features arise .
the method of approximate fundamental solutions for axisymmetric problems with laplace operator . <eos> the paper presents a new numerical technique for solving axisymmetric problems with laplace operator . it is similar to the method of fundamental solutions but it is based on the use of special basis functions which satisfy the majority of the boundary conditions of the problem considered . this reduces the number of unknowns and the size of the collocation matrix considerably . as it is shown in the paper , this technique can also be applied successfully in the cases when the solution domain has infinite boundaries in z z or r r directions . numerical examples justifying the method are presented .
bayesian ordinal and binary regression models with a parametric family of mixture links . <eos> an ordinal and binary regression model with parametric link is introduced . the link is a member of a one parameter family of mixture links , a family that comprises smooth mixtures of the extreme minimum value , extreme maximum value , and logistic distributions . a bayesian version of this flexible model serves as a vehicle for introducing a priori information regarding the choice of link . owing to non conjugacy , posterior and predictive distributions are approximated using markov chain monte carlo simulation methods . link independent , bayesian interpretations of covariate effects are described . the method is illustrated through the analyses of several data sets .
a generic implementation of replica exchange with solute tempering ( rest2 ) algorithm in namd for complex biophysical simulations . <eos> replica exchange with solute tempering ( rest2 ) is a powerful sampling enhancement algorithm of molecular dynamics ( md ) in that it needs significantly smaller number of replicas but achieves higher sampling efficiency relative to standard temperature exchange algorithm . in this paper , we extend the applicability of rest2 for quantitative biophysical simulations through a robust and generic implementation in greatly scalable md software namd . the rescaling procedure of force field parameters controlling rest2 hot region is implemented into namd at the source code level . a user can conveniently select hot region through vmd and write the selection information into a pdb file . the rescaling keyword parameter is written in namd tcl script interface that enables an on the fly simulation parameter change . our implementation of rest2 is within communication enabled tcl script built on top of charm , thus communication overhead of an exchange attempt is vanishingly small . such a generic implementation facilitates seamless cooperation between rest2 and other modules of namd to provide enhanced sampling for complex biomolecular simulations . three challenging applications including native rest2 simulation for peptide foldingunfolding transition , free energy perturbation rest2 for absolute binding affinity of proteinligand complex and umbrella sampling rest2 hamiltonian exchange for free energy landscape calculation were carried out on ibm blue gene q supercomputer to demonstrate efficacy of rest2 based on the present implementation . program title rest2 namd catalogue identifier aexx_v1_0 program summary url http cpc.cs.qub.ac.uk summaries aexx_v1_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions standard cpc licence , http cpc.cs.qub.ac.uk licence licence.html no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language c c , tcl8 .5 . computer not computer specific . operating system any . has the code been vectorized or parallelized yes , mpi and or pami parallelized depending on machine system software <digit> cores used on ibm blue gene q classification <digit> . external routines namd 2.10 ( http www.ks.uiuc.edu research namd ) nature of problem a generic implementation providing user friendly api including input file preparation and performing replica exchange , and high frequency exchange attempt frequency with minimal communication overhead . solution method the rescaling procedure of force field parameters controlling rest2 is implemented into namd at the source code level . a user can conveniently select hot region through vmd and write the selection information into a pdb file . the rescaling keyword parameter is written in namd tcl script interface that enables an on the fly simulation parameter change . the implementation of rest2 is within communication enabled tcl script built on top of charm , thus communication overhead of an exchange attempt is vanishingly small . running time <digit> min60 min
physically based modeling , simulation and rendering of fire for computer animation . <eos> we give an up to date survey on techniques and methods for fire simulation in computer graphics . physically based method prevails over traditional non physical methods for realistic visual effect . in this paper , we explore visual simulation of fire related phenomena in terms of physically modeling , numerical simulation and visual rendering . firstly , we introduce a physical and chemical coupled mathematical model to explain fire behavior and motion . several assumptions and constrains are put forward to simplify their implementations in computer graphics . we then give an overview of present methods to solve the most complicated processes in numerical simulation velocity advection and pressure projection . in addition , comparisons of these methods are also presented respectively . since fire is a participating medium as well as a visual radiator , we discuss techniques and problems of these issues as well . we conclude by addressing several open challenges and possible future research directions in fire simulation .
evaluating the success of an emergency response medical information system . <eos> statpack is an information system used to aid in the diagnosis of pathogens in hospitals and state public health laboratories . statpack is used as a communication and telemedicine diagnosis tool during emergencies . this paper explores the success of this emergency response medical information system ( ermis ) using a well known framework of information systems success developed by delone and mclean . using an online survey , the entire population of statpack users evaluated the success of the information system by considering system quality , information quality , system use , intention to use , user satisfaction , individual impact , and organizational impact . the results indicate that the overall quality of this ermis ( i.e. , system quality , information quality , and service quality ) has a positive impact on both user satisfaction and intention to use the system . however , given the nature of ermis , overall quality does not necessarily predict use of the system . moreover , the user 's satisfaction with the information system positively affected the intention to use the system . user satisfaction , intention to use , and system use had a positive influence on the system 's impact on the individual . finally , the organizational impacts of the system were positively influenced by use of the system and the system 's individual impact on the user . the results of the study demonstrate how to evaluate the success of an ermis as well as introduce potential changes in how one applies the delone and mclean success model in an emergency response medical information system context .
easy cases of probabilistic satisfiability . <eos> the probabilistic satisfiability problem ( psat ) can be considered as a probabilistic counterpart of the classical sat problem . in a psat instance , each clause in a cnf formula is assigned a probability of being true the problem consists in checking the consistency of the assigned probabilities . actually , psat turns out to be computationally much harder than sat , e.g. , it remains difficult for some classes of formulas where sat can be solved in polynomial time . a column generation approach has been proposed in the literature , where the pricing sub problem reduces to a weighted max sat problem on the original formula . here we consider some easy cases of psat , where it is possible to give a compact representation of the set of consistent probability assignments . we follow two different approaches , based on two different representations of cnf formulas . first we consider a representation based on directed hypergraphs . by extending a well known integer programming formulation of sat and max sat , we solve the case in which the hypergraph does not contain cycles a linear time algorithm is provided for this case . then we consider the co occurrence graph associated with a formula . we provide a solution method for the case in which the co occurrence graph is a partial <digit> tree , and we show how to extend this result to partial k trees with k > <digit> .
face description with local binary patterns application to face recognition . <eos> this paper presents a novel and efficient facial image representation based on local binary pattern ( lbp ) texture features . the face image is divided into several regions from which the lbp feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor . the performance of the proposed method is assessed in the face recognition problem under different challenges . other applications and several extensions are also discussed .
low complexity adaptive decision feedback equalization of mimo channels . <eos> a new adaptive mimo channel equalizer is proposed based on adaptive generalized decision feedback equalization and ordered successive interference cancellation . the proposed equalizer comprises equal length subequalizers , enabling any adaptive filtering algorithm to be employed for coefficient updates . a recently proposed computationally efficient recursive least squares algorithm based on dichotomous coordinate descents is utilized to solve the normal equations associated with the adaptation of the new equalizer . convergence of the proposed algorithm is examined analytically and simulations show that the proposed equalizer is superior to the previously proposed adaptive mimo channel equalizers by providing both enhanced bit error rate performance and reduced computational complexity . furthermore , the proposed algorithm exhibits stable numerical behavior and can deliver a trade off between performance and complexity .
performance of hsr and qpp based interleavers for turbo coding on power line communication systems . <eos> in this paper , the performance of different type and length interleavers for turbo codes is analyzed in the context of power line communication systems . this system typically operates in very noisy environments the noise , in this channel , is a combination of colored , narrow band and impulsive noises it has also strong amplitude attenuations . the digital modulation frequently employed in power line communication to counteract the channels noise effects is the orthogonal frequency division multiplexing due to its high spectral efficiency and robustness in multipath fading environments hence , it is also considered in our experimentation . we report the performance of turbo codes with the two types of interleavers the high spread random and the based quadratic permutation polynomial . the constituent codes are part of the 3gpp standard . finally , it is used a punctured matrix in order to achieve a coding rate of <digit> <digit> . the performance is evaluated in terms of bit error rate , through the way of simulations .
a numerical method for a model of two phase flow in a coupled free flow and porous media system . <eos> in this article , we study two phase fluid flow in coupled free flow and porous media regions . the model consists of coupled cahnhilliard and navierstokes equations in the free fluid region and the two phase darcy law in the porous medium region . we propose a robinrobin domain decomposition method for the coupled navierstokes and darcy system with the generalized beaversjosephsaffman condition on the interface between the free flow and the porous media regions . numerical examples are presented to illustrate the effectiveness of this method .
circular elm for the reduced reference assessment of perceived image quality . <eos> providing a satisfactory visual experience is one of the main goals for present day electronic multimedia devices . all the enabling technologies for storage , transmission , compression , rendering should preserve , and possibly enhance , the quality of the video signal to do so , quality control mechanisms are required . these mechanisms rely on systems that can assess the visual quality of the incoming signal consistently with human perception . computational intelligence ( ci ) paradigms represent a suitable technology to tackle this challenging problem . the present research introduces an augmented version of the basic extreme learning machine ( elm ) , the circular elm ( c elm ) , which proves effective in addressing the visual quality assessment problem . the c elm model derives from the original circular backpropagation ( cbp ) architecture , in which the input vector of a conventional multilayer perceptron ( mlp ) is augmented by one additional dimension , the circular input this paper shows that c elm can actually benefit from the enhancement provided by the circular input without losing any of the fruitful properties that characterize the basic elm framework . in the proposed framework , c elm handles the actual mapping of visual signals into quality scores , successfully reproducing perceptual mechanisms . its effectiveness is proved on recognized benchmarks and for four different types of distortions .
distributed reinforcement learning control for batch sequencing and sizing in just in time manufacturing systems . <eos> this paper presents an approach that is suitable for just in time ( jit ) production for multi objective scheduling problem in dynamically changing shop floor environment . the proposed distributed learning and control ( dlc ) approach integrates part driven distributed arrival time control ( datc ) and machine driven distributed reinforcement learning based control . with datc , part controllers adjust their associated parts ' arrival time to minimize due date deviation . within the restricted pattern of arrivals , machine controllers are concurrently searching for optimal dispatching policies . the machine control problem is modeled as semi markov decision process ( smdp ) and solved using q learning . the dlc algorithms are evaluated using simulation for two types of manufacturing systems family scheduling and dynamic batch sizing . results show that dlc algorithms achieve significant performance improvement over usual dispatching rules in complex real time shop floor control problems for jit production .
algorithm for calculating the noncentral chi square distribution . <eos> this correspondence presents a new algorithm for evaluating the noncentral chi square distribution based on parl 's method of neumann series expansion , it is applicable to both even and odd degrees of freedom , unlike most prior work , which has been directed at the even eases . convergence tests and procedures for detection of loss of precision are given . the overall method is extremely simple to program , accurate to many decimal places where applicable , and efficient over a wide range of parameters . the method is reliable provided the proper expansion is chosen based on the parameters .
new hierarchical architecture for ubiquitous wireless sensing and access with improved coverage using cwdm rof links . <eos> a novel hierarchical architecture for hybrid wireless sensor and access networks has been proposed based on cost effective radio over fiber ( rof ) links with the coarse wavelength division multiplexing ( cwdm ) technique . wireless fidelity ( wifi ) signals are distributed to the remote radio units transparently over optical fibers in a star shaped network topology . the wireless access traffic together with the perceiving usage scenarios including video monitoring and temperature sensing has been successfully demonstrated in the hybrid ieee 802.11 and 802.15.4 networks . the transmission performance of the cwdm rof links is evaluated in terms of the error vector magnitude ( evm ) and data throughput for both uplinks and downlinks . the results show that the wifi signals are successfully delivered through the cwdm rof links including a 4.5 km fiber and a <digit> m wireless channel with a <digit> % evm penalty . this cwdm rof technology can expand the application range of wireless sensor networks with the advantages of better capacity , larger coverage area , and lower investment on wired infrastructure .
bifurcation analysis of an inductorless chaos generator using 1d piecewise smooth map . <eos> in this work we investigate the dynamics of a one dimensional piecewise smooth map , which represents the model of a chaos generator circuit . in a particular ( symmetric ) case analytic results can be given showing that the chaotic region is wide and robust . in the general model only the border collision bifurcation can be analytically determined . however , the dynamics behave in a similar way , leading effectively to robust chaos . ( c ) <digit> published by elsevier b.v. on behalf of imacs .
a hierarchical approach for energy efficient scheduling of large workloads in multicore distributed systems . <eos> definition of a novel multi objective problem for energy efficient scheduling in distributed data centers . design of a hierarchical two level scheduler that allows dividing the problem into simpler and smaller sub problems . evaluation and comparison of <digit> different variants of the scheduler on large sets of workflows . accurate solutions found by the best performing schedulers , achieving important improvements over classical strategies .
collaborative feature based design via operations with a fine grain product database . <eos> this paper reports a collaborative product design framework and a prototype system that supports multiple cad systems . the key contribution is an ' operation ' based , multi application oriented , and near real time collaboration mechanism which can significantly reduce collaboration communication load over the network . the mechanism is discussed and demonstrated with examples . to support the proposed multi application collaboration system , a fine grain feature oriented product database is used . this research is a continued effort based on a shared common product modeling scheme , which covers fundamental issues of generic feature , feature level interoperability , engineering intent and operation definitions . ( c ) <digit> elsevier b.v. all rights reserved .
unified architecture for reed solomon decoder combined with burst error correction . <eos> reed solomon ( rs ) codes are widely used as forward correction codes ( fec ) in digital communication and storage systems . correcting random errors of rs codes have been extensively studied in both academia and industry . however , for burst error correction , the research is still quite limited due to its ultra high computation complexity . in this brief , starting from a recent theoretical work , a low complexity reformulated inversionless burst error correcting ( ribc ) algorithm is developed for practical applications . then , based on the proposed algorithm , a unified vlsi architecture that is capable of correcting burst errors , as well as random errors and erasures , is firstly presented for multi mode decoding requirements . this new architecture is denoted as unified hybrid decoding ( uhd ) architecture . it will be shown that , being the first rs decoder owning enhanced burst error correcting capability , it can achieve significantly improved error correcting capability than traditional hard decision decoding ( hdd ) design .
implementation aspects of 3d lattice bgk boundaries , accuracy , and a new fast relaxation method . <eos> in many realistic fluid dynamical simulations the specification of the boundary conditions , the error sources , and the number of time steps to reach a steady state are important practical considerations . in this paper we study these issues in the case of the lattice bgk model . the objective is to present a comprehensive overview of some pitfalls and shortcomings of the lattice bgk method and to introduce some new ideas useful in practical simulations . we begin with an evaluation of the widely used bounce back boundary condition in staircase geometries by simulating flow in an inclined tube . it is shown that the bounce back scheme is first order accurate in space when the location of the non slip wall is assumed to be at the boundary nodes . moreover , for a specific inclination angle of <digit> degrees , the scheme is found to be second order accurate when the location of the non slip velocity is fitted halfway between the last fluid nodes and the first solid nodes . the error as a function of the relaxation parameter is in that case qualitatively similar to that of flat walls . next , a comparison of simulations of fluid flow by means of pressure boundaries and by means of body force is presented . a good agreement between these two boundary conditions has been found in the creeping flow regime . for higher reynolds numbers differences have been found that are probably caused by problems associated with the pressure boundaries . furthermore , two widely used 3d models , namely d ( <digit> ) q ( <digit> ) and d ( <digit> ) q ( <digit> ) , are analysed . it is shown that the d ( <digit> ) q ( <digit> ) model may induce artificial checkerboard invariants due to the connectivity of the lattice . finally , a new iterative method , which significantly reduces the saturation time , is presented and validated on different benchmark problems . ( c ) <digit> academic press .
partition refinement of component interaction automata . <eos> component interaction automata provide a fitting model to capture and analyze the temporal facets of hierarchically structured component oriented software systems . however , the rules governing composition typically suffer from combinatorial state explosion , an effect that can impede modeling languages , like component interaction automata , from being successful in real world scenarios . we must , therefore , find some appropriate ways to counteract state explosion , one of which is partition refinement through bisimulation , in particular , weak bisimulation . while this technique can yield the desired state space reduction , it does not consider synchronization cliques , that is , groups of states that are interconnected solely by internal synchronization transitions . synchronization cliques give rise to action prefixes , local states that encapsulate preconditions for a component 's ability to interact with the environment . furthermore , both the existence and the size of synchronization cliques can be used as an indicator for the success of partition refinement . in particular , the more frequent synchronization cliques are and the more states they entail , the more likely it is that partition refinement can reduce the state space . but , there may be other factors that impact the refinement process . for this reason , we study , in this paper , how partition refinement behaves under weak bisimulation , how synchronization cliques emerge when using weak bisimulation , how we make state space reduction through partition refinement aware of the existence of synchronization cliques , and what other attributes of component interaction automata specifications can provides us with additional cues to forecast the possible outcome of the partition refinement process . ( c ) <digit> elsevier b.v. all rights reserved .
an on line replication strategy to increase availability in data grids . <eos> data is typically replicated in a data grid to improve the job response time and data availability . strategies for data replication in a data grid have previously been proposed , but they typically assume unlimited storage for replicas . in this paper , we address the system wide data availability problem assuming limited replica storage . we describe two new metrics to evaluate the reliability of the system , and propose an on line optimizer algorithm that can minimize the data missing rate ( mindmr ) in order to maximize the data availability . based on mindmr , we develop four optimizers associated with four different file access prediction functions . simulation results utilizing the optorsim show our mindmr strategies achieve better performance overall than other strategies in terms of the goal of data availability using the two new metrics .
power assignment for k connectivity in wireless ad hoc networks . <eos> the problem min power k connectivity seeks a power assignment to the nodes in a given wireless ad hoc network such that the produced network topology is k connected and the total power is the lowest . in this paper , we present several approximation algorithms for this problem . specifically , we propose a 3k approximation algorithm for any k ge <digit> , a ( k 12h ( k ) ) approximation algorithm for k ( 2k <digit> ) le n where n is the network size , a ( k <digit> ( k <digit> ) <digit> ) approximation algorithm for <digit> le k le <digit> , a <digit> approximation algorithm for k <digit> , and a <digit> approximation algorithm for k <digit> .
comparison of anova f and anom tests with regard to type i error rate and test power . <eos> a monte carlo simulation was conducted to compare the type i error rate and test power of the analysis of means ( anom ) test to the one way analysis of variance f test ( anova f ) . simulation results showed that as long as the homogeneity of the variance assumption was satisfied , regardless of the shape of the distribution , number of group and the combination of observations , both anova f and anom test have displayed similar type i error rates . however , both tests have been negatively affected from the heterogeneity of the variances . this case became more obvious when the variance ratios increased . the test power values of both tests changed with respect to the effect size ( ) , variance ratio and sample size combinations . as long as the variances are homogeneous , anova f and anom test have similar powers except unbalanced cases . under unbalanced conditions , the anova f was observed to be powerful than the anom test . on the other hand , an increase in total number of observations caused the power values of anova f and anom test approach to each other . the relations between effect size ( ) and the variance ratios affected the test power , especially when the sample sizes are not equal . as anova f has become to be superior in some of the experimental conditions being considered , anom is superior in the others . however , generally , when the populations with large mean have larger variances as well , anom test has been seen to be superior . on the other hand , when the populations with large mean have small variances , generally , anova f has observed to be superior . the situation became clearer when the number of the groups is <digit> or <digit> .
on mining multi time interval sequential patterns . <eos> sequential pattern mining is essential in many applications , including computational biology , consumer behavior analysis , web log analysis , etc. although sequential patterns can tell us what items are frequently to be purchased together and in what order , they can not provide information about the time span between items for decision support . previous studies dealing with this problem either set time constraints to restrict the patterns discovered or define time intervals between two successive items to provide time information . accordingly , the first approach falls short in providing clear time interval information while the second can not discover time interval information between two non successive items in a sequential pattern . to provide more time related knowledge , we define a new variant of time interval sequential patterns , called multi time interval sequential patterns , which can reveal the time intervals between all pairs of items in a pattern . accordingly , we develop two efficient algorithms , called the mi apriori and mi prefixspan algorithms , to solve this problem . the experimental results show that the mi prefixspan algorithm is faster than the mi apriori algorithm , but the mi apriori algorithm has better scalability in long sequence data .
fpga implementation of a wavelet neural network with particle swarm optimization learning . <eos> this paper introduces implementation of a wavelet neural network ( wnn ) with learning ability on field programmable gate array ( fpga ) . a learning algorithm using gradient descent method is not easy to implement in an electronic circuit and has local minimum . a more suitable method is the particle swarm optimization ( pso ) that is a population based optimization algorithm . the pso is similar to the ga , but it has no evolution operators such as crossover and mutation . in the approximation of a nonlinear activation function , we use a taylor series and a look up table ( lut ) to achieve a more accurate approximation . the results of the two experiments demonstrate the successful hardware implementation of the wavelet neural networks with the pso algorithm using fpga . from the results of the experiment , it can be seen that the performance of the pso is better than that of the simultaneous perturbation algorithm at sufficient particle sizes . ( c ) <digit> elsevier ltd. all rights reserved .
interface synthesis for heterogeneous multi core systems from transaction level models . <eos> this paper presents a tool for automatic synthesis of rtl interfaces for heterogeneous mpsoc from transaction level models ( tlms ) . the tool captures the communication parameters in the platform and generates interface modules called universal bridges between buses in the design . the design and configuration of the bridges depend on several platform components including heterogeneity of the components , traffic on the bus , size of messages and so on . we define these parameters and show how the synthesizable rtl code for the bridge can be automatically derived based on these parameters . we use industrial strength design drivers such as an mp3 decoder to test our automatically generated bridges for a variety of platforms and compare them to manually designed bridges on different quality metrics . our experimental results show that performance of automatically generated bridges are within <digit> % of manual design for simple platforms but surpasses them for more complex platforms . the area and rtl code size is consistently better than manual design while giving <digit> orders of improvement in development time .
efficient implementations of construction heuristics for the rectilinear block packing problem . <eos> the rectilinear block packing problem is a problem of packing a set of rectilinear blocks into a larger rectangular container , where a rectilinear block is a polygonal block whose interior angle is either <digit> or <digit> . there exist many applications of this problem , such as vlsi design , timber glass cutting , and newspaper layout . in this paper , we design efficient implementations of two construction heuristics for rectilinear block packing . the proposed algorithms are tested on a series of instances , which are generated from nine benchmark instances . the computational results show that the proposed algorithms are especially efficient for large instances with repeated shapes .
the effect of finite lattice size in lattice boltzmann model . <eos> in this paper , numerical results on two dimensional vapor liquid equilibrium calculated by lattice boltzmann method have been presented . artefacts resulted by the finite lattice size have been reviewed . a set of criteria for minimal lattice size to avoid lattice artefacts is given .
estimating vignetting function from a single image for image authentication . <eos> vignetting is the phenomenon of reduced brightness in an image at the peripheral region compared to the central region . as patterns of vignetting are characteristics of lens models , they can be used to authenticate digital images for forensic analysis . in this paper , we describe a new method for model based single image vignetting estimation and correction . we use the statistical properties of natural images in the discrete derivative domains and formulate the vignetting estimation problem as a maximum likelihood estimation . we further provide a simple and efficient procedure for better initialization of the numerical optimization . empirical evaluations of the proposed method using synthesized and real vignetted images show significant gain in both performance and running efficiency in correcting vignetting from digital images , and the estimated vignetting functions are shown to be effective in classifying different lens models .
effective utility mining with the measure of average utility . <eos> frequent itemset mining only considers the frequency of occurrence of the items but does not reflect any other factors , such as price or profit . utility mining is an extension of frequent itemset mining , considering cost , profit or other measures from user preference . traditionally , the utility of an itemset is the summation of the utilities of the itemset in all the transactions regardless of its length . the average utility measure is thus adopted in this paper to reveal a better utility effect of combining several items than the original utility measure . it is defined as the total utility of an itemset divided by its number of items within it . the average utility itemsets , as well as the original utility itemsets , does not have the downward closure property . a mining algorithm is then proposed to efficiently find the high average utility itemsets . it uses the summation of the maximal utility among the items in each transaction with the target itemset as the upper bound to overestimate the actual average utilities of the itemset and processes it in two phases . as expected , the mined high average utility itemsets in the proposed way will be fewer than the high utility itemsets under the same threshold . the proposed approach can thus be executed under a larger threshold than the original , thus with a more significant and relevant criterion . experimental results also show the performance of the proposed algorithm . ( c ) <digit> elsevier ltd. all rights reserved .
surgical workflow management schemata for cataract procedures process model based design and validation of workflow schemata . <eos> objective workflow guidance of surgical activities is a challenging task . because of variations in patient properties and applied surgical techniques , surgical processes have a high variability . the objective of this study was the design and implementation of a surgical workflow management system ( swfms ) that can provide a robust guidance for surgical activities . we investigated how many surgical process models are needed to develop a swfms that can guide cataract surgeries robustly . methods we used <digit> cases of cataract surgeries and acquired patient individual surgical process models ( ispms ) from them . of these , randomized subsets ispms were selected as learning sets to create a generic surgical process model ( gspm ) . these gspms were mapped onto workflow nets as workflow schemata to define the behavior of the swfms . finally , <digit> ispms from the disjoint set were simulated to validate the workflow schema for the surgical processes . the measurement was the successful guidance of an ispm . results we demonstrated that a swfms with a workflow schema that was generated from a subset of <digit> ispms is sufficient to guide approximately <digit> % of all surgical processes in the total set , and that a subset of <digit> ispms is sufficient to guide approx. <digit> % of all processes . conclusion we designed a swfms that is able to guide surgical activities on a detailed level . the study demonstrated that the high inter patient variability of surgical processes can be considered by our approach .
temspol a matlab thermal model for deep subduction zones including major phase transformations . <eos> temspol is an open matlab code suitable for calculating temperature and lateral anomaly of density distributions in deep subduction zones , taking into account the olivine to spinel phase transformation in a self consistent manner . the code solves , by means of a finite difference scheme , the heat transfer equation including adiabatic heating , radioactive heat generation , latent heat associated with phase changes and frictional heating . we show , with a few simulations , that temspol can be a useful tool for researchers studying seismic velocity , stress and seismicity distribution in deep subduction zones . deep earthquakes in subducting slabs are thought to be caused by shear instabilities associated with the olivine to spinel phase transition in metastable olivine wedges . we investigate the kinematic and thermal conditions of the subducting plate that lead to the formation of metastable olivine wedges . moreover , temspol calculates lateral anomalies of density within subducting slabs , which can be used to evaluate buoyancy forces that determine the dynamics of subduction and the stress distribution within the slab . we use temspol to evaluate the effects of heat sources such as shear heating and latent heat release , which are neglected in commonly used thermal models of subduction . we show that neglecting these heat sources can lead to significant overestimation of the depth reached by the metastable olivine wedge .
online social advertising via influential endorsers . <eos> in recent years , many web based services such as facebook and myspace have been making great progress and creating new opportunities . because online advertising is the main business model for social networking sites , in this paper we propose a social endorser based advertising system formulated on network influence and user preference analyses . by utilizing the social network and user preference analysis techniques , the theories of dynamic social influence and celebrity endorsement are realized in the proposed advertising approach . experiments show that our mechanism significantly improves advertising effectiveness and efficiency and outperforms other advertising approaches .
similarity measures between type <digit> fuzzy sets . <eos> in this paper , we give similarity measures between type <digit> fuzzy sets and provide the axiom definition and properties of these measures . for practical use , we show how to compute the similarities between gaussian type <digit> fuzzy sets . yang and shih 's <digit> algorithm , a clustering method based on fuzzy relations by beginning with a similarity matrix , is applied to these gaussian type <digit> fuzzy sets by beginning with these similarities . the clustering results are reasonable consisting of a hierarchical tree according to different levels .
a mechanization of unity in pc nqthm <digit> . <eos> this paper presents in detail how the unity logic for reasoning about concurrent programs was formalized within the mechanized theorem prover pc nqthm <digit> . most of unity 's proof rules were formalized in the unquantified logic of nqthm , and the proof system has been used to mechanically verify several concurrent programs . the mechanized proof system is sound by construction , since unity 's proof rules were proved about an operational semantics of concurrency , also presented here . skolem functions are used instead of quantifiers , and the paper describes how proof rules containing skolem function are used instead of unity 's quantified proof rules when verifying concurrent programs . this formalization includes several natural extensions to unity , including nondeterministic statements . the paper concludes with a discussion of the cost and value of mechanization .
reconciling while tolerating disagreement in collaborative data sharing . <eos> in many data sharing settings , such as within the biological and biomedical communities , global data consistency is not always attainable different sites ' data may be dirty , uncertain , or even controversial . collaborators are willing to share their data , and in many cases they also want to selectively import data from others but must occasionally diverge when they disagree about uncertain or controversial facts or values . for this reason , traditional data sharing and data integration approaches are not applicable , since they require a globally consistent data instance . additionally , many of these approaches do not allow participants to make updates if they do , concurrency control algorithms or inconsistency repair techniques must be used to ensure a consistent view of the data for all users.in this paper , we develop and present a fully decentralized model of collaborative data sharing , in which participants publish their data on an ad hoc basis and simultaneously reconcile updates with those published by others . individual updates are associated with provenance information , and each participant accepts only updates with a sufficient authority ranking , meaning that each participant may have a different ( though conceptually overlapping ) data instance . we define a consistency semantics for database instances under this model of disagreement , present algorithms that perform reconciliation for distributed clusters of participants , and demonstrate their ability to handle typical update and conflict loads in settings involving the sharing of curated data .
application of evolutionary strategies for 3d graphical model categorization and retrieval . <eos> in multimedia information processing , while the previous focus was on image video retrieval , content based categorization and retrieval of 3d computer graphics model is becoming increasingly important . this is due to the increased adoption of 3d graphics representations in multimedia applications and the resulting need for rapid virtual scene assembly from a repository of 3d models . motivated by these requirements , the main focus of this paper is on the content based classification and retrieval of 3d computer graphics models based on a histogram feature representation , and the search for an adaptive transformation of this representation such that the resulting classification and retrieval accuracies are optimized . observing that a histogram is basically an approximation of the probability density function of an underlying random variable , and that a suitable transformation , when applied to the random variable , will allow the classifier to attain better accuracy based on this new representation , we propose an evolutionary optimization approach to search for this set of optimal transformations due to the large size of the search space . in particular , we consider the special class of transformations that take the form of a piecewise continuous mapping . in this case , the transformed variable is a mixed random variable , with both discrete and continuous components , which provides added flexibility for modeling a number of more diverse random variable types . with a suitably defined fitness function for evolutionary strategies ( es ) that measures the capability of the transformed histogram representation to induce the correct class structure , our proposed approach is capable of improving the head model classification performance , which in turn allows , in the case of content based retrieval , the correct preassignment of a query object to its correct class for more efficient search , even in those cases where the query is ambiguous and difficult to characterize .
face recognition based on a novel linear discriminant criterion . <eos> as an effective technique for feature extraction and pattern classification fisher linear discriminant ( fld ) has been successfully applied in many fields . however , for a task with very high dimensional data such as face images , conventional fld technique encounters a fundamental difficulty caused by singular within class scatter matrix . to avoid the trouble , many improvements on the feature extraction aspect of fld have been proposed . in contrast , studies on the pattern classification aspect of fld are quiet few . in this paper , we will focus our attention on the possible improvement on the pattern classification aspect of fld by presenting a novel linear discriminant criterion called maximum scatter difference ( msd ) . theoretical analysis demonstrates that msd criterion is a generalization of fisher discriminant criterion , and is the asymptotic form of discriminant criterion large margin linear projection . the performance of msd classifier is tested in face recognition . experiments performed on the orl , yale , feret and ar databases show that msd classifier can compete with top performance linear classifiers such as linear support vector machines , and is better than or equivalent to combinations of well known facial feature extraction methods , such as eigenfaces , fisherfaces , orthogonal complementary space , nullspace , direct linear discriminant analysis , and the nearest neighbor classifier .
multi channel sampling on shift invariant spaces with frame generators . <eos> let phi be a continuous function in l <digit> ( r ) such that the sequence phi ( t n ) ( n is an element of z ) is a frame sequence in l <digit> ( r ) and assume that the shift invariant space v ( phi ) generated by phi has a multi banded spectrum sigma ( v ) . the main aim in this paper is to derive a multi channel sampling theory for the shift invariant space v ( phi ) . by using a type of fourier duality between the spaces v ( phi ) and l <digit> <digit> , <digit> pi we find necessary and sufficient conditions allowing us to obtain stable multi channel sampling expansions in v ( phi ) .
focus of b to b e commerce initiatives and related benefits in manufacturing small and medium sized enterprises . <eos> empirical research into business to business e commerce issues involving manufacturing small and medium sized enterprises ( smes ) is still embryonic . in an attempt to partially fill this gap , this paper presents empirical data from an electronic survey conducted among <digit> manufacturing smes to investigate e commerce initiatives and their related benefits . e commerce initiatives are assessed using a set of <digit> business processes that can be conducted electronically . these processes were classified according to their focus customer ( downstream ) , supplier ( upstream ) or in house . the research findings point to four main profiles of manufacturing smes with different e commerce focuses . the first group seems to lack any focus or may still be exploring e commerce opportunities . the second and third groups are supplier and customer focused , respectively . the fourth group consists of the more involved smes that have leveraged their e commerce initiatives with both their customers and their suppliers . results also suggest the existence of a close alignment between e commerce focus and related benefits .
incomplete information based decentralized cooperative control strategy for distributed energy resources of vsi based microgrids . <eos> this paper presents an effective method to control distributed energy resources ( ders ) installed in a microgrid ( mg ) to guarantee its stability after islanding occurrence . considering voltage and frequency variations after islanding occurrence and based on stability criteria , mg pre islanding conditions are divided into secure and insecure classes . it is shown that insecure mg can become secure , if appropriate preventive control is applied on the ders in different operating conditions of the mg . to select the most important variables of mg , which can estimate proper values of output power set points of ders , a feature selection procedure known as symmetrical uncertainty is used in this paper . among all the mg variables , critical ones are selected to calculate the appropriate output power of different ders for different conditions of the mg . the values of selected features are transmitted by the communication system to the control unit installed on each der to control its output power set point . in order to decrease the communication system cost , previous researchers have used local variables to control the set point of different ders . this approach decreases the accuracy of the controller because the controller uses incomplete information . in this paper , multi objective approach is used in order to decrease the cost of the communication system , while keeping the accuracy of the preventive control strategy in an allowable margin . the results demonstrate the effectiveness of the proposed method in comparison with other methods .
modeling and reasoning with paraconsistent rough sets . <eos> we present a language for defining paraconsistent rough sets and reasoning about them . our framework relates and brings together two major fields rough sets <digit> and paraconsistent logic programming <digit> . to model inconsistent and incomplete information we use a four valued logic . the language discussed in this paper is based on ideas of our previous work <digit> , <digit> , <digit> developing a four valued framework for rough sets . in this approach membership function , set containment and set operations are four valued , where logical values are t ( true ) , f ( false ) , i ( inconsistent ) and u ( unknown ) . we investigate properties of paraconsistent rough sets as well as develop a paraconsistent rule language , providing basic computational machinery for our approach .
a dynamical tikhonov regularization for solving ill posed linear algebraic systems . <eos> the tikhonov method is a famous technique for regularizing ill posed linear problems , wherein a regularization parameter needs to be determined . this article , based on an invariant manifold method , presents an adaptive tikhonov method to solve ill posed linear algebraic problems . the new method consists in building a numerical minimizing vector sequence that remains on an invariant manifold , and then the tikhonov parameter can be optimally computed at each iteration by minimizing a proper merit function . in the optimal vector method ( ovm ) three concepts of optimal vector , slow manifold and hopf bifurcation are introduced . numerical illustrations on well known ill posed linear problems point out the computational efficiency and accuracy of the present ovm as compared with classical ones .
mapping transit based access integrating gis , routes and schedules . <eos> accessibility is a concept that is not entirely easy to define . gould ( <digit> ) once stated that it is a ' slippery notion ... one of those common terms that everyone uses until faced with the problem of defining and measuring it ' . considerable research over the last <digit> years has been devoted to defining and measuring accessibility , ranging from access to jobs within an hour 's travel time to the ease at which given places can be reached . this article is concerned with the measurement of access provided by transit . it includes a review of past work on measuring accessibility in general and with respect to transit services in particular . from this overview of the literature , it can be seen that current methods fall short in measuring transit service access in several meaningful aspects . based on this review and critique , we propose new refinements that can be used to help overcome some of these shortcomings . as a part of this , we define an extended gis data structure to handle temporal elements of transit service . to demonstrate the value of these new measures , examples are presented with respect to mapping accessibility of transit services in santa barbara , california . finally , we show how these measures can be used to develop a framework for supporting transit service analysis and planning .
the complexity of compressing subsegments of images described by finite automata . <eos> we investigate how the compression size of the compressed version of a two dimensional image changes when we cut off a part of it , e.g. extract a photo of one person from a photo of a group of people , when compression is considered in terms of finite automata . denote by c ( t ) the compression size of a square image t in terms of deterministic automata , it is the smallest size of a deterministic acyclic automaton a describing t. the corresponding alphabet of a has only four letters , corresponding to four quadrants . we consider an independent useful combinatorial interpretation of c ( t ) in terms of regular subsquares of t. denote by ( n ) the largest compression size c ( r ) of a square subsegment r of the image t such that c ( t ) n. we show that there is a constant c > <digit> such that we also show how to construct efficiently ( in linear time w.r.t. the total size of the input and the produced output ) the compressed representation of subsegments given the compressed representation of the whole image .
accomplishing universal access through system reachabilitya management perspective . <eos> the aim of this paper is to describe the need of a method by which we can estimate the return on accessibility investments in information technology ( it ) systems . this paper reveals some of the reasons why accessibility still is a secondhand criterion when designing digital services . it also describes the authors experiences regarding the concept of accessibility and how it must develop in order to obtain the status of a basic business criterion for the benefit of disabled people who are currently excluded from public services and labour markets . the paper also questions the need of a separate accessibility standard . additionally , we discuss some of the hindering in the market and limiting perspectives that are blocking further development . one of the problems in the market seems to be that accessibility as a concept has been more of an issue about creating equal opportunities and therefore probably does not have the quality of a business criterion . in order to bridge that gap , we argue for replacing accessibility with reachability , which is a concept based on a measure used by media when estimating the reached percentage of a population or target group .
pothmf a program for computing potential curves and matrix elements of the coupled adiabatic radial equations for a hydrogen like atom in a homogeneous magnetic field . <eos> a fortran <digit> program is presented which calculates with the relative machine precision potential curves and matrix elements of the coupled adiabatic radial equations for a hydrogen like atom in a homogeneous magnetic field . the potential curves are eigenvalues corresponding to the angular oblate spheroidal functions that compose adiabatic basis which depends on the radial variable as a parameter . the matrix elements of radial coupling are integrals in angular variables of the following two types product of angular functions and the first derivative of angular functions in parameter , and product of the first derivatives of angular functions in parameter , respectively . the program calculates also the angular part of the dipole transition matrix elements ( in the length form ) expressed as integrals in angular variables involving product of a dipole operator and angular functions . moreover , the program calculates asymptotic regular and irregular matrix solutions of the coupled adiabatic radial equations at the end of interval in radial variable needed for solving a multi channel scattering problem by the generalized r matrix method . potential curves and radial matrix elements computed by the pothmf program can be used for solving the bound state and multichannel scattering problems . as a test desk , the program is applied to the calculation of the energy values , a short range reaction matrix and corresponding wave functions with the help of the kantbp program . benchmark calculations for the known photoionization cross sections are presented .
completely lazy learning . <eos> local classifiers are sometimes called lazy learners because they do not train a classifier until presented with a test sample . however , such methods are generally not completely lazy because the neighborhood size k ( or other locality parameter ) is usually chosen by cross validation on the training set , which can require significant preprocessing and risks overfitting . we propose a simple alternative to cross validation of the neighborhood size that requires no preprocessing instead of committing to one neighborhood size , average the discriminants for multiple neighborhoods . we show that this forms an expected estimated posterior that minimizes the expected bregman loss with respect to the uncertainty about the neighborhood choice . we analyze this approach for six standard and state of the art local classifiers , including discriminative adaptive metric knn ( dann ) , a local support vector machine ( svm knn ) , hyperplane distance nearest neighbor ( hknn ) , and a new local bayesian quadratic discriminant analysis ( local bda ) . the empirical effectiveness of this technique versus cross validation is confirmed with experiments on seven benchmark data sets , showing that similar classification performance can be attained without any training .
a tabu search approach for scheduling hazmat shipments . <eos> vehicle routing and scheduling are two main issues in the hazardous material ( hazmat ) transportation problem . in this paper , we study the problem of managing a set of hazmat transportation requests in terms of hazmat shipment route selection and actual departure time definition . for each hazmat shipment , a set of minimum and equitable risk alternative routes from origin to destination points and a preferred departure time are given . the aim is to assign a route to each hazmat shipment and schedule these shipments on the assigned routes in order to minimize the total shipment delay , while equitably spreading the risk spatially and preventing the risk induced by vehicles traveling too close to each other . we model this hazmat shipment scheduling problem as a job shop scheduling problem with alternative routes . no wait constraints arise in the scheduling model as well , since , supposing that no safe area is available , when a hazmat vehicle starts traveling from the given origin it can not stop until it arrives at the given destination . a tabu search algorithm is proposed for the problem , which is experimentally evaluated on a set of realistic test problems over a regional area , evaluating the provided solutions also with respect to the total route risk and length .
a comparative study of direct forcing immersed boundary lattice boltzmann methods for stationary complex boundaries . <eos> in this study , we assess several interface schemes for stationary complex boundary flows under the direct forcing immersed boundary lattice boltzmann methods ( ib lbm ) based on a split forcing lattice boltzmann equation ( lbe ) . our strategy is to couple various interface schemes , which were adopted in the previous direct forcing immersed boundary methods ( ibm ) , with the split forcing lbe , which enables us to directly use the direct forcing concept in the lattice boltzmann calculation algorithm with a second order accuracy without involving the navier stokes equation . in this study , we investigate not only common diffuse interface schemes but also a sharp interface scheme . for the diffuse interface scheme , we consider explicit and implicit interface schemes . in the calculation of velocity interpolation and force distribution , we use the <digit> and <digit> point discrete delta functions , which give the second order approximation . for the sharp interface scheme , we deal with the exterior sharp interface scheme , where we impose the force density on exterior ( solid ) nodes nearest to the boundary . all tested schemes show a second order overall accuracy when the simulation results of the taylor green decaying vortex are compared with the analytical solutions . it is also confirmed that for stationary complex boundary flows , the sharper the interface scheme , the more accurate the results are . in the simulation of flows past a circular cylinder , the results from each interface scheme are comparable to those from other corresponding numerical schemes . copyright ( c ) <digit> john wiley sons , ltd .
precise euclidean distance transforms in 3d from voxel coverage representation . <eos> we propose a method for computing euclidean distance transform ( edt ) in 3d images . the method utilizes voxel coverage information to increase precision of edt . the method can be used with any vector propagation based edt in 3d . synthetic tests confirm significant improvement in achieved precision . both the related binary and the existing coverage based methods are outperformed .
can we trust digital image forensics . <eos> compared to the prominent role digital images play in nowadays multimedia society , research in the field of image authenticity is still in its infancy . only recently , research on digital image forensics has gained attention by addressing tamper detection and image source identification . however , most publications in this emerging field still lack rigorous discussions of robustness against strategic counterfeiters , who anticipate the existence of forensic techniques . as a result , the question of trustworthiness of digital image forensics arises . this work will take a closer look at two state of the art forensic methods and proposes two counter techniques one to perform resampling operations undetectably and another one to forge traces of image origin . implications for future image forensic systems will be discussed .
max optimal and sum optimal labelings of graphs . <eos> given a graph g , a function f v ( g ) > <digit> , <digit> , ... , k is a k ranking of g if f ( u ) f ( v ) implies that every u v path contains a vertex w such that f ( w ) > f ( u ) . a k ranking is minimal if the reduction of any label greater than <digit> violates the described ranking property . we consider two norms for minimal rankings . the max optimal norm parallel to f ( g ) parallel to ( infinity ) is the smallest k for which g has a minimal k ranking . this value is also referred to as the rank number chi ( r ) ( g ) . in this paper we introduce the sum optimal norm parallel to f ( g ) parallel to ( <digit> ) which is the minimum sum of all labels over all minimal rankings . we investigate similarities and differences between the two norms . in particular we show rankings for paths and cycles that are sum optimal are also max optimal . ( c ) <digit> elsevier b.v. all rights reserved .
<digit> ( ( v , k ,1 ) ) designs with a point primitive rank <digit> automorphism group of affine type . <eos> <digit> ( ( v , k ,1 ) ) designs with a point primitive rank <digit> automorphism group of affine type are investigated and several new examples are provided .
increase in the releasable pool of synaptic vesicles underlies facilitation . <eos> facilitation is the ability of presynaptic terminals to release neurotransmitter more efficiently following repetitive stimulation . we demonstrated that facilitation can be explained by ca2 dependent vesicles priming and the increase in the number of synaptic vesicles activated for release . employing the model with two ca2 sensors , we computed ca2 concentration at the sites of priming and release , the size of the releasable pool of vesicles , and the rate of transmitter release during repetitive nerve stimulation . the calculated rates of vesicle release and the increase in the releasable pool during facilitation were in agreement with the results of electrophysiology experiments .
viscoelastic fracture of multiple cracks in functionally graded materials . <eos> in this paper , the viscoelastic fracture of multiple cracks in a functionally graded strip is studied . the solution of linear elastic crack tip field is investigated at first , using the finite element method . both applied stress load and applied strain load are taken into account . the effects of the crack length , crack spacing , material gradient index and the loading condition on the crack tip field intensity factor are plotted and discussed . according to the correspondence principle , the viscoelastic crack tip field under applied strain is obtained from the linear elastic results . variation of stress intensity factor of the viscoelastic functionally graded strip is analyzed . some useful information for the design of functionally graded materials is provided .
power distribution system optimization by an algorithm for capacitated steiner tree problems with complex flows and arbitrary cost functions . <eos> an algorithm called genetic shortest path algorithm is presented to solve capacitated minimal steiner tree problems in graphs with complex flows and arbitrary arc cost functions , but without negative cycles . voltage constraint can also been taken into consideration by the algorithm . hence , it can solve various power distribution system optimization problems with detailed mathematical models . in the proposed algorithm , a local optimization method based on shortest path algorithm and heuristics is used to find the local optimums , in which the minimum cost objective and all constraints are considered and the specialties of the problems are made good use of . genetic operations are only used to search the global optimum from the local optimums . therefore , this algorithm overcomes the disadvantage of general genetic algorithm in local searching . an example for distribution system planning problem with large scale is given to demonstrate the power of the algorithm .
qmbr ( i ) inverse quantization of minimum bounding rectangles for spatial data compression . <eos> in this paper , we propose qmbr ( i ) , the inverse representation of the quantized minimum bounding rectangles ( mbrs ) scheme , which compresses a minimum bounding rectangle key into one byte for spatial data compression . qmbr ( i ) is a novel spatial data compression scheme that is based on inverse quantization and overcomes the shortcomings of conventional relative coordination or quantization schemes . if a spatial data is far from the starting point of the search region , the relative coordination scheme does not guarantee compression . in a quantization scheme , since the mbrs are expanded , the overlapping of mbrs is increased and the search performance is reduced . the proposed scheme overcomes these shortcomings , and simulation results suggest that it performs better than other schemes .
two dimensional model of base force element method ( bfem ) on complementary energy principle for geometrically nonlinear problems . <eos> based on the concept of the base forces by gao , a new finite element methodthe base force element method ( bfem ) on complementary energy principle for two dimensional geometrically nonlinear problems is presented using arbitrary meshes . an arbitrary convex polygonal element model of the bfem for geometrically nonlinear problem is derived by assuming that the stress is uniformly distributed on each edges of a plane element . the explicit formulations of the control equations for the bfem are derived using the modified complementary energy principle . the bfem is naturally universal for small displacement and large displacement problems . a number of example problems are solved using the bfem and the results are compared with corresponding analytical solutions . a good agreement of the results using the arbitrary convex polygonal element model of bfem in the large displacement and large rotation calculations , are observed .
theoretical study of xn5 ( ) ( x o , s , se , te ) systems . <eos> a series of xn5 ( ) ( x o , s , se , te ) compounds has been examined with ab initio and density functional theory ( dft ) methods . the five membered nitrogen ring series of structures are global minima and may exist or be characterized due to their significant dissociation barriers ( 29.7 32.7 kcal mol ( <digit> ) ) . nucleus independent chemical shifts ( nics ) criteria and the presence of ( 4n <digit> ) pi electrons confirmed that the five membered nitrogen ring in their structures exhibits characteristics of aromaticity . thus , the strong stability of the five membered nitrogen ring structures may be attributed partially to their aromaticity .
incremental fault diagnosis . <eos> fault diagnosis is important in improving the circuit design process and the manufacturing yield . diagnosis of today 's complex defects is a challenging problem due to the explosion of the underlying solution space with the increasing number of fault locations and fault models . to tackle this complexity , an incremental diagnosis method is proposed . this method captures faulty lines one at a time using the novel linear time single fault diagnosis algorithms . to capture complex fault effects , a model free incremental diagnosis algorithm is outlined , which alleviates the need for an explicit fault model . to demonstrate the applicability of the proposed method , experiments on multiple stuck at faults , open interconnects and bridging faults are performed . extensive results on combinational and full scan sequential benchmark circuits confirm its resolution and performance .
dim , a portable , light weight package for information publishing , data transfer and inter process communication . <eos> the real time systems of hep experiments are presently highly distributed , possibly on heterogeneous cpus . in many applications , there is an important need to make information available to a large number of other processes in a transparent way . for this purpose the rpc like systems are not suitable , since most of them rely on polling from the client and one to one connections . dim is a very powerful alternative to those systems . it provides a named space for processes to publish information ( publishers ) and a very simple api for processes willing to use this information ( subscribers ) . it fully handles error recovery at the publisher and subscriber level , without additional software in the application , dim is available on a large variety of platforms and operating systems with c and c bindings . it is presently used in several hep experiments , while it was developed in the delphi experiment and is maintained at cern . we shall present its capabilities and examples of its use in hep experiments in domains ranging from simple data publishing to event transfer , process control or communication layer for an experiment control package ( smi ) . we shall also present prospectives for using it as communications layer for future experiment 's control systems . ( c ) <digit> elsevier science b.v. all rights reserved .
roswel workflow language a declarative , resource oriented approach . <eos> well defined business processes are a crucial success factor for deploying soa soku architectures . in this paper , the declarative business process description language roswel which supports applications compatible with roa , is discussed . roswel provides a declarative , reliable and semi automatic composition of restful web services , enriched by the knowledge representation . the paper discusses benefits of roswel , and presents an example of a simple workow that captures essential roswel features .
the ( <digit> <digit> ) dimensional burgers equation and its comparative solutions . <eos> in this paper , we will carry out an analytic comparative study between the adomian decomposition method and the differential transformation method . this is achieved by handling the ( <digit> <digit> ) dimensional burgers equation . two numerical simulations have also been carried out to validate and demonstrate efficiency of the two methods . ( c ) <digit> elsevier ltd. all rights reserved .
modeling the development of goal specificity in mirror neurons . <eos> neurophysiological studies have shown that parietal mirror neurons encode not only actions but also the goal of these actions . although some mirror neurons will fire whenever a certain action is perceived ( goal independently ) , most will only fire if the motion is perceived as part of an action with a specific goal . this result is important for the action understanding hypothesis as it provides a potential neurological basis for such a cognitive ability . it is also relevant for the design of artificial cognitive systems , in particular robotic systems that rely on computational models of the mirror system in their interaction with other agents . yet , to date , no computational model has explicitly addressed the mechanisms that give rise to both goal specific and goal independent parietal mirror neurons . in the present paper , we present a computational model based on a self organizing map , which receives artificial inputs representing information about both the observed or executed actions and the context in which they were executed . we show that the map develops a biologically plausible organization in which goal specific mirror neurons emerge . we further show that the fundamental cause for both the appearance and the number of goal specific neurons can be found in geometric relationships between the different inputs to the map . the results are important to the action understanding hypothesis as they provide a mechanism for the emergence of goal specific parietal mirror neurons and lead to a number of predictions ( <digit> ) learning of new goals may mostly reassign existing goal specific neurons rather than recruit new ones ( <digit> ) input differences between executed and observed actions can explain observed corresponding differences in the number of goal specific neurons and ( <digit> ) the percentage of goal specific neurons may differ between motion primitives .
a framework for designing and implementing the ada standard container library . <eos> an open issue of the ada language is the definition of a standard container library . containers in this library ( e.g. , sets , maps and lists ) shall offer some core functionalities that characterise their behaviour ( i.e. , different strategies for managing the elements stored therein ) as well as other general functionalities . among these general functionalities , we are interested in alternative ways for accessing the containers , namely direct access by position and traversals using iterators . in this paper , we present the shortcut based framework ( sbf ) , a framework aimed at providing suitable , uniform , accurate and secure access by position and iterators , while keeping other nice properties such as comprehensibility and changeability . the sbf should be considered as a baseline upon which the ada standard container library can be built . we assess the feasibility of our proposal defining a quality model for container libraries and evaluating the sbf using some metrics defined with the goal question metric approach .
stability of block lu factorization for block tridiagonal matrices . <eos> it is showed that if a is i block diagonally dominant ( ii block diagonally dominant ) , then the reduced matrix s preserves the same property . we also give a sufficient condition for the reduced matrix s also to be a block h matrix when a is a block h matrix , and some properties on the comparison matrices mu ( i ) ( a ( k ) ) , mu ( ii ) ( a ( k ) ) , mu ( i ) ( l ) , mu ( i ) ( u ) are obtained . finally , error analysis of block lu factroization for block tridiagonal matrix is presented . ( c ) <digit> elsevier ltd. all rights reserved .
russian dutch double degree masters programme in computational science in the age of global education . <eos> we present a new double degree graduate ( masters ) programme in computational science launched in <digit> by the itmo university , russia and university of amsterdam , the netherlands . we discuss the global aspects of integration of different educational systems and list some funding opportunities . we describe our double degree program curriculum , suggest the timeline of enrollment and studies , and give some examples of student research topics . finally , we discuss the issues of joint programs with russia and suggest possible solutions , analyze the results of the first three student intakes and reflect on the lessons learnt , and share our thoughts and experiences that could be of interest to the international community expanding the educational markets to the vast countries like russia , china or india . the paper is written for education professionals and contains useful information for potential students .
techno economic analysis of epon and wimax for future fiber wireless ( fiwi ) networks . <eos> hybrid fiber wireless ( fiwi ) networks become rapidly mature and represent a promising candidate for reducing power consumption , costs , and bandwidth bottlenecks of next generation broadband access networks . two key fiwi technologies with similar design goals are ethernet passive optical network ( epon ) and wimax . in this paper , we develop a powerful and flexible techno economic analysis to compare the two technologies , taking into account not only equipment and installation costs but also oam related costs such as power consumption and repairing costs for a wide range of different network failure scenarios , terrain types , and wireless channel conditions . the presented results give insight into the cost performance trade offs of current and next generation epon and wimax networks .
insight into goal directed movement strategies . <eos> the current paper proposes a novel method of analyzing goal directed movements by dividing them into distinct movement intervals . we demonstrate how the description of the first and second most prominent movement intervals in terms of duration and length can provide insight into the applied movement strategies under different conditions . this method , although demonstrated for goal directed movements , has the potential to be generalized to other types of movements , such as steering movements .
mobius invariant curve and surface energies and their applications . <eos> curvature based surface energies are frequently used in mathematics , physics , thin plate and shell engineering , and membrane chemistry and biology studies . invariance under rotations and shifts makes curvature based energies very attractive for modeling various phenomena . in computer aided geometric design , the willmore surfaces and the so called minimum variation surfaces ( mvs ) are widely used for shape modeling purposes . the willmore surfaces are invariant w.r.t conformal transformations ( mobius or conformal invariance ) , and studied thoroughly in differential geometry and related disciplines . in contrast , the minimum variation surfaces are not conformal invariant . in this paper , we suggest a simple modification of the minimum variation energy and demonstrate that the resulting modified mvs enjoy mobius invariance ( so we call them conformal invariant mvs or , shortly , ci mvs ) . we also study connections of ci mvs with the cyclides of dupin . in addition , we consider several other conformal invariant curve and surface energies involving curvatures and curvature derivatives . in particular , we show how filtering with a conformal invariant curve energy can be used for detecting salient subsets of the principal curvature extremum curves used by hosaka and co workers for shape quality inspection purposes .
calculation of delay characteristics for multiserver queues with constant service times . <eos> we consider a discrete time infinite capacity queueing system with a general uncorrelated arrival process , constant length service times of multiple slots , multiple servers and a first come first served queueing discipline . under the assumption that the queueing system can reach a steady state , we first establish a relationship between the steady state probability distributions of the system content and the customer delay . next , by means of this relationship , an explicit expression for the probability generating function of the customer delay is obtained from the known generating function of the system content , derived in previous work . in addition , several characteristics of the customer delay , namely the mean value , the variance and the tail distribution of the delay , are derived through some mathematical manipulations . the analysis is illustrated by means of some numerical examples .
converting computer integrated manufacturing into an intelligent information system by combining cim with concurrent engineering and knowledge management . <eos> some industrial organizations using computer integrated manufacturing ( cim ) for managing intelligent product and process data during a concurrent processing are facing acute implementation difficulties . some of the difficulties are due to the fact that cim in the current form is not able to adequately address knowledge management and concurrent engineering ( ce ) issues . also , with cim , it is not possible to solve problems related to decision and control even though there has been an increasing interest in artificial intelligence ( ai ) , knowledge based systems ( kbs ) , expert systems , etc. in order to improve the productivity gain through cim , eds focused its information technology ( it ) vision on the combined potential of concurrent engineering ( ce ) , knowledge management ( km ) and computer integrated manufacturing ( cim ) technologies . eds through a number of it and cim implementations realized that ce , km and cim do go hand in hand . the three together provide a formidable base , which is called intelligent information system ( iis ) in this paper . describes the rationales used for creating an iis framework at eds , its usefulness to our clients and a make up of this emerging iis framework for integrated product development .
extending fre and zelenyuk ( <digit> ) . <eos> in our recent work , fre and zelenyuk ( <digit> ) on aggregate farrell efficiency scores . european journal of operational research <digit> ( <digit> ) , <digit> , we have proposed a way of aggregating farrell type efficiency scores with weights ( and aggregation function ) derived from economic type optimization behaviour . in this comment we correct a mathematical error present in that work as well as generalize the price independent weights we have proposed earlier .
policy oscillation is overshooting . <eos> a majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value based policy gradient methods . the former approach , although fast , is well known to be susceptible to the policy oscillation phenomenon . we take a fresh view to this phenomenon by casting , within the context of non optimistic policy iteration , a considerable subset of the former approach as a limiting special case of the latter . we explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples . we also use it to derive the constrained natural actor critic algorithm that can interpolate between the aforementioned approaches . in addition , it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the tetris benchmark problem of all attempted approximate dynamic programming methods . based on empirical findings , we offer a hypothesis that might explain the inferior performance levels and the associated policy degradation phenomenon , and which would partially support the suggested connection . finally , we report scores in the tetris problem that improve on existing dynamic programming based results by an order of magnitude .
configuration and dynamic reconfiguration of components using the coordination paradigm . <eos> one of the most promising approaches in developing component based ( possibly distributed ) systems is that of coordination models and languages . coordination programming enjoys a number of advantages such as the ability to express different software architectures and abstract interaction protocols , support for multi linguality , reusability and programming in the large , etc. configuration programming is another promising approach in developing large scale , component based systems , with the increasing need for supporting the dynamic evolution of components . in this paper we explore and exploit the relationship between the notions of coordination and ( dynamic ) configuration and we illustrate the potential of control or event driven coordination languages to be used as languages for expressing dynamically reconfigurable software architectures . we argue that control driven coordination has similar goals and aims with the notion of dynamic configuration and we illustrate how the former can achieve the functionality required by the latter .
upper level scheduling supporting multimedia traffic in cellular data networks . <eos> wireless data networks such as cdma2000 1x ev do and umts hsdpa use downlink scheduling that exploits channel fading to increase the system throughput . as future wireless networks will eventually support multimedia and data traffic together , we need a proper criterion for scheduling that can count various service requirements such as delay and packet loss . although some previous approaches proposed opportunistic schedulers at the lower layer , it has not been investigated well whether they are able to meet explicit qos defined at the upper layer . hence , in this paper , we develop a hierarchical scheduling model that considers qos provisioning and the time varying channel feature separately . we focus on the upper level qos scheduling that supports various traffic classes in a unified manner . supposing that a user gets some satisfaction or utility when served , we introduce a novel concept of opportunity cost , which is defined as the maximum utility loss among users incurred by serving a particular user at the current turn . we obtain each user 's net profit by subtracting the opportunity cost from its expected utility , and then select a user with the maximum profit for service . simulation results reveal that our scheme supports various qos classes well that are represented by delay and packet loss under various traffic loadings . ( c ) <digit> elsevier b.v. all rights reserved .
repcidn a reputation based collaborative intrusion detection network to lessen the impact of malicious alarms . <eos> distributed and coordinated attacks in computer networks are causing considerable economic losses worldwide in recent years . this is mainly due to the transition of attackers operational patterns towards a more sophisticated and more global behavior . this fact is leading current intrusion detection systems to be more likely to generate false alarms . in this context , this paper describes the design of a collaborative intrusion detection network ( cidn ) that is capable of building and sharing collective knowledge about isolated alarms in order to efficiently and accurately detect distributed attacks . it has been also strengthened with a reputation mechanism aimed to improve the detection coverage by dropping false or bogus alarms that arise from malicious or misbehaving nodes . this model will enable a cidn to detect malicious behaviors according to the trustworthiness of the alarm issuers , calculated from previous interactions with the system . experimental results will finally demonstrate how entities are gradually isolated as their behavior worsens throughout the time .
differential evolution for system identification of self excited vibrations . <eos> a competitive version of the differential evolution algorithm is applied to regression on second order ordinary differential equations from only the original time signal . specific attention is devoted to application aspects of self excited vibrations in physics and engineering . extensive numerical experiments reveal the factors that influence test errors in free optimization . two novel approaches for a constrained optimization treatment of this inverse problem are proposed . this enables accurate identification of the target coefficients of the dynamical system .
mean curvature mapping for detection of corneal shape abnormality . <eos> corneal topography is used to measure the anterior surface of the cornea . it is conventionally represented as radial slope , radial curvature , and elevation . in this paper , we introduce the application of mean curvature mapping as an alternative representation of the corneal topography . the purpose is to improve the detection of keratoconus and other diseases characterized by local increase in corneal curvature . both simulated keratoconic cornea and real keratoconus data exported from the corneal topography system were analyzed . four representations of corneal topography were generated and compared . it was found that mean curvature mapping provided the most precise cone location in simulated keratoconus . in both actual and simulated keratoconus cases , the appearance of the cone like distortion is more consistent on mean curvature maps . mean curvature mapping may improve the detection and localization of corneal shape abnormalities .
client led information system creation ( clic ) navigating the gap . <eos> abstract . this paper offers a new framework to facilitate an interpretive approach to client led information system development , referred to as clic ( client led information system creation ) . the challenge of moving seamlessly through a process of information systems ( is ) design is still the subject of much research in the is field . attempts to address the difficulties of bridging the gap between a client 's business needs and an information system definition have hitherto not provided a coherent and practical approach . rather than attempting to bridge the gap , this paper describes an approach to managing this gap by facilitating the clients navigating through the information system design process ( or inquiry process ) in a coherent manner . the framework has been developed through practice , and the paper provides an example of navigating through the design phase taken from an action research field study in a major uk bank .
markov process based reliability model for laser diodes in space radiation environment . <eos> reliability model of irradiated laser diodes in space environment . degradation process is separated into discrete states . degradation of laser diodes is described as a markov process . reliability characteristics of laser diodes are simulated over 100,000 h.
novartis malaria initiative best practice example of pharmaceutical industry 's engagement in the fight against malaria . <eos> despite considerable advances in the treatment and prevention of malaria , plasmodium falciparum is still a threat to millions of people across the world , particularly in sub saharan africa , with infants and young children bearing the greatest burden in terms of morbidity and mortality . since <digit> , the artemisinin based combination therapy artemether lumefantrine ( al coartem ) has been made available . a wealth of evidence supports consistently high efficacy of al , and a favorable safety and tolerability profile has been demonstrated . the child friendly dispersible formulation of al has proven to be as effective and well tolerated as the standard tablets , and will encourage ease of administration and improved adherence to the drug regimen . this article reviews the significant impact made by al on the progress in malaria control and describes the way forward for the novartis malaria initiative in leading the fight against malaria .
scan architecture with align encode . <eos> scan architectures that provide compression capabilities have become mandatory due to the unbearable test costs imposed by high test data volume and prolonged test application . to alleviate these test costs , a stimulus decompressor and a response compactor block are inserted between the tester channels and the scan chains . as a result , a few tester channels drive a larger number of scan chains . in such an architecture , whether a particular test pattern can be delivered depends on the care bit distribution of that pattern . in this paper , we introduce a hardware block to be utilized in conjunction with a combinational stimulus decompressor block . this block. , namely , align encode , provides a deterministic per pattern control over care bit distribution of test vectors , improving pattern deliverability , and thus , the effectiveness of the particular stimulus decompressor . align encode is reconfigured on a per pattern basis to delay the shift in operations in selected scan chains . the number of cycles that a chain may be delayed can be between zero and the maximum allowable value , in order to align the scan slices in such a way that originally undeliverable test vectors become encodable . the reconfigurability of align encode provides a test pattern independent solution , wherein any , given set of test vectors can be analyzed to compute the proper delay information . we present efficient techniques for computing the scan chain delay values that lead to pattern encodability . experimental results also justify the test pattern encodability enhancements that align encode delivers , enabling significant test quality improvements and or test cost reductions .
efficient discovery of highly interrelated users in one way communications . <eos> in this paper , we introduce a new sequential pattern , the interactive user sequence pattern ( iusp ) . this pattern is useful for grouping highly interrelated users in one way communications such as e mail , sms , etc. , especially when the communications include many spam users . also , we propose an efficient algorithm for discovering iusps from massive one way communication logs containing only the following information senders , receivers , and dates and times . even though there is a difficulty in that our new sequential pattern violates the apriori property , the proposed algorithm shows excellent processing performance and low storage cost in experiments on a real dataset .
a pc cluster system employing ieee <digit> . <eos> in this paper , we describe the design and evaluation of a pc cluster system in which ieee <digit> is applied . networks for parallel cluster computing require low latency and high bandwidth . it is also important that the networks be commercially available at low cost . few network devices satisfy all of the above requirements . however , the ieee <digit> standard provides a good compromise for fulfilling these requirements . we have used ieee <digit> devices , which support a <digit> mbps data transfer rate , to connect the nodes of a pc cluster system which we have designed and implemented . we have implemented two communication libraries . one is a fast communication library called cf for ieee <digit> . the other is a mpi layer library on the cf library . experimental results show that cf achieves a 17.2 microsecond round trip time . on application benchmarks , the system was considerably faster than tcp ip over fast ethernet . even though the system was constructed at very low cost , it provides good performance . using the ieee <digit> standard is thus a good solution for low cost cluster systems . copyright ( c ) <digit> john wiley sons , ltd .
does habituation affect fingerprint quality . <eos> interest in the environmental factors that affect biometric image quality is increasing as biometric technologies are currently being implemented in various business applications . this study aims to determine , through repeated trials , the effects of various external factors on the image quality and usability of prints collected by an electronic reader . these factors include age and gender but also the absence or presence of immediate feedback . a key factor in biometric systems that will be used daily or routinely is habituation . the user 's behavior could potentially change as a result of acclimatization one 's input might increase in quality as one learns how to use the system better , or decrease in quality since comfort with the system could translate into carelessness .
on the relocation problem with a second working crew for resource recycling . <eos> in this paper , we introduce a variant of the relocation problem , which was formulated from a public house redevelopment project in boston . in the problem of interest , given some initial resources in a common pool there is a set of jobs to be processed on a two machine flowshop . each job acquires a specific number of resources to start its processing and will return a number of resources to the pool at its completion . the resource consumption and resource recycle processes are performed on machine one and machine two , respectively , in a two machine flowshop style . abiding by the resource constraints , the problem seeks to find a feasible schedule whose makespan is minimized . in this paper , we first present np hardness proofs for some special cases . three heuristic algorithms are designed to compose approximate schedules . two lower bounds are developed and then used to test the performance of our proposed heuristics . numerical results from computational experiments suggest that the proposed heuristics can produce quality solutions in a reasonable time .
equivariant pieri rule for the homology of the affine grassmannian . <eos> an explicit rule is given for the product of the degree two class with an arbitrary schubert class in the torus equivariant homology of the affine grassmannian . in addition a pieri rule ( the schubert expansion of the product of a special schubert class with an arbitrary one ) is established for the equivariant homology of the affine grassmannians of sl n and a similar formula is conjectured for sp 2n and so 2n <digit> . for sl n the formula is explicit and positive . by a theorem of peterson these compute certain products of schubert classes in the torus equivariant quantum cohomology of flag varieties . the sl n pieri rule is used in our recent definition of k double schur functions and affine double schur functions .
dual rail asynchronous logic multi level implementation . <eos> a synthesis flow oriented on producing the delay insensitive dual rail asynchronous logic is proposed . within this flow , the existing synchronous logic synthesis tools are exploited to design technology independent single rail synchronous boolean network of complex ( and or ) nodes . next , the transformation into a dual rail boolean network is done . each node is minimized under the formulated constraint to ensure hazard free implementation . then the technology dependent mapping procedure is applied . the mcnc and iscas benchmark sets are processed and the area overhead with respect to the synchronous implementation is evaluated . the implementations of the asynchronous logic obtained using the proposed ( with and or nodes ) and the state of the art ( nodes are designed based on dims , direct logic and ncl ) network structures are compared . a method , where nodes are designed as simple ( nand , nor , etc. ) gates is chosen for a detailed comparison . in our approach , the number of completion detection logic inputs is reduced significantly , since the number of nodes that should be supplied with the completion detection is less than in the case of the network structure that is based on simple gates . as a result , the improvement in sense of the total complexity and performance is obtained .
off line signature verification and forgery detection using fuzzy modeling . <eos> automatic signature verification is a well established and an active area of research with numerous applications such as bank check verification , atm access , etc. this paper proposes a novel approach to the problem of automatic off line signature verification and forgery detection . the proposed approach is based on fuzzy modeling that employs the takagisugeno ( ts ) model . signature verification and forgery detection are carried out using angle features extracted from box approach . each feature corresponds to a fuzzy set . the features are fuzzified by an exponential membership function involved in the ts model , which is modified to include structural parameters . the structural parameters are devised to take account of possible variations due to handwriting styles and to reflect moods . the membership functions constitute weights in the ts model . the optimization of the output of the ts model with respect to the structural parameters yields the solution for the parameters . we have also derived two ts models by considering a rule for each input feature in the first formulation ( multiple rules ) and by considering a single rule for all input features in the second formulation . in this work , we have found that ts model with multiple rules is better than ts model with single rule for detecting three types of forgeries random , skilled and unskilled from a large database of sample signatures in addition to verifying genuine signatures . we have also devised three approaches , viz. , an innovative approach and two intuitive approaches using the ts model with multiple rules for improved performance .
investigation of si sige si heterostructure implanted by h ion and annealed in vacuum and dry o2 ambient . <eos> the <digit> nm thick si cap layer <digit> nm thick si0 .72 ge0 .28 epilayer si heterostructures implanted by 25kev h ion to a dose of <digit> cm <digit> were annealed in ultra high vacuum ambient and dry o2 ambient at the temperature of 800c for <digit> min , respectively . rutherford backscattering ion channeling ( rbs c ) , raman spectra , high resolution x ray diffraction ( hrxrd ) and atomic force microscopy ( afm ) were used to characterize the structural characteristics of the si0 .72 ge0 .28 layer . investigations by rbs c demonstrated that the crystal quality of the si si0 .72 ge0 .28 si heterostructure sample implanted by 25kev h in conjunction with subsequent annealing in dry o2 ambient is superior to that of identical sample annealing in ultra high vacuum ambient . the less strain relaxation of sige layer of the si si0 .72 ge0 .28 si heterostructures implanted by h ion and annealed in dry o2 ambient at the temperature of 800c for <digit> min could be doublechecked by raman spectra as well as hrxrd , which was compared with that in an identical sample annealed in ultra high vacuum ambient for identical thermal budget . in addition , the sige layer of the h implanted si sige si heterostructural sample annealed in dry o2 ambient accompanied by better crystal quality and less strain relaxation made its surface morphology superior to that of the sample annealed in ultra high vacuum ambient at the temperature of 800c for <digit> min , which was also verified by afm images .
energy neutral scheduling and forwarding in environmentally powered wireless sensor networks . <eos> in environmentally powered wireless sensor networks ( epwsns ) , low latency wakeup scheduling and packet forwarding is challenging due to dynamic duty cycling , posing time varying sleep latencies and necessitating the use of dynamic wakeup schedules . we show that the variance of the intervals between receiving wakeup slots affects the expected sleep latency when the variance of the intervals is low ( high ) , the expected latency is low ( high ) . we therefore propose a novel scheduling scheme that uses the bit reversal permutation sequence ( brps ) a finite integer sequence that positions receiving wakeup slots as evenly as possible to reduce the expected sleep latency . at the same time , the sequence serves as a compact representation of wakeup schedules thereby reducing storage and communication overhead . but while low latency wakeup schedule can reduce per hop delay in ideal conditions , it does not necessarily lead to low latency end to end paths because wireless link quality also plays a significant role in the performance of packet forwarding . we therefore formulate expected transmission delay ( etd ) , a metric that simultaneously considers sleep latency and wireless link quality . we show that the metric is left monotonic and left isotonic , proving that its use in distributed algorithms such as the distributed bellmanford yields consistent , loop free and optimal paths . we perform extensive simulations using real world energy harvesting traces to evaluate the performance of the scheduling and forwarding scheme .
minimizing downtime in seamless migrations of mobile applications . <eos> application migration is a key enabling technology component of mobile computing that allows rich semantics involving location awareness , trust and timeliness of information processing by moving the application where the data is . seamlessness is one of the key properties of mobile computing and downtime must be eliminated minimized during the migration to achieve seamlessness . but migration involves large overheads , dominant of which are the overheads due to serialization and de serialization <digit> . to achieve seamless migration , an application state could be pre serialized during the program 's execution , and upon migration , the serialized data could be transmitted and de serialized to get the execution started . previous approach to this problem <digit> removed dead state but still suffered from large migration overheads due to serialization on demand that could lead to an unacceptable downtime.in this work , we develop a static compiler analysis plus runtime assisted framework to decrease the migration overhead to almost zero while minimizing the degradation in the program 's performance . we achieve such a goal by deciding which data to be pre serialized through analysis , and pre serializing the state in the program . a safe state is kept that would allow immediate migration upon the arrival of an interrupt while minimizing frequent pre serialization . when the migration interrupt comes in , the serialized data can be transmitted directly to the destination machine . this allows an application to resume its execution at the destination machine with almost no interruption ( only a small amount of non serialized data needs to be serialized during migration ) . the optimization serializes the data in such a way that a maximal number of functions can execute without interruption after migration . our experiments with multimedia applications show that the migration latency is significantly reduced leading to a small downtime . thus , the contribution of the paper is to provide an efficient methodology to perform seamless migration while limiting the overhead .
an efficient algorithm based on the differential quadrature method for solving navier stokes equations . <eos> in this paper , an approach to improve the application of the differential quadrature method for the solution of navierstokes equations is presented . in using the conventional differential quadrature method for solving navierstokes equations , difficulties such as boundary conditions ' implementation , generation of an ill conditioned set of linear equations , large memory storage requirement to store data , and matrix coefficients , are usually encountered . also , the solution of the generated set of equations takes a long running time and needs high computational efforts . an approach based on the point pressurevelocity iteration method , which is a variant of the newtonraphson relaxation technique , is presented to overcome these problems without losing accuracy . to verify its performance , four cases of two dimensional flows in single and staggered double lid driven cavity and flows past backward facing step and square cylinder , which have been often solved by researchers as benchmark solution , are simulated for different reynolds numbers . the results are compared with existing solutions in the open literature . very good agreement with low computational efforts of the approach is shown . it has been concluded that the method can be applied easily and is very time efficient . copyright ( c ) <digit> john wiley sons , ltd .
optimal transmission schemes for parallel and fading gaussian broadcast channels with an energy harvesting rechargeable transmitter . <eos> we consider an energy harvesting transmitter sending messages to two users over parallel and fading gaussian broadcast channels . energy required for communication arrives ( is harvested ) at the transmitter and a finite capacity battery stores it before being consumed for transmission . under off line knowledge of energy arrival and channel fading variations , we obtain the trade off between the performances of the users by characterizing the maximum departure region in a given interval . we first analyze the transmission with an energy harvesting transmitter over parallel broadcast channels . we show that the optimal total transmit power policy that achieves the boundary of the maximum departure region is the same as the optimal policy for the non fading broadcast channel , which does not depend on the priorities of the users , and therefore is the same as the optimal policy for the non fading scalar single user channel . the optimal total transmit power can be found by a directional water filling algorithm . the optimal splitting of the power among the parallel channels is performed in each epoch separately . next , we consider fading broadcast channels and obtain the transmission policies that achieve the boundary of the maximum departure region . the optimal total transmit power allocation policy is found using a specific directional water filling algorithm for fading broadcast channels . the optimal power allocation depends on the priorities of the users unlike in the case of parallel broadcast channels . finally , we provide numerical illustrations of the optimal policies and maximum departure regions for both parallel and fading broadcast channels . ( c ) <digit> elsevier b.v. all rights reserved .
efficient detection in hyperspectral imagery . <eos> hyperspectral sensors collect hundreds of narrow and contiguously spaced spectral bands of data . such sensors provide fully registered high resolution spatial and spectral images that are invaluable in discriminating between man made objects and natural clutter backgrounds . the price paid for this high resolution data is extremely large data sets , several hundred of mbytes for a single scene , that make storage and transmission difficult , thus requiring fast onboard processing techniques to reduce the data being transmitted . attempts to apply traditional maximum likelihood detection techniques for in flight processing of these massive amounts of hyperspectral data suffer from two limitations first , they neglect the spatial correlation of the clutter by treating it as spatially white noise second , their computational cost renders them prohibitive without significant data reduction like by grouping the spectral bands into clusters , with a consequent loss of spectral resolution . this paper presents a maximum likelihood detector that successfully confronts both problems rather than ignoring the spatial and spectral correlations , our detector exploits them to its advantage and it is computationally expedient , its complexity increasing only linearly with the number of spectral bands available . our approach is based on a gauss markov random held ( gmrf ) modeling of the clutter , which has the advantage of providing a direct parameterization of the inverse of the clutter covariance , the quantity of interest in the test statistic . we discuss in detail two alternative gmrf detectors one based on a binary hypothesis approach , and the other on a ' single ' hypothesis formulation . we analyze extensively with real hyperspectral imagery data ( hydice and sebass ) the performance of the detectors , comparing them to a benchmark detector , the rx algorithm . our results show that the gmrf ' single ' hypothesis detector outperforms significantly in computational cost the rx algorithm , while delivering noticeable detection performance improvement .
two way eye contact between humans and robots . <eos> eye contact is an effective means of controlling human communication , such as in starting communication . it seems that we can make eye contact if we simply look at each other . however , this alone does not establish eye contact . both parties also need to be aware of being watched by the other . we propose a method of two way eye contact for human robot communication . when a human wants to start communication with a robot , he she watches the robot . if it finds a human looking at it , the robot turns to him her , changing its facial expressions to let him her know its awareness of his her gaze . when the robot wants to initiate communication with a particular person , it moves its body and face toward him her and changes its facial expressions to make the person notice its gaze . we show several experimental results to prove the effectiveness of this method . moreover , we present a robot that can recognize hand gestures after making eye contact with the human to show the usefulness of eye contact as a means of controlling communication .
program representations for testing wireless sensor network applications . <eos> because of the growing complexity of wireless sensor network applications ( wsns ) , traditional software development tools are being developed that are specifically designed for their special characteristics . however , testing tools have yet to be proposed . one problem in developing testing tools is the need for a program representation that expresses the execution behavior . due to characteristics of wsn applications that use a concurrent , event based execution model , a representation is challenging to develop . in this paper , we present novel representations for wsns applications that express the execution behavior of event and tasks , the major components of a wsn application . our representations include a task posting graph , an event graph and finally an application graph that expresses the relationships among events and tasks as well as both timing and environmental interrupts . these representations are the first step in developing testing tools for wsn applications . based on the graphs , traditional and event based coverage criteria can be evaluated . when combined with individual control flow graphs ( cfgs ) of events and tasks , the graphs ' paths can be used as a criterion for evaluating the completeness of the test cases .
equalising stamp and substrate deformations in solid parallel plate uv based nanoimprint lithography . <eos> we deal with solid parallel plate uv based nanoimprint lithography ( uv nil ) using rigid quartz stamps and spin coated substrates . achieving a conformal contact of stamp and substrate in a parallel plate setup is challenging , since the solid stamp and the substrate usually are not perfectly flat . finite element simulations and experimental results show that a correctly designed compliant layer underneath the substrate guarantees a conformal contact of stamp and substrate and therefore imprints of a high quality with a homogeneous and thin residual layer down to 10nm can be achieved .
optimal inventory policies for profit maximizing eoq models under various cost functions . <eos> in this paper , we establish and analyze three eoq based inventory models under profit maximization via geometric programming ( gp ) techniques . through gp , we find optimal order quantity and price for each of these models considering production ( lot sizing ) as well as marketing ( pricing ) decisions . we also investigate the effects on the changes in the optimal solutions when different parameters are changed . in addition , a comparative analysis between the profit maximization models is conducted . by investigating the error in the optimal price , order quantity , and profit of these models , several interesting economic implications and insights can be observed .
augmented state gm phd filter with registration errors for multi target tracking by doppler radars . <eos> we build the linear gaussian dynamics and measurement model of the augmented state . we derive related equations for the augmented state gm phd filter with sensor biases . to effectively utilize the doppler , we propose the sequential processing method . the proposed gm phd r d is compared with the gm phd r for varying clutter rates .
restoring solvability of the electric network equations an approach based on the augmented lagrangean algorithm . <eos> this paper presents and discusses a new , robust approach for restoring solvability of the electric network equations in power systems . the unsolvable power flow is modelled as a constrained optimization problem . the cost function is the squared sum of the real and reactive power mismatches at the electric system buses which are subject to suffer load shedding . the equality constraints are the real and reactive power mismatches at null injection buses and or at buses whose power demands must be integrality supplied due to technical and or economical criteria . the mathematical model is solved using an algorithm based on augmented lagrangean function method which takes into account the special structure of the proposed problem . the inner iterations of the proposed methodology are solved using the levenberg marquardt ( lm ) algorithm . numerical results for both ieee test systems and a real equivalent electric system corresponding to brazil south southeast region are presented in order to analyze and test the performance of the proposed methodology .
combination of independent component analysis and support vector machines for intelligent faults diagnosis of induction motors . <eos> this paper studies the application of independent component analysis ( ica ) and support vector machines ( svms ) to detect and diagnose of induction motor faults . the ica is used for feature extraction and data reduction from original features . the principal components analysis is also applied in feature extraction process for comparison with ica does . in this paper , the training of the svms is carried out using the sequential minimal optimization algorithm and the strategy of multi class svms based classification is applied to perform the faults identification . also , the performance of classification process due to the choice of kernel function is presented to show the excellent of characteristic of kernel function . various scenarios are examined using data sets of vibration and stator current signals from experiments , and the results are compared to get the best performance of classification process .
context analysis to support development of virtual reality applications . <eos> to develop a usable virtual reality system , the prospective context of use of such a system may need to be considered in order to make sure it meets the requirements and restrictions of that context . in this paper , a contextual analysis is described for a virtual reality system to aid medical diagnosis and treatment planning of vascular disorders . semi structured interviews were coupled with observations in an ethnographic approach to requirements gathering in the daily work environment of ( interventional ) radiologists and vascular surgeons . the identified potential usability problems of a fully immersive prototype , coupled with the needs , requirements and real life environment of the end users lead to guidelines for the development of a vr application on a semi immersive desktop environment . the findings lead us to believe that contextual analysis can be a powerful way to inform the design of a vr application by offering an understanding of the context of use and to inform developers of the most appropriate degree of immersiveness of the vr environment .
efficient reconfigurable manchester adders for low power media processing . <eos> a new highly reconfigurable manchester adder for low power media signal processing is presented . the proposed circuit can be run time partitioned . its <digit> bit version performs one <digit> , two <digit> , four <digit> , or eight <digit> bit additions . when the ams 0.35 mm <digit> poly <digit> metal 3.3 v cmos ( csd ) process is used to produce a layout , an energy dissipation of only <digit> pj and a worst propagation delay of about 10.2 ns are obtained . the novelty demonstrated in this letter is that the introduction of dummy bit positions along the carry path can be avoided using on purpose dynamic logic stages .
numerical convergence and physical fidelity analysis for maxwells equations in metamaterials . <eos> in this paper , we develop a leap frog mixed finite element method for solving maxwells equations resulting from metamaterials . our scheme is similar to the popular yees fdtd scheme used in electrical engineering community , and is preferable for three dimensional large scale modeling since no storage of the large coefficient matrix is needed . our scheme is proved to obey the gausss law automatically if the initial fields satisfy that . furthermore , the conditional stability and optimal error estimate for the proposed scheme are proved . to our best knowledge , we are unaware of any other publications devoted to the convergence analysis of this leap frog explicit scheme for maxwells equations even in a simple medium , while our results for metamaterials automatically reduce to the standard maxwells equations in vacuum by dropping some terms resulting from the constitutive equations . numerical results confirming our analysis are presented .
weak convergence of finite element approximations of linear stochastic evolution equations with additive noise . <eos> a unified approach is given for the analysis of the weak error of spatially semidiscrete finite element methods for linear stochastic partial differential equations driven by additive noise . an error representation formula is found in an abstract setting based on the semigroup formulation of stochastic evolution equations . this is then applied to the stochastic heat , linearized cahn hilliard , and wave equations . in all cases it is found that the rate of weak convergence is twice the rate of strong convergence , sometimes up to a logarithmic factor , under the same or , essentially the same , regularity requirements .
numerical modeling of magnetic induction tomography using the impedance method . <eos> this article discusses the impedance method in the forward calculation in magnetic induction tomography ( mit ) . magnetic field and eddy current distributions were obtained numerically for a sphere in the field of a coil and were compared with an analytical model . additionally , numerical and experimental results for phase sensitivity in mit were obtained and compared for a cylindrical object in a planar array of sensors . the results showed that the impedance method provides results that agree very well with reality in the frequency range from 100khz to 20mhz and for low conductivity objects ( 10s m or less ) . this opens the possibility of using this numerical approach in image reconstruction in mit .
how to help seismic analysts to verify the french seismic bulletin . <eos> in this paper , classifiers based on multi layer perceptrons and support vector machines are used in order to classify seismic events that occurred in metropolitan france . the results are exploited in the software ramses to help the seismic analysts to conduct efficiently the revision of the weekly french seismic bulletin . with 96.5 % of good classification , and less than <digit> % of the events emphasized for verification , ramses strikingly improves the speed of the revision .
robust stability of uncertain fuzzy cohen grossberg bam neural networks with time varying delays . <eos> in this paper , the takagi sugeno ( ts ) fuzzy model representation is extended to the stability analysis for uncertain cohen grossberg type bidirectional associative memory ( bam ) neural networks with time varying delays using linear matrix inequality ( lmi ) theory . a novel lmi based stability criterion is obtained by using lmi optimization algorithms to guarantee the asymptotic stability of uncertain cohen grossberg bam neural networks with time varying delays which are represented by ts fuzzy models . finally , the proposed stability conditions are demonstrated with numerical examples . ( c ) <digit> elsevier ltd. all rights reserved .
fluid structure interaction problems in free surface flows application to boat dynamics . <eos> in this paper , we present some recent studies on fluid structure interaction problems in the presence of free surface flow . we consider the dynamics of boats simulated as rigid bodies . several hydrodynamic models are presented , ranging from full reynolds averaged navier stokes equations to reduced models based on potential flow theory . copyright ( c ) <digit> john wiley sons , ltd .
information theoretic approaches to branching in search . <eos> deciding what to branch on at each node is a key element of search algorithms . we present four families of methods for selecting what question to branch on . they are all information theoretically motivated to reduce uncertainty in remaining subproblems . in the first family , a good variable to branch on is selected based on lookahead . in real world procurement optimization , this entropic branching method outperforms default cplex and strong branching . the second family combines this idea with strong branching . the third family does not use lookahead , but instead exploits features of the underlying structure of the problem . experiments show that this family significantly outperforms the state of the art branching strategy when the problem includes indicator variables as the key driver of complexity . the fourth family is about branching using carefully constructed linear inequality constraints over sets of variables .
efficient solution to the 3d problem of automatic wall paintings reassembly . <eos> this paper introduces a new approach for the automated reconstruction reassembly of fragmented objects having one surface near to plane , on the basis of the 3d representation of their constituent fragments . the whole process starts by 3d scanning of the available fragments . the obtained representations are properly processed so that they can be tested for possible matches . next , four novel criteria are introduced , that lead to the determination of pairs of matching fragments . these criteria have been chosen so as the whole process imitates the instinctive reassembling method dedicated scholars apply . the first criterion exploits the volume of the gap between two properly placed fragments . the second one considers the fragments overlapping in each possible matching position . criteria 3,4 employ principles from calculus of variations to obtain bounds for the area and the mean curvature of the contact surfaces and the length of contact curves , which must hold if the two fragments match . the method has been applied , with great success , both in the reconstruction of objects artificially broken by the authors and , most importantly , in the virtual reassembling of parts of wall paintings belonging to the mycenaic civilization ( c. <digit> bc . ) , excavated in a highly fragmented condition in tyrins , greece
resolution enhancement of nondestructive testing from b scans . <eos> this article presents an approach to extend our previous work of the minimum weighted norm method in computerized tomography . in particular concentrating on applications of ultrasonic nondestructive testing , the resolution enhancement in the image reconstruction from b scans is achieved . to combat the degradation problem due to physical focus of finite sized ultrasonic transducer and incompleteness of b scan data , a profile oriented prior knowledge about the object being detected is incorporated in the image reconstruction , in the form of weighted summation of specific basis functions . each basis function is characterized by an image of coherent illumination pattern associated with a specific measuring time and a specific measuring position . from the demonstrations with both simulated and experimental data values , this technique proves a great potential in improving the image quality . <digit> wiley periodicals , inc. int j imaging syst technol , <digit> , <digit> , <digit>
user centric cloud service model in public sectors policy implications of cloud services . <eos> this study examines the acceptance of cloud computing services in government agencies by focusing on the key characteristics that affect behavioral intent . the study expanded upon the technology acceptance model by incorporating contextual factors such as availability , access , security , and reliability . the research model was empirically verified by investigating the perception of users working in public institutions . modeling results showed that user intentions and behaviors were largely influenced by the perceived features of cloud services . also these features were found to be the significant antecedents of cloud computing usefulness and ease of use . the findings should guide governments ' promotion of cloud public services to increase user awareness by enhancing usability and appeal and ensuring security .
a characterization of concordance relations . <eos> the notion of concordance is central to many multiple criteria techniques relying on ordinal information , e.g. outranking methods . it leads to compare alternatives by pairs on the basis of a comparison of coalitions of attributes in terms of importance . this paper proposes a characterization of the binary relations that can be obtained using such comparisons within a general framework for conjoint measurement that allows for intransitive preferences . we show that such relations are mainly characterized by the very rough differentiation of preference differences that they induce on each attribute .
impact of sourcing flexibility on the outsourcing of services under demand uncertainty . <eos> this paper investigates the relationship between market conditions and the value and use of sourcing flexibility for service processes . we develop and analyze a series of models , and we derive expressions for the optimal switching decision , the value of the option to outsource , the value of the option to backsource , and the probability and timing of switches between the alternative sources . one contribution is the models and associated derivations , which are largely new to the literature and may serve as a tool to support service sourcing plans and decisions . the second contribution is a series of results with managerial implications ( <digit> ) the probability of outsourcing is generally increasing in volatility for high skill process and decreasing in volatility for low skill processes . earlier work has found that the hysteresis band is increasing in volatility , which is interpreted as an indicator of increasing organizational inertia . we also find that the hysteresis band is increasing in volatility , but interestingly for the case of high skill processes , organizational inertia tends to be decreasing in volatility . ( <digit> ) the option to backsource is generally more valuable for high skill processes than for low skill processes . this result suggests that investments to make it easier to backsource should have a higher priority for high skill processes . ( <digit> ) the value of the option to backsource a high skill service process can be decreasing in volatility . the result suggests that a rather nuanced consideration of volatility is in order when considering investments in the flexibility to backsource a high skill process .
a robotic model of reaching and grasping development . <eos> we present a neurorobotic model that develops reaching and grasping skills analogous to those displayed by infants during their early developmental stages . the learning process is realized in an incremental manner , taking into account the reflex behaviors initially possessed by infants and the neurophysiological and cognitive maturation occurring during the relevant developmental period . the behavioral skills acquired by the robots closely match those displayed by children . the comparison between incremental and nonincremental experiments demonstrates how some of the limitations characterizing the initial developmental phase channel the learning process toward better solutions .
performance analyses of notch fourier transform ( nft ) and constrained notch fourier transform ( cnft ) . <eos> fourier analysis of sinusoidal and or quasi periodic signals in additive noise has been used in various fields . so far , many analysis algorithms including the well known dft have been developed . in particular , many adaptive algorithms have been proposed to handle non stationary signals whose discrete fourier coefficient ( dfcs ) are time varying . notch fourier transform ( nft ) and constrained notch fourier transform ( cnft ) proposed by tadokoro et al. and kilani et al. , respectively , are two of them , which are implemented by filter banks and estimate the dfcs via simple sliding algorithms of their own . this paper presents , for the first time , statistical performance analyses of the nft and the cnft . estimation biases and mean square errors ( mses ) of their sliding algorithms will be derived in closed form . as a result , it is revealed that both algorithms are unbiased , and their estimation mses are related to the signal frequencies , the additive noise variance and orders of comb filters used in their filter banks . extensive simulations are performed to confirm the analytical findings .
complementary cycles in almost regular multipartite tournaments , where one cycle has length four . <eos> let d d be a digraph with vertex set v ( d ) v ( d ) and independence number ( d ) ( d ) . if x v ( d ) x v ( d ) , then the numbers d ( x ) d ( x ) and d ( x ) d ( x ) are the outdegree and indegree of x x , respectively . the global irregularity of a digraph d d is defined by in <digit> , yeo conjectured that each regular c c partite tournament d d with c <digit> c <digit> and v ( d ) <digit> v ( d ) <digit> contains a pair of vertex disjoint directed cycles of lengths <digit> <digit> and v ( d ) <digit> v ( d ) <digit> . in <digit> , volkmann confirmed this conjecture for c <digit> c <digit> and c <digit> c <digit> and ( d ) <digit> ( d ) <digit> . as a supplement to this result , we prove in this paper the following theorem . let d d be an almost regular c c partite tournament with v ( d ) <digit> v ( d ) <digit> such that all partite sets have the same cardinality r r . if c <digit> c <digit> or c <digit> c <digit> and r <digit> r <digit> , then d d contains a pair of vertex disjoint directed cycles of lengths <digit> <digit> and v ( d ) <digit> v ( d ) <digit> .
data assimilation framework linking an open data assimilation library ( openda ) to a widely adopted model interface ( openmi ) . <eos> a generic framework for data assimilation is presented . it bridges together two open source projects openda and openmi . openmi compliant models can easily get access to data assimilation algorithms . tested on a catchment in denmark assimilating hydraulic head .
realistic scalability of noise in dynamic circuits . <eos> the usage of noise sensitive dynamic circuits has become commonplace due to speed and area requirements , making the noise issue even more prominent . this paper focuses on the trends of coupling and its effects on dynamic circuits . it presents closed form analytical solutions for noise , as well as noise tolerance metrics for dynamic circuits . these solutions are within <digit> % of dynamic simulations . it is shown that not all scaling trends are negative for noise , and that the scaling down of supply voltage and increasing frequency , help improve certain aspects of the noise immunity of dynamic circuits . most of the works treated the noise immunity and the noise content separately . this paper introduces an analysis of noise seatability by looking at the noise immunity and the noise content simultaneously .
envy , truth , and profit . <eos> we consider profit maximizing ( incentive compatible ) mechanism design in general environments that include , e.g. , position auctions ( for selling advertisements on internet search engines ) and single minded combinatorial auctions . we analyze optimal envy free pricings in these settings , and give economic justification for using the optimal revenue of envy free pricings as a benchmark for prior free mechanism design and analysis . moreover , we show that envy free pricing has a simple nice structure and a strong connection to incentive compatible mechanism design , and we exploit this connection to design prior free mechanisms with strong approximation guarantees .
model checking one dimensional cellular automata . <eos> we show that the first order theory of a one dimensional cellular automaton , construed as a structure with the global map and equality , is decidable . the argument employs bi infinite versions of buchi automata that can also be used to demonstrate that the spectra of cellular automata oil finite grids are regular . for existential properties our method can be used to produce witnesses .
comparison of visualization of optimal clustering using self organizing map and growing hierarchical self organizing map in cellular manufacturing system . <eos> we model visual clustering of machine part cell formation using ghsom model . we examine the optimal ghsom map that helps manager to visualize optimum cell formation . we compare som and ghsom models based on the network architecture and goodness of cell formation to find the efficacy of the performance on a set of <digit> benchmarked problems . ghsom algorithm concludes as the best model as it improves the gte performance measure for <digit> % of the cell formation problems than the som model and the other best models from the literature .
detecting non ergodic simulation models of logistics networks . <eos> simulation is a frequently applied method when analysing logistics networks . also within the collaborative research center <digit> modelling of large logistics networks simulation is broadly applied and process chains are used as a mutual basis for model development and description . previous research activities exposed non ergodicity of models as one of the typical application specific problems which are difficult to discover by simulation . in order to detect non ergodic models the problem has been reduced to its core employing the more analysis oriented modelling formalism of petri nets . with the help of the petri net formalism we developed an efficient method for the detection of non ergodic models . since petri nets is not the common modelling paradigm for logisticians , this method had to be made available in the process chain modelling world of the logistics area , additionally supported by an appropriate tool . this paper describes our corresponding approach and also demonstrates the process of identifying a problem class in an application area , reducing it to its core , establishing a solution in an analysis oriented formalism and making corresponding techniques available in the application oriented modelling world and thus also available for the end user .
simd , smp and mimd dm approaches for real time 2d image stabilization . <eos> we present a real time image stabilization method , based on a 2d motion model , and exploiting different levels of parallelism in its implementation . this stabilization method is decomposed into three parts . first , the image matching is determined by a feature based technique . in the second part , the motion between consecutive frames is estimated and filtered to extract the unwanted motion component . finally , these component is used to correct ( warp ) the images , resulting in a stable sequence . to validate our stabilization approach in a real time on board system context , the algorithm was implemented and tested over different hardware platforms , allowing a performance evaluation in function of the adopted architecture . in this paper , we present some results concerning the parallel implementation of the algorithm , using the simd altivec instructions set , a symmetric multi processor architecture ( smp ) and a mimd dm architecture .
simulation modeling decision support through belief networks . <eos> this paper presents an automated approach aimed at optimizing simulation model performance by means of intelligent agents and belief networks . the method described embeds intelligent agents in simulation models to conduct simulation real time model evaluation . these agents can be considered intelligent observers placed within a model that inspect and make real time decisions regarding the overall model performance . the knowledge encapsulation for the agents is provided via belief networks , which allow the agents to record their observations and make inferences . to exemplify this approach , a general purpose resource allocation model describing an earthmoving operation is provided .
a fast and stable algorithm for downdating the singular value decomposition . <eos> in this paper , we modify a classical downdating svd algorithm and reduce its complexity significantly . we use a structured low rank approximation algorithm to compute an hierarchically semiseparable ( hss ) matrix approximation to the eigenvector matrix of a diagonal matrix plus rank one modification . the complexity of our downdating algorithm is analyzed . we further show that the structured low rank approximation algorithm is backward stable . numerous experiments have been done to show the efficiency of our algorithm . for some matrices with large dimensions , our algorithm can be much faster than that using plain matrixmatrix multiplication routine in intel mkl in both sequential and parallel cases .
augmented reality vehicle system left turn maneuver study . <eos> augmented reality ar is a promising paradigm that can offer users with real time , high quality visualization of a wide variety of information . in ar , virtual objects are added to the real world view in real time . the ar technology can offer a very realistic environment for enhancing drivers performance on the road and testing drivers ability to react to different road design and traffic operations scenarios . this can be achieved by adding virtual objects ( people , vehicles , hazards , and other objects ) to the normal view while driving an actual vehicle in a real environment . this paper explores a new augmented reality vehicle arv system and attempts to apply this new concept to a selected traffic engineering application namely the left turn maneuver at two way stop controlled twsc intersection . this twsc intersection experiment , in addition to testing the feasibility of the application , tries to quantify the size of gaps accepted by different drivers characteristics ( age and gender ) . the arv system can be installed in any vehicle where the driver can see the surrounding environment through a head mounted display hmd and virtual objects are generated through a computer and added to the scene . these different environments are generated using a well defined set of scenarios . the results from this study supported the feasibility and validity of the proposed arv system and they showed promise for this system to be used in the field testing for the safety and operation aspects of transportation research . results of the left turn maneuver study revealed that participants accepted gaps in the range of 4.09.0 s. this finding implies that all gaps below 4s are rejected and all gaps above 9s are likely to be accepted . the mean value of the left turn time was 4.67 s which is a little bit higher than reported values in the literature ( 4.04.3 s ) . older drivers were found to select larger gaps to make left turns than younger drivers . the conservative driving attitude of older drivers indicates the potential presence of reduced driving ability of elderly . drivers characteristics ( age and gender ) did not significantly affect the left turn time . based on the survey questions that were handed to participants , most participants indicated good level of comfort with none or small level of risk while driving the vehicle with the arv system . none of the participants felt any kind of motion sickness and the participants answers indicated a good visibility and realism of the scene with overall good system fidelity .
granulometric analysis of corneal endothelium specular images by using a germgrain model . <eos> specular microscopy is widely used to study the human corneal endothelium status in vivo . in this paper , the corneal endothelium is represented as a binary image composed of the cell inscribed circles . the granulometric distribution function of the complement of this image is used as a functional descriptor , which provides information about the shape , size and spatial arrangement of cells . experimental evaluation using bootstrap techniques shows its ability to discriminate between controls and pathological cases . it represents a reliable and graphical alternative to the classical indices ( cell density , hexagonality and coefficient of variation of cell areas ) , which behave poorly when detecting subtle abnormalities .
maximum studentized score tests for the detection of outliers in time series regression models . <eos> efficient score tests exist among others , for testing the presence of additive and or innovative outliers that are the result of the shifted mean of the error process under the regression model . a sample influence function of autocorrelation based diagnostic technique also exists for the detection of outliers that are the result of the shifted autocorrelations . the later diagnostic technique is however not useful if the outlying observation does not affect the autocorrelation structure but is generated due to an inflation in the variance of the error process under the regression model . in this paper , we develop a unified maximum studentized type test which is applicable for testing the additive and innovative outliers as well as variance shifted outliers that may or may not affect the autocorrelation structure of the outlier free time series observations . since the computation of the p values for the maximum studentized type test is not easy in general , we propose a satterthwaite type approximation based on suitable doubly non central f distributions for finding such p values f.e. satterthwaite , an approximate distribution of estimates of variance components , biometrics <digit> ( <digit> ) , pp. <digit> <digit> . the approximations are evaluated through a simulation study , for example , for the detection of additive and innovative outliers as well as variance shifted outliers that do not affect the autocorrelation structure of the outlier free time series observations . some simulation results on model misspecification effects on outlier detection are also provided .
cursor movement control development by using anfis algorithm . <eos> our non invasive brain computer interface uses eeg signals and beta frequency bands over sensorimotor cortex to control cursor movement horizontally ( i.e. , one dimension ) . the main goal of this study is to help people with sever motor disabilities ( i.e. , spinal cord injuries ) and pro vide them a new way of communication and control options by which they can move the cursor in one dimension . in this study , offline analysis of the data collected was used to make the user able controlling the movement of the cursor horizontally ( i.e. , one dimension ) . the data was collected during a session in which the user selected among two targets by thinking and moving either the right hand little finger or the left hand little finger . the adaptive network based fuzzy inference system algorithm was examined for the classification method with some parameters . in the offline analysis , the method used showed a significant performance in the classification accuracy level and it gave an accuracy level of more than <digit> % . this result suggests that using the adoptive network based fuzzy inferences system algorithm will improve online operation of the current bci system .
images of health technology in national and local strategies . <eos> objectives this paper examines the potential of various models relating technology to society and institutional structures to inform health policy . among the models discussed are various versions of technological determinism , social constructivism , actor network theory and critical theory . methods the paper considers recent developments in policy and strategy that aim to shape the way the uk 's national health service ( nhs ) integrates information and communication technologies ( icts ) into health care and considers what these alternative models highlight or emphasise , and how they might influence the activities of setting local implementation strategies . results and conclusions contemporary icts are often presented as having a particular relevance and power in reforming or transforming the delivery of health care . understanding how such technologies might be conceived of , implemented and become an integral part of some future health care system is an important and challenging task that requires innovative theoretical treatments .
random heuristic search applications to gas and functions of unitation . <eos> describing a wide range of search methods at various levels of detail , the theory of random heuristic search speaks of their qualitative and quantitative aspects . this paper begins by outlining the theory , reviewing some of the more basic principles and results , and then goes on to illustrate its application by presenting both fine grained and coarse grained models for a genetic algorithm applied to functions of unitation . particular emphasis is given to the interrelationships between the models .
evolving model trees for mining data sets with continuous valued classes . <eos> this paper presents a genetic programming ( gp ) approach to extract symbolic rules from data sets with continuous valued classes , called gpmcc . the gpmcc makes use of a genetic algorithm ( ga ) to evolve multi variate non linear models potgieter , g. , engelbrecht , a. ( <digit> ) . genetic algorithms for the structural optimisation of learned polynomial expressions . applied mathematics and computation at the terminal nodes of the gp . several mechanisms have been developed to optimise the gp , including a fragment pool of candidate non linear models , k means clustering of the training data to facilitate the use of stratified sampling methods , and specialized mutation and crossover operators to evolve structurally optimal and accurate models . it is shown that the gpmcc is insensitive to control parameter values . experimental results show that the accuracy of the gpmcc is comparable to that of neurolinear and cubist , while producing significantly less rules with less complex antecedents .
key dependency for a wavelet based blind watermarking algorithm . <eos> when a host image is watermarked multiple times by the same algorithm collisions can occur . this makes it difficult for an image to host multiple watermarks . but this hosting is necessary for an image distribution chain , where several persons all watermark the same image . wavelet domain transformations provide several possibilities to customize the transformation process . we discuss the applicability of the methods of wavelet filter parametrization and wavelet packet decomposition for secret watermark embedding on the algorithm of dugad et al. we conclude that filter parametrization is not suited while wavelet packet decomposition shows good results .
finitely additive extensions of distribution functions and moment sequences the coherent lower prevision approach . <eos> we study the information that a distribution function provides about the finitely additive probability measure inducing it . we show that in general there is an infinite number of finitely additive probabilities associated with the same distribution function . secondly , we investigate the relationship between a distribution function and its given sequence of moments . we provide formulae for the sets of distribution functions , and finitely additive probabilities , associated with some moment sequence , and determine under which conditions the moments determine the distribution function uniquely . we show that all these problems can be addressed efficiently using the theory of coherent lower previsions . ( c ) <digit> elsevier inc. all rights reserved .
an application of heterogeneous agents to fabricate large , realistic corporate transaction data sets for data mining tool testing and evaluation . <eos> we describe methods used to specify and instantiate hundreds of heterogeneous agents and their use in a simulation of national trade and shipping . the agents , representing synthetic corporate entities , interacted to produce hundreds of thousands of trade transaction documents . the goal for the system was for the corpus of documents to evidence diverse , but realistic linkage patterns of corporate entities engaged in emergent shipping behaviors . we then used the documents to test and evaluate a data mining tool that purported to be able to detect these types of behavioral patterns . our contributions include a design algorithm for a heterogeneous mas that produces multi featured outcomes , and a method for instantiating realistic heritages and preferences of agents that extends recent work in heterogeneous random utility modeling .
comparative study of approximation algorithms and heuristics for sinr scheduling with power control . <eos> various recent theoretical studies have achieved considerable progress in understanding combined link scheduling and power control in wireless networks with sinr constraints . these analyses were mainly focused on designing and analyzing approximation algorithms with provable approximation guarantees . while these studies revealed interesting effects from a theoretical perspective , so far there has not been a systematic evaluation of the theoretical results in simulations . in this paper , we examine the performance of various approximation algorithms and heuristics for the common scheduling problems on instances generated by different random network models , e.g. , taking clustering effects into account . using ( mixed ) integer linear programming , we are able to compute the theoretical optima for some of these instances such that the performance of the different algorithms can be compared with these optima . the simulations support the practical relevance of the theoretical findings . for example , setting transmission powers by a square root power assignment , the network 's capacity increases significantly in comparison to uniform power assignments . furthermore , the developed approximation algorithms are able to exploit this gap providing in general a better performance than any algorithm using uniform transmission powers , even with unlimited computational power . the obtained results are robust against changes in parameters and network generation models .
robust design optimization by polynomial dimensional decomposition . <eos> this paper introduces four new methods for robust design optimization ( rdo ) of complex engineering systems . the methods involve polynomial dimensional decomposition ( pdd ) of a high dimensional stochastic response for statistical moment analysis , a novel integration of pdd and score functions for calculating the second moment sensitivities with respect to the design variables , and standard gradient based optimization algorithms . new closed form formulae are presented for the design sensitivities that are simultaneously determined along with the moments . the methods depend on how statistical moment and sensitivity analyses are dovetailed with an optimization algorithm , encompassing direct , single step , sequential , and multi point single step design processes . numerical results indicate that the proposed methods provide accurate and computationally efficient optimal solutions of rdo problems , including an industrial scale lever arm design .
circadian rhythm changes in heart rate variability during chronic sound stress . <eos> to study the circadian rhythm changes of the heart rate variability ( hrv ) during chronic sound stress , wistar rats were implanted with telemetry transmitters and exposed to chronic ultrasound stress for <digit> days . the heart rate , mean r r intervals ( mean r r ) and body temperature were monitored hourly . the spectra of five minute heart rate variability were plotted on a log log scale of frequency versus power spectral density , and the spectral exponent beta of the regression line of this plot was calculated . the exponent beta , heart rate ( hr ) and body temperature recorded hourly were plotted and fitted to sine curves to observe the circadian rhythm of these parameters . the correlation coefficient of the fitted sine curves in beta decreased from 0.644 in the control period to 0.105 in the stress period , indicating that the rhythm of beta deceased during stress . this did not occur in the other two parameters , demonstrating that the hrv exponent can assess chronic stress .
editable polycube map for gpu based subdivision surfaces . <eos> in this paper we propose an editable polycube mapping method that , given an arbitrary high resolution polygonal mesh and a simple polycube representation plus optional sketched features indicating relevant correspondences between the two , provides a uniform , regular and artist controllable quads only mesh with a parameterized subdivision scheme . the method introduces a global parameterization , based on a divide and conquer strategy , which allows to create polycube maps with a much smaller number of patches , and gives much more control over the quality of the induced subdivision surface . all this makes it practical for real time rendering on modern hardware ( e.g. ogl 4.1 and d3d11 tessellation hardware ) . by sketching these correspondence features , processing large scale models with complex geometry and topology is now feasible . this is crucial for obtaining watertight displaced catmull clark subdivision surfaces and high quality texturing on real time applications .
beyond schema evolution to database reorganization . <eos> while the contents of databases can be easily changed , their organization is typically extremely rigid . some databases relax the rigidity of database organization somewhat by supporting simple changes to individual schemas . as described in this paper , otgen supports not only more complex schema changes , but also database reorganization . a database administrator uses a declarative notation to describe mappings between objects created with old versions of schemas and their corresponding representations using new versions . otgen generates a transformer that applies the mappings to update the database to the new definitions , thus facilitating improvements in performance , functionality , and usability of the database . <digit>
motor pattern selection by combinatorial code of interneuronal pathways . <eos> we use a modeling approach to examine ideas derived from physiological network analyses , pertaining to the switch of a motor control network between two opposite control modes . we studied the femurtibia joint control system of the insect leg , and its switch between resistance reflex in posture control and active reaction in walking , both elicited by the same sensory input . the femurtibia network was modeled by fitting the responses of model neurons to those obtained in animals . the strengths of <digit> interneuronal pathways that integrate sensory input were then assigned three different values and varied independently , generating a database of more than <digit> million network variants . we demonstrate that the same neural network can produce the two different behaviors , depending on the combinatorial code of interneuronal pathways . that is , a switch between behaviors , such as standing to walking , can be brought about by altering the strengths of selected sensory integration pathways .
animal models got you puzzled think pig . <eos> swine are an excellent large animal model for human health and disease because their size and physiology are similar to humans , in particular , with respect to the skin , heart , gastrointestinal tract , and kidneys . in addition , the pig has many emerging technologies that will only enhance the development of the pig as the nonrodent biomedical model of choice .
multimodal neuroimaging computing a review of the applications in neuropsychiatric disorders . <eos> multimodal neuroimaging is increasingly used in neuroscience research , as it overcomes the limitations of individual modalities . one of the most important applications of multimodal neuroimaging is the provision of vital diagnostic data for neuropsychiatric disorders . multimodal neuroimaging computing enables the visualization and quantitative analysis of the alterations in brain structure and function , and has reshaped how neuroscience research is carried out . research in this area is growing exponentially , and so it is an appropriate time to review the current and future development of this emerging area . hence , in this paper , we review the recent advances in multimodal neuroimaging ( mri , pet ) and electrophysiological ( eeg , meg ) technologies , and their applications to the neuropsychiatric disorders . we also outline some future directions for multimodal neuroimaging where researchers will design more advanced methods and models for neuropsychiatric research .
an ic manufacturing yield model considering intra die variations . <eos> in deep submicron feature sizes continue to shrink aggressively beyond the natural capabilities of the <digit> nm lithography used to produce those features thanks to all the innovations in the field of resolution enhancement techniques ( ret ) . with reduced feature sizes and tighter pitches die level variations become an increasingly dominant factor in determining manufacturing yield . thus a prediction of design specific features that impact intra die variability and correspondingly its yield is extremely valuable as it allows for altering such features in a manner that reduces intra die variability and improves yield . in this paper , a manufacturing yield model which takes into account both physical layout features and manufacturing fluctuations is proposed . the intra die systematic variations are evaluated using a physics based model as a function of a design 's physical layout . the random variations and their across die spatial correlations are obtained from data harvested from manufactured test structures . an efficient algorithm is proposed to reduce the order of the numerical integration in the yield model . the model can be used to ( i ) predict manufacturing yields at the design stage and ( ii ) enhance the layout of a design for higher manufacturing yield .
on selecting an optimal wavelet for detecting singularities in traffic and vehicular data . <eos> serving as a powerful tool for extracting localized variations in non stationary signals , applications of wavelet transforms ( wts ) in traffic engineering have been introduced however , lacking in some important theoretical fundamentals . in particular , there is little guidance provided on selecting an appropriate wt across potential transport applications . this research described in this paper contributes uniquely to the literature by first describing a numerical experiment to demonstrate the shortcomings of commonly used data processing techniques in traffic engineering ( i.e. , averaging , moving averaging , second order difference , oblique cumulative curve , and short time fourier transform ) . it then mathematically describes wts ability to detect singularities in traffic data . next , selecting a suitable wt for a particular research topic in traffic engineering is discussed in detail by objectively and quantitatively comparing candidate wavelets performances using a numerical experiment . finally , based on several case studies using both loop detector data and vehicle trajectories , it is shown that selecting a suitable wavelet largely depends on the specific research topic , and that the mexican hat wavelet generally gives a satisfactory performance in detecting singularities in traffic and vehicular data .
haskell program coverage . <eos> we describe the design , implementation and use of hpc , a tool kit to record and display haskell program coverage . hpc includes tools that instrument haskell programs to record program coverage , run instrumented programs , and display information derived from coverage data in various ways .
the zeta image , illuminant estimation , and specularity manipulation . <eos> a novel log chromaticity illumination constraint we call the zeta image . the new feature is equivalent to a novel application of the kullbackleibler divergence . for illumination estimation the method outperforms other unsupervised methods . post processing any color constancy method , that methods accuracy is improved . using the zeta image we are able to manipulate specular content in the image .
binary synthesis . <eos> recent high level synthesis approaches and c based hardware description languages attempt to improve the hardware design process by allowing developers to capture desired hardware functionality in a well known high level source language . however , these approaches have yet to achieve wide commercial success due in part to the difficulty of incorporating such approaches into software tool flows . the requirement of using a specific language , compiler , or development environment may cause many software developers to resist such approaches due to the difficulty and possible instability of changing well established robust tool flows . thus , in the past several years , synthesis from binaries has been introduced , both in research and in commercial tools , as a means of better integrating with tool flows by supporting all high level languages and software compilers . binary synthesis can be more easily integrated into a software development tool flow by only requiring an additional backend tool , and it even enables completely transparent dynamic translation of executing binaries to configurable hardware circuits . in this article , we survey the key technologies underlying the important emerging field of binary synthesis . we compare binary synthesis to several related areas of research , and we then describe the key technologies required for effective binary synthesis decompilation techniques necessary for binary synthesis to achieve results competitive with source level synthesis , hardware software partitioning methods necessary to find critical binary regions suitable for synthesis , synthesis methods for converting regions to custom circuits , and binary update methods that enable replacement of critical binary regions by circuits .
a note on the existence and uniqueness of mild solutions to neutral stochastic partial functional differential equations with non lipschitz coefficients . <eos> in this note , we study the existence and uniqueness of mild solutions to neutral stochastic partial functional differential equations under some carathodory type conditions on the coefficients by means of the successive approximation . in particular , we generalize and improve the results that appeared in govindan t.e. govindan , almost sure exponential stability for stochastic neutral partial functional differential equations , stochastics <digit> ( <digit> ) <digit> and bao and hou j. bao , z. hou , existence of mild solutions to stochastic neutral partial functional differential equations with non lipschitz coefficients , comput . math . appl . <digit> ( <digit> ) <digit> .
patinformatics tasks to tools . <eos> this article starts with an overview of the field of patinformaticsthe science of analyzing patent information to discover relationships and trends . this is followed by a survey of many common analysis tasks in this field , and many of the software tools available to tackle these tasks . the survey is set out under the tasks of list cleanup and grouping of concepts list generation co occurrency matrices and circle graphs clustering of structured data clustering of unstructured data mapping document clusters adding temporal component to cluster map citation analysis subject action object functions . the author concludes that patinformatics has developed very rapidly over the last few years , and provides continuing challenges and opportunities in making optimal use of the resources available to achieve reliable and meaningful results . useful tables summarizing aspects of this survey are included .
modular static scheduling of synchronous data flow networks . <eos> this paper addresses the question of producing modular sequential imperative code from synchronous data flow networks . precisely , given a system with several input and output flows , how to decompose it into a minimal number of classes executed atomically and statically scheduled without restricting possible feedback loops between input and output though this question has been identified by raymond in the early years of lustre , it has almost been left aside until the recent work of lublinerman , szegedy and tripakis . the problem is proven to be intractable , in the sense that it belongs to the family of optimization problems where the corresponding decision problem there exists a solution with size c is np complete . then , the authors derive an iterative algorithm looking for solutions for c 1,2 , aeuro broken vertical bar where each step is encoded as a satisfiability ( sat ) problem . despite the apparent intractability of the problem , our experience is that real programs do not exhibit such a complexity . based on earlier work by raymond , the current paper presents a new encoding of the problem in terms of input output relations . this encoding simplifies the problem , in the sense that it rejects some solutions , while keeping all the optimal ones . it allows , in polynomial time , ( <digit> ) to identify nodes for which several schedules are feasible and thus are possible sources of combinatorial explosion ( <digit> ) to obtain solutions which in some cases are already optimal ( <digit> ) otherwise , to get a non trivial lower bound for c to start an iterative combinatorial search . the method has been validated on several industrial examples . the solution applies to a large class of block diagram formalisms based on atomic computations and a delay operator , ranging from synchronous languages such as lustre or scade to modeling tools such as simulink .
exact and approximate construction of offset polygons . <eos> the minkowski sum of two sets a , b r2 a , b r <digit> , denoted a b a b , is defined as a b a a , b b a b a a , b b . we describe an efficient and robust implementation of the construction of the minkowski sum of a polygon in r2 r <digit> with a disc , an operation known as offsetting the polygon . our software package includes a procedure for computing the exact offset of a straight edge polygon , based on the arrangement of conic arcs computed using exact algebraic number types . we also present a conservative approximation algorithm for offset computation that uses only rational arithmetic and decreases the running times by an order of magnitude in some cases , while having a guarantee on the quality of the result . the package will be included in the next public release of the computational geometry algorithms library , cgalversion 3.3 . it also integrates well with other cgalpackages in particular , it is possible to perform regularized boolean set operations on the polygons the offset procedures generate .
ontology based affective context representation . <eos> in this paper we propose an ontology based representation of the affective states for context aware applications that allows expressing the complex relations that are among the affective states and between these and the other context elements . this representation is open to map different affective spaces basic and secondary states relation ( using fuzzy logic ) , the relation between these states and other context elements as location , time , person , activity etc. the proposed affective context model is encoded in owl . due to difficulties in direct detection of the secondary affective states we propose a method to infer the characteristic values of these states from other context elements values . the deduces states are used here to improve the behavior of a context aware museum guide in order to react more intuitively and more intelligent by taking into account the users affective states .
biased mutation operators for subgraph selection problems . <eos> many graph problems seek subgraphs of minimum weight that satisfy a set of constraints . examples include the minimum spanning tree problem ( mstp ) , the degree constrained minimum spanning tree problem ( d mstp ) , and the traveling salesman problem ( tsp ) . low weight edges predominate in optimum solutions to such problems , and the performance of evolutionary algorithms ( eas ) is often improved by biasing variation operators to favor these edges . we investigate the impact of biased edge exchange mutation . in a large scale empirical investigation on euclidean and uniform random instances , we describe the distributions of edges in optimum solutions of the mstp , the d mstp , and the tsp in terms of the edges ' weight based ranks . we approximate these distributions by exponential functions and derive approximately optimal probabilities for selecting edges to be incorporated into candidate solutions during mutation . a theoretical analysis of the expected running time of a ( <digit> <digit> ) ea on nondegenerate instances of the mstp shows that when using the derived probabilities for edge selection in mutation , the ( <digit> <digit> ) ea is asymptotically as fast as a classical implementation of kruskal 's minimum spanning tree algorithm . in experiments on the mstp , d mstp , and the tsp , we compare the new edge selection strategy to four alternative methods . the results of a ( <digit> <digit> ) ea on instances of the mstp support the theory and indicate that the new strategy is superior to the other methods in practice . on instances of the d mstp , a more sophisticated ea with a larger population and unbiased recombination performs better with the new biased mutation than with alternate mutations . on the tsp , the advantages of weight biased mutation are generally smaller , because the insertion of a specific new edge into a tour requires the insertion of a second dependent edge as well . although we considered euclidean and uniform random instances only , we conjecture that the same biasing toward low weight edges also works well on other instance classes structured in different ways .
discrete approximations to real valued leaf sequencing problems in radiation therapy . <eos> for a given mn m n nonnegative real matrix a a , a segmentation with <digit> norm relative error e e is a set of pairs ( , s ) ( <digit> , s1 ) , ( <digit> , s2 ) , , ( k , sk ) ( , s ) ( <digit> , s <digit> ) , ( <digit> , s <digit> ) , , ( k , s k ) , where each i i is a positive number and si s i is an mn m n binary matrix , and e a i <digit> k i s i <digit> a <digit> , where a <digit> a <digit> is the <digit> norm of a vector which consists of all the entries of the matrix a a . in certain radiation therapy applications , given a a and positive scalars , , , we consider the optimization problem of finding a segmentation ( , s ) ( , s ) that minimizes z i <digit> k i k e subject to certain constraints on si s i . this problem poses a major challenge in preparing a clinically acceptable treatment plan for intensity modulated radiation therapy ( imrt ) and is known to be np hard . known discrete imrt algorithms use alternative objectives for this problem and an l l level entrywise approximation a ( i.e.each entry in a a is approximated by the closest entry in a set of l l equally spaced integers ) , and produce a segmentation that satisfies a i <digit> k i s i . in this paper we present two algorithms that focus on the original non discretized intensity matrix and consider measures of delivery quality and complexity ( i k ) ( i k ) as well as approximation error e e . the first algorithm uses a set partitioning approach to approximate a a by a matrix a that leads to segmentations with smaller k k for a given e e . the second algorithm uses a constrained least square approach to post process a segmentation ( i , s i ) of a to replace i with real valued i i in order to reduce k k and e e .
gpu based computation of discrete periodic centroidal voronoi tessellation in hyperbolic space . <eos> periodic centroidal voronoi tessellation ( cvt ) in hyperbolic space provides a nice theoretical framework for computing the constrained cvt on high genus ( genus > <digit> ) surfaces . this paper addresses two computational issues related to such a hyperbolic cvt framework ( <digit> ) efficient reduction of unnecessary site copies in neighbor domains on the universal covering space , based on two special rules ( <digit> ) gpu based parallel algorithms to compute a discrete version of the hyperbolic cvt . our experiments show that with the dramatically reduced number of unnecessary site copies in neighbor domains and the gpu based parallel algorithms , we significantly speed up the computation of cvt for high genus surfaces . the proposed discrete hyperbolic cvt guarantees to converge and produces high quality results .
efficient transient analysis of markovian models using a block reduction approach . <eos> one of the most widely used techniques to obtain transient measures is the uniformization method . however , although uniformization has many advantages , the computational cost required to calculate transient probabilities is very large for stiff models . we study efficient solutions that can be applied to an approximate method developed for calculating transient state probabilities of markov models and cumulative expected reward measures over a finite interval . our work is based on a method that approximates the state probabilities at time t by the state probabilities calculated at a random time with erlangian distribution . the original method requires an inversion of a matrix obtained from the state transition rate matrix that destroys special structures such as sparseness and banded matrices . this precludes the use of the technique for large models . in our work we propose efficient solutions that can take advantage of special structures . finally , we present examples that show that the proposed technique is computationally very efficient for stiff models when compared with uniformization .
feature selection for improved 3d facial expression recognition . <eos> an entropy based feature selection process for 3d facial expression recognition is proposed . mpeg <digit> facial definition parameters are used as a base for feature selection . two level svm classifier system is employed to classify six basic expressions of the face . tests are performed on bu 3dfe database and the system achieves <digit> % average recognition rate .
a data driven stochastic approach to model and analyze test data on fatigue response . <eos> a stochastic approach to model and analyze test data on the fatigue response of materials and laminated composites is developed . the developed approach is data driven in nature . it has been customary to describe the fatigue response of metallic and laminated composite materials using a suitable parameter that can serve as the indicator and descriptor of damage accumulation . in the present methodology , the fatigue response of the material is quantified by interpreting the corresponding material parameter to be an embedded markov process . the true probability distributions of the fatigue response parameter are extracted from sample test data based on an analytical approach , and they are used in the formulation . to this end , the maximum entropy method is incorporated into the formulation . a recursive stochastic matrix equation is developed based on the test data using the theory of reliability and fokkerplankkolmogorov equation . application of the methodology to a composite laminate is demonstrated .
analyzing requirements evolution in engineering design using the method of problem reduction . <eos> traditional requirements definition activities begin with the engineer or design team performing a needs analysis to identify user requirements <digit> . while recent studies have focused on conceptual design activities , research into the requirements definition pro cess has for the most part been lacking . needs analysis is generally subjective , and varies according to the composition and experience of the design team . systematic procedures for defining and ranking requirements could consolidate the foundation on which the design process is predicated and enhance its outcome by providing the designer with a consistent , reliable approach to product development <digit> . before such systematic procedures could be developed , it would be necessary to establish an understanding of the existing process by which requirements evolve . and to create a model for evaluating this process . therefore , a pilot study was conducted at stanford university using empirical data from an actual spaceflight experiment , sponsored by nasa ames research center ( arc ) , and flown aboard the space shuttle . a large body of empirical evidence was examined , and on the basis of this evidence , the method of problem reduction using and or graphs proved to be an effective framework for analyzing requirements evolution .
choosing the optimal set of instruments from large instrument sets . <eos> it is well known that instrumental variables ( iv ) estimation is sensitive to the choice of instruments both in small samples and asymptotically . recently , a simple method has been suggested in the literature for choosing the instrument set . the method involves minimising the approximate mean square error ( mse ) of a given iv estimator where the mse is obtained using refined asymptotic theory . an issue with this method is the fact that when considering large sets of valid instruments , it is not clear how to order the instruments in order to choose which ones ought to be included in the estimation . a possible solution to the problem using nonstandard optimisation algorithms is provided . the properties of the algorithms are discussed . a monte carlo study illustrates the potential of the new method . ( c ) <digit> elsevier b.v. all rights reserved .
a hybrid mac scheme to improve the transmission performance in body sensor networks . <eos> wireless body sensor networks ( wbsns ) constitute a key technology for closing the loop between patients and healthcare providers , as wbsns provide sensing ability , as well as mobility and portability , essential characteristics for wide acceptance of wireless healthcare technology . however , one important and difficult aspect of wbsns is to provide data transmissions with quality of service , among other factors due to the antennas being small size and placed close to the body . such transmissions can not be fully provided without the assumption of a mac protocol that solves the problems of the medium sharing . a vast number of mac protocols conceived for wireless networks are based on random or scheduled schemes . this paper studies firstly the suitability of two mac protocols , one using csma and the other tdma , to transmit directly to the base station the signals collected continuously from multiple sensor nodes placed on the human body . tests in a real scenario show that the beaconed tdma mac protocol presents an average packet loss ratio lower than csma . however , the average packet loss ratio is above 1.0 % . to improve this performance , which is of vital importance in areas such as e health and ambient assisted living , a hybrid tdma csma scheme is proposed and tested in a real scenario with two wbsns and four sensor nodes per wbsn . an average packet loss ratio lower than 0.2 % was obtained with the hybrid scheme . to achieve this significant improvement , the hybrid scheme uses a lightweight algorithm to control dynamically the start of the superframes . scalability and traffic rate variation tests show that this strategy allows approximately ten wbsns operating simultaneously without significant performance degradation .
playing the language games of design and use on skill and participation . <eos> this paper deals with computers and cooperative work . focus in not on applications for cooperative work , but on the cooperative process of designing such and other computer applications . focus is on the role of skill and participation in design as a creative and communicative process . the paper suggests a need to go beyond the cartesian philosophical assumptions of rationalistic reasoning as epistemology and dualism as ontology , so strongly embedded in traditional design methods . there are many philosophical candidates for such a reinterpretation . in this paper i have chosen to elaborate on language games and the ordinary language philosophy of ludwig wittgenstein . hence , focus is on the shift in design from language as description towards language as action . some consequences of such a shift is illustrated with reflections on examples from utopia ( a research and development project for skill enhancing computer based tools for graphic workers ) , and with design ideas on an application simulator from a new research programme on cooperative design and communication .
co sizing of an electromechanical device by using optimisation process . <eos> purpose this paper deals with the collaborative design of electromagnetic devices over the internet network . the design is made by both mechanical and electrical engineers . so , the paper tries to show the importance but also constraints to size such a system using a collaborative optimisation process . design methodology approach the paper compares two approaches in order to size an electromechanical actuator between mechanical and electrical engineers . in the first one , each profession designs its part , and only common constrained are negotiated . this can result in a design process with many iterations . in the second one , electrical and mechanical engineers built together a common model of the structure and a common list of specifications this allows a global optimisation that is more efficient . findings the main result of the paper is that the second approach in which a global model is built between electrical and mechanical engineers is more efficient . originality value the originality of the paper is to explore the problems and difficulties of an optimisation of an electromechanical device between engineers of different culture working together over the internet network .
an algorithm for trading off quantization error with hardware resources for matlab based fpga design . <eos> most practical fpga designs of digital signal processing ( dsp ) applications are limited to fixed point arithmetic owing to the cost and complexity of floating point hardware . while mapping dsp applications onto fpgas , a dsp algorithm designer must determine the dynamic range and desired precision of input , intermediate , and output signals in a design implementation . the first step in a matlab based hardware design flow is the conversion of the floating point matlab code into a fixed point version using quantizers from the filter design and analysis ( fda ) toolbox for matlab . this paper describes an approach to automate the conversion of floating point matlab programs into fixed point matlab programs , for mapping to fpgas by profiling the expected inputs to estimate errors . our algorithm attempts to minimize the hardware resources while constraining the quantization error within a specified limit . experimental results on five matlab benchmarks are reported for xilinx virtex ii fpgas .
an aig based qbf solver using sat for preprocessing . <eos> in this paper we present a solver for quantified boolean formulas ( qbfs ) which is based on and inverter graphs ( aigs ) . we use a new quantifier elimination method for aigs , which heuristically combines cofactor based quantifier elimination with quantification using bdds and thus benefits from the strengths of both data structures . moreover , we present a novel sat based method for preprocessing qbfs that is able to efficiently detect variables with forced truth assignments , allowing for an elimination of these variables from the input formula . we describe the used algorithm which heavily relies on the incremental features of modern sat solvers . experimental results demonstrate that our preprocessing method can significantly improve the performance of qbf preprocessing and thus is able to accelerate the overall solving process when used in combination with state of the art qbf solvers . in particular , we integrated the preprocessing technique as well as the quantifier elimination method into the qbf solver aigsolve , allowing it to outperform state of the art solvers .
a proactive wireless self protection system . <eos> though mobile computing systems constitute the core of the next generation ubiquitous pervasive services , they still have many flaws in their security . this paper describes a novel framework for wireless anomaly based intrusion detection and response system , which is capable of detecting complex malicious attacks . this framework is based on multi channel online monitoring and analysis of wireless network features with respect to multiple observation time windows . these features are related to data link layer frame behaviors and the mobility of stations . a general purpose wireless self protection system ( wsps ) is presented . wsps has the following modules wireless network probes , wireless features filtration and generation module , wireless network flow generator , behavior analysis module , and action module . wsps self protects against attacks by online monitoring and analyzing anomalies and misuses in the network features , and utilizes the low false alerts of the analysis module . the validation and effectiveness of this framework is carried out by experimenting with more than <digit> different types of wireless attacks using wireless lans ( wlans ) . our experimental results show that our approach can protect from wireless network attacks with average false positive rate of 2.234 % , and average detection rate of 99.13 % for all the experimented attacks .
data visualization optimization via computational modeling of perception . <eos> we present a method for automatically evaluating and optimizing visualizations using a computational model of human vision . the method relies on a neural network simulation of early perceptual processing in the retina and primary visual cortex . the neural activity resulting from viewing flow visualizations is simulated and evaluated to produce a metric of visualization effectiveness . visualization optimization is achieved by applying this effectiveness metric as the utility function in a hill climbing algorithm . we apply this method to the evaluation and optimization of 2d flow visualizations , using two visualization parameterizations streaklet based and pixel based . an emergent property of the streaklet based optimization is head to tail streaklet alignment . it had been previously hypothesized the effectiveness of head to tail alignment results from the perceptual processing of the visual system , but this theory had not been computationally modeled . a second optimization using a pixel based parameterization resulted in a lic like result . the implications in terms of the selection of primitives is discussed . we argue that computational models can be used for optimizing complex visualizations . in addition , we argue that they can provide a means of computationally evaluating perceptual theories of visualization , and as a method for quality control of display methods .
robust dissipative control for internet based switching systems . <eos> a class of hybrid multi rate control models with time delay and switching controllers are formulated based on combined remote control and local control strategies . the problem of robust dissipative control for this discrete system is investigated . an improved lyapunovkrasovskii functional is constructed and the subsequent analysis provides some new sufficient conditions in the form of lmis for both nominal and uncertain representations . several special cases of practical interests are derived . a numerical simulation example is given to illustrate the effectiveness of the theoretical result .
estimating receptive fields in the presence of spike time jitter . <eos> neurons in sensory systems are commonly characterized by their receptive fields . these are experimentally often obtained by reverse correlation analyses , for example , by calculating the spike triggered average . the reverse correlation approach , however , generally assumes a fixed temporal relation between spike generating stimulus features and measured spikes . temporal jitter of spikes will therefore distort the estimated receptive fields . here , a novel extension of widely used reverse correlation techniques ( spike triggered average as well as spike triggered covariance ) is presented that allows accurate measurements of receptive fields even in the presence of considerable spike time jitter . it is shown that the method correctly recovers the receptive fields from simulated spike trains . when applied to recordings from auditory receptor cells of locusts , a considerable sharpening of receptive fields as compared to standard spike triggered averages is observed . in addition , the multiple filters that are obtained from a conventional spike triggered covariance analysis of these data can be collapsed into a single component if spike jitter is accounted for . finally , it is shown how further effects on spike timing , such as systematic shifts in spike latency , can be included in the approach .
topological conceptual model of geological relative time scale for geoinformation systems . <eos> in geological information systems , the methods of encoding geological age , both relative and absolute , play an important role . interoperable exchange of information between these systems requires application of solutions based upon recognized international standards . the iso <digit> standard can serve as a basis for elaboration of a conceptual model for geological data , as well as its implementation , for instance in extensible markup language ( xml ) . the nature of the relative geological time scale requires application of topological elements in this model , and the model presented here is a complex construction of such elements . as a result , operations of the interface used for objects belonging to classes derived from the timetopologicalordinalera class may return a value in the form of topological time relations such as before , after , during and overlappedby .
the involvement of thyroid hormone metabolism in gilthead sea bream ( sparus auratus ) osmoregulation . <eos> abstract we have investigated the effect of adaptation to low salinity water on the thyroid status of the euryhaline teleost , sparus auratus . we show that , following low salinity adaptation , the plasma t4 concentration increases and branchial deiodination activities of t4 , t3 , and rt3 decrease . moreover , branchial and hepatic enzyme activities that are putatively involved in thyroid hormone metabolism respond differentially in low salinity conditions . our results indicate the involvement of thyroid hormones in sparus auratus osmoregulation . moreover , the gills appear well equipped to play an important role in the modulation of plasma thyroid hormone titers .
a lower bound formulation for the geometry and topology optimization of truss structures under multiple loading . <eos> in this contribution , we propose an effective formulation to address the stress based minimum volume problem of truss structures . starting from the lower bound formulation in topology optimization , the problem is further expanded to geometry optimization and multiple loading scenarios , and systematically reformulated to alleviate numerical difficulties related to the melting node effect and stress singularities . the subsequent simultaneous analysis and design ( sand ) formulation is well suited for a direct treatment by introducing a barrier function . using exact second derivatives , this difficult class of problem is solved by sequential quadratic programming with trust regions . these building blocks result into an integrated design process . two examples including a large scale application illustrate the robustness of the proposed formulation .
flexible and stretchable micro electrodes for in vitro and in vivo neural interfaces . <eos> microelectrode arrays ( meas ) are designed to monitor and or stimulate extracellularly neuronal activity . however , the biomechanical and structural mismatch between current meas and neural tissues remains a challenge for neural interfaces . this article describes a material strategy to prepare neural electrodes with improved mechanical compliance that relies on thin metal film electrodes embedded in polymeric substrates . the electrode impedance of micro electrodes on polymer is comparable to that of mea on glass substrates . furthermore , meas on plastic can be flexed and rolled offering improved structural interface with brain and nerves in vivo . meas on elastomer can be stretched reversibly and provide in vitro unique platforms to simultaneously investigate the electrophysiological of neural cells and tissues to mechanical stimulation . adding mechanical compliance to meas is a promising vehicle for robust and reliable neural interfaces .
study of temporal stationarity and spatial consistency of fmri noise using independent component analysis . <eos> spatial independent component analysis ( ica ) was used to study the temporal stationarity and spatial consistency of structured functional mri ( fmri ) noise . spatial correlations have been used in the past to generate filters for the removal of structured noise for each time course in an fmri dataset . it would be beneficial to produce a multivariate filter based on the same principles . ica is examined to determine if it has properties that are beneficial for this type of filtering . six fmri baseline datasets were decomposed via spatial ica . the time courses associated with each component were tested for wide sense stationarity using the wide sense stationarity quotient ( wss ) . each dataset was divided into three subsets and each subset was decomposed . the components of first and third subset were matched by the strength of their correlation . the components produced by ica were found to have largely nonstationary time courses . despite the temporal nonstationarity in the data , ica was found to produce consistent spatial components . the degree of correlation among components differed depending on the amount of dimension reduction performed on the data . it was found that a relatively small number of dimensions produced components that are potentially useful for generating a spatial fmri filter .
petri net based ftl architecture for parametric wcet estimation via ftl operation sequence derivation . <eos> a flash translation layer ( ftl ) provides file systems with transparent access to nand flash memory . although many applications running on it require real time guarantees , it is difficult to provide tight worst case execution time ( wcet ) bounds with conventional static wcet analysis since an ftl exhibits a large variance in execution time depending on its runtime state . parametric wcet analysis could be an effective alternative but it is also challenging to formulate a parametric wcet function for an ftl program because traditional ftl architecture does not properly model the runtime availability of flash resources in its code structure . to overcome such a limitation , we propose petri net based ftl architecture where a petri net explicitly specifies dependencies between ftl operations and the runtime resource availability . it comes with an ftl operation sequencer that derives at runtime the shortest sequence of ftl operations for servicing an incoming ftl request under the current resource availability . the sequencer computes the wcet of the request by merely summing the wcets of only those ftl operations in the sequence . our experimental results show the effectiveness of our ftl architecture . it allowed for tight wcet estimation that yielded wcets shorter by a factor of <digit> than statically analyzed ones .
students participation intention in an online discussion forum why is computer mediated interaction attractive . <eos> anecdotal evidence indicates that an online discussion forum may not be utilized to its full potential in enhancing the effectiveness and efficiency of teaching due to a lower than expected student participation rate . this paper seeks to identify the motivational behavioral factors influencing students intention to participate in an online discussion forums ( odf ) . drawing on the literature on social psychology and applying the theory of reasoned action , we develop a conceptual model of intention to participate in an online discussion forum and empirically test the hypotheses in a cross sectional quantitative survey . the findings indicate that expectancy on hedonic outcome and utilitarian outcome and peer pressure positively influence the participation intention of students . also , the perceived importance of learning positively moderates the relationship between utilitarian outcome expectancy and participation intention . theoretical and practical implications of the findings are discussed .
transient response of a plateliquid system under an aerial detonation simulations and experiments . <eos> the unsteady flow equations for air blasts were solved with a home house cfd code . the simulation of the plateliquid system under this blast load has been validated . space and time scales of the cfd and structural models are made compatible . experimental strains and pressures show good agreement with the simulations .
on equations for union free regular languages . <eos> in this paper we consider the variety uf , generated by all algebras of binary relations equipped with the operations of composition , reflexive transitive closure , and the empty set and the identity relation as constants . this variety coincides with the variety generated by the union free reducts of kleene algebras of languages and its free objects are formed by union free regular languages , that is , regular languages represented by regular expressions having no occurrence of . we show that the variety uf is not finitely based . the situation does not change if we consider the variety ufv generated by the above algebras of binary relations equipped with the conversion operation . ( c ) <digit> academic press .
a reappraisal on advanced planning and scheduling systems . <eos> purpose this paper sets out to present a reappraisal on advanced planning and scheduling ( aps ) systems in industrial settings and propose an effective approach for aps implementation . design methodology approach a case study approach is adopted , and a research framework comprising human , technological , and organizational dimensions is developed to analyze the evidence database which includes business flows , system design documents , archival records , post system assessment , participant observation and semi structured interviews . findings the findings indicate that real world production planning problems are ill defined , complex and dynamic . a post implementation evaluation reveals major pitfalls in the technology dominant approach , whose negative ramifications are usually overlooked . besides , these aps implementation pitfalls are found to be attributable to the real world context , human factors and organizational aspects . research limitations implications despite advances in information technology ( it ) and computer modeling techniques , humans still play critical roles in the production planning processes especially in a complex and dynamic manufacturing environment where incomplete , ambiguous , inconsistent and untimely data make automatic planning unrealistic . a rational human computer collaboration scheme under an effective organizational structure would be in a better position to take advantage of the it . originality value this paper presents a humans technology organization framework of real planning systems , which is employed to analyze a case of aps implementation . practical insights are extracted as a result of this field research , and a realist approach is proposed to cope with the problems and pitfalls of aps implementation in industrial settings .
dominant parameter selection in the marginally identifiable case . <eos> often a rather limited set of experimental data is available for the identification of a dynamic model , which contains many parameters . this is , e.g. the usual case for crop growth models . in this situation , only some parameter values can be estimated . based on an analysis of the fisher information matrix , a method for a reasonable selection of parameters is suggested here . the method chooses the most sensitive parameters , i.e. those to which the model under the considered experimental conditions is most sensitive , and excludes both coupled parameters and those that exhibit multiplecorrelation . a comparison with different ridge regression methods is made . the methodology is illustrated with a simple lettuce growth model . ( c ) <digit> imacs . published by elsevier b.v all rights reserved .
parallels between the analytic hierarchy and network processes ( ahp anp ) and fractal geometry . <eos> the aim of this work is to show the parallelisms and analogies that exist in modeling and measuring of dependence and feedback processes , in physical and in decision making processes , that is , to compare among the scales of measurement of the physical world ( geometry ) and the scales of measurement of the human being 's internal decision process , in other words , the brain 's internal generation of a relative measure scale . ( c ) <digit> elsevier ltd. all rights reserved .
development of a hybrid methodology for dimensionality reduction in mahalanobis taguchi system using mahalanobis distance and binary particle swarm optimization . <eos> mahalanobis taguchi system ( mts ) is a pattern recognition method applied to classify data into categories healthy and unhealthy or acceptable and unacceptable . mts has found applications in a wide range of problem domains . dimensionality reduction of the input set of attributes forms an important step in mts . the current practice is to apply taguchi 's design of experiments ( doe ) and orthogonal array ( oa ) method to achieve this end . maximization of signal to noise ( sin ) ratio forms the basis for selection of the optimal combination of variables . however the doe oa method has been reviewed to be inadequate for the purpose in this research study , we propose a dimensionality reduction method by addressing the problem as feature selection exercise . the optimal combination of attributes minimizes a weighted sum of total fractional misclassification and the percentage of the total number of variables employed to obtain the misclassification . mahalanobis distances ( mds ) of healthy and unhealthy conditions are used to compute i he misclassification . a mathematical model formulates the feature selection approach and it is solved by binary particle swarm optimization ( pso ) . data from an indian foundry shop is adopted to test the mathematical model and the swarm heuristic . results are compared with that of doe oa method of mts . ( c ) <digit> elsevier ltd. all rights reserved
real time design and animation of fractal plants and trees . <eos> the goal of science is to understand why things are the way they are . by emulating the logic of nature , computer simulation programs capture the essence of natural objects , thereby serving as a tool of science . when these programs express this essence visually , they serve as an instrument of art as well.this paper presents a fractal computer model of branching objects . this program generates pictures of simple orderly plants , complex gnarled trees , leaves , vein systems , as well as inorganic structures such as river deltas , snow flakes , etc. the geometry and topology of the model are controlled by numerical parameters which are analogous to the organism 's dna . by manipulating the genetic parameters , one can modify the geometry of the object in real time , using tree based graphics hardware . the random effects of the environment are taken into account , to produce greater diversity and realism . increasing the number of significant parameters yields more complex and evolved species.the program provides a study in the structure of branching objects that is both scientific and artistic . the results suggest that organisms and computers deal with complexity in similar ways .
the fully implicit stochastic method for stiff stochastic differential equations . <eos> a fully implicit integration method for stochastic differential equations with significant multiplicative noise and stiffness in both the drift and diffusion coefficients has been constructed , analyzed and illustrated with numerical examples in this work . the method has strong order 1.0 consistency and has user selectable parameters that allow the user to expand the stability region of the method to cover almost the entire drift diffusion stability plane . the large stability region enables the method to take computationally efficient time steps . a system of chemical langevin equations simulated with the method illustrates its computational efficiency .
looking beyond learning notes towards the critical study of educational technology . <eos> this paper makes a case for academic research and writing that looks beyond the learning potential of technology and , instead , seeks to develop social scientific accounts of the often compromised and constrained realities of education technology use on the ground . the paper discusses how this critical approach differs from the ways that educational technology scholarship has tended to be pursued to date . these differences include viewing technology as being socially constructed and negotiated rather than imbued with pre determined characteristics developing objective and realistic accounts of technology use in situ and producing context rich analyses of the social conflicts and politics that underpin the use of technology in educational settings . the paper concludes by encouraging academic researchers and writers to show greater interest in the issues of democracy and social justice that surround educational technology .
a soft computing method to predict sludge volume index based on a recurrent self organizing neural network . <eos> the structure of rsonn can be self organized based on the contributions of each hidden node , which uses not only the past states but also the current states . the appropriately adjusted learning rates of rsonn is derived based on the lyapunov stability theorem . moreover , the convergence of the proposed rsonn is discussed . an experimental hardware , including the proposed soft computing method is set up . the experimental results have confirmed that the soft computing method exhibits satisfactory predicting performance for svi .
varieties of helmholtz machine . <eos> the helmholtz machine is a new unsupervised learning architecture that uses top down connections to build probability density models of input and bottom up connections to build inverses to those models . the wake sleep learning algorithm for the machine involves just the purely local delta rule . this paper suggests a number of different varieties of helmholtz machines , each with its own strengths and weaknesses , and relates them to cortical information processing . copyright <digit> elsevier science ltd .
cyclophosphamides as hypoxia activated diffusible cytotoxins a theoretical study . <eos> cyclophosphamides have been in clinical use as anti cancer drugs for a long time and much research has been directed towards reducing their side effects . here we have performed a theoretical investigation into the possibility of designing bioreductive analogues of cyclophosphamides . our calculations have employed semiempirical molecular orbital am1 sm2 and pm3 sm3 calculations , as implemented in mopac <digit> , which include a modified born method for the treatment of solvation . we have investigated the effect of bioreductive activation on the beta elimination reaction that is central to the activation of cyclophosphamides . the approach was tested on two known bioreductive agents , including cb1954 , and gave results in agreement with experiment . non local density functional calculations on cb1954 and its metabolites , including the radical anion , were in agreement with the semiempirical calculations . the calculations have identified a number of potentially novel bioreductive cyclophosphamides . in particular , our calculations identified compounds in which the initial one electron reduction was not activating . such compounds are likely to be more effective bioreductive agents , as the beta elimination will not compete under oxic conditions with the important re oxidation required for the protection of oxic tissue .
power aware operand delivery . <eos> based on operand delivery , existing microprocessors can be categorized into architected register file ( arf ) or physical register file ( prf ) machines , both with or without payload ram ( pl ) . though many previous generation microprocessors use a prf without pl , the trend of newer microprocessors targeting lower power environments seem to be moving towards arf with pl . we quantitatively analyze power consumption of different machine styles arf with pl , arf without pl , prf with pl , and prf only machine . our result shows that prf without pl consumes the least amount of power and is fundamentally the best approach for building power aware out of order microprocessors .
nanomechanical strength mechanisms of hierarchical biological materials and tissues . <eos> biological protein materials ( bpms ) , intriguing hierarchical structures formed by assembly of chemical building blocks , are crucial for critical functions of life . the structural details of bpms are fascinating they represent a combination of universally found motifs such as helices or sheets with highly adapted protein structures such as cytoskeletal networks or spider silk nanocomposites . bpms combine properties like strength and robustness , self healing ability , adaptability , changeability , evolvability and others into multi functional materials at a level unmatched in synthetic materials . the ability to achieve these properties depends critically on the particular traits of these materials , first and foremost their hierarchical architecture and seamless integration of material and structure , from nano to macro . here , we provide a brief review of this field and outline new research directions , along with a review of recent research results in the development of structure property relationships of biological protein materials exemplified in a study of vimentin intermediate filaments .
dense crystalline dimer packings of regular tetrahedra . <eos> we present the densest known packing of regular tetrahedra with density phi <digit> <digit> 0.856341 ... . like the recently discovered packings of kallus et al. and torquato jiao , our packing is crystalline with a unit cell of four tetrahedra forming two triangular dipyramids ( dimer clusters ) . we show that our packing has maximal density within a three parameter family of dimer packings . numerical compressions starting from random configurations suggest that the packing may be optimal at least for small cells with up to <digit> tetrahedra and periodic boundaries .
a generic architecture to synchronise design models issued from heterogeneous business tools towards more interoperability between design expertises . <eos> product development involves many experts collaborating to the same design goal . every expert has his own formalisms and tools leading to a high heterogeneity of information systems supporting design activities . interoperability became a major challenge to avoid information incompatibility along the product life cycle . to synchronise heterogeneous representations of product will be a major step to integrate expert activities . in this paper , the authors propose a meta model framework to connect together heterogeneous design models . this meta model framework is used to formalise possible interactions between heterogeneous representations . interaction formalisation is considered as a key point to synchronise heterogeneous models and to provide more interoperability between various computer assisted systems . the synchronisation loop is also presented as a major sequence of activities to manage collaborative design . tools to support synchronisation are proposed . however , through a basic case study , authors highlight what can be automated and where human intervention is still expected .
oh behave agent based behavioral representations in problem solving environments . <eos> the development of deregulated electricity systems around the world has produced the need for simulation systems that are capable of addressing the complexities that arise in the new markets . agent based models allow the use of complex adaptive systems approaches that are capable of producing tools or problem solving environments that can address the behavior of each of the participants within the electricity market . the agents in the tools are allowed to establish their own objectives and apply their own decision rules . they can be developed to learn from their previous experiences and change their behavior when future opportunities arise . in this paper , we will argue that the same type of agent based technology that is used to produce realistic agent behavior in agent based simulation tools at argonne national laboratory can also be used to embed these tools in problem solving environments .
keypoint recognition using randomized trees . <eos> in many 3d object detection and pose estimation problems , runtime performance is of critical importance . however , there usually is time to train the system , which we will show to be very useful . assuming that several registered images of the target object are available , we developed a keypoint based approach that is effective in this context by formulating wide baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem . this shifts much of the computational burden to a training phase , without sacrificing recognition performance . as a results , the resulting algorithm is robust , accurate , and fast enough for frame rate performance . this reduction in runtime computational complexity is our first contribution . our second contribution is to show that , in this context , a simple and fast keypoint detector suffices to support detection and tracking even under large perspective and scale variations . while earlier methods require a detector that can be expected to produce very repeatable results , in general , which usually is very time consuming , we simply find the most repeatable object keypoints for the specific target object during the training phase . we have incorporated these ideas into a real time system that detects planar , nonplanar , and deformable objects . it then estimates the pose of the rigid ones and the deformations of the others .
driver behaviour at roadworks . <eos> road networks around the world are reaching a critical stage in their lifecycle . transport authorities are planning significant maintenance activities with associated roadworks and traffic management . traffic microsimulation is used to plan these roadworks but modelled drivers are not behaving in the same way as real drivers . a range of psychological explanations for this difference are reviewed . guidance for incorporating these psychological factors into future models is proposed .
sally a tool for embedding strings in vector spaces . <eos> strings and sequences are ubiquitous in many areas of data analysis . however , only few learning methods can be directly applied to this form of data . we present sally , a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data . sally implements a generalized form of the bag of words model , where strings are mapped to a vector space that is spanned by a set of string features , such as words or n grams of words . the implementation of sally builds on efficient string algorithms and enables processing millions of strings and features . the tool supports several data formats and is capable of interfacing with common learning environments , such as weka , shogun , matlab , or pylab . sally has been successfully applied for learning with natural language text , dna sequences and monitored program behavior .
photon splatting for participating media . <eos> since the beginning of image synthesis , much research have been done on global illumination simulation . however , simulation in participating media is still an open problem as far as computing time is concerned . recently some methods , like photon mapping , proposed an optimization of the resolution of global illumination in participating media . nevertheless , the computing costs of these methods remain very expensive.in this paper , we present a method which takes advantage of density estimation to efficiently reconstruct volume irradiance from the photon map . our idea is to separate the computation of emission , absorption and out scattering from the computation of in scattering . then we use a dual approach of density estimation to optimize this last part as it is the most computational expensive . our method extends photon splatting , which optimizes the computation time of photon mapping for surface rendering , to participating media , and then considerably reduce participating media rendering times . even though our method is faster than photon mapping for equal quality , we also propose a gpu based optimization of our algorithm .
a genetic algorithm based approach for simultaneously balancing and sequencing of mixed model u lines with parallel workstations and zoning constraints . <eos> this paper presents a priority based genetic algorithm ( pga ) based method for the simultaneously tackling of the mixed model u shape assembly line ( mmul ) line balancing model sequencing problems ( mmul bs ) with parallel workstations and zoning constraints and allows the decision maker to control the process to create parallel workstations and to work in different scenarios . in the presented method , simulated annealing based fitness evaluation approach ( sabfea ) is developed to be able to make fitness function calculations easily and effectively . a new fitness function is adapted to mmuls for aiming at minimizing the number of workstations as primary goal and smoothing the workload between within workstations by taking all cycles into consideration . a numerical example to clarify the solution methodology is presented . performance of the proposed approach is tested through sets of test problem with randomly generated minimum part sets . the results of the computational experiments indicate that sabfea works with pga very concordantly and it is an effective method in solving mmul bs with parallel workstations and zoning constraints . ( c ) <digit> elsevier ltd. all rights reserved .
ursa a system for uniform reduction to sat . <eos> there are a huge number of problems , from various areas , being solved by reducing them to sat . however , for many applications , translation into sat is performed by specialized , problem specific tools . in this paper we describe a new system for uniform solving of a wide class of problems by reducing them to sat . the system uses a new specification language ursa that combines imperative and declarative programming paradigms . the reduction to sat is defined precisely by the semantics of the specification language . the domain of the approach is wide ( e.g. , many np complete problems can be simply specified and then solved by the system ) and there are problems easily solvable by the proposed system , while they can be hardly solved by using other programming languages or constraint programming systems . so , the system can be seen not only as a tool for solving problems by reducing them to sat , but also as a general purpose constraint solving system ( for finite domains ) . in this paper , we also describe an open source implementation of the described approach . the performed experiments suggest that the system is competitive to state of the art related modelling systems .
transient analysis of zero attracting nlms algorithm without gaussian inputs assumption . <eos> we present the individual weight error variance analysis ( iwv ) of the za nlms algorithm without gaussian inputs assumption . our iwv analysis is based on exact individual weight error relation . we use iwv analysis to derive the transient and steady state behavior of the za nlms algorithm without restricting the input to being gaussian or white . we use iwv analysis to deduce the mean square convergence condition on step size for the za nlms algorithm .
an entropy based query expansion approach for learning researchers dynamic information needs . <eos> proposing an entropy based query expansion with a reweighting ( e_qe ) approach . the e_qe used to learn the researchers evolving information needs at different levels of topic change . adopting a simulation pseudo relevance feedback process to evaluate the proposed approach . the results show that the proposed e_qe approach can achieve better search results than the tfidf .
laplacian spectrum characterization of extensions of vertices of wheel graphs and multi fan graphs . <eos> the graph cn1 kk c n <digit> k k is the product of a circuit cn1 c n <digit> and a clique kk k k . in this paper , we will prove that it is determined by their laplacian spectrum except when n1 <digit> n <digit> <digit> . if n1 <digit> n <digit> <digit> , there are several counterexamples . we also prove that the product of s s vertexdisjoint paths and a clique ( pn1 pn2 pns ) kk ( p n <digit> p n <digit> p n s ) k k is also determined by the laplacian spectrum .
a travel efficient driving assistance scheme in vanets by providing recommended speed . <eos> vehicles ' speed is one of the key factors in vehicle travel efficiency , as speed is related to vehicle travel time , travel safety , fuel consumption , and exhaust gas emissions ( e.g. , co2 emissions ) . therefore , to improve the travel efficiency , a recommended speed calculation scheme is proposed to assist driving in vehicle ad hoc networks ( vanets ) circumstances . in the proposed scheme , vehicles ' current speed and space headway are obtained by vehicle to roadside unit ( v2r ) communication and vehicle to vehicle ( v2v ) communication . based on the vehicles ' current speed and adjacent vehicles ' space headway , a recommended speed is calculated by on board units installed in the vehicles , and then this recommended speed is provided to drivers . the drivers can change their speed to the recommended speed . at the recommended speed , vehicle travel efficiency can be improved vehicles can arrive at destinations in a shorter travel time with fewer stop times , lower fuel consumption , and less co2 emission . in particular , when approaching intersections , vehicles can pass through the intersections with less red light waiting time and a higher nonstop passing rate .
open access to scholarly full text documents . <eos> purpose the purpose of this article is to discuss open access to scholarly full text documents . design methodology approach discusses open access to scholarly full text documents . findings the paper shows that while open access archives are good for the majority , for publishers , editors and authors , open access articles can substantially increase their impact , and the impact factor for the source journals . originality value the paper offers insights into scholarly full text documents .
the numerical methods for oscillating singularities in elliptic boundary value problems . <eos> the singularities near the crack tips of homogeneous materials are monotone of type r ( alpha ) and r ( alpha ) log ( delta ) r , ( depending on the boundary conditions along nonsmooth domains ) . however . the singularities around the interfacial cracks of the heterogeneous bimaterials are oscillatory of type , r ( alpha ) sin ( epsilon logr ) . the method of auxiliary mapping ( mam ) . introduced by babuska and oh , was proven to be successful in dealing with , r ( alpha ) type singularities . however , the effectiveness of mam is reduced in handling oscillating singularities . this paper deals with oscillating singularities as well as the monotone singularities by extending mam through introducing the power auxiliary mapping and the exponential auxiliary mapping . ( c ) <digit> academic press .
hope a genetic algorithm for the unequal area facility layout problem . <eos> the paper discusses the application of an evolutionary computation technique for the design of efficient facilities . genetic algorithms ( ga ) have been applied to heuristically solve a number of combinatorial problems such as scheduling , the traveling salesman problem and the quadratic assignment problem . we apply ga to the layout problem which arises frequently in the design of manufacturing and service organizations to find good solutions . in this paper we outline a ga based algorithm for solving the single floor facility layout problem . we consider departments of both equal and unequal sizes . the gas performance is evaluated using several test problems available in the literature . the results indicate that ga may provide a better alternative in a realistic environment where the objective is to find a number of reasonably good layouts . the implementation also provides the flexibility of having fixed departments and to interactively modify the layouts produced .
a multi society based intelligent association discovery and selection for ambient intelligence environments . <eos> this article presents a novel intelligent embedded agent approach for reducing the number of associations and interconnections between various agents operating within ad hoc multiagent societies of an ambient intelligent environment ( aie ) in order to reduce the processing latency and overheads . the main goal of the proposed fuzzy based intelligent embedded agents ( f ias ) includes learning the overall network configuration and adapting to the system functionality to personalize themselves to the user needs based on monitoring the user in a lifelong nonintrusive mode . in addition , the f ias agents aim to reduce the agent interconnections to the most relevant set of agents in order to reduce the processing overheads and thus implicitly improving the system overall efficiency . we employ embedded ambassador agents , namely embassadors , which are designated f ias agents utilized with additional novel characteristics to not only act as a gateway filtering the number of messages multicast across societies but also discover , recommend , and establish associations between agents residing in separate societies . in order to validate the efficiency of the proposed methods for multiagent and society based intelligent association discovery and learning of f ias agents embassadors we will present two sets of unique experiments . the first experiment describes the obtained results carried out within the intelligent dormitory ( idorm ) which is a real world testbed for aie research . here we specifically demonstrate the utilization of the f ias agents and discuss that by optimizing the set of associations the agents increase efficiency and performance . the second set of experiments is based on emulating an idorm like large scale multi society based aie environment . the results illustrate how embassadors discover strongly correlated agent pairs and cause them to form associations so that relevant agents of separate societies can start interacting with each other .
sorting in column stores . <eos> in recent years , we have seen a number of new database architectures based on the idea of vertical fragmentation of relations . these architectures target the analysis of huge amounts of relational data , because vertical fragmentation facilitates column scans which are common in analytic applications at the expense of single tuple operations . although sorting is a common operation for analytics , few is known about sorting vertically fragmented relations . this paper compares various possibilities to apply ( external ) merge sort to vertically fragmented relations on different layers of the memory hierarchy and gives hints on when to apply which one . we propose a greedy algorithm to find the optimum mixture of steps that leads to a sorted version of a given relation which is stored column wise .
an integrated model for site selection and space determination of warehouses . <eos> in this paper we present an integrated model for site selection and space determination for warehouses in a two stage network in which products are shipped from part suppliers to warehouses , where they are stored for an uncertain length of time and then delivered to assembly plants . the objective is to minimize the total inbound and outbound transportation costs and the total warehouse operation costs , which include the fixed costs related to their locations and the variable costs related to their space requirements for given service levels . each warehouse is modeled as an m g c m g c queueing system in which each storage slot acts as a server . we formulate this problem as a nonlinear mixed integer program with a probabilistic constraint . two cases are considered . for the continuous unbounded size case , we find an approximate formula for the overflow probability and reformulate the problem into a set covering problem . for the discrete size option case , we reformulate the problem into a capacitated connection location problem with discrete size options . computational experiments are performed and the results show that the continuous model is appropriate for the small and median size problems and the discrete model is a good choice for the large size problems .
fast spectral solution of the generalized enskog equation for dense gases . <eos> we propose a fast spectral method for solving the generalized enskog equation for dense gases . for elastic collisions , the method solves the enskog collision operator with a computational cost of o ( md 1ndlog n ) o ( m d <digit> n d log n ) , where d is the dimension of the velocity space , and md <digit> m d <digit> and nd n d are the number of solid angle and velocity space discretizations , respectively . for inelastic collisions , the cost is n times higher . the accuracy of this fast spectral method is assessed by comparing our numerical results with analytical solutions of the spatially homogeneous relaxation of heated granular gases . we also compare our results for force driven poiseuille flow and fourier flow with those from molecular dynamics and monte carlo simulations . although it is phenomenological , the generalized enskog equation is capable of capturing the flow dynamics of dense granular gases , and the fast spectral method is accurate and efficient . as example applications , fourier and couette flows of a dense granular gas are investigated . in addition to the temperature profile , both the density and the high energy tails in the velocity distribution functions are found to be strongly influenced by the restitution coefficient .
a new approach to chaotic image encryption based on quantum chaotic system , exploiting color spaces . <eos> a new color image encryption based on quantum chaotic system is proposed . it comprises efficient confusion and diffusion properties for image cipher . it achieves brilliant characteristics for color image encryption applications . experimental results and analysis show better performance than other schemes .
a circular visualization of people s activities in distributed teams . <eos> a visualization technique that shows daily patterns of people activities . visualization of activities of several individuals at once . the goal of the visualization is to foster interpersonal awareness . the tool implementing the technique is also described . the technique can be generalized and used in different contexts .
simulation of geometrical and electronic structure of quasi two dimensional layer consisting of fullerenes d6h c36 . <eos> this article describes a computer simulation of the geometrical and electronic structure of a quasi two dimensional carbon layer with a trigonal lattice consisting of fullerenes c36 ( <digit> ) with topological symmetry d6h . every polyhedral cluster <digit> of this polymeric layer ( <digit> ) is surrounded by six similar fullerenes and connected with every such a fullerene by two covalent bonds . atomic coordinates of the repeating unit are estimated on the basis of mndo pm3 calculations of hydrocarbon molecule ( d6h ) c132h48 ( <digit> ) . the carbon skeleton of <digit> coincides with a sufficiently large fragment of the polymeric layer <digit> . the electronic spectrum of the quasi two dimensional layer <digit> is calculated by the crystalline orbital method in the eht approximation . the band gap in the electronic spectrum of <digit> was found to be equal to 1.5 ev . the geometric and electronic structure of some oligomers of cluster c36 , quasi linear macromolecule c36 n , and hypergraphite layer is also discussed .
generating chordal graphs included in given graphs . <eos> a chordal graph is a graph which contains no chordless cycle of at least four edges as an induced subgraph . the class of chordal graphs contains many famous graph classes such as trees , interval graphs , and split graphs , and is also a subclass of perfect graphs . in this paper , we address the problem of enumerating all labeled chordal graphs included in a given graph . we think of some variations of this problem . first we introduce an algorithm to enumerate all connected labeled chordal graphs in a complete graph of n vertices . next , we extend the algorithm to an algorithm to enumerate all labeled chordal graphs in a n vertices complete graph . then , we show that we can use , with small changes , these algorithms to generate all ( connected or not necessarily connected ) labeled chordal graphs in arbitrary graph . all our algorithms are based on reverse search method , and time complexities to generate a chordal graph are o ( l ) , and also o ( l ) delay . additionally , we present an algorithm to generate every clique of a given chordal graph in constant time . using these algorithms we obtain combinatorial gray code like sequences for these graph structures in which the differences between two consecutive graphs are bounded by a constant size .
optimal camera placement for automated surveillance tasks . <eos> camera placement has an enormous impact on the performance of vision systems , but the best placement to maximize performance depends on the purpose of the system . as a result , this paper focuses largely on the problem of task specific camera placement . we propose a new camera placement method that optimizes views to provide the highest resolution images of objects and motions in the scene that are critical for the performance of some specified task ( e.g. motion recognition , visual metrology , part identification , etc. ) . a general analytical formulation of the observation problem is developed in terms of motion statistics of a scene and resolution of observed actions resulting in an aggregate observability measure . the goal of this system is to optimize across multiple cameras the aggregate observability of the set of actions performed in a defined area . the method considers dynamic and unpredictable environments , where the subject of interest changes in time . it does not attempt to measure or reconstruct surfaces or objects , and does not use an internal model of the subjects for reference . as a result , this method differs significantly in its core formulation from camera placement solutions applied to problems such as inspection , reconstruction or the art gallery class of problems . we present tests of the system 's optimized camera placement solutions using real world data in both indoor and outdoor situations and robot based experimentation using an all terrain robot vehicle jr robot in an indoor setting .
on some geometric problems of color spanning sets . <eos> in this paper we study several geometric problems of color spanning sets given n points with m colors in the plane , selecting m points with m distinct colors such that some geometric properties of the m selected points are minimized or maximized . the geometric properties studied in this paper are the maximum diameter , the largest closest pair , the planar smallest minimum spanning tree , the planar largest minimum spanning tree and the planar smallest perimeter convex hull . we propose an o ( n <digit> ) time algorithm for the maximum diameter color spanning set problem where could be an arbitrarily small positive constant . then , we present hardness proofs for the other problems and propose two efficient constant factor approximation algorithms for the planar smallest perimeter color spanning convex hull problem .
feature guided convolution for pencil rendering . <eos> we re render a photographic image as a simulated pencil drawing using two independent line integral convolution ( lic ) algorithms that express tone and feature lines . the lic for tone is then applied in the same direction across the image , while the lic for features is applied in pixels close to each feature line in the direction of that line . features are extracted using the coherent line scheme . changing the direction and range of the lics allows a wide range of pencil drawing style to be mimicked . we tested our algorithm on diverse images and obtained encouraging results .
a fuzzy model for human fall detection in infrared video . <eos> fall detection , especially for elderly people , is a challenging problem which demands new products and technologies . in this paper a fuzzy model for fall detection and inactivity monitoring in infrared video is presented . the classification features proposed include geometric and kinematic parameters associated with more or less sudden changes in the tracked human related regions of interest . a complete segmentation and tracking algorithm for infrared video as well as a fuzzy fall detection and confirmation algorithm are introduced . the proposed system is capable of identifying true and false falls , enhanced with inactivity monitoring aimed at confirming the need for medical assistance and or care . the fall indicators used as well as their fuzzy model is explained in detail . the fuzzy model has been tested for a wide number of static and dynamic falls , demonstrating exciting initial results .
multiple hypothesis tracking for data association in vehicular networks . <eos> the introduction of vehicle to vehicle ( v2v ) and vehicle to infrastructure ( v2i ) communications in intelligent transportation systems of the future brings new opportunities and new challenges into the automotive scene . vehicular communications broaden the information spectrum that is available to each vehicle , allowing the enhancement of existing applications and the introduction of new ones . undoubtedly , the impact of this new technology in transportation safety , efficiency and infotainment is expected to be very important . a significant part of research in vehicular networks ( vanets ) is dedicated to networking issues like routing and safety . however , perception systems which until now were based on onboard sensors only , need to incorporate the wirelessly received information in order to extend the situation awareness of the vehicle and the driver . this paper presents an algorithm for associating targets tracked from an onboard radar sensor with the position and motion data received from the vanet . the core of the algorithm is a track oriented multiple hypothesis tracker that is modified for incorporating information included in vanet messages . the algorithm is tested in real scenarios using two experimental vehicles and then compared with two other algorithmic approaches . one is using a simpler single hypothesis algorithm for association of vanet messages and the second is using only the onboard sensors for environment perception . as a result , the advantages of the multiple hypothesis algorithm regarding association performance and the added value of wireless information in the perception system are highlighted . ( c ) <digit> elsevier b.v. all rights reserved .
basic competitive neural networks as adaptive mechanisms for non stationary colour quantisation . <eos> in this paper we consider the application of two basic competitive neural networks ( cnn ) to the adaptive computation of colour representatives on image sequences that show non stationary distributions of pixel colours . the tested algorithms are the simple competitive learning ( scl ) algorithm and the frequency sensitive competitive learning ( fscl ) algorithm . both , scl and fcsl are the simplest adaptive methods based , respectively , on minimising the distortion and on the search for a uniform quantisation . the aim of this paper is to study several computational properties of these methods when applied to non stationary clustering as adaptive vector quantisation algorithms . nonstationary colour quantisation is , therefore , representative of the more general class of non stationary clustering problems . we expect our results to be meaningful for other algorithms that involve either the minimisation of the distortion or the search for uniform quantisers . we study experimentally the effect of the size of the image sample employed in the one pass adaptation , their robustness to initial conditions , and the effect of local versus global scheduling of the learning rate .
an efficient pd data mining method for power transformer defect models using som technique . <eos> laboratory pd measurement set up is developed . hfct is employed to pick up pd current pulses . texture feature extraction method is applied on time domain pd data . capability in different pd sources discrimination ( using texture features ) is assessed by som .
an expeditious cum efficient algorithm for salt and pepper noise removal and edge detail preservation using cardinal spline interpolation . <eos> salt and pepper impulse noise removal for noise densities up to <digit> % . edge detail preservation with psnr for noise densities up to <digit> % . the variation of the parameters and to maximize the psnr . the comparative analysis and time complexity with existing algorithms .
semi supervised learning by disagreement . <eos> in many real world tasks , there are abundant unlabeled examples but the number of labeled training examples is limited , because labeling the examples requires human efforts and expertise . so , semi supervised learning which tries to exploit unlabeled examples to improve learning performance has become a hot topic . disagreement based semi supervised learning is an interesting paradigm , where multiple learners are trained for the task and the disagreements among the learners are exploited during the semi supervised learning process . this survey article provides an introduction to research advances in this paradigm .
applying hierarchical grey relation clustering analysis to geographical information systems a case study of the hospitals in taipei city . <eos> deng proposed grey clustering analysis ( gca ) in <digit> . later , jin presented a new method in <digit> , called grey relational clustering ( grc ) method that combined grey relational analysis with clustering . however , the crc method can not use a tree diagram to make appropriate classification decisions without re computation . this study thus attempts to combine crc and hierarchical clustering analysis . given the existence of an excess of medical resources in the taipei area , this study attempts to understand the degree of concentration of medical resources in this area . specifically , this study applies a geographical information system ( gis ) to present the geographical distribution of hospitals in taipei . additionally , a new type of cluster analysis , known as hierarchical grey relation clustering analysis , is used to analyze the distribution of hospitals and understand how they compete with one another . the analytical results demonstrate that hierarchical grey relation clustering analysis is a suitable method of analyzing geographical position . tree diagrams can help policymakers make appropriate classification decisions without re computation . the study results can inform hospitals of their competitors and help them to develop appropriate responses . additionally , the analytical results can also provide a reference to government or hospital policymakers to help them position hospitals in areas , thus achieving a better distribution of medical resources in taipei . ( c ) <digit> elsevier ltd. all rights reserved .
optimization of test power and data volume in bist scheme based on scan slice overlapping . <eos> in order to further reduce test data storage and test power of deterministic bist based on scan slice overlapping , this paper proposes a novel optimization approach . firstly , a san cell grouping method considering layout constraint is introduced to shorten the scan chain . secondly , a novel scan cell ordering approach considering layout constraint is proposed to optimize the order of scan chain . lastly , the authors propose an improved test pattern partition algorithm which selects the scan slice with the most specified bits as the first scan slice of the current overlapping block . experimental results indicate that the proposed optimization approach significantly reduces the scan in transitions and test data storage by <digit> % and <digit> % , respectively .
relational databases as a massive information source for defeasible argumentation . <eos> argumentation provides a sophisticated yet powerful mechanism for the formalization of commonsense reasoning in knowledge based systems , with application in many areas of artificial intelligence . nowadays , most argumentation systems build their arguments on the basis of a single , fixed knowledge base , often under the form of a logic program as in defeasible logic programming or in assumption based argumentation . currently , adding new information to such programs requires a manual encoding , which is not feasible for many real world environments which involve large amounts of data , usually conceptualized as relational databases . this paper presents a novel approach to compute arguments from premises obtained from relational databases , identifying several relevant aspects . in our setting , different databases can be updated by external , independent applications , leading to changes in the spectrum of available arguments . we present algorithms for integrating a database management system with an argument based inference engine . empirical results and running time analysis associated with our approach show that it provides a powerful alternative for efficiently achieving massive argumentation , taking advantage of modern dbms technologies . we contend that our proposal is significant for developing new architectures for knowledge based applications , such as decision support systems and recommender systems , using argumentation as the underlying inference model .
iterative inversion of fuzzified neural networks . <eos> the inversion of a neural network is a process of computing inputs that produce a given target when fed into the neural network . the inversion algorithm of crisp neural networks is based on the gradient descent search in which a candidate inverse is iteratively refined to decrease the error between its output and the target . in this paper , me derive an inversion algorithm of fuzzified neural networks from that of crisp neural networks . first , we present a framework of learning algorithms of fuzzified neural networks and introduce the idea of adjusting schemes for fuzzy variables . next , we derive the inversion algorithm of fuzzified neural networks by applying the adjusting scheme for fuzzy variables to total inputs in the input layer , finally , we make three experiments on the parity three problem we examine the effect of the size of training sets on the inversion and investigate how the fuzziness of inputs and targets of training sets affects the inversion .
upper bounds on overloaded ldpcoptimum combinied system over rayleigh fading channel . <eos> in this paper a closed form expression for the bit error rate of ldpcoc system in the presence of interferers is derived over an i.i.d rayleigh fading channel using message passing algorithm . all interferers are assumed to have equal power . the analysis is done for the case when the number of interferes is more than the number of receive antenna elements . in this paper , analytical results showed that for a ber of <digit> <digit> , the ldpcoc system provides an additional gain of 6.3 db over oc system alone . both the systems provide identical diversity gain of 1.2 db when the number of receive antennas are increased from <digit> to <digit> .
accelerating boolean matching using bloom filter . <eos> boolean matching is a fundamental problem in fpga synthesis , but existing boolean matchers are not scalable to complex plbs ( programmable logic blocks ) and large circuits . this paper proposes a filter based boolean matching method , f bm , which accelerates boolean matching using lookup tables implemented by bloom filters storing pre calculated matching results . to show the effectiveness of the proposed f bm , a post mapping re synthesis minimizing area which employs boolean matching as the kernel has been implemented . tested on a broad selection of benchmarks , the re synthesizer using f bm is 80x faster with 0.5 % more area , compared with the one using a sat based boolean matcher .
a binary method for fast computation of inter and intra cluster similarities for combining multiple clusterings . <eos> in this paper , we introduce a novel binary method for fast computation of an objective function to measure inter and intra class similarities , which is used for combining multiple clusterings . our method has the advantages of using less memory and cpu time . moreover , compared with the conventional technique , we reduce the time complexity of the problem considerably . experimental test results demonstrate the effectiveness of our new method .
creating animation with personal photo collections and map for storytelling . <eos> the goal of our research is to support the sharing of stories with digital photographs . some map sites are now collecting stories associated with peoples ' relationships to places . users are mapping collections of places including intangible emotional associations of places along with photographs , videos . though this framework of mapping stories is important for accelerating individual creation and transmission of map content , it is not expressive enough to communicate stories narratively . for example , especially when the number of the mapped collections of places is large , it is not easy for viewers to read the map , and it is not easy for creators to express stories as series of events in the real world . that is because one narrative story in a form of a sequence of text narrations , a sequence of photographs , a movie , and an audio like podcasting , etc. is mapped to just one point . as a result , it 's up to the viewers which point on the map to read , and in what order . the common framework is rather suitable for mapping and expressing fragments or snapshots of a whole story , and not suitable for expressing a whole story narratively by using a whole area of a map as the setting of the story . we therefore propose a new framework for mapping personal photo collections and constructing them as stories such as route guidances , sightseeing guidances , historical topics , fieldwork records , personal diaries , and so on . we named this framework as spatial slideshow . a spatial slideshow is a fusion of personal photo mapping and photo storytelling . each story of a spatial slideshow is made of a sequence of mapped photographs , and presented as synchronized animations of a map and an enhanced photo slideshow . the main technical suggestion of this paper is a method for creating a three dimensional animation of photo image which has a visual effect of moving from photo to photo . we have developed a personal photo album software and released it on the web , which works as an editor and a browser of spatial slideshows . we assume the proposed framework has a significance of helping with grassroots development of spatial content driven by visual communications about places or events in the real world .
key issues in data center security an investigation of government audit reports . <eos> the rising volume of electronic data , the growth in cloud computing and the need for secure and affordable large scale data storage all contribute to the increasing reliance on data centers in society . this paper provides an overview of security issues relevant to data centers . we offer an aggregation and exploratory analysis of four audit reports of government data centers operating in the united states . using the information security common body of knowledge to categorize audit findings , we identify the key issues from the reoccurring findings in the reports , particularly in regards to operations security , data center management , physical security , and disaster planning . the security of data centers has become a paramount concern for both government and the information technology industry . both practitioners and academics can benefit from our research results because it provides insight into the key security issues facing modern data centers .
charged partial surface area ( cpsa ) descriptors qsar applications . <eos> the charged partial surface area , or cpsa descriptors were originally designed for use in structure physical relationship studies to capture information about the features of molecules responsible for polar intermolecular interactions . since their development , they have found applications in abroad variety of both structure property and structure activity relationship studies . in the present work , the cpsa descriptors are examined in more detail , evaluating their characteristics with regard to conformational dependence , sources of partial atomic charges , utility of whole molecule and substructure varieties , and the inclusion or exclusion of explicit hydrogens . additionally , an examination of the physical interpretation that can be derived from structure activity relationships that incorporate the cpsa descriptors is made . most recently , the cpsa descriptors have been found to be practically useful in the study of acute aquatic toxicity where they appear to provide an alternative to lumo energy level measures for describing global and local electrophilicity in cases of non covalent molecular interactions . a second example illustrates the ability of the cpsa descriptors to discriminate agonists and antagonists among compounds that bind strongly at the estrogen receptor . while measures of global and local nucleophilicity and interatomic distances are required to explain receptor binding , volumetric parameters , such as cpsas , were found to be necessary to provide separation between reactivity patterns for agonists and antagonists , all having high binding affinity to estrogen receptor .
the structural and electronic properties of cumagn cu m ag n ( m n <digit> m n <digit> ) clusters . <eos> the structural and electronic properties of cumagn cu m ag n ( m n <digit> m n <digit> ) clusters have been investigated through density functional theory . our results show that the triangular shape ( a ) clusters and capped shape ( b ) clusters are generally stabler than w shaped ( c ) clusters . it is shown that the homo ( highest occupied molecular orbital ) lumo ( lowest unoccupied molecular orbital ) gaps vary inversely to the average bond length . we also investigated spectra of cumagn cu m ag n ( m n <digit> m n <digit> ) clusters , which shows that the dominant peaks near 3.5 ev are contributed from the electrons of cu3d cu <digit> d , cu4s cu <digit> s and ag5s ag <digit> s .
a tv agent system that integrates knowledge and answers users ' questions . <eos> aiming to close the digital divide in the television viewing environment , we are developing a tv system with an agent that controls the tv and peripherals on behalf of the user and provides information to the user . we propose a tv system function that answers viewers ' questions about tv programs by calling upon multiple question answering agents that search for relevant information .
new current controlled current mode sinusoidal quadrature oscillators using cdtas . <eos> this article presents new current mode oscillator circuits using cdtas which is designed from block diagram . the proposed circuits consist of three cdtas and two grounded capacitors . the condition of oscillation can be adjusted electronically orthogonally from the oscillation frequency by adjusting bias current of the cdtas . the proposed quadrature oscillators have high output impedance and use only grounded capacitors without any external resistor which is very appropriate for future development into an integrated circuit . the pspice simulation and experimental results are corresponding to the theoretical analysis .
identification of unknown pure component spectra by indirect hard modeling . <eos> indirect hard modeling ( ihm ) is a physically motivated spectral analysis principle . it utilizes nonlinear spectral hard models generated by peak fitting of the pure spectra . this approach allows the consideration of various nonlinear effects such as peak variations or spectral shifts . compared to established methods , less calibration samples are required and basic calibration transfer is performed inherently . to extend the applicability of ihm , which currently requires knowledge of the pure component spectra , two methods for the identification of pure spectra are presented in this work . these methods work automatically on a mathematically objective basis and do thus not depend on the expertise of the user . as ihm relies on an underlying physical picture of the spectra , the relevant information in the input data is exploited very efficiently especially for selective spectra , and nonideal spectral behavior is captured throughout the identification process . compared to established smcr methods the number of required spectra is reduced . the first method , complemental hard modeling ( chm ) , is introduced for the case that a single pure spectrum is unknown . the method is based on a deconvolution approach and only requires a single mixture spectrum as input data . the second method , hard modeling factor analysis ( hmfa ) , is conceptually related to smcr methods . it allows the identification of all pure spectra in a completely unknown mixture from a limited set of mixture spectra . as shown in this work , even highly collinear data can be employed . ( c ) <digit> elsevier b.v. all rights reserved .
a multi parameter regularization model for image restoration . <eos> based on total variation and wavelet frame , a multi parameter regularization model for image restoration is proposed . an effective algorithm based on admm is given for solving the new model , i.e. , tvframe . convergence analysis of the new algorithm is given . numerical experiments show that tvframe outperforms several state of the art image restoration approaches .
distributed version control in the classroom . <eos> modern distributed version control systems offer compelling advantages for teaching students professional software development practices and skills . in this paper , we explore the potential for incorporating mercurial into introductory , intermediate , and advanced computing courses . by incorporating version control into the entire cs curriculum , instructors create unique opportunities to engage students in collaborative , real world projects and activities , giving them critical early exposure to the expectations and assumptions prevalent in the software development community . early introduction to version control provides students with an important foundation in both personal and collaborative development excellence , offering them a competitive edge in the marketplace and a superior understanding of software development best practice .
a statistical model for synthesis of detailed facial geometry . <eos> detailed surface geometry contributes greatly to the visual realism of 3d face models . however , acquiring high resolution face geometry is often tedious and expensive . consequently , most face models used in games , virtual reality , or computer vision look unrealistically smooth . in this paper , we introduce a new statistical technique for the analysis and synthesis of small three dimensional facial features , such as wrinkles and pores . we acquire high resolution face geometry for people across a wide range of ages , genders , and races . for each scan , we separate the skin surface details from a smooth base mesh using displaced subdivision surfaces . then , we analyze the resulting displacement maps using the texture analysis synthesis framework of heeger and bergen , adapted to capture statistics that vary spatially across a face . finally , we use the extracted statistics to synthesize plausible detail on face meshes of arbitrary subjects . we demonstrate the effectiveness of this method in several applications , including analysis of facial texture in subjects with different ages and genders , interpolation between high resolution face scans , adding detail to low resolution face scans , and adjusting the apparent age of faces . in all cases , we are able to re produce fine geometric details consistent with those observed in high resolution scans .
development of an autonomous distributed control system for optical packet and circuit integrated networks . <eos> in this paper , we describe an autonomous distributed control system that we have been developing for an optical packet and circuit integrated network , and we experimentally evaluate its performance . colored ( i.e. , multi wavelength ) optical packet switched links transfer both control signals for circuit switching ( e.g. , signaling and routing ) and best effort packet data . we successfully transmitted high definition uncompressed real time video signals on two lightpaths established by our control system without degradation of video quality , simultaneously with other optical packet data transferred on the same optical fibers . our developed control system not only achieved autonomous distributed signaling and routing but also has a function that can adjust wavelength resources for optical packet and circuit switching autonomously in each link at each node . controllers achieved lightpath establishment within approximately <digit> ms and dynamic resource adjustment within approximately <digit> ms , in the best possible case in our experimental setup .
fun and games player profiles . <eos> this paper presents findings about player preferences regarding characteristics of gaming experiences . it describes a quantitative investigation employing a survey with <digit> items as a basis to categorise players in profiles , and it shows how the fun factors represented by the survey items relate to each other . the survey was applied to <digit> students in florianpolis ( brazil ) . a factorial analysis method was used to understand how the fun factors are associated with one other in the players opinions , and it generated six dimensions of fun , every one of which can be described as a way to have fun ( <digit> ) improvement , ( <digit> ) distinction , ( <digit> ) immersion , ( <digit> ) decoration , ( <digit> ) empathy , and ( <digit> ) grotesque . a cluster analysis method was used to divide players into eight profiles , every one of which describes the preferences of a group of players who declared to have fun with similar aspects . the profiles were named as ( <digit> ) competitive and enthusiastic ( <digit> ) competitive and selfish ( <digit> ) competitive and overcoming ( <digit> ) immersed in the beauty ( <digit> ) immersed and selfish ( <digit> ) distracted and uninterested ( <digit> ) quitters and , ( <digit> ) friendly and overcoming . the results are a useful source to reflect on game design and fun , and to consider topics such as violence , verisimilitude , distraction , socialization , as well as the multiplicity of modes of engagement with digital game .
analyzing operational risk reward trade offs for start ups . <eos> we model the advertising and inventory decisions of a start up . we develop a novel methodology to track variance as well as expected rewards of the operational decisions . we call this methodology variance retentive stochastic dynamic programming . we develop a heuristic ( risktrackr ) to construct risk reward efficient frontiers .
identity based deniable authentication for ad hoc networks . <eos> deniable authentication is an important security requirement for ad hoc networks . however , all known identity based deniable authentication ( ibda ) protocols are lack of formal security proof which is very important for cryptographic protocol design . in this paper , we propose a non interactive ibda protocol using bilinear pairings . our protocol admits formal security proof in the random oracle model under the bilinear diffie hellman assumption . our protocol is faster than all known ibda protocols of its type . in addition , our protocol supports batch verification that can speed up the verification of authenticators . this characteristic makes our protocol useful in ad hoc networks .
lossless trace compression . <eos> the tremendous storage space required for a useful data base of program traces has prompted a search for trace reduction techniques . in this paper , we discuss a range of information lossless address and instruction trace compression schemes that can reduce both storage space and access time by an order of magnitude or more , without discarding either references or interreference timing information from the original trace . the pdats family of trace compression techniques achieves trace coding densities of about six references per byte . this family of techniques is now in use as the standard in the nmsu tracebase , an extensive trace archive that has been established for use by the international research and teaching community .
diffusion tensor based fast marching for modeling human brain connectivity network . <eos> diffusion tensor imaging ( dti ) is an effective modality in studying the connectivity of the brain . to eliminate possible biases caused by fiber extraction approaches due to low spatial resolution of dti and the number of fibers obtained , the fast marching ( fm ) algorithm based on the whole diffusion tensor information is proposed to model and study the brain connectivity network . our observation is that the connectivity extracted from the whole tensor field would be more robust and reliable for constructing brain connectivity network using dti data . to construct the connectivity network , in this paper , the arrival time map and the velocity map generated by the fm algorithm are combined to define the connectivity strength among different brain regions . the conventional fiber tracking based and the proposed tensor based fm connectivity methods are compared , and the results indicate that the connectivity features obtained using the fm based method agree better with the neuromorphical studies of the human brain .
vertical handover management scheme using multiple tcp connections for heterogeneous networks . <eos> although mobile nodes will handle vertical handovers to achieve transparent mobility in the heterogeneous network , the communication quality will be drastically degraded . we proposed vertical handover management scheme that solves all of the issues . in this paper , to verify the effectiveness of the scheme in a real environment , we implement it and show a demonstration .
multimedia applications and security in mapreduce opportunities and challenges . <eos> cloud computing has recently attracted great attention , both commercially and academically . mapreduce is a popular programming model for distributed storage and computation in the cloud . in this paper , we survey cloud based multimedia applications , identifying the open issues and challenges which arise when mapreduce is used for cloud computing . copyright ( c ) <digit> john wiley sons , ltd .
internal states on equality algebras . <eos> this paper investigates properties of equality algebras introduced by jenei as a possible algebraic semantic for fuzzy type theory . we define and study the pointed equality algebras and its subclass of compatible pointed equality algebras . we introduce and investigate the internal states and the state morphism operators on equality algebras and on their corresponding bck meet semilattices . we prove that any internal state ( state morphism ) on an equality algebra is also an internal state ( state morphism ) on its corresponding bck meet semilattice , and we prove the converse for the case of linearly ordered equality algebras . another main result consists of proving that any state morphism on a linearly ordered equality algebra is an internal state on it . we show that any internal state on a linearly ordered bck meet semilattice satisfying the distributivity condition is also an internal state on its corresponding equality algebra and a state morphism on a bck meet semilattice satisfying the distributivity condition is also a state morphism on its corresponding equality algebra .
distributed data replenishment . <eos> we propose a distributed data replenishment mechanism for some distributed peer to peer based storage systems that automates the process of maintaining a sufficient level of data redundancy to ensure the availability of data in presence of peer departures and failures . the dynamics of peers entering and leaving the network are modeled as a stochastic process . a novel analytical time backward technique is proposed to bound the expected time for a piece of data to remain in p2p systems . both theoretical and simulation results are in agreement , indicating that the data replenishment via random linear network coding ( rlnc ) outperforms other popular strategies . specifically , we show that the expected time for a piece of data to remain in a p2p system , the longer the better , is exponential in the number of peers used to store the data for the rlnc based strategy , while they are quadratic for other strategies .
knowledge management for context aware , policy based ubiquitous computing systems . <eos> ubiquitous computing systems depend on a distributed intelligence that relates the context of the task being performed to the available system resources and services . currently , this is hampered by the problems inherent in heterogeneous devices and technologies used to manage network resources and services , and the associated lack of a network language lingua franca that all systems can use and understand . this paper describes a new approach for representing , using and managing knowledge for ubiquitous computing systems , based on a novel combination of extracting knowledge from models and ontologies to form such a lingua franca . an extensible context model is used to select applicable policies to govern system behaviour as context changes , policies change , which in turn causes system behaviour to change accordingly .
path space regularization for holistic and robust light transport . <eos> we propose a simple yet powerful regularization framework for robust light transport simulation . it builds on top of existing unbiased methods and resorts to a consistent estimation using regularization only for paths which can not be sampled in an unbiased way . to introduce as little bias as possible , we selectively regularize individual interactions along paths , and also derive the regularization consistency conditions . our approach is compatible with the majority of unbiased methods , e. g. ( bidirectional ) path tracing and metropolis light transport ( mlt ) , and only a simple modification is required to adapt existing renderers . we compare to recent unbiased and consistent methods and show examples of scenes with difficult light paths , where regularization is required to account for all illumination features . when coupled with mlt we are able to sample all phenomena , like recent consistent methods , while achieving superior convergence .
integrating generations with advanced reference counting garbage collectors . <eos> we propose the use of generations with modern reference counting . a reference counting collector is well suited to collect the old generation , containing a large fraction of live objects that are modified infrequently . such a collector can be combined with a tracing collector to collect the young generation , typically containing a small fraction of live objects . we have designed such a collector appropriate for running on a multiprocessor . as our building blocks , we used the sliding views on the fly collectors . we have implemented the new collector on the jikes research java virtual machine ( jikes rvm ) and compared it with the concurrent reference counting collector supplied with the jikes rvm package . our measurements demonstrate short pause times , retaining those of the original on the fly collectors and a gain in application throughput time . it turns out that a modern reference counting collector may benefit from the use of generations . copyright ( c ) <digit> john wiley sons , ltd .
on the use of visual motion in particle filter tracking . <eos> particle filtering is now established as one of the most popular methods for visual tracking . within this framework , a basic assumption is that the data are temporally independent given the sequence of object states . in this paper , we argue that in general the data are correlated , and that modeling such dependency should improve tracking robustness . besides , the choice of using the transition prior as proposal distribution is also often made . thus , the current observation data is not taken into account in the generation of the new samples , requesting the noise process of the prior to be large enough to handle abrupt trajectory changes between the previous image data and the new one . therefore , many particles are either wasted in low likelihood area , resulting in a low efficiency of the sampling , or , more importantly , propagated on near distractor regions of the image , resulting in tracking failures . in this paper , we propose to handle both issues using motion . explicit motion measurements are used to drive the sampling process towards the new interesting regions of the image , while implicit motion measurements are introduced in the likelihood evaluation to model the data correlation term . the proposed model allows to handle abrupt motion changes and to filter out visual distractors when tracking objects with generic models based on shape or color distribution representations . experimental results compared against the condensation algorithm have demonstrated superior tracking performance .
numerical model for two bolted joints subjected to compressive loading . <eos> this paper deals with the dimensioning of two bolted assemblies made up from joint prismatic subassemblies subjected to fatigue compressive loads coplanar with the screw axis . the presented study is complementary to a previous study relative to tension loading a. daidie , j. chakhari , a. zghal , numerical model for bolted t stubs with two bolt rows , struct . eng . mech . <digit> ( <digit> ) ( <digit> ) <digit> . however , one should note that the two loading cases are not similar since an additional influential corner contact problem occurs in the case of compressive loading . the main development in this paper relative to the previous study a. daidie , j. chakhari , a. zghal , numerical model for bolted t stubs with two bolt rows , struct . eng . mech . <digit> ( <digit> ) ( <digit> ) <digit> is taking into consideration this complex corner contact problem . in this framework , the local deformation between the subassemblies and the corner of the supporting structure is formulated and a non linear expression of a constrained displacement is established . moreover , the evolution of the contact zone under the compressive loading , which is not similar in the case of tension loading , is taken into consideration . consequently , a new extended numerical model for compressive loading is established from unidirectional finite elements and validated by 3d finite element simulations . an algorithm which updates the contact stiffness matrix and sets out forces and displacements at each node of the subassembly is developed using c language program . finally , a statistical software method is used as in the case of tensile loading . it is important to note that in the case of compressive loading , this statistical software method is not only used to establish the effect of the joint parameters , but also to identify and tune up parameters relative to the complex problem of the corner contact .
comment on support vector machine for classification based on fuzzy training data by a. b. ji , j. h. pang , h. j. qiu expert systems with applications <digit> ( <digit> ) <digit> . <eos> this paper comments on the recently published work dealing with support vector machine for classification based on fuzzy data ji , a. b. , pang , j. h. , qiu , h. j. ( <digit> ) . support vector machine for classification based on fuzzy training data . expert systems with applications <digit> ( <digit> ) , <digit> . the authors have claimed that their proposed program is a classical convex quadratic program . but , we show that their proposed program is neither convex nor classical quadratic . then , we propose a convex program by using the similar strategy proposed by ji et al. ( <digit> ) .
performance of the ensemble kalman filter outside of existing wells for a channelized reservoir . <eos> the ensemble kalman filter ( enkf ) appears to give good results for matching production data at existing wells . however , the predictive power of these models outside of the existing wells is much more uncertain . in this paper , for a channelized reservoir for five different cases with different levels of information the production history is matched using the enkf . the predictive power of the resulting model is tested for the existing wells and for new wells . the results show a consistent improvement for the predictions at the existing wells after assimilation of the production data , but not for prediction of production at new well locations . the latter depended on the settings of the problem and prior information used . the results also showed that the fit during the history match was not always a good predictor for predictive capabilities of the history match model . this suggests that some form of validation outside of observed wells is essential .
the dynamics of an elastic hopping hoop . <eos> this paper discusses the motion of a hoop which is loaded with a heavy particle fixed to the rim and which is rolling in a vertical plane . in contrast to previous analyses , the hoop is not rigid the elasticity in the system produces results that are in agreement with previously reported observations of hopping hula hoops . the main result of this analysis is the identification of the conditions that are required for hopping to occur after a rotation through less than 90degrees after starting with the particle at the highest point . ( c ) <digit> elsevier science ltd. all rights reserved .
opportunities for actuated tangible interfaces to improve protein study . <eos> we outline strategies for actuated tangible user interfaces ( tuis ) to improve the study of proteins . current protein study tools miss fundamental biology concepts because graphical and symbolic interfaces do not allow users to intuitively manipulate complex physical forms . actuated , tangible tools may enhance understanding at all levels of protein study . to advance tui awareness of protein study , we present an overview of protein concepts and current protein study tools . thirty six protein researchers , engineers , professors and students recommend design guidelines for tangible interfaces in protein study , and we outline research directions for tuis to improve protein study at all educational levels .
point rendering of non manifold surfaces with features . <eos> we are concerned with producing high quality images of implicit surfaces , in particular those with non manifold features . in this work we present a point based technique that improves the rendering of non manifold implicit surfaces by using point and gradient information to prune plotting nodes resulting from using octree spatial subdivision based on the natural interval extension of the surface 's function . the use of intervals guarantees that no parts of the surfaces are missed in the view volume and the combination of point and gradient sampling preserves this feature while greatly enhacing the quality of point based rendering of implicit surfaces . we also sucessfully render non manifold features of implicit surfaces such as rays and thin sections . we illustrate the technique with a number of example surfaces .
a novel reordering write buffer to improve write performance of log structured file systems . <eos> this paper presents a novel reordering write buffer which improves the performance of log structured file systems ( lfs ) . while lfs has a good write performance , high garbage collection overhead degrades its performance under high disk space utilization . previous research concentrated on how to improve the efficiency of the garbage collector after data is written to disk . we propose a new method that reduces the amount of work the garbage collector would do before data reaches disk . by classifying active and inactive data in memory into different segment buffers and then writing them to different disk segments , we force the disk segments to form a bimodal distribution . most data blocks in active segments are quickly invalidated , while inactive segments remain mostly intact . simulation results based on a wide range of both real world and synthetic traces show that our method significantly reduces the garbage collection overhead , slashing the overall write cost of lfs by up to <digit> percent , improving the write performance of lfs by up to <digit> percent , and the overall system performance by up to <digit> percent .
doppler ultrasound wall removal based on the spatial correlation of wavelet coefficients . <eos> in medical doppler ultrasound systems , a high pass filter is commonly used to reject echoes from the vessel wall . however , this leads to the loss of the information from the low velocity blood flow . here a spatially selective noise filtration algorithm cooperating with a threshold denoising based on wavelets coefficients is applied to estimate the wall clutter . then the blood flow signal is extracted by subtracting the wall clutter from the mixed signal . experiments on computer simulated signals with various clutter to blood power ratios indicate that this method achieves a lower mean relative error of spectrum than the high pass filtering and other two previously published separation methods based on the recursive principle component analysis and the irregular sampling and iterative reconstruction , respectively . the method also performs well when applied to in vivo carotid signals . all results suggest that this approach can be implemented as a clutter rejection filter in doppler ultrasound instruments .
evaluation of probabilistic queries over imprecise data in constantly evolving environments . <eos> sensors are often employed to monitor continuously changing entities like locations of moving objects and temperature . the sensor readings are reported to a database system , and are subsequently used to answer queries . due to continuous changes in these values and limited resources ( e.g. , network bandwidth and battery power ) , the database may not be able to keep track of the actual values of the entities . queries that use these old values may produce incorrect answers . however , if the degree of uncertainty between the actual data value and the database value is limited , one can place more confidence in the answers to the queries . more generally , query answers can be augmented with probabilistic guarantees of the validity of the answers . in this paper , we study probabilistic query evaluation based on uncertain data . a classification of queries is made based upon the nature of the result set . for each class , we develop algorithms for computing probabilistic answers , and provide efficient indexing and numeric solutions . we address the important issue of measuring the quality of the answers to these queries , and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries . extensive experiments are performed to examine the effectiveness of several data update policies . ( c ) <digit> elsevier b.v. all rights reserved .
understanding and modeling pedestrian mobility of train station scenarios . <eos> this work presents the observations of pedestrian mobility characteristics based on the traces collected in a train station provides a mobility model using these observations .
an anytime assignment algorithm from local task swapping to global optimality . <eos> the assignment problem arises in multi robot task allocation scenarios . inspired by existing techniques that employ task exchanges between robots , this paper introduces an algorithm for solving the assignment problem that has several appealing features for online , distributed robotics applications . the method may start with any initial matching and incrementally improve the current solution to reach the global optimum , producing valid assignments at any intermediate point . it is an any time algorithm with a performance profile that is attractive quality improves linearly with stages ( or time ) . additionally , the algorithm is comparatively straightforward to implement and is efficient both theoretically ( complexity of ( o ( n <digit> lg n ) ) is better than many widely used solvers ) and practically ( comparable to the fastest implementation , for up to hundreds of robots tasks ) . the algorithm generalizes swap primitives used by existing task exchange methods already used in the robotics community but , uniquely , is able to obtain global optimality via communication with only a subset of robots during each stage . we present a centralized version of the algorithm and two decentralized variants that trade between computational and communication complexity . the centralized version turns out to be a computational improvement and reinterpretation of the little known method of balinskigomory proposed half a century ago . thus , deeper understanding of the relationship between approximate swap based techniquesdeveloped by roboticistsand combinatorial optimization techniques , e.g. , the hungarian and auction algorithmsdeveloped by operations researchers but used extensively by roboticistsis uncovered .
on minimum reload cost paths , tours , and flows . <eos> the concept of reload cost , that is of a cost incurred when two consecutive arcs along a path are of different types , naturally arises in a variety of applications related to transportation , telecommunication , and energy networks . previous work on reload costs is devoted to the problem of finding a spanning tree of minimum reload cost diameter ( with no arc costs ) or of minimum reload cost . in this article , we investigate the complexity and approximability of the problems of finding optimum paths , tours , and flows under a general cost model including reload costs as well as regular arc costs . some of these problems , such as shortest paths and minimum cost flows , turn out to be polynomially solvable while others , such as minimum shortest path tree and minimum unsplittable multicommodity flows , are np hard to approximate within any polynomial time computable function . ( c ) <digit> wiley periodicals , inc. networks , vol . <digit> ( <digit> ) , 254 260 2011
natural scene text detection with multi layer segmentation and higher order conditional random field based analysis . <eos> the contrasts in rgb channels are integrated to segment image into multi layers . the multi layer segmentation is implemented with a graph cuts based model . a higher order crf based connected component analysis is used .
operations research practice on logistics management in taiwan an academic view . <eos> the opinions of logistics educators in taiwan on or practices in the domestic logistics industry are explored in this research . in this study , questionnaires were given to <digit> pre screened educators at <digit> logistics departments and graduate institutes . according to the <digit> valid returned questionnaires , <digit> % of the responding educators believe the major source of or techniques in taiwans logistics industry originates from individual employee training , while <digit> % believe or is not widely used primarily because companies are unfamiliar with or techniques . or techniques were considered helpful in solving problems by <digit> % . generally speaking , familiarity with or techniques is not associated with implementation of or techniques by educators . additionally , logistics educators express concern about insufficient training for logistics students in taiwan . however , they are optimistic about the logistics industrys willingness to more actively adopt or techniques in taiwan in the next two years .
interference mitigation through limited receiver cooperation . <eos> interference is a major issue limiting the performance in wireless networks . cooperation among receivers can help mitigate interference by forming distributed mimo systems . the rate at which receivers cooperate , however , is limited in most scenarios . how much interference can one bit of receiver cooperation mitigate in this paper , we study the two user gaussian interference channel with conferencing decoders to answer this question in a simple setting . we identify two regions regarding the gain from receiver cooperation linear and saturation regions . in the linear region , receiver cooperation is efficient and provides a degrees of freedom gain , which is either one cooperation bit buys one over the air bit or two cooperation bits buy one over the air bit . in the saturation region , receiver cooperation is inefficient and provides a power gain , which is bounded regardless of the rate at which receivers cooperate . the conclusion is drawn from the characterization of capacity region to within two bits s hz , regardless of channel parameters . the proposed strategy consists of two parts <digit> ) the transmission scheme , where superposition encoding with a simple power split is employed and <digit> ) the cooperative protocol , where one receiver quantize bin and forwards its received signal and the other after receiving the side information decode bin and forwards its received signal .
a note on permutations and rank aggregation . <eos> in this brief note we consider rank aggregation , a popular method in voting theory , social choice , business decisions , etc. mathematically the problem is to find a permutation viewed as a vector that minimizes the sum of the l ( <digit> ) distances to a given family of permutations . the problem may be solved as an assignment problem and we establish several properties of optimal solutions in this problem . ( c ) <digit> elsevier ltd. all rights reserved .
mapping search relevance to social networks . <eos> this paper explores how information contained in the structure of the social graph can improve search result relevance on social networking websites . traditional approaches to search include scoring documents for relevance based on a set of keywords or using the link structure across documents to infer quality and relevance . these approaches attempt to optimally match keywords to documents with little or no information about the searcher and no information about his network . this study analyzes 3.8 m profile search queries from a large social networking site in conjunction with the tie structure of a 21m member social graph . the key finding is that a measure of social distance , when used in conjunction with standard search relevance methods , improves the ordering of profiles in search results .
some refinements of rough k means clustering . <eos> lingras et at . proposed a rough cluster algorithm and successfully applied it to web mining . in this paper we analyze their algorithm with respect to its objective function , numerical stability , the stability of the clusters and others . based on this analysis a refined rough cluster algorithm is presented . the refined algorithm is applied to synthetic , forest and microarray gene expression data . ( c ) <digit> pattern recognition society . published by elsevier ltd. all rights reserved .
taming hardware event samples for precise and versatile feedback directed optimizations . <eos> feedback directed optimization ( fdo ) is effective in improving application runtime performance , but has not been widely adopted due to the tedious dual compilation model , the difficulties in generating representative training data sets , and the high runtime overhead of profile collection . the use of hardware event sampling overcomes these drawbacks by providing a lightweight approach to collect execution profiles in the production environment , which naturally consumes representative input . yet , hardware event samples are typically not precise at the instruction or basic block granularity . these inaccuracies lead to missed performance when compared to instrumentation based fdo . in this paper , we use performance monitoring unit ( pmu ) based sampling to collect the instruction frequency profiles . by collecting profiles using multiple events , and applying heuristics to predict the accuracy , we improve the accuracy of the profile . we also show how emerging techniques can be used to further improve the accuracy of the sample based profile . additionally , these emerging techniques are used to collect value profiles , as well as to assist a lightweight interprocedural optimizer . all these profiles are represented in a portable form , thus they can be used across different platforms . we demonstrate that sampling based fdo can achieve an average of <digit> percent of the performance gains obtained using instrumentation based exact profiles for both spec cint2000 and cint2006 benchmarks . the overhead of collection is only 0.93 percent on average , while compiler based instrumentation incurs 2.0 351.5 percent overhead ( and 10x overhead on an industrial web search application ) .
discrete particle swarm optimization for high order graph matching . <eos> high order graph matching aims at establishing correspondences between two sets of feature points using high order constraints . it is usually formulated as an np hard problem of maximizing an objective function . this paper introduces a discrete particle swarm optimization algorithm for resolving high order graph matching problems , which incorporates several re defined operations , a problem specific initialization method based on heuristic information , and a problem specific local search procedure . the proposed algorithm is evaluated on both synthetic and real world datasets . its outstanding performance is validated in comparison with three state of the art approaches .
the structure of continuous uni norms . <eos> the concept of uni norm . aggregation operators ( uni norms ) was introduced by yager and rybalov to unify and generalize the t norms and t conorms . considering that uni norms continuous on <digit> , <digit> ( <digit> ) must be t norms or t conorms , we concentrate our attention on uni norms continuous in ( <digit> , <digit> ) ( <digit> ) in this paper . we mainly investigate their properties , representation and structure . ( c ) <digit> elsevier science b.v. all rights reserved .
undergraduate research experiences in data mining . <eos> the new interdisciplinary field of data mining emerged in the early 1990s as a response to the profusion of digital data generated in numerous fields such as biology , chemistry , astronomy , advertising , banking and finance , retail market , stock market , and the www . in this paper , i describe an undergraduate course in data mining offered at the college of saint benedict and saint john 's university in spring of <digit> as a csci <digit> upper division topics in computer science course , entitled data intelligence . one of the main objectives of the course was to engage students in experimental computing research through a number of carefully planned research activities resulting in better understanding of the course contents and deeper insights into the challenges faced by the data mining community .
a single tri axial accelerometer based real time personal life log system capable of human activity recognition and exercise information generation . <eos> recording a personal life log ( pll ) of daily activities in a ubiquitous environment is an emerging application of information technology . in this work , we present a single tri axial accelerometer based pll system capable of human activity recognition and exercise information generation . our pll system exhibits two main functions activity recognition and exercise information generation . for activity recognition , the system first recognizes a state of daily activities based on the statistical and spectral features of the accelerometer signals . an activity within the recognized state is then recognized using a set of augmented features , including autoregressive coefficients , signal magnitude area , and tilt angle , via linear discriminant analysis and hierarchical artificial neural networks . upon the recognition of each activity , the system further estimates exercise information that includes energy expenditure based on metabolic equivalents , stride length , step count , walking distance , and walking speed . our pll system operates in real time , and the life log information it generates is archived in a daily log database . we have validated our pll system for six daily activities ( i.e. , lying , standing , walking , going upstairs , going downstairs , and driving ) via subject independent and subject dependent recognition on a total of twenty subjects , achieving an average recognition accuracy of 94.43 and 96.61 % , respectively . our results demonstrate the feasibility of a portable real time pll system that could be used for u lifecare and u healthcare services in the near future .
simulation optimization with qualitative variables and structural model changes a genetic algorithm approach . <eos> in many common simulation optimization methods the structure of the system stays the same and only the set of values for certain parameters of the system such as the number of machines in a station or the in process inventory is varied from one evaluation to the next . the methodology described in this paper is a simulation optimization process where the qualitative variables and the structure of the system are the subjects of optimization . here , the optimum response sought is a function of design and operation characteristics of the system such as the type of machines to use , dispatching rules , sequence of processing operations , etc. in the methodology developed here simulation models are automatically generated through an object oriented process and are evaluated for various candidate configurations of the system . these candidates are suggested by a genetic algorithm ( ga ) that automatically guides the system towards better solutions . after simulating the alternatives , the results are returned to the ga to be utilized in selection of the next generation of configurations to be evaluated . this process continues until a satisfactory solution is obtained for the system .
clinical decision support for diagnosing stress related disorders by applying psychophysiological medical knowledge to an instance based learning system . <eos> an important procedure in diagnosing stress related disorders caused by dysfunction in the interaction of the heart with breathing , i.e. , respiratory sinus arrhythmia ( rsa ) , is to analyse the breathing first and then the heart rate . analysing these measurements is a time consuming task for the diagnosing clinician . a decision support system in this area would reduce the analysis task of the clinician and enable him her to give more attention to the patient . we have created a decision support system which contains a signal classifier and a pattern identifier . the system performs an analysis of the physiological time series concerned which would otherwise be performed manually by the clinician . the signal classifier , hr3modul , classifies heart rate patterns by analysing both cardio and pulmonary signals , i.e. , physiological time series . hr3modul uses case based reasoning ( cbr ) , using a wavelet based method for retrieving features from the signals . the system searches for familiar shapes in the signals by comparing them with shapes already stored . we have applied a best fit scheme for handling signals of different lengths , as the length of a breath is highly dynamic . we also apply automatic weighting to the features to obtain a more autonomous system . the classified heart signals indicate if a patient may be suffering from a stress related disorder and the nature of the disorder . these classified signals are thereafter sent to the second subsystem , the pattern identifier . the pattern identifier analyses the classified signals and searches for familiar patterns by identifying sequences in the classified signals . the identified sequences give clinicians a more complete analysis of the measurements , providing them with a better basis for diagnosis . we have shown that a case based classifier with a wavelet feature extractor and automatic weighting is a viable option for building a decision support system for the psychophysiological domain , as it is at par , or even outperforms other retrieval techniques and is less complex .
the case for research in game engine architecture . <eos> this paper is a call for research in the field of game engine architecture and design , a more comprehensive and thorough understanding of which we consider to be essential for its development . we present a number of key aspects that may help to define the problem space and provide a catalogue of questions that we believe identify areas of interest for future investigation .
a geometry based soft shadow volume algorithm using graphics hardware . <eos> most previous soft shadow algorithms have either suffered from aliasing , been too slow , or could only use a limited set of shadow casters and or receivers . therefore , we present a strengthened soft shadow volume algorithm that deals with these problems . our critical improvements include robust penumbra wedge construction , geometry based visibility computation , and also simplified computation through a four dimensional texture lookup . this enables us to implement the algorithm using programmable graphics hardware , and it results in images that most often are indistinguishable from images created as the average of <digit> hard shadow images . furthermore , our algorithm can use both arbitrary shadow casters and receivers . also , one version of our algorithm completely avoids sampling artifacts which is rare for soft shadow algorithms . as a bonus , the four dimensional texture lookup allows for small textured light sources , and , even video textures can be used as light sources . our algorithm has been implemented in pure software , and also using the geforce fx emulator with pixel shaders . our software implementation renders soft shadows at 0.5 <digit> frames per second for the images in this paper . with actual hardware , we expect that our algorithm will render soft shadows in real time . an important performance measure is bandwidth usage . for the same image quality , an algorithm using the accumulated hard shadow images uses almost two orders of magnitude more bandwidth than our algorithm .
management and analysis of unstructured construction data types . <eos> compared with structured data sources that are usually stored and analyzed in spreadsheets , relational databases , and single data tables , unstructured construction data sources such as text documents , site images , web pages , and project schedules have been less intensively studied due to additional challenges in data preparation , representation , and analysis . in this paper , our vision for data management and mining addressing such challenges are presented , together with related research results from previous work , as well as our recent developments of data mining on text based , web based , image based , and network based construction databases .
nonlinear multiresolution signal decomposition schemes part ii morphological wavelets . <eos> in its original form , the wavelet transform is a linear tool . however , it has been increasingly recognized that nonlinear extensions are possible . a major impulse to the development of nonlinear wavelet transforms has been given by the introduction of the lifting scheme by sweldens , the aim of this paper , which is a sequel of a previous paper devoted exclusively to the pyramid transform , is to present an axiomatic framework encompassing most existing linear and nonlinear wavelet decompositions . furthermore , it introduces some , thus far unknown , wavelets based on mathematical morphology , such as the morphological haar wavelet , both in one and two dimensions . a general and flexible approach for the construction of nonlinear ( morphological ) wavelets is provided by the lifting scheme . this paper briefly discusses one example , the max lifting scheme , which has the intriguing property that preserves local maxima in a signal over a range of scales , depending on how local or global these maxima are .
a learner centered approach to teaching ethics in computing . <eos> this paper presents an approach to teaching computer ethics that blends the use of contemporary media , subscriptions to digests of current technology news , and reflective writing in a learner centered strategy . this approach is designed to make use of activities and assignments that take advantage of ( <digit> ) student interest in contemporary media ( video and film ) to provide motivation and context beyond historical case studies , ( <digit> ) breaking news about technology and technology use in education to provide current real world context , and ( <digit> ) reflective writing to stimulate thinking critically about the course content outside the classroom context . digests published three times weekly provide a constant flow of current real world issues that can be used for focused reflective writing . contemporary media productions are viewed and then a writing assignment in a structured learning log is used to focus on ethical issues raised by the film . we present an example using a feature length film and subsequent learning log assignment .
a pattern theoretic characterization of biological growth . <eos> mathematical and statistical modeling of biological growth is an important problem in medical diagnostics . here , we seek tools to analyze changes in anatomical parts using images collected over time . we introduce a structured model , called growth by random iterated diffeomorphisms ( grid ) , that treats a cumulative growth deformation as a composition of several elementary deformations . each elementary deformation applies to a small region by capturing deformation local to that region and is characterized by a seed and a radial deformation pattern around that seed . these grid variables seed locations and radial deformation patterns are estimated from observed images in two steps <digit> ) estimate a cumulative deformation over an observation interval <digit> ) estimate grid variables using maximum likelihood criterion from this estimated cumulative deformation . we demonstrate this framework using an mri image data of a rat 's brain growth . for future statistical analysis , we propose a time varying poisson process for the seed placements and a random drawing from a predetermined catalog of deformations for the radial deformation patterns .
low power low voltage class ab linear ota for hf filters with a large tuning range . <eos> this letter presents a new low voltage class ab differential linear ota . the proposed transconductor uses a novel scheme based on two cross coupled class ab pseudo differential pairs biased by a flipped voltage follower <digit> . the transconductor has been designed using a 0.8 mum cmos technology to operate at <digit> v supply voltage with only <digit> muw of quiescent power consumption . simulation results show <digit> mhz bandwidth with more than two decades of transconductance tuning range .
on ( q t , t ) arcs of type ( 0,2 , t ) . <eos> in this paper we construct an infinite series of ( q t , t ) arcs of type ( <digit> , <digit> , t ) . we show that this construction includes the korchmaros mazzocca arcs , and we gain new infinite series of examples , too .
semidiscrete formulations for transient transport at small time steps . <eos> solutions of direct time integration schemes for transient advection diffusion reaction problems that converge in time to conventional semidiscrete formulations may be polluted at small time steps by spurious spatial oscillations . this degradation is not an artifact of the time marching scheme , but rather a property of the solution of the semidiscrete galerkin approximation itself . an analogy to steady advection diffusion reaction problems with a modified reaction coefficient by the rothe method of discretizing in time prior to spatial discretization provides an upper bound on the time step for the onset of spatial instability . spatial stabilization removes this pathology , leading to stabilized implicit time integration schemes that are free of spurious oscillations at small time steps . copyright ( c ) <digit> john wiley sons , ltd .
adaptive security management of real time storage applications over nand based storage systems . <eos> establishing the model of security critical storage applications based on security protection of nand flash systems . . deriving a dynamic model that captures the vulnerability and utilization constraints in nand based storage systems . designing a feedback control loop to guarantee security performance and soft real time requirements . employing two pi controllers to achieve the utilization and vulnerability control in one framework .
sensor placement and coordination via distributed multi agent cooperative control . <eos> this paper examines the problem of sensor placement and coordination to maximize the sensor utilization when monitoring different types of environments . our assumption is that the sensors are mobile and each sensor can have more than one type of sensing capabilities which can be active or not at each specific moment . the goal is to maximize the amount of information collected from the environment , given the limited amount of resources that the total of the available sensors can provide , and at the same time to be fault tolerant in failures of individual sensors by using a decentralized approach that re organizes their placement in case of failures . we tackle this problem by employing a decentralized multi agent coordination framework using message passing and the max sum algorithm for building and maintaining a common picture of the area to be monitored . we show that by representing each sensor as an independent agent which can take decisions individually and at the same time can affect the decisions of its neighbouring sensor agents we can provide a robust and efficient system for the monitoring of life critical environments such as assistive environments or governmental infrastructures .
implementation and evaluation of the msc course in health informatics in greece . <eos> objectives health informatics is a well established and important multi disciplinary and inter disciplinary field that not only involves informatics but also medicine , nursing , engineering , biology and other related subjects . the program has been organized on the basis of an inter university approach with the participation of five greek universities . the paper aims at providing a current description of the academic program and a preliminary evaluation of the implementation phase . methods the paper presents a case study of a curriculum implementation from the phase of curriculum development to the phase of implementation and evaluation . due to the interdisciplinary character of the course appropriate procedures were undertaken to ensure that mixed backgrounds can assimilate the broad spectrum of the teaching material taught . in the first stages of the implementation international students mainly from europe attended the course . in addition , local graduates provided an extra dimension to the multi layered difficulties and challenges of such a course implementation . results the students registered in the course were from different backgrounds and disciplines . they were mainly from health sciences and engineering schools . the interdisciplinary arrangement of the course facilitated the proper exchange of thoughts , skills , and knowledge among and between students and teachers . conclusions the postgraduate course in health informatics at the university of athens has now been running for more than fifteen consecutive years and is one of the first and longest standing courses in europe . continuous evaluation and adaptation is required to fit within the changing and evolving amazing field of biomedical and health informatics .
a novel fpga architecture and an integrated framework of cad tools for implementing applications . <eos> a complete system for the implementation of digital logic in a field programmable gate array ( fpga ) platform is introduced . the novel power efficient fpga architecture was designed and simulated in stm 0.18 mu m cmos technology . the detailed design and circuit characteristics of the configurable logic block , the interconnection network , the switch box and the connection box were determined and evaluated in terms of energy , delay and area . a number of circuit level low power techniques were employed because power consumption was the primary concern . additionally , a complete tool framework for the implementation of digital logic circuits in fpga platforms is introduced . having as input vhdl description of an application , the framework derives the reconfiguration bitstream of fpga . the framework consists of i ) non modified academic tools , ii ) modified academic tools and iii ) new tools . furthermore , the framework can support a variety of fpga architectures . qualitative and quantitative comparisons with existing academic and commercial architectures and tools are provided , yielding promising results .
ode a tool for distributing object oriented applications . <eos> object oriented applications are increasingly being deployed in distributed computing environments . technologies , such as java rmi , and architectures , such as corba , dcom , and enterprise java beans , are facilitating and enhancing this trend . the performance and eventual success of these applications is dependent on distribution decisions made by the application designer . this decision is a complex one , involving a large number of alternatives and multiple conflicting criteria . rigorous approaches for effective distribution of object oriented applications are still lacking . this paper describes the implementation of a practical and effective approach for distributing object oriented applications . a prototype decision support systemobject distribution environment ( ode ) that implements the approach in the form of a user friendly tool for the design of distributed object oriented applications is described . ode has been successfully used in the distribution of a real world distributed object oriented system .
fault tolerance of hypercubes and folded hypercubes . <eos> let ( g ( v , e ) ) be a connected graph . the conditional edge connectivity ( lambda _ delta k ( g ) ) is the cardinality of the minimum edge cuts , if any , whose deletion disconnects ( g ) and each component of ( g f ) has ( delta ge k ) . we assume that ( f subseteq e ) is an edge set , ( f ) is called edge extra cut , if ( g f ) is not connected and each component of ( g f ) has more than ( k ) vertices . the edge extraconnectivity ( lambda _ mathrm e k ( g ) ) is the cardinality of the minimum edge extra cuts . in this paper , we study the conditional edge connectivity and edge extraconnectivity of hypercubes and folded hypercubes .
reuse of manufacturing knowledge to facilitate platform based product realization . <eos> product platforming is a technique for exploiting commonality across a family of products . while utilizing a common platform can have many advantages when developing and manufacturing products , the approach places greater demands on collaboration , in particular the sharing and reuse of knowledge and information . repositories are intended to facilitate information sharing across organizational groups and geographically distributed collaborators . a particular challenge in utilizing repositories is culling a search for the most appropriate information for the problem at hand . the reuse existing unit for shape and efficiency ( r.e.u.s.e. ) method facilitates the search of information in a repository through three stages that consider similarity , efficiency , and configuration . automated search and filter techniques are implemented with interaction with the user to effectively obtain the desired results . the similarity stud ) , uses thresholds to clarify different opportunities for reuse . the user can then select alternatives for further examination based on efficiency of satisfaction of desired characteristics . the degree of modification of the similar alternatives is reported to assist in the configuration of the new design . this method contributes to the field by ( a ) accounting for the variety of the product family during the reuse of existing process design information ( b ) integrating an efficiency assessment for retrieval by considering characteristics beyond cost and ( c ) addressing the search with a multicriteria method . the implementation of the r.e.u.s.e method is supported with an example of assembly line design for an air conditioner module in automobile production .
precise rates in the law of the logarithm for negatively associated random variables . <eos> let xn n <digit> x n n <digit> be a strictly stationary sequence of negatively associated random variables with mean zero and finite variance . set s n k <digit> n x k , mn maxk n sk , n <digit> m n max k n s k , n <digit> . suppose <digit> e x <digit> <digit> <digit> k <digit> e x <digit> x k . we study the precise rates of a kind of weighted infinite series of p m n n log n and p s n n log n as <digit> <digit> , and p m n <digit> n <digit> log n as . the results are related to the convergence rates of the law of the logarithm and the chung type law of the logarithm .
antivirus security naked during updates . <eos> the security of modern computer systems heavily depends on security tools , especially on antivirus software solutions . in the anti malware research community , development of techniques for evading detection by antivirus software is an active research area . this has led to malware that can bypass or subvert antivirus software . the common strategies deployed include the use of obfuscated code and staged malware whose first instance ( usually installer such as dropper and downloader ) is not detected by the antivirus software . increasingly , most of the modern malware are staged ones in order for them to be not detected by antivirus solutions at the early stage of intrusion . the installers then determine the method for further intrusion including antivirus bypassing techniques . some malware target boot and or shutdown time when antivirus software may be inactive so that they can perform their malicious activities . however , there can be another time frame where antivirus solutions may be inactive , namely , during the time of update . all antivirus software share a unique characteristic that they must be updated at a very high frequency to provide up to date protection of their system . in this paper , we suggest a novel attack vector that targets antivirus updates and show practical examples of how a system and antivirus software itself can be compromised during the update of antivirus software . local privilege escalation using this vulnerability is also described . we have investigated this design vulnerability with several of the major antivirus software products such as avira , avg , mcafee , microsoft , and symantec and found that they are vulnerable to this new attack vector . the paper also discusses possible solutions that can be used to mitigate the attack in the existing versions of the antivirus software as well as in the future ones . copyright <digit> john wiley sons , ltd .
collaboratory use by peripheral scientists . <eos> recent years have seen an increasing use of collaboratories in scientific work . it is hypothesized that by enabling scientists to reach remotely located data , instruments and experts , collaboratories will benefit peripheral scientists ( e.g. , scientists from developing countries and scientists from minority colleges in the u.s. ) more than core scientists . however , previous studies on computer network use have shown mixed results regarding peripherality effects . adopting a qualitative approach , this study intends to investigate cultural , political , and technical factors that influence collaboratory use by peripheral scientists .
government records and records management law on the right to information in turkey . <eos> operating of the laws on right to information is related to effective management of government records and information having social value . this article contains the relation of government records and records management considering the role of records management at institutions in execution of law on right to information in turkey , and the evaluation of turkish law on the right to information that came into force in <digit> in view of records management and archival approaches .
a random number . <eos> this pedagogical tip presents a physical means for generating a random number that turns out to be not so random after all .
an efficient resource allocation scheme using particle swarm optimization . <eos> developing techniques for optimal allocation of limited resources to a set of activities has received increasing attention in recent years . in this paper , an efficient resource allocation scheme based on particle swarm optimization ( pso ) is developed . different from many existing evolutionary algorithms for solving resource allocation problems ( raps ) , this pso algorithm incorporates a novel representation of each particle in the population and a comprehensive learning strategy for the pso search process . the novelty of this representation lies in that the position of each particle is represented by a pair of points , one on each side of the constraint hyper plane in the problem space . the line joining these two points intersects the constraint hyper plane and their intersection point indicates a feasible solution . with the evaluation value of the feasible solution used as the fitness value of the particle , such a representation provides an effective way to ensure the equality resource constraints in raps are met . without the distraction of infeasible solutions , the particle thus searches the space smoothly . in addition , particles search for optimal solutions by learning from themselves and their neighborhood using the comprehensive learning strategy , helping prevent premature convergence and improve the solution quality for multimodal problems . this new algorithm is shown to be applicable to both single objective and multiobjective raps , with performance validated by a number of benchmarks and by a real world bed capacity planning problem . experimental results verify the effectiveness and efficiency of the proposed algorithm .
enhancing integrated earthquake simulation with high performance computing . <eos> integrated earthquake simulation ( ies ) is a seamless simulation of the three earthquake processes , namely , the earthquake hazard process , the earthquake disaster process and the anti disaster action process . high performance computing ( hpc ) is essential if ies , or particularly , the simulation of the earthquake disaster process is applied to an urban area in which <digit> <digit> structures are located . ies is enhanced with parallel computation , and its performance is examined , so that virtual earthquake disaster simulation will be made for a model of an actual city by inputting observed strong ground motion . it is shown that parallel ies has fairly good scalability even when advanced non linear seismic structure analysis is used .
event based modeling and processing of digital media . <eos> capture , processing , and assimilation of digital media based information such as video , images , or audio requires a unified framework within which signal processing techniques and data modeling and retrieval approaches can act and interact . in this paper we present the rudiments of such a framework based on the notion of events . this framework serves the dual roles of a conceptual data model as well as a prescriptive model that defines the requirements for appropriate signal processing . amongst the key advantages of this framework , lies the fact that it fundamentally brings together the traditionally diverse disciplines of databases and ( various areas of ) digital signal processing . in addition to the conceptual event based framework , we present a physical implementation of the event model . our implementation specifically targets the problem of processing , storage , and querying of multimedia information related to indoor group oriented activities such as meetings . such multimedia information may comprise of video , image , audio , and text based data . we use this application context to illustrate many of the practical challenges that are encountered in this area , our solutions to them , and the open problems that require research across databases , computer vision , audio processing , and multimedia .
a blocked qr decomposition for the parallel symmetric eigenvalue problem . <eos> new parallel algorithm for the qr decomposition of tall and skinny matrices ( based on choleskyqr ) . reduced synchronization requirements , compared to the classic householder qr decomposition . adaptive blocking during householder vector generation , to guarantee numerical stability . considerable speedups were achieved on a bluegene p and power6 system .
compiling cryptographic protocols for deployment on the web . <eos> cryptographic protocols are useful for trust engineering in web transactions . the cryptographic protocol programming language ( cppl ) provides a model wherein trust management annotations are attached to protocol actions , and are used to constrain the behavior of a protocol participant to be compatible with its own trust policy . the first implementation of cppl generated stand alone , single session servers , making it unsuitable for deploying protocols on the web . we describe a new compiler that uses a constraint based analysis to produce multi session server programs . the resulting programs run without persistent tcp connections for deployment on traditional web servers . most importantly , the compiler preserves existing proofs about the protocols . we present an enhanced version of the cppl language , discuss the generation and use of constraints , show their use in the compiler , formalize the preservation of properties , present subtleties , and outline implementation details .
association of pre pregnancy weight and weight gain with perinatal mortality . <eos> reducing infant mortality is one of the primary millennium development goals <digit> . a lot of effort has been made to reduce infant mortality but it remains high in most of the developing countries and the underdeveloped world . perinatal mortality is a cause of great emotional pain and social unrest . the main cause of pregnancy failure in the developed world is obesity but in the under developed world the main cause remains malnutrition . however , their are a mix of factors that affect pregnancy failure in the developing countries . pakistan has a very high infant mortality rate which stands at <digit> deaths per <digit> births . the reasons for this are many including lack of proper healthcare . this is because of a severe shortage of healthcare professionals and specialists in pakistan . the gap in healthcare may be overcome by leveraging it to provide automated healthcare . in this paper , we show how machine learning may be used to predict perinatal failure . we examine the relationship between pre pregnancy weight , weight gain during pregnancy and the body mass index ( bmi ) to investigate how they relate to foetal failure . we employ the k nearest neighbor ( k nn ) technique to automatically differentiate between successful and failed pregnancies . our method is able to predict the the outcome of a pregnancy with about <digit> % accuracy .
a linearly implicit conservative scheme for the coupled nonlinear schrodinger equation . <eos> the coupled nonlinear schrodinger equation models several intersting physical phenomena . it presents a model equation for optical fiber with linear birefringence . in this paper , we present a linearly implicit conservative method to solve this equation . this method is second order accurate in space and time and conserves the energy exactly . many numerical experiments have been conducted and have shown that this method is quite accurate and describe the interaction picture clearly . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
self adaptive multimethod search for global optimization in real parameter spaces . <eos> many different algorithms have been developed in the last few decades for solving complex real world search and optimization problems . the main focus in this research has been on the development of a single universal genetic operator for population evolution that is always efficient for a diverse set of optimization problems . in this paper , we argue that significant advances to the field of evolutionary computation can be made if we embrace a concept of self adaptive multimethod optimization in which multiple different search algorithms are run concurrently , and learn from each other through information , exchange using a common population of points . we present an evolutionary algorithm , entitled a multialgorithm genetically adaptive method for single objective optimization ( amalgam so ) , that implements this concept of self adaptive multimethod search . this method simultaneously merges the strengths of the covariance matrix adaptation ( cma ) evolution strategy , genetic algorithm ( ga ) , and particle swarm optimizer ( pso ) for population evolution and implements a self adaptive learning strategy to automatically tune the number of offspring these three individual algorithms are allowed to contribute during each generation . benchmark results in <digit> , <digit> , and <digit> dimensions using synthetic functions from the special session on real parameter optimization of cec <digit> show that amalgam so obtains similar efficiencies as existing algorithms on relatively simple unimodal problems , but is superior for more complex higher dimensional multimodal optimization problems . the new search method scales well with increasing number of dimensions , converges in the close proximity of the global minimum for functions with noise induced multimodality , and is designed to take full advantage of the power of distributed computer networks .
pilot study of the development of a theory based instrument to evaluate the communication process during multidisciplinary team conferences in rheumatology . <eos> coordinated teams with multidisciplinary team conferences are generally seen as a solution to the management of complex health conditions . however , problems regarding the process of communication during team conferences are reported , such as the absence of a common language or viewpoint and the exchange of irrelevant or repeated information . to determine the outcome of interventions aimed at improving communication during team conferences , a reliable and valid assessment method is needed . to investigate the feasibility of a theory based measurement instrument for assessing the process of the communication during multidisciplinary team conferences in rheumatology . an observation instrument was developed based on communication theory . the instrument distinguishes three types of communication ( i ) grounding activities , ( ii ) coordination of non team activities , and ( iii ) coordination of team activities . to assess the process of communication during team conferences in a rheumatology clinic with inpatient and day patient facilities , team conferences were videotaped . to determine the inter rater reliability , in <digit> conferences concerning <digit> patients with rheumatoid arthritis admitted to the inpatient unit , the instrument was applied by two investigators independently . content validity was determined by analysing and comparing the results of initial and follow up team conferences of <digit> consecutive patients with rheumatoid arthritis admitted to the day patient unit ( wilcoxon signed rank test ) . the inter rater reliability was excellent with the intra class correlation coefficients being > 0.98 for both types i and iii communications in <digit> initial and <digit> follow up conferences ( type ii was not observed ) . an analysis of an additional <digit> initial and <digit> follow up team conferences showed that time spent on grounding ( type i ) made up the greater part of the contents of communication ( <digit> % s.d. <digit> and <digit> % s.d. <digit> in initial and follow up conferences , respectively ) , which is significantly more compared to time spent on co ordination ( p < 0.001 and 0.02 for categories ii and iii , respectively ) . moreover , significantly less time spent was spent on grounding in follow up as compared to initial team conferences , whereas the time spent on coordination ( type iii ) increased ( both p values < 0.001 ) . this theory based measurement instrument for describing and evaluating the communication process during team conferences proved to be reliable and valid in this pilot study . its usefulness to detect changes in the communication process , e.g. after implementing systems for re structuring team conferences mediated by ict applications , should be further examined .
traffic aware cross site virtual machine migration in future mobile cloud computing . <eos> by moving virtual machines ( vms ) to the sites closest to their users , cross site vm migration is promising to improve users experience . however , when multiple vms are required to be migrated , an arbitrary migration sequence will possibly congest the inter site links . to avoid such congestion and also maximize the number of successful migrations , in this paper , we formulate vm migration sequence planning problem as a mixed integer linear programming ( milp ) problem , which considers both inter vm communication traffic and migration traffic . due to the high computational complexity to get the optimal results , we further propose a heuristic algorithm referred as minuti o to approximate the optimal results with low complexity . the extensive simulation results show that the success ratio achieved by minuti o is close to the optimal results with less than <digit> % gap under different topology with variation of migration requests and network conditions .
efficient and accurate numerical methods for the klein gordon schrodinger equations . <eos> in this paper , we present efficient , unconditionally stable and accurate numerical methods for approximations of the klein gordon schrodinger ( kgs ) equations with without damping terms . the key features of our methods are based on ( i ) the application of a time splitting spectral discretization for a schrodinger type equation in kgs , ( ii ) the utilization of fourier pseudospectral discretization for spatial derivatives in the klein gordon equation in kgs , ( iii ) the adoption of solving the ordinary differential equations ( odes ) in phase space analytically under appropriate chosen transmission conditions between different time intervals or applying crank nicolson leap frog for linear nonlinear terms for time derivatives . the numerical methods are either explicit or implicit but can be solved explicitly , unconditionally stable , and of spectral accuracy in space and second order accuracy in time . moreover , they are time reversible and time transverse invariant when there is no damping terms in kgs , conserve ( or keep the same decay rate of ) the wave energy as that in kgs without ( or with a linear ) damping term , keep the same dynamics of the mean value of the meson field , and give exact results for the plane wave solution . extensive numerical tests are presented to confirm the above properties of our numerical methods for kgs . finally , the methods are applied to study solitary wave collisions in one dimension ( id ) , as well as dynamics of a 2d problem in kgs . ( c ) <digit> elsevier inc. all rights reserved .
openmp versus mpi for pde solvers based on regular sparse numerical operators . <eos> two parallel programming models represented by openmp and mpi are compared for pde solvers based on regular sparse numerical operators . as a typical representative of such an operator , a finite difference approximation of the euler equations for fluid flow is considered . the comparison of programming models is made with regard to uniform memory access ( uma ) , non uniform memory access ( numa ) , and self optimizing numa ( numa opt ) computer architectures . by numa opt , we mean numa systems extended with self optimization algorithms , in order to reduce the non uniformity of the memory access time . the main conclusions of the study are ( <digit> ) that openmp is a viable alternative to mpi on uma and numa opt architectures ( <digit> ) that openmp is not competitive on numa platforms , unless special care is taken to get an initial data placement that matches the algorithm ( <digit> ) that for openmp to be competitive in the numa opt case , it is not necessary to extend the openmp model with additional data distribution directives , nor to include user level access to the page migration library .
sentiment analysis of movie reviews on discussion boards using a linguistic approach . <eos> we propose a linguistic approach for sentiment analysis of message posts on discussion boards . a sentence often contains independent clauses which can represent different opinions on the multiple aspects of a target object . therefore , the proposed system provides clause level sentiment analysis of opinionated texts . for each sentence in a message post , it generates a dependency tree , and splits the sentence into clauses . then it determines the contextual sentiment score for each clause utilizing grammatical dependencies of words and the prior sentiment scores of the words derived from sentiwordnet and domain specific lexicons . negation is also delicately handled in this study , for instance , the term not superb is assigned a lower negative sentiment score than the term not good . we have experimented with a dataset of movie review sentences , and the experimental results show the effectiveness of the proposed approach .
design guidelines for the integration of geiger mode avalanche diodes in standard cmos technologies . <eos> the goal of this paper is to provide some useful design guidelines at the device level regarding the main challenges to be typically faced in the design and integration of geiger mode avalanche diodes in a standard cmos process . different techniques are found in literature in order to avoid premature edge breakdown with the aim of limiting the electric field at the edges to be weaker than in the multiplication region . in this article , the use of such techniques , the conditions where they can effectively work and above all their limitations are studied by means of tcad simulations for various diode architectures . additionally , the noise performance is discussed by focusing on the band to band tunneling and shallow trench isolation enhanced dark count rates . geiger mode bias techniques as well as a synthesis on the pros and cons of the various avalanche diode architectures are finally presented aiming at facilitating future design choices .
dummy fill aware buffer insertion after layer assignment based on an effective estimation model . <eos> this paper studies the impact of dummy fill for chemical mechanical polishing ( cmp ) induced capacitance variation on buffer insertion based on a virtual cmp fill estimation model . compared with existing methods , our algorithm is more feasible by performing buffer insertion not in post process but during early physical design . our contributions are threefold . first , we introduce an improved fast dummy fill amount estimation algorithm based on <digit> , and use some speedup techniques ( tile merging , fill factor and amount assigning ) for early estimation . second , based on some reasonable assumptions , we present an optimum virtual dummy fill method to estimate dummy position and the effect on the interconnect capacitance . then the dummy fill estimation model was verified by our experiments . third , we use this model in early buffer insertion after layer assignment considering the effects of dummy fill . experimental results verified the necessity of early dummy fill estimation and the validity of our algorithm . buffer insertion considering dummy fill during early physical design is necessary and our algorithm is promising .
reconsidering assessment in online hybrid courses knowing versus learning . <eos> this study explores the influence of assessment on students ' online written discussions . a two by two design was used to understand students ' expression of knowledge and of learning in the contexts of regular online discussions versus final test online discussions . findings suggested that assessment had an impact on how students interacted online and in their use of rhetorical moves and that knowing and learning are related but distinct constructs , correlated within each writing context , dissociated across contexts , and performing differentially as a function of students ' perceptions of academic demands . we discuss the limitations of traditional assessment , offer an alternative approach , and conclude with practical suggestions for online hybrid course instructors . ( c ) <digit> elsevier ltd. all rights reserved .
energy flow prediction in built up structures through a hybrid finite element wave and finite element approach . <eos> numerical tool for the energy flow evaluation in a periodic substructure from the near field to the far field domain . the near field part is then modeled by finite element method ( fem ) . the periodic structure and the far field part are regarded as waveguides and modeled by an enhanced wave finite element method ( wfem ) where a modal reduction technique is employed to accelerate the calculation . . a multi scale strategy is used such that the final matrices dimensions of the built up structure are largely reduced disorders . an application is presented , a structural dynamical system coupled with periodic resistive piezoelectric shunts is discussed .
structural optimization of 3d masonry buildings . <eos> in the design of buildings , structural analysis is traditionally performed after the aesthetic design has been determined and has little influence on the overall form . in contrast , this paper presents an approach to guide the form towards a shape that is more structurally sound . our work is centered on the study of how variations of the geometry might improve structural stability . we define a new measure of structural soundness for masonry buildings as well as cables , and derive its closed form derivative with respect to the displacement of all the vertices describing the geometry . we start with a gradient descent tool which displaces each vertex along the gradient . we then introduce displacement operators , imposing constraints such as the preservation of orientation or thickness or setting additional objectives such as volume minimization .
a neuro dynamic programming based optimal controller for tomato seedling growth in greenhouse systems . <eos> this work proposes a neuro dynamic programming based optimal controller to guide the growth of tomato seedling crops by manipulating its environmental conditions in a greenhouse . the neurocontroller manages the growth development of the crop , while minimizing a predefined cost function that considers the operative costs and the final state errors under physical constraints on process variables and actuator signals . the aim is to guide the growth of tomato seedlings by controlling the microclimate of the greenhouse . the design process of the neurocontroller considers the nonlinear dynamic behavior of the crop greenhouse system model and the real climate data . simulations of the proposed approach allow for contrasting its performance against those of other strategies for tomato seedling crop development subject to various climatic conditions .
on the capacity of bicm with qam constellations . <eos> in this tutorial paper we analyze the capacity of bit interleaved coded modulation ( bicm ) with quadrature amplitude modulation ( qam ) constellations , and we pay special attention to different bit to symbol labeling strategies . the relation between the bicm capacity and the capacity of other cm schemes such as trellis coded modulation ( tcm ) and multilevel codes ( mlc ) is analyzed . motivated by the fact that for bicm with some particular labelings , the same e b n <digit> maps to more than one bicm capacity value , we study the relation between the capacity and e b n <digit> . in particular , we present some analytical results on this relation , and we also give an intuitive explanation for the somehow contradictory behavior of these curves .
sparse communication networks and efficient routing in the plane ( extended abstract ) . <eos> traditional approaches to network design separate the issues of designing the network itself and designing its management and control subsystems . this paper proposes an approach termed routing oriented network design , which is based on designing the network topology and its routing scheme together , attempting to optimize some of the relevant parameters of both simultaneously . this approach is explored by considering the design of communication networks supporting efficient routing in the special case of points located in the euclidean plane . the desirable network parameters considered include low degree and small number of communication links . the desirable routing parameters considered include small routing tables , small number of hops and low routing stretch . two rather different schemes are presented , one based on direct navigation in the plane and the other based on efficient hierarchical tree covers . on a collection of n sites with diameter d , these methods yield networks with maximum degree o ( log d ) ( hence a total of o ( n log d ) communication links ) , coupled with routing schemes with constant routing stretch , o ( log n log d ) memory bits per vertex and routes with at most log n or log d hops .
evolutionary repair of faulty software . <eos> testing and fault localization are very expensive software engineering tasks that have been tried to be automated . although many successful techniques have been designed , the actual change of the code for fixing the discovered faults is still a human only task . even in the ideal case in which automated tools could tell us exactly where the location of a fault is , it is not always trivial how to fix the code . in this paper we analyse the possibility of automating the complex task of fixing faults . we propose to model this task as a search problem , and hence to use for example evolutionary algorithms to solve it . we then discuss the potential of this approach and how its current limitations can be addressed in the future . this task is extremely challenging and mainly unexplored in the literature . hence , this paper only covers an initial investigation and gives directions for future work . a research prototype called jaff and a case study are presented to give first validation of this approach .
fuzzy unidirectional force control of constrained robotic manipulators . <eos> the end effector of a robotic arm is required to keep a contact on the contour of the constraint surface in tasks such as deburring and grinding . being different from contacts resulted from general mechanical pairs , such a contact is unidirectional , or equivalently , the contact force can only act along the outward normal of the constraint surface at the contact point . how to achieve this specification was not addressed explicitly in many position force control schemes developed so far , instead it was assumed in the development of controllers . in this paper , the unidirectionality of the contact force is explicitly included in modeling and control of constrained robot system . a fuzzy tuning mechanism is developed to generate the impedance model resulted from the continuous contact made by the end effector of the robotic manipulator on the constraint surface while it is in motion . a controller is then developed based on the fuzzy rule bases and the nonlinear feedback technique . the simulation is carried out to verify the effectiveness of the approach . ( c ) <digit> elsevier science b.v. all rights reserved .
a quadratic construction for zielonka automata with acyclic communication structure . <eos> asynchronous automata are parallel compositions of finite state processes synchronizing over shared variables . a deep theorem due to zielonka says that every regular trace language can be recognized by a deterministic asynchronous automaton . the construction is rather involved and the most efficient variant produces automata which are exponential in the number of processes and polynomial in the size of the dfa . in this paper we show a simple , quadratic construction in the case where the synchronization actions are binary and define an acyclic communication graph .
palmprint verification based on 2d gabor wavelet and pulse coupled neural network . <eos> to alleviate the limitation that the recent texture based algorithms for palmprint recognition yield unsatisfactory robustness to the variations of orientation , position and illumination in capturing palmprint images , this paper describes a novel texture based algorithm for palmprint recognition combining 2d gabor wavelets and pulse coupled neural network ( pcnn ) . in the proposed algorithm , palmprint images are decomposed by 2d gabor wavelets , and then pcnn is employed to imitate the creatural vision perceptive process and decompose each gabor subband into a series of binary images . entropies for these binary images are calculated and regarded as features . a support vector machine based classifier is employed to implement classification . experimental results show that the proposed approach yields a better performance in terms of the correct classification percentages and relatively high robustness to the variations of orientation , position and illumination compared with the recent texture based approaches .
a distributed activity scheduling algorithm for wireless sensor networks with partial coverage . <eos> one of the most important design objectives in wireless sensor networks ( wsn ) is minimizing the energy consumption since these networks are expected to operate in harsh conditions where the recharging of batteries is impractical , if not impossible . the sleep scheduling mechanism allows sensors to sleep intermittently in order to reduce energy consumption and extend network lifetime . in applications where <digit> % coverage of the network field is not crucial , allowing the coverage to drop below full coverage while keeping above a predetermined threshold , i.e. , partial coverage , can further increase the network lifetime . in this paper , we develop the distributed adaptive sleep scheduling algorithm ( dassa ) for wsns with partial coverage . dassa does not require location information of sensors while maintaining connectivity and satisfying a user defined coverage target . in dassa , nodes use the residual energy levels and feedback from the sink for scheduling the activity of their neighbors . this feedback mechanism reduces the randomness in scheduling that would otherwise occur due to the absence of location information . the performance of dassa is compared with an integer linear programming ( ilp ) based centralized sleep scheduling algorithm ( cssa ) , which is devised to find the maximum number of rounds the network can survive assuming that the location information of all sensors is available . dassa is also compared with the decentralized dgt algorithm . dassa attains network lifetimes up to <digit> % of the centralized solution and it achieves significantly longer lifetimes compared with the dgt algorithm .
the enhanced quality function deployment for developing virtual items in massive multiplayer online role playing games . <eos> because of the huge potential profit , the development of virtual items in massive multiplayer online role playing games ( mmorpgs ) has lately begun receiving attention . as a successful means for developing new products , the quality function deployment ( qfd ) has been widely used in devising virtual items . in traditional qfd , information about the customers , needs and their priorities can be gained through some marketing methods . however , these approaches heavily rely on the subjective results and can not identify the demands of each customer because of bewildering amount of information . thus , we adopt the genetic chaotic neural network ( gcnn ) technique to identify each customer 's needs and their priorities and propose the enhanced qualify function deployment ( eqfd ) . however , in most of the existing literature , the equations to describe chaos dynamics are fixed and rigid corresponding to different nonlinear dynamic systems . in fact , for many chaotic systems in applications , it is often difficult to obtain accurate and faithful mathematical models , regarding their physically complex structures and hidden parameters . therefore , gcnn is proposed in this paper , where ga is embedded into the chaotic neural network to generate and refine the equations of chaotic systems . by experimenting our methods with several benchmark methods , the proposed gcnn is found to demonstrate a clear advantage over other identifying methods , and eqfd is proven to be a feasible technique for developing the virtual items in mmorpgs . ( c ) <digit> elsevier ltd. all rights reserved .
an agent based approach to the two dimensional guillotine bin packing problem . <eos> the two dimensional guillotine bin packing problem consists of packing , without overlap , small rectangular items into the smallest number of large rectangular bins where items are obtained via guillotine cuts . this problem is solved using a new guillotine bottom left ( gbl ) constructive heuristic and its agent based ( ab ) implementation . gbl , which is sequential , successively packs items into a bin and creates a new bin every time it can no longer fit any unpacked item into the current one . ab , which is pseudo parallel , uses the simplest system of artificial life . this system consists of active agents dynamically interacting in real time to jointly fill the bins while each agent is driven by its own parameters , decision process , and fitness assessment . ab is particularly fast and yields near optimal solutions . its modularity makes it easily adaptable to knapsack related problems .
identification and characterization of thymosin <digit> in chicken macrophages using whole cell maldi tof . <eos> the aim of the study was to determine chicken monocyte and granulocyte associated peptides and proteins using whole cell matrix assisted laser desorption ionization timeofflight mass spectrometry ( maldi tof ms ) and to characterize the peptides based on their abundance . the mass spectra showed a prominent peak at m z <digit> in monocytes macrophages but not in the granulocytes . subsequent purification and characterization of the m z <digit> peptide from an avian macrophage cell line htc , revealed it to be thymosin <digit> ( t <digit> ) , an actin modulating peptide . htc cells when treated with bacterial lipopolysaccharide and peptidoglycan to determine the modulation of t <digit> gene expression or its secretion , showed no changes
explaining and predicting the adoption intention of mobile data services a value based approach . <eos> this study comes to examine the adoption intention of mobile data services ( mds ) in jordan . this study develops a value based approach where value is used as a multidimensional construct . our results show that utilitarian value is according to previous studies an important adoption factor . our results show that economic factor is also significant as cost of using the mobile service comes into play . in the context of this study , hedonic , uniqueness , and epistemic value dimensions are not as important for the use of mds .
a bridge across the bosphorus returned migrants , their internet and media use and social capital . <eos> relatively few studies have focused on migrants who returned to the country of their origin or their parents ' origin . still fewer have examined the communication patterns of call center workers who live in one country but conduct all of their business in the language and culture of another country . drawing on work by portes and bourdieu , this study treats the use of traditional media and the internet and its relation to the bridging and bonding behavior of a group of turks who returned to istanbul from the netherlands and who are now employed by a dutch call center company . based on a survey and three focus groups of employee participants , this research finds that more recent and younger returnees primarily bond with family and friends in holland through use of dutch media and internet use , whereas longer term returnees connect more with turkish media . some bridging was occurring through interpersonal communication with turks by the younger and more recent arrivals .
extracting user web browsing patterns from non content network traces the online advertising case study . <eos> online advertising is a rapidly growing industry currently dominated by the search engine ' giant ' google . in an attempt to tap into this huge market , internet service providers ( isps ) started deploying deep packet inspection techniques to track and collect user browsing behavior . however , these providers have the fear that such techniques violate wiretap laws that explicitly prevent intercepting the contents of communication without gaining consent from consumers . in this paper , we explore how it is possible for isps to extract user browsing patterns without inspecting contents of communication . our contributions are threefold . first , we develop a methodology and implement a system that is capable of extracting web browsing features from stored non content based network traces , which could be legally shared . when such browsing features are correlated with information collected by independently crawling the web , it becomes possible to recover the actual web pages accessed by clients . second , we evaluate our system on the internet and check that it can successfully recover user browsing patterns with high accuracy . ( c ) <digit> elsevier b.v. all rights reserved .
cloud enabled fractal based ecg compression in wireless body sensor networks . <eos> we proposed a cloud efficient compression technique suitable for a wireless body sensor network . the compression ratio achieved is <digit> with percentage residual difference ( prd ) of less than <digit> % . the decompression technique is designed to support partial retrieval of ecg data which make it suitable for cloud solutions . better compression ratios compared with other available compression techniques .
temporal reasoning and bayesian networks . <eos> this work examines important issues in probabilistic temporal representation and reasoning using bayesian networks ( also known as belief networks ) . the representation proposed here utilizes temporal ( or dynamic ) probabilities to represent facts , events , and the effects of events . the architecture of a belief network may change with time to indicate a different causal context . probability variations with time capture temporal properties such as persistence and causation . they also capture event interaction , and when the interaction between events follows known models such as the competing risks model , the additive model , or the dominating event model , the net effect of many interacting events on the temporal probabilities can be calculated efficiently . this representation of reasoning also exploits the notion of temporal degeneration of relevance due to information obsolescence to improve the efficiency .
vertex transitive cubic graphs of square free order . <eos> a classification of connected vertex transitive cubic graphs of square free order is provided . it is shown that such graphs are well characterized metacirculants ( including dihedrants , generalized petersen graphs , mbius bands ) , or tutte 's <digit> cage , or graphs arisen from simple groups psl ( <digit> , p ) .
the two weighted inequalities for sublinear operators generated by b singular integrals in weighted lebesgue spaces . <eos> in this paper , the authors establish several general theorems for the boundedness of sublinear operators ( b sublinear operators ) satisfies the condition ( 1.2 ) , generated by b singular integrals on a weighted lebesgue spaces ( l _ p , omega , gamma ( mathbb r _ k , n ) ) , where ( b sum _ i <digit> k ( frac partial <digit> partial x _ k <digit> frac gamma _ i x _ i frac partial partial x _ i ) ) . the condition ( 1.2 ) are satisfied by many important operators in analysis , including b maximal operator and b singular integral operators . sufficient conditions on weighted functions and <digit> are given so that b sublinear operators satisfies the condition ( 1.2 ) are bounded from ( l _ p , omega , gamma ( mathbb r _ k , n ) ) to ( l _ p , omega _ <digit> , gamma ( mathbb r _ k , n ) ) .
the union of congruent cubes in three dimensions . <eos> a em dihedral ( trihedral ) wedge is the intersection of two ( resp . t hree ) half spaces in reals <digit> . it is called em alpha fat if the angle ( resp. , solid angle ) determined by these half spaces is at least alpha > <digit> . if , in addition , the sum of the three face angles of a trihedral wedge is at least gamma > <digit> pi <digit> , then it is called em ( gamma , alpha ) substantially fat . we prove that , for any fixed gamma > <digit> pi <digit> , alpha > <digit> , the combinatorial complexity of the union of n ( a ) alpha fat dihedral wedges , ( b ) ( gamma , alpha ) substantially fat trihedral wedges is at most o ( n <digit> eps ) , for any eps > <digit> , where the constants of proportionality depend on eps , alpha ( and gamma ) . we obtain as a corollary that the same upper bound holds for the combinatorial complexity of the union of n ( nearly ) congruent cubes in reals <digit> . these bounds are not far from being optimal .
read , write , and present for acm siguccs conferences . <eos> the association of computing machinery special interest group in university and college computing services ( acm siguccs ) is made up of professionals who support and manage of information technology services at higher education institutions . siguccs sponsors an annual conference that is drawn together by volunteers . the conference program takes the form of paper authors presenting their findings in <digit> minute talks , as part of a panel , or in a poster session . papers are presented on a variety of tracks such as management , technology , customer support , documentation and training , or instructional technology . the track titles can change over time . attending and contributing to the siguccs conference program is an opportunity for professional development . this paper seeks to demystify the process of contributing to the siguccs conference program as a reader , author , and presenter and thus make the opportunity to obtain professional development through contributing to the siguccs conference program easier .
web based information content and its application to concept based video retrieval . <eos> semantic similarity between words or phrases is frequently used to find matching correlations between search queries and documents when straightforward matching of terms fails . this is particularly important for searching in visual databases , where pictures or video clips have been automatically tagged with a small set of semantic concepts based on analysis and classification of the visual content . here , the textual description of documents is very limited , and semantic similarity based on wordnet 's cognitive synonym structure , along with information content derived from term frequencies , can help to bridge the gap between an arbitrary textual query and a limited vocabulary of visual concepts . this approach , termed concept based retrieval , has received significant attention over the last few years , and its success is highly dependent on the quality of the similarity measure used to map textual query terms to visual concepts . in this paper , we consider some issues of semantic similarity measures based on information content ( ic ) , and propose a way to improve them . in particular , we note that most ic based similarity measures are derived from a small and relatively outdated corpus ( the brown corpus ) , which does not adequately capture the usage pattern of many contemporary terms for example , out of more than 150,000 wordnet terms , only about 36,000 are represented . this shortcoming reflects very negatively on the coverage of typical search query terms . we therefore suggest using alternative ic corpora that are larger and better aligned with the usage of modern vocabulary . we experimentally derive two such corpora using the www google search engine , and show that they provide better coverage of vocabulary , while showing comparable frequencies for brown corpus terms . finally , we evaluate the two proposed ic corpora in the context of a concept based video retrieval application using the trecvid <digit> , <digit> , and <digit> datasets , and we show that they increase average precision results by up to <digit> % .
comparison between two muscle models under dynamic conditions . <eos> one fundamental problem when trying to calculate the force developed by one muscle during a motor task is the muscle model . usually , one control signal is juxtaposed to one musclotendon unit . the question is how is this signal connected to the activation of the motor units ( mus ) that compose the muscle and fire differently . the aim of the paper is to compare a hill type muscle model to a model composed of mus . a fast elbow flexion performed by only one muscle is considered . the activation necessary for performing the motion and the corresponding frequencies are calculated for cases of fast and slow muscles using hill type model . then the muscle is modelled as a mixture of with uniformly distributed twitch parameters . using motco software the moments of impulsation of all mus and their mechanical responses are predicted . the activation characteristics obtained by the two muscle models are compared . it is concluded that there are two essential parameters for proper muscle modelling the lead time and the mus composition .
local joint entropy based non rigid multimodality image registration . <eos> we present a new variational model for image registration . the local joint entropy is used to measure the similarity of the images to be aligned . the weighted horn type regularization term is used to protect displacement fields from over smoothing . the proposed model has the advantage of aligning local edges of images well . a fast iteration algorithm is designed to solve our model .
survey of experimental evaluation studies for wireless mesh network deployments in urban areas towards ubiquitous internet . <eos> establishing wireless networks in urban areas that can provide ubiquitous internet access to end users is a central part of the efforts towards defining the internet of the future . in recent years , wireless mesh network ( wmn ) backbone infrastructures are proposed as a cost effective technology to provide city wide internet access . studies that evaluate the performance of city wide mesh network deployments via experiments provide essential information on various challenges of building them . in this survey , we particularly focus on such studies and provide brief conclusions on the problems , benefits , and future research directions of city wide wmns .
extremal sizes of subspace partitions . <eos> a subspace partition i of v v ( n , q ) is a collection of subspaces of v such that each <digit> dimensional subspace of v is in exactly one subspace of i . the size of i is the number of its subspaces . let sigma ( q ) ( n , t ) denote the minimum size of a subspace partition of v in which the largest subspace has dimension t , and let rho ( q ) ( n , t ) denote the maximum size of a subspace partition of v in which the smallest subspace has dimension t. in this article , we determine the values of sigma ( q ) ( n , t ) and rho ( q ) ( n , t ) for all positive integers n and t. furthermore , we prove that if n a parts per thousand yen 2t , then the minimum size of a maximal partial t spread in v ( n t <digit> , q ) is sigma ( q ) ( n , t ) .
visual trustworthy monitoring system ( v tms ) for behavior of trusted computing . <eos> as the platform mobility increases , it becomes increasingly susceptible to theft stolen data is often regarded as being more valuable than the notebook hardware itself thus , the need to protect user data and secrets is underscored in a mobile computing environment the trusted platform module ( tpm ) is defined as a hardware instantiation which has been proposed by the tcg ( trusted computing group ) for trust computing tpm offers facilities for the secure generation of cryptographic keys , and limitations on their use , in addition to a hardware pseudo random number generator it also includes capabilities such as remote attestation and sealed storage a tpm can be used to authenticate hardware devices since each tpm chip has a unique and secret rsa key burned in during production , it is capable of performing platform authentication the tpb ( trusted platform board ) is an expansion of the tpm for enhancing the efficiency and usability of the tpm chip in addition to the tpb functions supporting high standard trust environments within the hardware standard of the system in this paper , we develop the v tms ( visual trustworthy monitoring system ) that provides visualization of real time monitoring for the behavior of system resources ( process , memory , network , users , etc ) with tpb and system software for hardening of os and applications moreover , v tms is not only a web based computing environment for system resources but also a real time monitoring system for a trust computing environment
a homotopy based approach for computing defocus blur and affine transform simultaneously . <eos> this paper presents a homotopy based algorithm for a simultaneous recovery of defocus blur and the affine parameters of apparent shifts between planar patches of two pictures . these parameters are recovered from two images of the same scene acquired by a camera evolving in time and or space and for which the intrinsic parameters are known . using limited taylor 's expansion one of the images ( and its partial derivatives ) is expressed as a function of the partial derivatives of the two images , the blur difference , the affine parameters and a continuous parameter derived from homotopy methods . all of these unknowns can thus be directly computed by resolving a system of equations at a single scale . the proposed algorithm is tested using synthetic and real images . the results confirm that dense and accurate estimation of the previously mentioned parameters can be obtained .
architectures for functional imagination . <eos> imagination can be defined broadly as the manipulation of information that is not directly available to an agent 's sensors . however , the topic of imagination raises representational , physiological , and phenomenological issues that can not be tackled easily without using the body as a reference point . within this framework , we define functional imagination as the mechanism that allows an embodied agent to simulate its own actions and their sensory consequences internally , and to extract behavioural benefits from doing so . in this paper , we present five necessary and sufficient requirements for the implementation of functional imagination , as well as a minimal architecture that meets all these criteria . we also present a taxonomy for categorising possible architectures according to their main attributes . finally , we describe experiments with some simple architectures designed using these principles and implemented on simulated and real robots , including an extremely complex anthropomimetic humanoid .
quantifying patterns of agentenvironment interaction . <eos> this article explores the assumption that a deeper ( quantitative ) understanding of the information theoretic implications of sensorymotor coordination can help endow robots not only with better sensory morphologies , but also with better exploration strategies . specifically , we investigate by means of statistical and information theoretic measures to what extent sensorymotor coordinated activity can generate and structure information in the sensory channels of a simulated agent interacting with its surrounding environment . the results show how the usage of correlation , entropy , and mutual information can be employed ( a ) to segment an observed behavior into distinct behavioral states ( b ) to analyze the informational relationship between the different components of the sensorymotor apparatus and ( c ) to identify patterns ( or fingerprints ) in the sensorymotor interaction between the agent and its local environment .
an inventory model for deteriorating items with stock dependent consumption rate and shortages under inflation and time discounting . <eos> this paper derives an inventory model for deteriorating items with stock dependent consumption rate and shortages under inflation and time discounting over a finite planning horizon . we show that the total cost function is convex . with the convexity , a simple solution algorithm is presented to determine the optimal order quantity and the optimal interval of the total cost function . the results are discussed with a numerical example and particular cases of the model are discussed in brief . a sensitivity analysis of the optimal solution with respect to the parameters of the system is carried out .
significance of q fever serologic diagnosis in clinically suspect patients . <eos> q fever is caused by c. burnetii , an intracellular obligate bacterium . for clinical confirmation of q fever , diagnosis of interstitial pneumonia is of significance . the acute disease varies in severity from minor to fatal , with the possibility of serious complications . chronic endocarditis is a well known outcome . symptoms of q fever can vary fixing diagnosis is done by serology with the phase i and the phase ii antibody . we tested <digit> sera of <digit> clinically suspect patients . from these , <digit> patients were taken to the infection clinic , <digit> to the pulmonary clinic , and one to the general hospital . from the <digit> patients , <digit> patients had one serum , <digit> patients , <digit> sera , and <digit> patients , <digit> sera . blood samples were collected by vein puncture , and serum samples were kept at 20c until testing . all sera were processed by indirect imunofluorescent assay ( ifa ) q fever igm and igg . of <digit> processed sera , <digit> were seropositive . specific igm antibody was found in sera of <digit> patients ( 19.4 % ) , and specific igg antibody in sera of <digit> patients ( 51.2 % ) . in sera of <digit> clinically suspect patients ( 48.3 % ) , no specific anticoxiella antibody was found . from these results we can confirm the importance of serology in laboratory diagnosis and clinical affirmation of suspect q fever . indirect imunofluorescent assay ( ifa ) is reliable and appropriate for daily , routine diagnosis of human q fever .
residual stress estimation in damascene copper interconnects using embedded sensors . <eos> mechanical stress in damascene copper low k interconnects has been studied by means of micro rotating sensors embedded in chips and directly integrated in cmos process flow . a new hinge sensor design has been elaborated and a new analytical model of the mechanical equilibrium of sensors is validated . these sensors allow the study of the average residual stress as a function of the line width in a range from few hundred nanometers to several microns . it was found that the residual stress increases from <digit> to 850mpa in , respectively , <digit> and 0.25 m wide lines . this trend shows a yield stress increase with the line width reduction . copper grains microstructure change between large and narrow lines is probably one of the reasons for yield stress and so residual stress increase . this microstructure change has been observed by means of transmission electron microscopy ( tem ) observations .
visually localizing design problems with disharmony maps . <eos> assessing the quality of software design is difficult , as design is expressed through guidelines and heuristics , not rigorous rules . one successful approach to assess design quality is based on detection strategies , which are metrics based composed logical conditions , by which design fragments with specific properties are detected in the source code . such detection strategies , when executed on large software systems usually return large sets of artifacts , which potentially exhibit one or more design disharmonies , which are then inspected manually , a cumbersome activity . in this article we present disharmony maps , a visualization based approach to locate such flawed software artifacts in large systems . we display the whole system using a 3d visualization technique based on a city metaphor . we enrich such visualizations with the results returned by a number of detection strategies , and thus render both the static structure and the design problems that affect a subject system . we evaluate our approach on a number of open source java systems and report on our findings .
optimal database search waves and catalysis . <eos> grover 's database search algorithm , although discovered in the context of quantum computation , can be implemented using any system that allows superposition of states . a physical realization of this algorithm is described using coupled simple harmonic oscillators , which can be exactly solved in both classical and quantum domains . classical wave algorithms are far more stable against decoherence compared to their quantum counterparts . in addition to providing convenient demonstration models , they may have a role in practical situations , such as catalysis .
on numerical methods for highly oscillatory problems in circuit simulation . <eos> purpose the purpose of this paper is to analyse a novel technique for an efficient numerical approximation of systems of highly oscillatory ordinary differential equations ( odes ) that arise in electronic systems subject to modulated signals . design methodology approach the paper combines a filon type method with waveform relaxation techniques for nonlinear systems of odes . findings the analysis includes numerical examples to compare with traditional methods such as the trapezoidal rule and runge kutta methods . this comparison shows that the proposed approach can be very effective when dealing with systems of highly oscillatory differential equations . research limitations implications the present paper constitutes a preliminary study of filon type methods applied to highly oscillatory odes in the context of electronic systems , and it is a starting point for future research that will address more general cases . originality value the proposed method makes use of novel and recent techniques in the area of highly oscillatory problems , and it proves to be particularly useful in cases where standard methods become expensive to implement .
type inference and semi unification . <eos> the milner calculus is the typed lgr calculus underlying the type system for the programming language ml har86 and several other strongly typed polymorphic functional languages such as miranda tur86 and sps wan84 . mycroft myc84 extended the problematical typing rule for recursive definitions and proved that the resulting calculus , termed milner mycroft calculus here , is sound with respect to milner 's mil78 semantics and that it preserves the principal typing property dm82 of the milner calculus . the extension is of practical significance in typed logic programming languages mo84 and , more generally , in any language with ( mutually ) recursive definitions . mycroft did n't solve the decidability problem for typings in this calculus , though . this was an open problem independently raised also by meertens mee83 . the decidability question was answered in the affirmative just recently by kfoury et al. in ktu88 . we show that the type inference problems in the milner and the milner mycroft calculi can be reduced to solving equations and inequations between first order terms , a problem we have termed semi unification . we show that semi unification problems have most general solutions in analogy to unification problems which translates into principal typing properties for the underlying calculi . in contrast to the ( essentially ) nonconstructive methods of ktu88 we present functional specifications , which we prove partially correct , for computing the most general solution of semi unification problems , and we devise a concrete nondeterministic algorithm on a graph theoretic representation for computing these most general solutions . finally , we point out some erroneous statements about the efficiency of polymorphic type checking that have persisted throughout the literature including an incorrect claim , submitted by ourselves , of polynomial time type checking in the milner mycroft calculus .
srtm resample with short distance low nugget kriging . <eos> the shuttle radar topography mission ( srtm ) , was flow on the space shuttle endeavour in february <digit> , with the objective of acquiring a digital elevation model of all land between <digit> degrees north latitude and <digit> degrees south latitude , using interferometric synthetic aperture radar ( insar ) techniques . the srtm data are distributed at horizontal resolution of <digit> arc second ( similar to 30m ) for areas within the usa and at <digit> arc second ( similar to 90m ) resolution for the rest of the world . a resolution of 90m can be considered suitable for the small or medium scale analysis , but it is too coarse for more detailed purposes . one alternative is to interpolate the srtm data at a finer resolution it will not increase the level of detail of the original digital elevation model ( dem ) , but it will lead to a surface where there is the coherence of angular properties ( i.e. slope , aspect ) between neighbouring pixels , which is an important characteristic when dealing with terrain analysis . this work intents to show how the proper adjustment of variogram and kriging parameters , namely the nugget effect and the maximum distance within which values are used in interpolation , can be set to achieve quality results on resampling srtm data from <digit> to <digit> . we present for a test area in western usa , which includes different adjustment schemes ( changes in nugget effect value and in the interpolation radius ) and comparisons with the original <digit> model of the area , with the national elevation dataset ( ned ) dems , and with other interpolation methods ( splines and inverse distance weighted ( idw ) ) . the basic concepts for using kriging to resample terrain data are ( i ) working only with the immediate neighbourhood of the predicted point , due to the high spatial correlation of the topographic surface and omnidirectional behaviour of variogram in short distances ( ii ) adding a very small random variation to the coordinates of the points prior to interpolation , to avoid punctual artifacts generated by predicted points with the same location than original data points and ( iii ) using a small value of nugget effect , to avoid smoothing that can obliterate terrain features . drainages derived from the surfaces interpolated by kriging and by splines have a good agreement with streams derived from the <digit> ned , with correct identification of watersheds , even though a few differences occur in the positions of some rivers in flat areas . although the <digit> surfaces resampled by kriging and splines are very similar , we consider the results produced by kriging as superior , since the spline interpolated surface still presented some noise and linear artifacts , which were removed by kriging .
one set of pliers for more tasks in installation work the effects on ( dis ) comfort and productivity . <eos> in installation work , the physical workload is high . awkward postures , heavy lifting and repetitive movements are often seen . to improve aspects of the work situation , frequently used pliers were redesigned to make them suitable for more cutting tasks . in this study these multitask pliers are evaluated in comparison to the originally used pliers in a field study and a laboratory study . for the field study <digit> subjects participated divided into two groups according to their type of work . ten subjects participated in the laboratory study . the multitask plier appeared to result in more comfort during working , more relaxed working and more satisfaction . no differences in productivity were found . in conclusion , the multitask pliers can replace the originally used pliers and are suitable for more tasks than the original pliers . the installation workers have to carry less pliers by using the multitask pliers .
simulated unbound structures for benchmarking of protein docking in the dockground resource . <eos> proteins play an important role in biological processes in living organisms . many protein functions are based on interaction with other proteins . the structural information is important for adequate description of these interactions . sets of protein structures determined in both bound and unbound states are essential for benchmarking of the docking procedures . however , the number of such proteins in pdb is relatively small . a radical expansion of such sets is possible if the unbound structures are computationally simulated .
threshold based admission control for a multimedia grid analysis and performance evaluation . <eos> in a grid based services system facing a large number of requests with different services and profits significance , there is always a trade off between the system profits and the quality of service ( qos ) . in such systems , admission control plays an important role the system has to employ a proper strategy to make admission control decisions and reserve resources for the coming requests thus to achieve greater profits without violating the qos of the requests already admitted . in this paper , we introduce three essential admission control strategies with threshold on resource reservation and a newly proposed strategy with layered threshold . through comprehensive theoretical analyses and extensive simulations , we demonstrate that the strategy with layered threshold is more efficient and flexible than the existing strategies for grid based multimedia services systems . copyright ( c ) <digit> john wiley sons , ltd .
diffusion kernels on statistical manifolds . <eos> a family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models . the kernels are based on the heat equation on the riemannian manifold defined by the fisher information metric associated with a statistical family , and generalize the gaussian kernel of euclidean space . as an important special case , kernels based on the geometry of multinomial families are derived , leading to kernel based learning algorithms that apply naturally to discrete data . bounds on covering numbers and rademacher averages for the kernels are proved using bounds on the eigenvalues of the laplacian on riemannian manifolds . experimental results are presented for document classification , for which the use of multinomial geometry is natural and well motivated , and improvements are obtained over the standard use of gaussian or linear kernels , which have been the standard for text classification .
self monitoring for sensor networks . <eos> local monitoring is an effective mechanism for the security of wireless sensor networks ( wsns ) . existing schemes assume the existence of sufficient number of active nodes to carry out monitoring operations . such an assumption , however , is often difficult for a large scale sensor network . in this work , we focus on designing an efficient scheme integrated with good self monitoring capability as well as providing an infrastructure for various security protocols using local monitoring . to the best of our knowledge , we are the first to present the formal study on finding optimized self monitoring topology for wsns . we show the problem is np complete even under the unit disk graph ( udg ) model , and give the upper bound on the approximation ratio . we further propose two distributed polynomial algorithms with provable approximation ratio to address this issue . through comprehensive simulations , we evaluate the effectiveness of this design .
hci and business practices in a collaborative method for augmented reality systems . <eos> every interactive system is composed of a functional core and a user interface . however , the software engineering ( se ) and humancomputer interaction ( hci ) communities do not share the same methods , models or tools . this usually induces a large work overhead when specialists from the two domains try to connect their applicative studies , especially when developing augmented reality systems that feature complex interaction cores . we present in this paper the essential activities and concepts of a development method integrating the se and hci development practices , from the specifications down to the design , as well as their application on a case study . the efficiency of the method was tested in a qualitative study involving four pairs of se and hci experts in the design of an application for which an augmented reality interaction would provide better user performance than a classic interactive system . the effectivity of the method was evaluated in a qualitative study comparing the quality of three implementations of the same application fragment ( based on the same analysis model ) , using software engineering metrics . the first evaluation confirmed the ease of use of our method and the relevance of our tools for guiding the design process , but raised concerns on the handling of conflicting collaborative activities . the second evaluation gave indications that the structure of the analysis model facilitates the implementation of quality software ( in terms of coupling , stability and complexity ) . it is concluded that our method enables design teams with different backgrounds in application development to collaborate for integrating augmented reality applications with information systems . areas of improvement are also described .
dimensions of emotion in expressive musical performance . <eos> abstract this paper explores the dimensions of emotion conveyed by music . participants rated emotion terms after seeing and or hearing recordings of clarinet performances that varied in expressive content . a factor analysis revealed four independent dimensions of emotion . changes to the clarinetists ' expressive intentions did not significantly affect emotions conveyed by sound . it was largely through the visual modality that expressive intentions influenced the experience for observers .
study of a plane free jet exhausting from a channel by vortex in cell method . <eos> a two dimensional simulation of a plane jet exhausting from a channel has been performed using the vortex in cell algorithm in the reynolds number range of <digit> <digit> . the vorticity is generated on the wall of the entrance channel whose length has been fixed in order to obtain a fully developed velocity profile at the entry of the jet . the transient behaviour of the velocity field starting from rest has been observed until reaching a quasi steady regime . the mean value of the velocity field is compared with the results of a finite volume computation on the same mesh . the velocity fluctuations obtained using this method are analysed . their effect on the mean flow is estimated to be smaller than the viscous effect . copyright ( c ) <digit> john wiley sons , ltd .
an improved relaxed complex scheme for receptor flexibility in computer aided drug design . <eos> the interactions among associating ( macro ) molecules are dynamic , which adds to the complexity of molecular recognition . while ligand flexibility is well accounted for in computational drug design , the effective inclusion of receptor flexibility remains an important challenge . the relaxed complex scheme ( rcs ) is a promising computational methodology that combines the advantages of docking algorithms with dynamic structural information provided by molecular dynamics ( md ) simulations , therefore explicitly accounting for the flexibility of both the receptor and the docked ligands . here , we briefly review the rcs and discuss new extensions and improvements of this methodology in the context of ligand binding to two example targets kinetoplastid rna editing ligase <digit> and the w191g cavity mutant of cytochrome c peroxidase . the rcs improvements include its extension to virtual screening , more rigorous characterization of local and global binding effects , and methods to improve its computational efficiency by reducing the receptor ensemble to a representative set of configurations . the choice of receptor ensemble , its influence on the predictive power of rcs , and the current limitations for an accurate treatment of the solvent contributions are also briefly discussed . finally , we outline potential methodological improvements that we anticipate will assist future development .
twin support vector machine with universum data . <eos> the universum , which is defined as the sample not belonging to either class of the classification problem of interest , has been proved to be helpful in supervised learning . in this work , we designed a new twin support vector machine with universum ( called u u tsvm ) , which can utilize universum data to improve the classification performance of tsvm . unlike u u svm , in u u tsvm , universum data are located in a nonparallel insensitive loss tube by using two hinge loss functions , which can exploit these prior knowledge embedded in universum data more flexible . empirical experiments demonstrate that u u tsvm can directly improve the classification accuracy of standard tsvm that use the labeled data alone and is superior to u u svm in most cases .
compiler analysis of irregular memory accesses . <eos> irregular array accesses are array accesses whose array subscripts do not have closed form expressions in terms of loop indices . traditional array analysis and loop transformation techniques can not handle irregular array accesses . in this paper , we study two kinds of simple and common cases of irregular array accesses single indexed access and indirect array access . we present techniques to analyze these two cases at compile time , and we provide experimental results showing the effectiveness of these techniques in finding more implicit loop parallelism at compile time and improved speedups .
relying on time synchronization for security in ad hoc networks . <eos> mobile ad hoc networks are networks composed of mobile computing devices with wireless connections and no fixed infrastructure . the unique attributes of these networks cause new security problems . some security protocols , such as the timed , efcient , streaming , loss tolerant authentication protocol , tesla <digit> , rely on loose time synchronization for security . these protocols have not fully analyzed the implications of relying on time synchronization in this environment . this paper looks into some of the difficulties in time synchronization and points out further areas for research .
model execution an approach based on extending domain specific modeling with action reports . <eos> in this paper , we present an approach to development and application of domain specific modeling ( dsm ) tools in the model based management of business processes . the level of model to text ( m2t ) transformations in the standard architecture for domain specific modeling solutions is extended with action reports , which allow synchronization between models , generated code , and target interpreters . the basic idea behind the approach is to use m2t transformation languages to construct submodels , client application components , and operations on target interpreters . in this manner , m2t transformations may be employed to support not only generation of target platform code from domain specific graphical language ( dsgl ) models but also straightforward use of models and appropriate dsm tools as client applications . the applicability of action reports is demonstrated by examples from document engineering , and measurement and control systems .
fast and scalable parallel processing of scalar multiplication in elliptic curve cryptosystems . <eos> to secure parallel systems in communication networks , in this paper , we propose a fast and scalable parallel scalar multiplication method over generic elliptic curves for elliptic curve cryptosystems , by means of our proposed scalar folding and unfolding techniques . in contrast to previous parallel scalar multiplication methods , our method can be implemented into scalable parallel computers . the optimal time complexity is k point doublings ( d ) plus log k point additions ( a ) , denoted as kd ( logk ) a , where k is the bit length of the scalar . if our method is applied to koblitz curves , the optimal time complexity can be reduced to ( logk ) a. furthermore , previous simple side channel protected scalar multiplication methods can be integrated into our method for resisting against simple side channel attacks . copyright ( c ) <digit> john wiley sons , ltd .
construction of dual ovsf codes with lower correlations . <eos> in wide band code division multiple access ( wcdma ) , orthogonal variable spreading factors ( ovsf ) codes are assigned to different users to preserve the orthogonality between users ' physical channels . in this letter , we present the dual ovsf code , which can transmit the variable data rates by suing two different modulated signals without loss of the orthogonality . the bit error rate ( ber ) performance under a multi user environment suffering the additive white gaussian noise ( awgn ) channel and correlations of those codes are evaluated . the results demonstrate that the proposed dual ovsf scheme could provide flexible rates and lower correlation values with a slight increase in complexity .
layout optimization of cmos functional cells . <eos> an optimal non exhaustive method of minimizing the layout area of complementary series parallel cmos functional cells in the standard cell style is presented . this generalizes earlier work of uehara and van cleemput which is heuristic and nonoptimal . a complete graph theoretical framework for cmos cell layout is developed and illustrated . the approach demonstrates a new class of graph based algebras which characterize this layout problem .
some theoretical aspects of algorithmic routing . <eos> in this paper we describe a router which has the capabilities of automatic rip up and rerouting . the system employs a look ahead feature for determining if as yet unrouted nets are unroutable . backtrack programming is used to implement an implicit enumeration scheme which sequentially produces alternative paths for a given net . the relationship between this type of router and the problem of ordering nets is discussed . by varying parameters in our router which deal with back tracking and path length deviations , we can obtain a hierarchy of routing systems . one extreme case corresponds to a classical lee type router another to a perfect router which guarantees finding a solution if one exists .
classifier subset selection to construct multi classifiers by means of estimation of distribution algorithms . <eos> this paper proposes a novel approach to select the individual classifiers to take part in a multiple classifier system . individual classifier selection is a key step in the development of multi classifiers . several works have shown the benefits of fusing complementary classifiers . nevertheless , the selection of the base classifiers to be used is still an open question , and different approaches have been proposed in the literature . this work is based on the selection of the appropriate single classifiers by means of an evolutionary algorithm . different base classifiers , which have been chosen from different classifier families , are used as candidates in order to obtain variability in the classifications given . experimental results carried out with <digit> databases from the uci repository show how adequate the proposed approach is stacked generalization multi classifier has been selected to perform the experimental comparisons .
energy cost evaluation of parallel algorithms for multiprocessor systems . <eos> with the continuous development of hardware and software , graphics processor units ( gpus ) have been used in the general purpose computation field . they have emerged as a computational accelerator that dramatically reduces the application execution time with cpus . to achieve high computing performance , a gpu typically includes hundreds of computing units . the high density of computing resource on a chip brings in high power consumption . therefore power consumption has become one of the most important problems for the development of gpus . this paper analyzes the energy consumption of parallel algorithms executed in gpus and provides a method to evaluate the energy scalability for parallel algorithms . then the parallel prefix sum is analyzed to illustrate the method for the energy conservation , and the energy scalability is experimentally evaluated using sparse matrix vector multiply ( spmv ) . the results show that the optimal number of blocks , memory choice and task scheduling are the important keys to balance the performance and the energy consumption of gpus .
large scale multi task image labeling with adaptive relevance discovery and feature hashing . <eos> this paper proposes a novel multi label classification approach . it seamlessly incorporates the idea of multi task feature hashing learning . it can capture the task relationships at task level as well as feature level .
global change and the evolution of phenotypic plasticity in plants . <eos> global change drivers create new environmental scenarios and selective pressures , affecting plant species in various interacting ways . plants respond with changes in phenology , physiology , and reproduction , with consequences for biotic interactions and community composition . we review information on phenotypic plasticity , a primary means by which plants cope with global change scenarios , recommending promising approaches for investigating the evolution of plasticity and describing constraints to its evolution . we discuss the important but largely ignored role of phenotypic plasticity in range shifts and review the extensive literature on invasive species as models of evolutionary change in novel environments . plasticity can play a role both in the short term response of plant populations to global change as well as in their long term fate through the maintenance of genetic variation . in new environmental conditions , plasticity of certain functional traits may be beneficial ( i.e. , the plastic response is accompanied by a fitness advantage ) and thus selected for . plasticity can also be relevant in the establishment and persistence of plants in novel environments that are crucial for populations at the colonizing edge in range shifts induced by climate change . experimental studies show taxonomically widespread plastic responses to global change drivers in many functional traits , though there is a lack of empirical support for many theoretical models on the evolution of phenotypic plasticity . future studies should assess the adaptive value and evolutionary potential of plasticity under complex , realistic global change scenarios . promising tools include resurrection protocols and artificial selection experiments .
a computer aided cost estimation system for bga dca technology . <eos> the last decade has seen an increasing demand for smaller and more densely populated printed circuit boards ( pcbs ) . this is partly due to the market driven need within the electronics industry of reducing the size of products while concurrently enhancing their capabilities . consequently , the electronics packaging industry is relying upon area array technologies such as ball grid array ( bga ) and direct chip attach ( dca ) as possible replacements for the traditional peripherally leaded surface mount packaging formats . however , since these technologies are still in their nascent stage , the cost benefits obtained from them need to be quantified in an effort to aid in justifying their use . this paper describes a computer aided cost estimation ( cage ) system which has been developed to justify the use of bga dca devices .
a new model for health care e insurance using credit points and service oriented architecture ( soa ) . <eos> nowadays the concept of e insurance is about how to apply online for insurance services ( e.g. policy plan , claim ) . in this study we tried to develop a new model which is not based on insurance company plans . this new model is based on credits , points and rewards for health care insurance . with using this new model , clients will be able to choose their own created health care insurance plan . on the other hand small business groups involved a few number of clinics or hospitals can create their own health and care insurance . the new model is more appropriate for short term insurance and small town or areas . as the insurance risk is higher than available insurance plans , in this model the insurance price and investing the premiums are more than normal insurer company 's policies . moreover it can cover all who wants to be insured and also it is more flexible in terms of clients ' needs . finally researcher used service oriented architecture ( soa ) to connect all clinics and hospitals which are involved in the plan behind an e insurance website . therefore insured party just needs to connect e insurance website in order to make an appointment for receiving medical care and etc .
eulerian rotations of deformed nuclei for tddft calculations . <eos> we discuss three practical methods for performing eulerian rotations of slater determinants in a three dimensional cartesian geometry . in addition to the straightforward application of the active form of the quantum mechanical rotation operator , we introduce two methods using a passive position space rotation followed by an active spin space rotation , one after variation and the other before variation . these methods can be used to initialize reactions involving deformed nuclei where a particular alignment of the deformed nuclei with respect to the collision axis is desired . we show that doing the rotation before the variation is the most efficient way of generating such initial states .
a fast and memory efficient discrete focal stack transform for plenoptic sensors . <eos> plenoptic cameras are a new type of sensors that capture the four dimensional lightfield of a scene . processing the recorded lightfield , they extend the capabilities of current commercial cameras . conventional cameras obtain photographs focusing at a determined depth . this photograph can be described through a projection of the four dimensional lightfield onto two spatial dimensions . the collection of such images is the focal stack of the scene . the focal stack can be used to select an image refocused at a certain depth , to recover 3d information or to obtain all in focus images . there are several approaches to the computation of the focal stack . in this paper we propose a new technique to compute the focal stack by means of its frequency decomposition that can be seen as an extension of the discrete focal stack transform ( dfst ) . this new approach decreases the computational complexity of the dfst maintaining an efficient memory use . experimental results are provided to show the validity of the technique and its extension to 3d processing and all in focus image computation is also studied .
managing information systems for health services in a developing country a case study using a contextualist framework . <eos> investments in information technology ( it ) have been escalating in the health sector in both developed and developing countries . however , the failure rate of applications is of concern especially for countries with scarce resources . there is insufficient understanding of factors that lead to such failures in developing countries . a case study of implementing a computerised information system ( is ) for health services in the philippines is analysed using a contextualist framework . factors that led to the failure included ambiguity in the organisation and in responsibility for the project , lack of capacity to undertake large information systems development projects and inability to retain appropriate staff . however , when the historical and contextual issues were revealed and the interplay between the content , process and context of the change was analysed it was revealed that the content of the is was not responsive to the changes in the wider health system . the case study confirms the need to analyse and understand organisational , environmental and cultural issues in adopting models and procedures used elsewhere when managing information systems in developing countries .
an extensible , lightweight architecture for adaptive j2ee applications . <eos> server applications with adaptive behaviors can adapt their functionality in response to environmental changes , and significantly reduce the on going costs of system deployment and administration . however , developing adaptive server applications is challenging due to the complexity of server technologies and highly dynamic application environments . this paper presents an architecture framework , known as the adaptive server framework ( asf ) . asf provides a clear separation between the implementation of adaptive behaviors and the server application business logic . this means a server application can be cost effectively extended with programmable adaptive features through the definition and implementation of control components defined in asf . furthermore , asf is a lightweight architecture in that it incurs low cpu overhead and memory usage . we demonstrate the effectiveness of asf through a case study , in which a server application dynamically determines the resolution and quality to scale an image based on the load of the server and network connection speed . the experimental evaluation demonstrates the performance gains possible by adaptive behaviors and the low overhead introduced by asf .
very high order compact finite difference schemes on non uniform grids for incompressible navier stokes equations . <eos> this article presents a family of very high order non uniform grid compact finite difference schemes with spatial orders of accuracy ranging from 4th to 20th for the incompressible navier stokes equations . the high order compact schemes on non uniform grids developed in shukla and zhong r.k. shukla , x. zhong , derivation of high order compact finite difference schemes for non uniform grid using polynomial interpolation , j. comput . phys . <digit> ( <digit> ) <digit> for linear model equations are extended to the full navier stokes equations in the vorticity and streamfunction formulation . two methods for the solution of helmholtz and poisson equations using high order compact schemes on non uniform grids are developed . the schemes are constructed so that they maintain a high order of accuracy not only in the interior but also at the boundary . second order semi implicit temporal discretization is achieved through an implicit backward differentiation scheme for the linear viscous terms and an explicit adam bashforth scheme for the non linear convective terms . the boundary values of vorticity are determined using an influence matrix technique . the resulting discretized system with boundary closures of the same high order as the interior is shown to be stable , when applied to the two dimensional incompressible navier stokes equations , provided enough grid points are clustered at the boundary . the resolution characteristics of the high order compact finite difference schemes are illustrated through their application to the one dimensional linear wave equation and the two dimensional driven cavity flow . comparisons with the benchmark solutions for the two dimensional driven cavity flow , thermal convection in a square box and flow past an impulsively started cylinder show that the high order compact schemes are stable and produce extremely accurate results on a stretched grid with more points clustered at the boundary . ( c ) <digit> elsevier inc. all rights reserved .
emergence of un correlated common mode oscillations in the sensory cortex . <eos> simultaneous eeg recordings from various cortical areas indicate the presence of common mode , spatially coherent oscillations . these oscillations are characterized by a common wave form with a spatially distributed pattern of amplitude modulation ( am ) . we observe highly reproducible am patterns across spatially separated channels within various areas , yet the temporal correlations between the channels are low . in the framework of the present research , a nonlinear , spatially distributed dynamical model of neuronal populations ( kiii ) is used for the interpretation of the observed spatial coherence . the theoretical findings are in good agreement with experiments performed with chronically implanted rabbits .
optimization and coordination of svc based supplementary controllers and psss to improve power system stability using a genetic algorithm . <eos> in this paper , a lead lag structure is proposed as a main damping controller for a static var . compensator ( s vc ) to diminish power system oscillations . to confirm the transient performance of the proposed controller , it was compared to a proportional integral derivative ( pid ) damping controller . power system stability improvement was thoroughly examined using these supplementary damping controllers as well as a power system . stabilizer ( pss ) . the generic algorithm , ( ca ) is well liked in the academic environment due to its immediate perceptiveness , ease of performance , and ability to impressively solve highly nonlinear objectives . thus , the ca optimization technique was applied to solve an optimization problem , and to achieve optimal parameters of the svc based supplementary damping controllers and pss . the coordinated design problem of these devices was formulated as an optimization problem , to reduce power system . oscillations . the transient performance of the damping controllers and pss were evaluated under a , severe disturbance for a single machine infinite bus ( smib ) and multimachine power system . the nonlinear simulation results of the smib power system suggest that power system stability was increasingly improved using the coordinated design of the svc based lead lag controller and ps 's , rather than the coordinated design of the svc based p id controller and pss . furthermore , the interarea and local modes of the oscillations were superiorly damped using the proposed controller in the multimachine power system .
planning the location and rating of distributed energy storage in lv networks using a genetic algorithm with simulated annealing . <eos> to fix voltage rise in lv networks , dnos traditionally reinforce lv networks . we show that the severity of the voltage problem depends on the penetration of pv . a new planning tool is produced for locating and sizing energy storage in networks . storage is shown to be cheaper than reinforcement depending on the pv penetration . we use the tool to show that small single phase storage reduces rating capacity .
mapping trigger conditions onto trigger units during post silicon validation and debugging . <eos> on chip trigger units are employed for detecting events of interest during post silicon validation and debugging . their implementation constrains the trigger conditions that can be programmed at runtime . it is often the case that some trigger events of interest , which were not accounted for during design time , can not be detected due to the constraints imposed by the hardware implementation of the trigger units . to address this issue , we present architectural features that can be included into the trigger units and discuss the algorithmic approach for automatically mapping trigger conditions onto the trigger units .
buzz based recommender system . <eos> in this paper , we describe a buzz based recommender system based on a large source of queries in an ecommerce application . the system detects bursts in query trends . these bursts are linked to external entities like news and inventory information to find the queries currently in demand which we refer to as buzz queries . the system follows the paradigm of limited quantity merchandising , in the sense that on a per day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity , and improving activity and stickiness on the site . a semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood .
test data compression and decompression based on internal scan chains and golomb coding . <eos> we present a data compression method and decompression architecture for testing embedded cores in a system on a chip ( soc ) . the proposed approach makes effective use of golomb coding and the internal scan chain ( s ) of the core under test and provides significantly better results than a recent compression method that uses golomb coding and a separate cyclical scan register ( csr ) . the major advantages of golomb coding of test data include very high compression , analytically predictable compression results , and a low cost and scalable on chip decoder . the use of the internal scan chain for decompression obviates the need for a csr , thereby reducing hardware overhead considerably . in addition , the novel interleaving decompression architecture allows multiple cores in an soc to be tested concurrently using a single ate i o channel . we demonstrate the effectiveness of the proposed approach by applying it to the iscas <digit> benchmark circuits .
optimal sleep transistor synthesis under timing and area constraints . <eos> leakage power reduction in nano cmos designs has gained tremendous interest both in academia and industry . many techniques have been proposed in the literature for leakage power reduction and one of the prominent techniques for leakage power reduction is the use of sleep transistors as power gating elements to cut off sub threshold leakage current in circuits when they are in stand by mode . although sleep transistor insertion is very effective in cutting off leakage , it also incurs timing , area and routing overhead . since most of the sleep transistor insertion methodologies do post layout insertion , care should be taken such that there is minimal perturbation of the original layout . over design of sleep transistors cells and sub optimal sleep transistor placement must be avoided to achieve final design closure . since the sleep transistor area plays an important and prominent role in this aspect , it necessitates for optimal sleep transistor sizing and synthesis technique under area constraints . in this paper , we first provide a methodology for optimal sleep transistor synthesis under given area constraints . we then apply our technique to the general timing and area constraint driven row based power gating methodology proposed in <digit> and show how optimal low leakage designs with constraints on timing and area can be designed .
a framework for comparing task performance in real and virtual scenes . <eos> in this paper , we describe a framework for comparing task performance in real and virtual environments . realistic graphics , rear projection , haptics and rapid prototyping are used to match the virtual scene to the real scene . we describe some preliminary placement tasks which were evaluated using eye tracking and discuss our future plans for this framework .
a scalable algorithm to order and annotate continuous observations reveals the metastable states visited by dynamical systems . <eos> advances in it infrastructure have enabled the generation and storage of very large data sets describing complex systems continuously in time . these can derive from both simulations and measurements . analysis of such data requires the availability of scalable algorithms . in this contribution , we propose a scalable algorithm that partitions instantaneous observations ( snapshots ) of a complex system into kinetically distinct sets ( termed basins ) . to do so , we use a combination of ordering snapshots employing the methods only essential parameter , i.e. , a definition of pairwise distance , and annotating the resultant sequence , the so called progress index , in different ways . specifically , we propose a combination of cut based and structural annotations with the former responsible for the kinetic grouping and the latter for diagnostics and interpretation . the method is applied to an illustrative test case , and the scaling of an approximate version is demonstrated to be o ( nlogn ) o ( n log n ) with n n being the number of snapshots . two real world data sets from river hydrology measurements and protein folding simulations are then used to highlight the utility of the method in finding basins for complex systems . both limitations and benefits of the approach are discussed along with routes for future research .
worse a workbench for model based security engineering . <eos> we propose a tool supported approach to enhance the quality of security policies . for this goal , a workbench for model based security engineering has been developed . it implements a uniform method to engineer policy specific security models . it allows for detailed model analysis based on this uniform calculus . it implements a novel method for heuristic safety analysis of access control models .
current feedback operational amplifier based oscillators . <eos> this paper demonstrates the practicality of the current feedback operational amplifier ( cfoa ) in realizing grounded capacitor ( or grounded resistor ) oscillator circuits . the paper begins with a description of the minimum passive component oscillators , using one or two cfoas . next , two new single cfoa oscillators with independent control on the condition of oscillation are generated from the single cfoa minimum component oscillator . three new oscillator circuits using two cfoas are introduced . grounded capacitor and grounded resistor oscillators using three cfoas with independent control on the condition of oscillation and on the frequency of oscillation are also included . the proposed grounded component oscillators are suitable for vlsi implementation . pspice simulations and experimental results demonstrating the performance of some of the proposed oscillators are given .
simulation of service oriented systems for mobile ad hoc networks . <eos> the simulation of distributed systems that implement a service oriented architecture ( soa ) imposes a set of requirements on their simulation models . these models must replicate the interactions that characterise the soa operational paradigm and the role of the entities that support an soa . when those interactions occur over mobile ad hoc networks ( manets ) , simulation models must accurately reproduce the effects that mobility and other networking factors have on the system . this paper presents the architecture of a simulation environment for manet interconnected service oriented systems . the simulation environment combines the realistic soa support provided by osgi with the network simulation capabilities of ns <digit> . the application layer elements of the simulation models built for this environment are also soa implementations , thus promoting structural model validity . this approach provides the opportunity of utilising elements or prototypes from the modelled systems as part of the simulation model , promoting its continuity . in addition , it takes advantage of the benefits that soa has brought to field of modelling and simulation .
adaptive wavelet thresholding for image denoising and compression . <eos> the first part of this paper proposes an adaptive , data driven threshold for image denoising via wavelet soft thresholding , the threshold is derived in a bayesian framework , and the prior used on the wavelet coefficients is the generalized gaussian distribution ( ggd ) widely used in image processing applications , the proposed threshold is simple and closed form , and it is adaptive to each subband because it depends on data driven estimates of the parameters . experimental results show that the proposed method , called bayesshrink , is typically within <digit> % of the mse of the best soft thresholding benchmark with the image assumed known , it also outperforms donoho and johnstone 's sureshrink most of the time . the second part of the paper attempts to further validate recent claims that lossy compression can be used for denoising , the bayesshrink threshold can aid in the parameter selection of a coder designed with the intention of denoising , and thus achieving simultaneous denoising and compression . specifically , the zero zone in the quantization step of compression is analogous to the threshold value in the thresholding function . the remaining coder design parameters are chosen based on a criterion derived from rissanen 's minimum description length ( mdl ) principle , experiments show that this compression method does indeed remove noise significantly , especially for large noise power , however , it introduces quantization noise and should he used only if bitrate were an additional concern to denoising .
debugging distributed c programs by real time reply . <eos> bugnet is a portable unix system designed to debug c programs distributed within a local area network . a graphics interface allows the user of a sun workstation to manage process groups and monitor process interactions very conveniently . bugnet gives information about interprocess communication , i o events , and execution traces for each component process . it allows the user to detect an error visually , to roll global program state back to a time before the error , and to replay events almost exactly as they previously occurred . current work on bugnet is making the implementation easier to port , seeing if replay accuracy can be improved by minor adjustments in the unix process scheduler , linking it with unix dbx to control individual processes , and determining useful tools for filtering of long event strings and for detecting errors . the bugnet project is testing how well existing multiprocess c programs can be debugged without special hardware features that make porting difficult . an initial version is running on a network of suns . it currently reproduces real time execution sequences with an accuracy of 0.01 to 0.10 seconds .
an analytic approach to the asymptotic variance of trie statistics and related structures . <eos> we develop analytic tools for the asymptotics of general trie statistics , which are particularly advantageous for clarifying the asymptotic variance . many concrete examples are discussed for which new fourier expansions are given . the tools are also useful for other splitting processes with an underlying binomial distribution . we specially highlight philippe flajolet 's contribution in the analysis of these random structures .
object oriented biomedical system modelingthe rationale . <eos> a short tutorial and a rationale for object oriented biomedical ( continuous ) system modelling ( oobsm ) are given . the paper investigates and defines what is needed in order to make the work with complex bio medical and pathophysiological models easier , less error prone and conceptually clearer than is possible by using the existing modelling techniques . it also contains a specification of what is required in order to make such models and corresponding knowledge communicable among different research groups and in order to use such models as components in even more complex models . the work shows that hitherto available continuous system modelling languages and tools are less suitable for the construction of complex , interdisciplinary , multilevel , hierarchical models and model components and that those modelling languages do not allow for easy exchange and communication of the model knowledge between different research groups and sites . it concludes that object oriented and distributed objects methodologies are both feasible and suitable for such modelling .
on the quest for optimal rule learning heuristics . <eos> the primary goal of the research reported in this paper is to identify what criteria are responsible for the good performance of a heuristic rule evaluation function in a greedy top down covering algorithm . we first argue that search heuristics for inductive rule learning algorithms typically trade off consistency and coverage , and we investigate this trade off by determining optimal parameter settings for five different parametrized heuristics . in order to avoid biasing our study by known functional families , we also investigate the potential of using metalearning for obtaining alternative rule learning heuristics . the key results of this experimental study are not only practical default values for commonly used heuristics and a broad comparative evaluation of known and novel rule learning heuristics , but we also gain theoretical insights into factors that are responsible for a good performance . for example , we observe that consistency should be weighted more heavily than coverage , presumably because a lack of coverage can later be corrected by learning additional rules .
an approach to extracting trunk from an image . <eos> rendering realistic trees is quite important for simulating a 3d natural scene . separating the trunk from its background is the first step toward the 3d model construction of the tree . in this paper , a three phase algorithm is developed to extract the trunk structure of the tree and hence segment the trunk from the image . some experiments were conducted and results confirmed the feasibility of proposed algorithm .
disulfide connectivity prediction based on structural information without a prior knowledge of the bonding state of cysteines . <eos> previous studies predicted the disulfide bonding patterns of cysteines using a prior knowledge of their bonding states . in this study , we propose a method that is based on the ensemble support vector machine ( svm ) , with the structural features of cysteines extracted without any prior knowledge of their bonding states . this method is useful for improving the predictive performance of disulfide bonding patterns . for comparison , the proposed method was tested with the same dataset spx that was adopted in previous studies . the experimental results demonstrate that bridge classification and disulfide connectivity predictions achieve 96.5 % and 89.2 % accuracy , respectively , using the ensemble svm model , which outperforms the traditional method ( 51.5 % and 51.0 % , respectively ) and the model that is based on a single kernel svm classifier ( 94.6 % and 84.4 % , respectively ) . for protein chain and residue classifications , the sensitivity , specificity , and accuracy of ensemble and single kernel svm approaches are better than those of the traditional methods . the predictive performances of the ensemble svm and single kernel models are identical , indicating that the ensemble model can converge to the single kernel model for some applications .
a parallel network emulation method for evaluating the correctness and performance of applications . <eos> network emulation system constructs a virtual network environment which has the characteristics of controllable and repeatable network conditions . this makes it possible to predict the correctness and performance of proposed new technology before deploying to internet . in this paper we present a methodology for evaluating the correctness and performance of applications based on the parnem , a parallel discrete event network emulator . parnem employs a bsp based real time event scheduling engine , provides flexible interactive mechanism and facilitates legacy network models reuse . parnem allows detailed and accurate study of application behavior . comprehensive case studies covering bottleneck bandwidth measurement and distributed cooperative web caching system demonstrate that network emulation technology opens a wide range of new opportunities for examining the behavior of applications .
diconic addition of failsafe fault tolerance . <eos> we present a divide and conquer method , called diconic , for automatic addition of failsafe fault tolerance to distributed programs , where a failsafe program guarantees to meet its safety specification even when faults occur . specifically , instead of adding fault tolerance to a program as a whole , we separately revise program actions so that the entire program becomes failsafe fault tolerant . our diconic algorithm has the potential to utilize the processing power of a large number of machines working in parallel , thereby enabling automatic addition of failsafe fault tolerance to distributed programs with a large number of processes . we formulate our diconic synthesis algorithm in terms of the satisfiability problem and demonstrate our approach for the byzantine generals problem and an industrial application .
application of neural networks and kano 's method to content recommendation in web personalization . <eos> as customers become more skilled in the use of internet , many companies have gradually established their websites with more and more enormous information to get future competition in electronic commerce ( ec ) . however , the miscellaneous information often brings the users at a loss . web personalization provides a solution to improvement of information overloading on websites . the objective of web personalization is to give users a website they want or need , and thus knowing the needs of users is an important task for content recommendation in web personalization . in this article , we propose a hybrid approach for this task . the proposed approach trains the artificial neural networks to group users into different clusters , and applies the well established kano 's method to extracting the implicit needs from users in different clusters . finally , a real case of tour and travel websites applying the approach is presented to demonstrate the improvement of information overloading . ( c ) <digit> elsevier ltd. all rights reserved .
multi objective optimization with a max t t norm fuzzy relational equation constraint . <eos> in this paper , we consider minimizing multiple linear objective functions under a max t t norm fuzzy relational equation constraint . since the feasible domain of a maxarchimedean t t norm relational equation constraint is generally nonconvex , traditional mathematical programming techniques may have difficulty in yielding efficient solutions for such problems . in this paper , we apply the two phase approach , utilizing the min operator and the average operator to aggregate those objectives , to yield an efficient solution . a numerical example is provided to illustrate the procedure .
the effect of a pro ( <digit> ) thr point mutation on the local structure and stability of human galactokinase enzyme a theoretical study . <eos> galactokinase is responsible for the phosphorylation of alpha d galactose , which is an important step in the metabolism of the latter . malfunctioning of galactokinase due to a single point mutation causes cataracts and , in serious cases , blindness . this paper reports a study of the pro ( <digit> ) thr point mutation using a variety of theories including molecular dynamics ( md ) , mm pbsa gbsa calculations and aim analysis . altered h bonding networks were detected based on geometric and electron density criteria that resulted in local unfolding of the beta sheet secondary structure . another consequence was the decrease in stability ( <digit> <digit> kcal mol ( <digit> ) ) around this region , as confirmed by delta g ( bind ) calculations for the extracted part of the whole system . local unfolding was verified by several other md simulations performed with different duration , initial velocities and force field . based on the results , we propose a possible mechanism for the unfolding caused by the pro ( <digit> ) thr point mutation .
unmanned aerial vehicle aided communications system for disaster recovery . <eos> after natural disasters such as earthquakes , floods , hurricanes , tornados and fires , providing emergency management schemes which mainly rely on communications systems is essential for rescue operations . to establish an emergency communications system during unforeseen events such as natural disasters , we propose the use of a team of unmanned aerial vehicles ( uavs ) . the proposed system is a post disaster solution and can be used whenever and wherever required . each uav in the team has an onboard computer which runs three main subsystems responsible for end to end communication , formation control and autonomous navigation . the onboard computer and the low level controller of the uav cooperate to accomplish the objective of providing local communications infrastructure . in this study , the subsystems running on each uav are explained and evaluated by simulation studies and field tests using an autonomous helicopter . while the simulation studies address the efficiency of the end to end communication subsystem , the field tests evaluate the accuracy of the navigation subsystem . the results of the field tests and the simulation studies show that the proposed system can be successfully used in case of disasters to establish an emergency communications system .
nonparametric fuzzy regression k nn and kernel smoothing techniques . <eos> fuzzy regression without predefined functional form , or nonparametric fuzzy regression , is investigated . the two most basic nonparametric regression techniques in statistics , namely , k nearest neighbor smoothing and kernel smoothing , are fuzzified and analyzed . algorithms are proposed to obtain the best smoothing parameters based on the minimization of cross validation criteria . ( c ) <digit> elsevier science ltd. all rights reserved .
a laminate parametrization technique for discrete ply angle problems with manufacturing constraints . <eos> in this paper we present a novel laminate parametrization technique for layered composite structures that can handle problems in which the ply angles are limited to a discrete set . in the proposed technique , the classical laminate stiffnesses are expressed as a linear combination of the discrete options and design variable weights . an exact penalty function is employed to drive the solution toward discrete <digit> <digit> designs . the proposed technique can be used as either an alternative or an enhancement to simp type methods such as discrete material optimization ( dmo ) . unlike mixed integer approaches , our laminate parametrization technique is well suited for gradient based design optimization . the proposed laminate parametrization is demonstrated on the compliance design of laminated plates and the buckling design of a laminated stiffened panel . the results demonstrate that the approach is an effective alternative to dmo methods .
consert constructing optimal name based routing tables . <eos> name based routing belongs to a routing category different from address based routing , it is usually adopted by content oriented networks sharma etal. , <digit> , koponen etal. , <digit> , rajahalme etal. ,2011 , thaler etal. ,1998 , hwang etal. , <digit> , gritter etal. , <digit> , chawathe etal. , <digit> , caesar etal. , <digit> , carzaniga etal. , <digit> , koponen etal. , <digit> , hwang etal. , <digit> singla etal. , <digit> , detti etal. , <digit> , jain etal. , <digit> xu etal. , <digit> , katsaros etal. , <digit> . <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> and <digit> e.g. , the recently proposed named data networking ( ndn ) . it populates routers with name based routing tables , which are composed of name prefixes and their corresponding next hop ( s ) . name based routing tables are believed to have much larger size than ip routing tables , because of the large amount of name prefixes and the unbounded length of each prefix . this paper presents consertan algorithm that , given an arbitrary name based routing table as input , computes a routing table with the minimal number of prefixes , while keeping equivalent forwarding behavior . the optimal routing table also supports incremental update . we formulate the consert algorithm and prove its optimality with an induction method . evaluation results show that , consert can reduce <digit> to <digit> % prefixes in the synthetic routing tables depending on the distribution of the next hops , and meanwhile improve the lookup performance by more than <digit> % . prior efforts usually focus on compact data structures and lookup algorithms so as to reduce memory consumption and expedite lookup speed of the routing table , while consert compresses the routing table from another perspective it removes the inherent redundancy in the routing table . therefore , consert is orthogonal to these prior efforts , thus the combination of consert and a prior compressing method would further optimize the memory consumption and lookup speed of the routing table . e.g. , we can first adopt consert to achieve the optimal routing table , and afterwards apply namefilter wang etal. ,2013 . <digit> , a two stage bloom filter method , to that optimal table . this combination diminishes the memory consumption of the routing table data structure by roughly <digit> % , and increases the lookup throughput by around <digit> % simultaneously . the joint method outperforms each individual method in terms of memory savings and absolute lookup throughout increase .
idd a supervised interval distance based method for discretization . <eos> this paper introduces a new method for supervised discretization based on interval distances by using a novel concept of neighborhood in the target 's space . the proposed method takes into consideration the order of the class attribute , when this exists , so that it can be used with ordinal discrete classes as well as continuous classes , in the case of regression problems . the method has proved to be very efficient in terms of accuracy and faster than the most commonly supervised discretization methods used in the literature . it is illustrated through several examples , and a comparison with other standard discretization methods is performed for three public data sets by using two different learning tasks a decision tree algorithm and svm for regression .
symbolic analysis of network security policies using rewrite systems . <eos> first designed to enable private networks to be opened up to the outside world in a secure way , the growing complexity of organizations make firewalls indispensable to control information flow within a company . the central role they hold in the security of the organization information make their management a critical task and that is why for years many works have focused on checking and analyzing firewalls . the composition of firewalls , taking into account routing rules , has nevertheless often been neglected . in this paper , we propose to specify all components of a firewall , ie filtering and translation rules , as a rewrite system . we show that such specifications allow us to handle usual problems such as comparison , structural analysis and query analysis . we also propose a formal way to describe the composition of firewalls ( including routing ) in order to build a whole network security policy . the properties of the obtained rewrite system are strongly related to the properties of the specified networks and thus , classical theoretical and practical tools can be used to obtain relevant security properties of the security policies .
three dimensional packings with rotations . <eos> we present approximation algorithms for the three dimensional strip packing problem , and the three dimensional bin packing problem . we consider orthogonal packings where <digit> degrees rotations are allowed . the algorithms we show for these problems have asymptotic performance bounds 2.64 , and 4.89 , respectively . these algorithms are for the more general case in which the bounded dimensions of the bin given in the input are not necessarily equal ( that is , we consider bins for which the length . the width and the height are not necessarily equal ) . moreover , we show that these problems in the general version are as hard to approximate as the corresponding oriented version . ( c ) <digit> elsevier ltd. all rights reserved .
perception for collision avoidance and autonomous driving . <eos> the navlab group at carnegie mellon university has a long history of development of automated vehicles and intelligent systems for driver assistance . the earlier work of the group concentrated on road following , cross country driving , and obstacle detection . the new focus is on short range sensing , to look all around the vehicle for safe driving . the current system uses video sensing , laser rangefinders , a novel light stripe rangefinder , software to process each sensor individually , a map based fusion system , and a probability based predictive model . the complete system has been demonstrated on the navlab i i vehicle for monitoring the environment of a vehicle driving through a cluttered urban environment , detecting and tracking fixed objects , moving objects , pedestrians , curbs , and roads . ( c ) <digit> elsevier ltd. all rights reserved .
a low cost and calibration free gaze estimator for soft biometrics an explorative study . <eos> an explorative study about gaze as soft biometrics . a calibration free gaze estimator . gaze estimation from low cost devices . a non invasive approach for gaze estimation . quantitative and qualitative evaluation in real environments .
taxonomy development and assessment of global information technology outsourcing decisions . <eos> purpose this paper seeks to provide taxonomy and assessment methodologies for executives of global conglomerates with a selection of variables which can help them evaluate outsourcing decisions . design methodology approach a range of established theories , which addressed outsourcing decisions , are identified . the major determinants of global outsourcing were then formulated into an integrated model . each of the variables identified was validated using multiple theories . finally , a weighted score index was used to demonstrate how the variables can be used to evaluate outsourcing decisions . findings it provides an over view of outsourcing theories about the variety of major reasons and their associated determinants as well as attributes that are relevant to decision makers . it showed that the individual theories can be integrated into a global taxonomy . this taxonomy can be assessed using a weighted index because the inputs and computation processes contain realistic qualitative and quantitative information . research limitations implications it is an assessment methodology that requires input and judgment from a variety of experts . persons having such expertise may be fairly costly and difficult to find . practical implications a very simple , yet comprehensive , and useful taxonomy for executives making outsourcing decisions . the assessment index is a proven methodology that is used by business consultants for a variety of related applications involving decision choices . originality value this paper fulfills the need for a more comprehensive view and a systematic approach to the assessment of outsourcing decisions . the index of attributes identified and the evaluation technique proposed are a practical approach .
segmentation of tongue muscles from super resolution magnetic resonance images . <eos> the first attempt to segment tongue muscles from in vivo mr images is presented . the focus is on segmenting the genioglossus and inferior longitudinalis muscles . the game theoretic approach is applied for landmark based segmentation . the database will be publicly released for objective method comparison .
framework for user acceptance clustering for fine grained results . <eos> in an attempt to uncover intra group behavior similarities , we developed an open multi level framework for understanding the process of technology acceptance by its users . we partitioned our population into groups by clustering at several levels and then for each level it was divided into subgroups with a measurement layer added to uncover subgroup influence . thus , by intersecting the resulting clusters of the set of models , the population was divided into subgroups that have similarities in the factors measured by the cluster layer models . subsequently we tested our framework in a university hospital setting personality and prior technology background models were used in clustering via the five factor model and the technology readiness index . utaut was used in the measurement layer . our hypothesis that the subgroups have differing degrees of explained variance and different predictors was confirmed . our framework was open , because any model that results in a taxonomy of the population can be used to obtain meaningful clusters .
navigation of interactive sonifications and visualisations of time series data using multi touch computing . <eos> this paper discusses interaction design for interactive sonification and visualisation of data in multi touch contexts . interaction design for data analysis is becoming increasingly important as data becomes more openly available . we discuss how navigation issues such as zooming , selection , arrangement and playback of data relate to both the auditory and visual modality in different ways , and how they may be linked through the modality of touch and gestural interaction . for this purpose we introduce a user interface for exploring and interacting with representations of time series data simultaneously in both the visual and auditory modalities .
wikis in enterprise settings a survey . <eos> the wiki technology is increasingly being used in corporate environments to facilitate a broad range of tasks . this survey examines the use of wikis on a variety of organisational tasks that include the codification of explicit and tacit organisational knowledge and the formulation of corporate communities of practice , as well as more specific processes such as the collaborative information systems development , the interactions of the enterprise with third parties , management activities and organisational response in crisis situations . for each one of the aforementioned corporate functions , the study examines the findings of related research literature to highlight the advantages and concerns raised by the wiki usage and to identify specific solutions addressing them . finally , based on the above findings , the study discusses various aspects of the wiki usage in the enterprise and identifies trends and future research directions on the field .
red tides prediction system using fuzzy reasoning and the ensemble method . <eos> a red tide is a temporary natural phenomenon in which harmful algal blooms ( habs ) can lead to fin fish and shellfish dying en masse . for example , habs can damage sea farming on the coast of south korea , and generally have a bad influence on the coastal environment and sea ecosystem . prediction of red tide blooms , which consists of a categorical type and a numerical type , can minimize the mitigation cost of hab disasters and the suffering caused by the damage from red tide events . the first type of prediction has high precision but it represents a simple binary result , and the second can predict how much harm an algal increase causes , but its prediction has lower accuracy than the results of the categorical type . to enhance the automatic forecast of red tide , this paper proposes a red tide prediction method that uses fuzzy reasoning and the ensemble method to obtain prediction results for the categorical and numerical types . the proposed method improves the precision of categorical prediction because the ensemble classifier is enhanced by optimal data of the proposed preprocessing . the method forecasts a numerical prediction , such as the increasing density of red tide algae , using the fuzzy reasoning by which the accuracy of numerical results is improved by the proposed post processing . the experimental results demonstrate that the proposed method achieves a better red tide prediction performance than other single classifiers .
large time behaviour of solutions to the equations of one dimensional nonlinear thermoviscoelasticity with memory . <eos> this paper is concerned with the large time behaviour of globally defined smooth solutions of the initial boundary value problem for the one dimensional nonlinear thermoviscoelasticity system with memory .
a multilevel input system with force sensitive elements . <eos> force sensitive multilevel input elements are introduced as the basic building blocks for compact size input devices in mobile environments . compared with switch type keys , multilevel elements can decrease the number of keys on a keyboard while maintaining the input capacity . a multilevel input mechanism using force sensitive sensor pads is demonstrated in a three level three element tactile chording system with multimodal feedback . two schemes are introduced to segment the output range of the sensor into levels . for relatively unpracticed users , the scheme based on maximum finger forces gives an average error rate of 20.2 % and an input time of 2.24 s for a chord of three inputs . reclassification of the experimental data using gaussian segmentation shows that significant improvement of the performance can be expected . ( c ) <digit> academic press .
data visualization sliders . <eos> computer sliders are a generic user input mechanism for specifying a numeric value from a range . for data visualization , the effectiveness of sliders may be increased by using the space inside the slider as an interactive color scale , a barplot for discrete data , and a density plot for continuous data . the idea is to show the selected values in relation to the data and its distribution . furthermore , the selection mechanism may be generalized using a painting metaphor to specify arbitrary , disconnected intervals while maintaining an intuitive user interface .
semi explicit solution and fast minimization scheme for an energy with a ( <digit> ) fitting and tikhonov like regularization . <eos> regularized energies with a ( <digit> ) fitting have attracted a considerable interest in the recent years and numerous aspects of the problem have been studied , mainly to solve various problems arising in image processing . in this paper we focus on a rather simple form where the regularization term is a quadratic functional applied on the first order differences between neighboring pixels . we derive a semi explicit expression for the minimizers of this energy which shows that the solution is an affine function in the neighborhood of each data set . we then describe the volumes of data for which the same system of affine equations leads to the minimum of the relevant energy . our analysis involves an intermediate result on random matrices constructed from truncated neighborhood sets . we also put in evidence some drawbacks due to the a ( <digit> ) fitting . a fast , simple and exact optimization method is proposed . by way of application , we separate impulse noise from gaussian noise in a degraded image .
a variant of inductive counting . <eos> we present a new version of the inductive counting , accepting the complement of an nspace ( s ( n ) ) language nondeterministically in space o ( s ( n ) ) , independent of whether s ( n ) greater than or equal to log n , but using an additional '' one way pebble a movable marker placed on the input tape . this reduces the space used by inductive counting to log n o ( s ( n ) ) bits on the binary work tape and gives the weakest known nondeterministic device accepting a co nspace ( o ( log n ) ) language . ( c ) <digit> elsevier science b.v. all rights reserved .
temperature distribution in steel during hot rolling pseudo bond graph view . <eos> in hot rolling , a metal is given its final shape by plastically deforming the original stock . in this present work , the above mentioned deformation process is modelled , i.e. , temperature , flow stress at each point in and around the deformation zone . a viable bond graph model has been developed to study the essential dynamics inside the material . low carbon steel has been considered and the modelling is described for single pass hot rolling .
capacity bounds in random wireless networks . <eos> we consider a receiving node , located at the origin , and a poisson point process ( ppp ) that models the locations of the desired transmitter as well as the interferers . interference is known to be non gaussian in this scenario . the capacity bounds for additive non gaussian channels depend not only on the power of interference ( i.e. , up to second order statistics ) but also on its entropy power which is influenced by higher order statistics as well . therefore , a complete statistical characterization of interference is required to obtain the capacity bounds . while the statistics of sum of signal and interference is known in closed form , the statistics of interference highly depends on the location of the desired transmitter . in this paper , we show that there is a tradeoff between entropy power of interference on the one hand and signal and interference power on the other hand which have conflicting effects on the channel capacity . we obtain closed form results for the cumulants of the interference , when the desired transmitter node is an arbitrary neighbor of the receiver . we show that to find the cumulants , joint statistics of distances in the ppp will be required which we obtain in closed form . using the cumulants , we approximate the interference entropy power and obtain bounds on the capacity of the channel between an arbitrary transmitter and the receiver . our results provide insight and shed light on the capacity of links in a poisson network . in particular , we show that , in a poisson network , the closest hop is not necessarily the highest capacity link .
convex copy number variation estimation in exome sequencing data using hmm . <eos> one of the main types of genetic variations in cancer is copy number variations ( cnv ) . whole exome sequenicng ( wes ) is a popular alternative to whole genome sequencing ( wgs ) to study disease specific genomic variations . however , finding cnv in cancer samples using wes data has not been fully explored .
analogy considered harmful . <eos> the computer is like a typewriter . the computer is like a filing cabinet . the computer is a personal servant ready to obey your every command . it is often claimed ( e.g. , carroll and thomas <digit> , rumelhart and norman <digit> ) that the best way to introduce a new user to a computer system is to draw an analogy between the computer and some situation familiar to the user . given the analogy , the new user can draw upon his knowledge about the familiar situation in order to reason about the workings of the mysterious new computer system . for example , if the new user wants to understand about how the computer file system works , he need only think about how an office filing cabinet works and then carry over this same way of thinking to the computer file system .
risking trust in a public key infrastructure old techniques of managing risk applied to new technology . <eos> installing a public key infrastructure ( pki ) can change in the security model of an it operation in several ways . this article gives a layman 's overview of what exactly a pki is , and how one can be built and operated safely and securely . first , the pki must be designed using the familiar principles of risk management , rather than trust management . next , although it is not widely appreciated , digital signatures are not equivalent to traditional signatures , and understanding this difference is crucial to understanding how a pki needs to be audited . lastly , i will show that for a pki to provide ongoing security , the principles of compromisecontainment and regular auditing must be adhered to .
a fixed point theorem and applications to problems on sets with convex sections and to nash equilibria . <eos> a new fixed point theorem for a family of maps defined on product spaces is obtained . the new result requires the functions involved to satisfy the local intersection properties . previous results required the functions to have the open lower sections which are more restrictive conditions . new properties of multivalued maps are provided and applied to prove the new fixed point theorem . applications to problems on sets with convex sections and to the existence of nash equilibria for a family of continuous functions are given . ( c ) <digit> elsevier science ltd. all rights reserved .
on stability delay bounds of simple input delayed linear and non linear systems computational results . <eos> this paper deals with the problem of delay size stability analysis of single input delayed linear and nonlinear systems . conventional reduction , reduction linked by sliding mode , and linear memoryless control approaches are used for simple input delayed systems to obtain the stability conditions . several first order examples are investigated systematically to demonstrate the capabilities and limitations of the advanced stability analysis techniques including lyapunov krasovskii functionals , newton leibniz formula , and a newly addressed lagrange mean value theorem . numerical comparative results show the usefulness and effectiveness of the advanced delay size analysis techniques proposed in this paper .
the ackermann approach for modal logic , correspondence theory and second order reduction . <eos> the problem of eliminating second order quantification over predicate symbols is in general undecidable . since an application of second order quantifier elimination is correspondence theory in modal logic , understanding when second order quantifier elimination methods succeed is an important problem that sheds light on the kinds of axioms that are equivalent to first order correspondence properties and can be used to obtain complete axiomatizations for modal logics . this paper introduces a substitution rewrite approach based on ackermann 's lemma to second order quantifier elimination in modal logic . compared to related approaches , the approach includes a number of enhancements the quantified symbols that need to be eliminated can be flexibly specified . the inference rules are restricted by orderings compatible with the elimination order , which provides more control and reduces non determinism in derivations thereby increasing the efficiency and success rate . the approach is equipped with a powerful notion of redundancy , allowing for the flexible definition of practical simplification and optimization techniques . we present correctness , termination and canonicity results , and consider two applications ( i ) computing first order frame correspondence properties for modal axioms and rules , and ( ii ) rewriting second order modal problems to equivalent simpler forms . the approach allows us to define and characterize two new classes of formulae , which are elementary and canonical , and subsume the class of sahlqvist formulae and the class of monadic inductive formulae . ( c ) <digit> elsevier b.v. all rights reserved .
compressed histograms with arbitrary bucket layouts for selectivity estimation . <eos> selectivity estimation is an important step of query optimization in a database management system , and multi dimensional histogram techniques have proved promising for selectivity estimation . recent multi dimensional histogram techniques such as genhist and stholes use an arbitrary bucket layout . this layout has the advantage of requiring a smaller number of buckets to model tuple densities than those required by the traditional grid or recursive layouts . however , the arbitrary bucket layout brings an inherent disadvantage of requiring more memory to store each bucket location information . this diminishes the advantage of requiring fewer buckets and , therefore , has an adverse effect on the resulting selectivity estimation accuracy . to our knowledge , however , no existing histogram based technique with arbitrary layout addresses this issue . in this paper , we introduce the idea of bucket location compression and then demonstrate its effectiveness for improving selectivity estimation accuracy by proposing the stholes technique . stholes extends stholes by quantizing each coordinate of a bucket relative to the coordinate of the smallest enclosing bucket . this quantization increases the number of histogram buckets that can be stored in the histogram . our quantization scheme allows stholes to trade precision of histogram bucket locations for storing more buckets . experimental results show that stholes outperforms stholes on various data distributions , query distributions , and other factors such as available memory size , quantization resolution , and dimensionality of the data space .
facial expression transfer method based on frequency analysis . <eos> the subtle expression changes are salient in the frequency domain . a local expression deformation transfer method based on frequency analysis . dynamic expression synthesis for the source subject using expression manifold . a unified framework automatic dynamic expression transfer method .
eaf energy aware adaptive free viewpoint video wireless transmission . <eos> by transmitting views captured using two adjacent cameras in texture plus depth format , any middle view in between could be synthesized with depth image based rendering ( dibr ) technique . this provides users with continuous viewing angles for selection and is a key technology for a number of emerging applications such as free viewpoint tv . how to deliver the free viewpoint video contents to clients is one fundamental problem for these applications and the challenges exist in the large bandwidth requirement and frequent change of network environments . source encoding rates could be varied to fit different network conditions by changing frame rates , with extra battery life consumption due to skipped frames interpolation before synthesizing the requested middle view . note that battery development is lagging behind mobile devices function and computation capability development . how to trade off the source encoding rate reduction and extra energy consumption has not been formally solved as far as we understand . in this paper , we proposed eaf an energy aware adaptive free viewpoint video wireless transmission system , where we jointly considered the source encoding rate reduction and extra energy consumption with network constraints . simulation results showed that the proposed scheme advantages over competing schemes significantly in typical network scenarios .
implementation of a plasticity bond model for reinforced concrete . <eos> this study examines numerical integration schemes for incremental , elastoplastic interface characterizations . the motivating problem is modeling bond of reinforcing bars to concrete in finite element analyses . five first order accurate numerical integration schemes are examined . four algorithms use backward euler integration , but the iterative methods for solving the resulting nonlinear systems of equations ( i.e. , the plastic correctors ) differ . a proposed algorithm minimizes the number of nonlinear equations that are solved numerically for the bond model usually only one nonlinear equation is solved numerically . the algorithm is comparatively robust with respect to local convergence for arbitrary interface displacements .
joint reconstruction of image and motion in gated positron emission tomography . <eos> we present a novel intrinsic method for joint reconstruction of both image and motion in positron emission tomography ( pet ) . intrinsic motion compensation methods exclusively work on the measured data , without any external motion measurements . most of these methods separate image from motion estimation they use deformable image registration optical flow techniques in order to estimate the motion from individually reconstructed gates . then , the image is estimated based on this motion information . with these methods , a main problem lies in the motion estimation step , which is based on the noisy gated frames . the more noise is present , the more inaccurate the image registration becomes . as we show both visually and quantitatively , joint reconstruction using a simple deformation field motion model can compete with state of the art image registration methods which use robust multilevel b spline motion models .
the dynamics of two entangled qubits exposed to classical noise role of spatial and temporal noise correlations . <eos> we investigate the decay of two qubit entanglement caused by the influence of classical noise . we consider the whole spectrum of cases ranging from independent to fully correlated noise affecting each qubit . we take into account different spatial symmetries of noises , and the regimes of noise autocorrelation time . the latter can be either much shorter than the characteristic qubit decoherence time ( markovian decoherence ) , or much longer ( approaching the quasi static bath limit ) . we express the entanglement of two qubit states in terms of expectation values of spherical tensor operators which allows for transparent insight into the role of the symmetry of both the two qubit state and the noise for entanglement dynamics .
optimization of artificial neural networks for prediction of the unit cell parameters in orthorhombic perovskites . comparison with multiple linear regression . <eos> the unit cell parameters ( a , b , c ) of orthorhombic perovskites ( of a ( <digit> ) b ( <digit> ) o ( <digit> ) and a ( <digit> ) b ( <digit> ) o ( <digit> ) type ) were predicted both using multiple linear regression analysis ( mlr ) and two types of artificial neural networks ( ann ) . in these analyses , <digit> compounds of above perovskite type were included <digit> in calibration set and <digit> in test set , which were randomly chosen . in multiple linear regression , the unit cell parameters of <digit> perovskites were expressed as bilinear function of the effective ionic radii of a and b cations , and then , using the obtained regression equation , the unit cell parameters of <digit> perovskites were calculated and compared with the experimental data . predictions using the same sets and the same dependent and independent variables were also done by feed forward and cascade forward ann . the two different ann models were compared to mlr model by f test using their root mean square error ( rmsep ) . although the two models give excellent results , it could be concluded that ann have significantly better prediction abilities compared to mlr . ( c ) <digit> elsevier b.v. all rights reserved .
cherub power consumption aware cluster resource management . <eos> this paper presents an evaluation of acpi energy saving modes , and deduces the design and implementation of an energy saving daemon for clusters called cherub . the design of the cherub daemon is modular and extensible . since the only requirement is a central approach for resource management , cherub is suited for server load balancing ( slb ) clusters managed by dispatchers like linux virtual server ( lvs ) , as well as for high performance computing ( hpc ) clusters . our experimental results show that cherub 's scheduling algorithm works well , i.e. it will save energy , if possible , and avoids state flapping .
exor opportunistic multi hop routing for wireless networks . <eos> this paper describes exor , an integrated routing and mac protocol that increases the throughput of large unicast transfers in multi hop wireless networks . exor chooses each hop of a packet 's route after the transmission for that hop , so that the choice can reflect which intermediate nodes actually received the transmission . this deferred choice gives each transmission multiple opportunities to make progress . as a result exor can use long radio links with high loss rates , which would be avoided by traditional routing . exor increases a connection 's throughput while using no more network capacity than traditional routing . exor 's design faces the following challenges . the nodes that receive each packet must agree on their identities and choose one forwarder . the agreement protocol must have low overhead , but must also be robust enough that it rarely forwards a packet zero times or more than once . finally , exor must choose the forwarder with the lowest remaining cost to the ultimate destination . measurements of an implementation on a <digit> node 802.11 b test bed show that exor increases throughput for most node pairs when compared with traditional routing . for pairs between which traditional routing uses one or two hops , exor 's robust acknowledgments prevent unnecessary retransmissions , increasing throughput by nearly <digit> % . for more distant pairs , exor takes advantage of the choice of forwarders to provide throughput gains of a factor of two to four .
reducing message length variations in resource constrained embedded systems implemented using the controller area network ( can ) protocol . <eos> the controller area network ( can ) protocol is widely used in low cost embedded systems . can uses non return to zero ( nrz ) coding and includes a bit stuffing mechanism . whilst providing an effective mechanism for clock synchronization , the bit stuffing mechanism causes the can frame length to become ( in part ) a complex function of the data contents variations in frame length can have a detrimental impact on the real time behaviour of systems employing this protocol . in this paper , two software based mechanisms for reducing the impact of can bit stuffing are considered and compared . the first approach considered is a modified version of a technique described elsewhere ( e.g. nolte et al. t. nolte , h.a. hansson , c. norstrm , minimizing can response time jitter by message manipulation , in proceedings of the eighth ieee real time and embedded technology and applications symposium ( rtas <digit> ) , san jose , california , <digit> ) . the second approach considered is a form of software bit stuffing ( sbs ) . in both cases , not only the impact on message length variations is addressed but also the implementation costs ( including cpu and memory requirements ) involved in creating practical implementation of each technique on a range of appropriate hardware platforms . it is concluded that the sbs technique is more effective in the reduction of message length variations , but at the cost of an increase in cpu time and memory overheads and a reduction in the available data bandwidth . the choice of the most appropriate technique will , therefore , depend on the application requirements and the available resources .
transition from traditional to ict enhanced learning environments in undergraduate chemistry courses . <eos> this paper describes a three year study conducted among chemistry instructors ( professors and teaching assistants ) at a post secondary institution . the goal was to explore the integration process of information and communication technologies ( ict ) into traditional teaching . four undergraduate chemistry courses incorporated a course website , an electronic forum , computerized visualizations , and web based projects , into their curriculum . the learning technologies were integrated to enhance inquiry based learning , visualizations , and knowledge sharing . the current study investigated chemistry instructors ' perceptions toward ict and their activities while practicing the newly introduced technologies . the findings showed that integrating new practices is a phase dependent process that consists of promises as well as complexities . four transition steps were found to characterize the integration of ict learning environments non active , support dependant , partial independant , and total independant . findings indicated that the transition from traditional to ict enhanced learning environments involves ambivalent feelings and dichotomy among instructors . ( c ) <digit> elsevier ltd. all rights reserved .
experiences teaching data structures with java . <eos> this paper describes our experiences incorporating java in a data structures course . we describe the features of java that made for a more interesting course , the difficulties that we encountered , and compare java to the prior languages used in this course , ada and c . all in all , we found java to be a reasonable , but not overwhelming better , alternative . our students were particularly happy with the experiment .
detecting emotional state of a child in a conversational computer game . <eos> the automatic recognition of users communicative style within a spoken dialog system framework , including the affective aspects , has received increased attention in the past few years . for dialog systems , it is important to know not only what was said but also how something was communicated , so that the system can engage the user in a richer and more natural interaction . this paper addresses the problem of automatically detecting frustration , politeness , and neutral attitudes from a childs speech communication cues , elicited in spontaneous dialog interactions with computer characters . several information sources such as acoustic , lexical , and contextual features , as well as , their combinations are used for this purpose . the study is based on a wizard of oz dialog corpus of <digit> children , <digit> years of age , playing a voice activated computer game . three way classification experiments , as well as , pairwise classification between polite vs. others and frustrated vs. others were performed . experimental results show that lexical information has more discriminative power than acoustic and contextual cues for detection of politeness , whereas context and acoustic features perform best for frustration detection . furthermore , the fusion of acoustic , lexical and contextual information provided significantly better classification results . results also showed that classification performance varies with age and gender . specifically , for the politeness detection task , higher classification accuracy was achieved for females and <digit> years olds , compared to males and other age groups , respectively .
a simple method to design dielectric resonator based filters and diplexers using implicit space mapping technique . <eos> we present the design of a six pole chebyshev filter and a cascaded quadruple dielectric resonator ( dr ) filter using space mapping technique . implicit space mapping technique is used throughout and the design emerges within few iterations in both the cases . finite element method based hfss is used in constructing the fine model and agilent ads is used in constructing the coarse model . fine details such as tuning screws are included in the fine model . the same technique is also applied to a dr based diplexer and is explained . in all the cases , the results obtained with the hardware match well with the analyzed results . the same procedure can be applied in designing much more complex structures such as multiplexers . <digit> wiley periodicals , inc. int j rf and microwave cae 24 204216 , <digit> .
on the expressiveness and complexity of atl . <eos> atl is a temporal logic geared towards the specification and verification of properties in multi agents systems . it allows to reason on the existence of strategies for coalitions of agents in order to enforce a given property . in this paper , we first precisely characterize the complexity of atl model checking over alternating transition systems and concurrent game structures when the number of agents is not fixed . we prove that it is delta ( p ) ( <digit> ) and delta ( p ) ( <digit> ) complete , depending on the underlying multi agent model ( ats and cgs resp . ) . we also consider the same problems for some extensions of atl . we then consider expressiveness issues . we show how ats and cgs are related and provide translations between these models w.r.t. alternating bisimulation . we also prove that the standard definition of atl ( built on modalities next , always and until ) can not express the duals of its modalities it is necessary to explicitely add the modality release .
deformable templates guided discriminative models for robust 3d brain mri segmentation . <eos> automatically segmenting anatomical structures from 3d brain mri images is an important task in neuroimaging . one major challenge is to design and learn effective image models accounting for the large variability in anatomy and data acquisition protocols . a deformable template is a type of generative model that attempts to explicitly match an input image with a template ( atlas ) , and thus , they are robust against global intensity changes . on the other hand , discriminative models combine local image features to capture complex image patterns . in this paper , we propose a robust brain image segmentation algorithm that fuses together deformable templates and informative features . it takes advantage of the adaptation capability of the generative model and the classification power of the discriminative models . the proposed algorithm achieves both robustness and efficiency , and can be used to segment brain mri images with large anatomical variations . we perform an extensive experimental study on four datasets of t1 weighted brain mri data from different sources ( 1,082 mri scans in total ) and observe consistent improvement over the state of the art systems .
accurate retinal blood vessel segmentation by using multi resolution matched filtering and directional region growing . <eos> a new method to extract retinal blood vessels from a colour fundus image is described . digital colour fundus images are contrast enhanced in order to obtain sharp edges . the green bands are selected and transformed to correlation coefficient images by using two sets of gaussian kernel patches of distinct scales of resolution . blood vessels are then extracted by means of a new algorithm , directional recursive region growing segmentation or d rrgs . the segmentation results have been compared with clinically generated ground truth and evaluated in terms of sensitivity and specificity . the results are encouraging and will be used for further application such as blood vessel diameter measurement .
technology mapping for fpgas with embedded memory blocks . <eos> modern field programmable gate arrays ( fpgas ) provide embedded memory blocks ( embs ) to be used as on chip memories . in this paper , we explore the possibility of using embs to implement logic functions when they are not used as on chip memory . we propose a general technology mapping problem for fpgas with embs for area and delay minimization and develop an efficient algorithm based on the concepts of maximum fanout free cone ( mffc ) <digit> and maximum fanout free subgraph ( mffs ) <digit> , named emb_pack , which minimizes the area after or before technology mapping by using embs while maintaining the circuit delay . we have tested emb_pack on mcnc benchmarks on altera 's flex10k device family <digit> . the experimental results show that compared with the original mapped circuits generated from cutmap <digit> without using embs , emb_pack as postprocessing can further reduce up to <digit> % of the area on the mapped circuits while maintaining the layout delay by making efficient use of available emb resources . compared with cutmap e without using embs , emb_pack as pre mapping processing followed by cutmap e can reduce <digit> % of the area while maintaining the circuit optimal delay .
b spline surfaces for ship hull design . <eos> the use of true sculptured surface descriptions for design applications has been proposed by numerous authors . the actual implementation and use of interactive sculptured surface description techniques for design and production has been limited . the use of such techniques for ship hull design has been even more limited . the present paper describes a preliminary implementation of such a system for the design of ship hulls and for the production of towing tank models using numerical control techniques . the present implementation is based on a cartesian product b spline surface description . implementation is on an evans and sutherland picture system supported by a pdp <digit> <digit> minicomputer . the b spline surface is manipulated by its associated polygonal net . both surface and net are three dimensional . techniques both good and bad for <digit> d picking of a polygon point when the net , its associated surface , and the <digit> d picking cue independently exist and can be independently manipulated in three space are presented and discussed . the shape of a b spline surface of fixed order is controlled by the location of the polygon net points , the number of multiple points at a particular net point , and the knot vector . frequently multiple points imply multiple knot vectors . practical techniques for controlling and shaping the surface with and without this assumption are discussed and the results illustrated . experience attained by interactively fitting a single fourth order b spline surface patch to the forebody half of an actual ship hull described by three dimensional digitized points is discussed and the results illustrated .
fpga implementation of a near computation free image compression scheme based on adaptive decimation . <eos> adaptive decimation is a technique that can be applied to compress images with good visual quality at low bit rate . despite the low complexity of the method , the encoding process involves floating point computation that requires the use of medium speed processors in order to achieve real time operation . in this paper , a new approach has been taken to restructure the adaptive decimation algorithm to a form that includes only small amount of arithmetic operations . the revised algorithm can be implemented with simple logic circuits . the encoder is practically free from complicated numerical computation . a fpga implementation of the proposed algorithm was realized . the architecture of the encoder is simple and suitable for valuable applications in the development of low cost non processor based multimedia products .
the study of wsn routing . <eos> zigbee network layer , the main purpose of the agreement is to provide reliable and secure transmission . there are three kinds of networks topology in zigbee network layer star topology , tree topology , mesh topology . this study was used in the experiment of the packet size and speed are not great , postponed plans did not change significantly . however , the same packet would be delayed with the lost , only a very small number , but if increasing the amount of packets with the transmission speed , delay will increase with the loss of . when over load , will be a substantial loss . from the experiments , the tree to delay longer than the star topology and mesh topology a long time , star topology is also informed that easy to synchronize the delay time is to do the least . although the delay of the mesh topology is better than the tree topology , but the mesh topology to play its characteristics , his performance is better than the star and tree topology .
effect of varying concentration and temperature on steady and dynamic parameters of low concentration photovoltaic energy system . <eos> low concentration photovoltaic ( lcpv ) system is designed and developed . commercially available crystalline silicon solar pv cells used under concentration . dynamic behavior of lcpv system is modeled and estimated . real time analysis of lcpv system is presented .
on the numerical modelling of the multiphysics self piercing riveting process based on the finite element technique . <eos> the development of reliable numerical models permits to investigate the manufacturing processes with very low incremental costs or prototyping efforts hence it provides a relevant help in process optimisation and gives great opportunity for making maximum use of sparse process data shercliff hr , lovatt am . selection of manufacturing process in design and the role of process modelling . prog mater sci <digit> <digit> <digit> . among others the metal forming processes have heavily benefited from the finite element numerical computing technology chenot jl , massoni e. finite element modelling and control of new metal forming processes . int j machine tool manuf 2006 46 1194200 . the self piercing riveting ( spr ) is a cold forming process which creates a strong mechanical interlock between two or more sheets by means of a semi tubular rivet , which , pressed by a punch , pierces the upper sheet and flares into the bottom one . it is governed by complex multiphysics phenomena whose governing equations can be resolved using the finite element method . in this paper all the governing equations are fully reported along with the mathematics of the resolving method needed for setting up and simulate a finite element model of the self piercing riveting of an aluminium alloy . a case study of the spr of two sheets of the 6060t4 aluminium alloy using a steel rivet was investigated . the calculations were performed using the lsdyna finite element commercial code . the problems encountered and the solutions applied for the preparation of the model and the run of the calculation were presented and discussed . the obtained results were validated by comparison with data coming from a laboratory experiment .
adaptive available bandwidth estimation for internet video streaming . <eos> in this study , an adaptive available bandwidth estimation approach that is suitable for internet video streaming is developed . the algorithm exploits repetitive measurements and uses this redundancy to improve its video adaptation decision . the importance of available bandwidth estimation in internet applications has recently increased particularly because of the heterogeneity of the network links . many of the internet paths may contain wired and wireless links in which loss may happen due to congestion as well as link errors . hence , loss rate by itself is not a sufficient statistics for monitoring purposes . if the loss is due to congestion , video quality can then be decreased whereas if the loss is due to link error , no such action is necessary . moreover , in video streaming , such an estimate can be used to determine the new video rate if the quality is to be increased . in our approach , active probing packets are used to estimate bandwidth in very short time duration . the novelty of our estimator is its adaptivity in the sense that the overhead caused by the estimator is automatically reduced when congestion builds up . the trade off is reduced accuracy . such accuracy is not needed under congestion anyway and when things get back to normal , our estimator turns back to normal operation mode . we have integrated our algorithm into our video streamer and carried out experiments on both simulated and actual streaming applications on the internet . the results indicate that our estimator algorithm increases streaming performance substantially .
cost oriented task allocation and hardware redundancy policies in heterogeneous distributed computing systems considering software reliability . <eos> task allocation policy and hardware redundancy policy for distributed computing system ( dcs ) are of great importance as they affect many system characteristics such as system cost , system reliability and performance . in recent years , abundant research has been carried out on the optimal task allocation and or hardware redundancy problem , most of which took a reliability oriented approach , i.e. , the optimization criterion was system reliability maximization . nevertheless , besides system reliability , other system characteristics such as system cost may be of great concern to management . in this paper , we take a cost oriented approach to the optimal task allocation and hardware redundancy problem for dcs , which addresses both system cost and system reliability issues . a system cost model which could reflect the impact of system unreliability on system cost is developed , and by minimizing the total system cost , a satisfactory level of system reliability could be reached simultaneously . in the reliability modeling and analysis of dcs , we take both hardware reliability and software reliability into account . two numerical examples are given to illustrate the formulation and solution procedures , in which genetic algorithm is used . results show that based on the developed system cost model , appropriate decision makings on task allocation and hardware redundancy policies for dcs could be made , and the result obtained seems to be a fairly good trade off between system cost and system reliability . ( c ) <digit> elsevier ltd , all rights reserved .
a qualitative case study of the adoption and use of an agricultural decision support system in the australian cotton industry the socio technical view . <eos> in response to the call for research that considers the human as well as the technical aspects of information systems implementation , the authors report on an interpretive case study which explores the adoption and use of an agricultural decision support system ( dss ) cottonlogic in the australian cotton industry . the study was informed through the innovation decision model by rogers and the technology in practice model by orlikowski using a socio technical approach . it was found that participants who achieved a high level of implementation success were reflexive and resourceful in adapting the technology to their changing needs , often in ways unanticipated by dss builders .
an innovationdiffusion view of implementation of enterprise resource planning ( erp ) systems and development of a research model . <eos> firms around the world have been implementing enterprise resource planning ( erp ) systems since the 1990s to have an uniform information system in their respective organizations and to reengineer their business processes . through a case type analysis conducted in six manufacturing firms that have one of the widely used erp systems , various contextual factors that influenced these firms to implement this technology were understood using the six stage model proposed by kwon and zmud . three types of erp systems , viz. sap , baan and oracle erp were studied in this research . implementation of erp systems was found to follow the stage model . the findings from the process model were used to develop the items for the causal model and in identifying appropriate constructs to group those items . in order to substantiate that the constructs developed to measure the causal model were congruent with the findings based on qualitative analysis , i.e. that the instrument appropriately reflects the understanding of the case interview triangulation technique was used . the findings from the qualitative study and the results from the quantitative study were found to be equivalent , thus , ensuring a fair assessment of the validity and reliability of the instrument developed to test the causal model . the quantitative measures done only at these six firms are not statistically significant but the samples were used as a part of the triangulation method to collect data from multiple sources , to verify the respondents understanding of the scales and as an initial measure to see if my understanding from the qualitative studies were accurately reflected by the instrument . this instrument will be pilot tested first and administered to a large sample of firms .
biasing bayesian optimization algorithm using case based reasoning . <eos> studies show that application of the prior knowledge in biasing the estimation of distribution algorithms ( edas ) , such as bayesian optimization algorithm ( boa ) , increases the efficiency of these algorithms significantly . one of the main advantages of the edas over other optimization algorithms is that the former provides a trail of probabilistic models of candidate solutions with increasing quality . some recent studies have applied these probabilistic models , obtained from previously solved problems in biasing the boa algorithm , to solve the future problems . in this paper , in order to improve the previous works and reduce their disadvantages , a method based on case based reasoning ( cbr ) is proposed for biasing the boa algorithm . herein , after running boa for solving optimization problems , each problem , the corresponding solution , as well as the last bayesian network obtained from the boa algorithm , will be stored as an entry in the case base . upon introducing a new problem , similar problems from the case base are retrieved and the last bayesian networks of these solved problems are combined according to the degree of their similarity with the new problem hence , a compound bayesian network is constructed . the compound bayesian network is sampled and the initial population for the boa algorithm is generated . this network will be applied efficiently for biasing future probabilistic models during the runs of boa for the new problem . the proposed method is tested on three well known combinatorial benchmark problems . experimental results show significant improvements in algorithm execution time and quality of solutions , compared to previous methods .
a numerical study on mechanical performance of asphalt mixture using a meso scale finite element model . <eos> this paper concerns with meso scale finite element ( fe ) modeling of asphalt mixtures . the proposed fe model is capable of simulating the complex geometry of several types of asphalt mixture in which different gradations of aggregates could be properly modeled . the meso structure of the mixture is constructed by a novel technique identified in this paper . the main idea of such technique is that the aggregate particles could be randomly packed together into the simulation region , by defining a kind of artificial interaction forces among the particles . after that by the voronoi tessellation method , the set of the generated discrete grains will alter to space filling , adjoining polyhedrons with respect to the real geometry so that it were possible to investigate the behavior of mixture by finite element method . such a model considers the main components of the asphalt mixture consisting of aggregate particles , mastic , interfacial zone and air voids . moreover , different moving wheel loads with different passing velocities are considered and their effects on the mechanical responses of the asphalt mixture are examined . the fe model gives some better insights into the behavior of mixture 's components under moving loads . the responses include vertical displacement of the pavement surface as well as the development of stress in mixture components . the proposed models give results that are in agreement with theoretical predictions and previous studies .
expressive and efficient pattern languages for tree structured data ( extended abstract ) . <eos> it would be desirable to have a query language for tree structured data that is ( <digit> ) as easily usable as sql , ( <digit> ) as expressive as monadic second order logic ( mso ) , and ( <digit> ) efficiently evaluable . the paper develops some ideas in this direction . towards ( <digit> ) the specification of sets of vertices of a tree by combining conditions on their induced subtree with conditions on their path to the root is proposed . existing query languages allow regular expressions ( hence mso logic ) in path conditions but are limited in expressing subtree conditions . it is shown that such query languages fall short of capturing all mso queries . on the other hand , allowing a certain guarded fragment of mso logic in the specification of subtree conditions results in a language fulfilling ( <digit> ) , ( <digit> ) and , anguably , ( <digit> ) .
a fuzzy approach to resource aware automatic parallelization . <eos> any realistic approach to automatic program parallelization must take into account practical issues related to the resource usage of parallel executions , such as the overheads associated with parallel tasks creation , migration of tasks to remote processors , and communication . the aim of granularity control techniques is avoiding such overheads undermining the benefits of parallel executions . for example , sufficient conditions have been proposed to ensure that the parallel execution of some given tasks will not take longer than their corresponding sequential execution . however , when the goal is to optimize the average execution time of several runs , such conditions can be very conservative , causing a loss in parallelization opportunities . to solve this problem , we have proposed novel conditions based on fuzzy logic and performed an experimental assessment with real programs . the results show that such conditions select the optimal type of execution in most cases and behave much better than the conservative conditions .
a survey of experienced user perceptions about software design patterns . <eos> although the concept of the software design pattern is well established , there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable . to identify which patterns from the set catalogued by the gang of four are considered to be useful by experienced users , which ones are considered as not being useful , and why this is so . we undertook a web based survey of experienced pattern users , seeking information about their experiences as software developers and maintainers . our sampling frame consisted of the authors of all of the pattern papers that we had identified in a preceding systematic review of studies of patterns . we received <digit> usable responses , corresponding to a response rate of <digit> % from the original sampling frame . most respondents were involved with software development rather than maintenance . while patterns can provide a means of sharing knowledge schemas between designers , only three patterns were widely regarded as valuable . around one quarter of the patterns gained very low approval or worse . these observations need to be considered when using patterns teaching students about the pattern concept and planning empirical studies about patterns .
integrating radial basis function networks with case based reasoning for product design . <eos> this paper presents a case based design expert system that automatically determines the design values of a product . we focus on the design problem of a shadow mask which is a core component of monitors in the electronics industry . in case based reasoning ( cbr ) , it is important to retrieve similar cases and adapt them to meet design specifications exactly . notably , difficulties in automating the adaptation process have prevented designers from being able to use design expert systems easily and efficiently . in this paper , we present a hybrid approach combining cbr and artificial neural networks in order to solve the problems occurring during the adaptation process . we first constructed a radial basis function network ( rbfn ) composed of representative cases created by k means clustering . then , the representative case most similar to the current problem was adjusted using the network . the rationale behind the proposed approach is discussed , and experimental results acquired from real shadow mask design are presented . using the design expert system , designers can reduce design time and errors and enhance the total quality of design . furthermore , the expert system facilitates effective sharing of design knowledge among designers . ( c ) <digit> elsevier ltd. all rights reserved .
rna mediated inhibition of hiv in a gene therapy setting . <eos> at present , treatment for hiv <digit> infection employs highly active anti retroviral therapy ( haart ) , which utilizes a combination of rt and protease inhibitors . unfortunately , hiv can escape many therapies because of its high mutation rate and the complexity of its pathogenesis . hiv <digit> integrates into the cellular genome , which facilitates persistence and acts as a reservoir for reactivation and replication . as an alternative or adjuvant to chemotherapy we have been developing an rna based gene therapy approach for the treatment of hiv <digit> infection . this article summarizes the various rna based technologies that we have developed for potential application in a gene therapy setting .
identifying and quantifying metabolites by scoring peaks of gc ms data . <eos> metabolomics is one of most recent omics technologies . it has been applied on fields such as food science , nutrition , drug discovery and systems biology . for this , gas chromatography mass spectrometry ( gc ms ) has been largely applied and many computational tools have been developed to support the analysis of metabolomics data . among them , amdis is perhaps the most used tool for identifying and quantifying metabolites . however , amdis generates a high number of false positives and does not have an interface amenable for high throughput data analysis . although additional computational tools have been developed for processing amdis results and to perform normalisations and statistical analysis of metabolomics data , there is not yet a single free software or package able to reliably identify and quantify metabolites analysed by gc ms.
optical properties of sic azo plasmonic nano composite at infrared frequencies . <eos> we report a study of the optical properties of a mixture of spherical inclusions within a host continuum . the structure of the designed sic azo material is a binary composite of two different kinds of nano spheres , one made from a semiconductor material ( sic ) and the other from a plasmonic material ( azo ) . its electromagnetic response is studied using the extended maxwellgarnett effective medium theory and a rigorous method based on layer multiple scattering theory . a very nice agreement between both treatments is established , rendering the effective medium approximation a useful guide for the experimentalist in the field .
linear broadcast encryption schemes . <eos> a new family of broadcast encryption schemes , which will be called linear broadcast encryption schemes ( lbess ) , is presented in this paper by using linear algebraic techniques . this family generalizes most previous proposals and provides a general framework to the study of broadcast encryption schemes . we present a method to construct , for a general specification structure , lbess with a good trade off between the amount of secret information stored by every user and the length of the broadcast message . in this way , we are able to find schemes that fit in situations that have not been considered before .
programming living cells to function as massively parallel computers . <eos> we have reprogrammed the genomes of living cells to construct massively parallel biological computers capable of processing two dimensional images at a theoretical resolution of greater than <digit> megapixels per square inch . first , we rewired a signal transduction pathway in escherichia coli to express a pigment producing enzyme under the control of red light . we then use the engineered bacteria as pixels in biological film . next , use the ' bacterial photography ' technology as tool for the engineering of a massively parallel biological computer which uses cell cell communication to compute the edges ( light dark boundaries ) within images .
performance optimization of large non negatively constrained least squares problems with an application in biophysics . <eos> solving large non negatively constrained least squares systems is frequently used in the physical sciences to estimate model parameters which best fit experimental data . analytical ultracentrifugation ( auc ) is an important hydrodynamic experimental technique used in biophysics to characterize macromolecules and to determine parameters such as molecular weight and shape . we previously developed a parallel divide and conquer method to facilitate solving the large systems obtained from auc experiments . new auc instruments equipped with multi wavelength ( mwl ) detectors have recently increased the data sizes by three orders of magnitude . analyzing the mwl data requires significant compute resources . to better utilize these resources , we introduce a procedure allowing the researcher to optimize the divide and conquer scheme along a continuum from minimum wall time to minimum compute service units . we achieve our results by implementing a preprocessing stage performed on a local workstation before job submission .
influence of equivalent bolt length in finite element modeling of t stub steel connections . <eos> in this paper the development and implementation of a finite element model for simple t stub steel connections is presented . material and geometric non linearities as well as contact and friction have been implemented in the model . the model is validated by comparison with experimental data found in the literature , for configurations exhibiting different failure mechanisms and featuring different bolt preloading levels . the impact of bolt length considered in the model is investigated and is shown to be of primary importance . this issue is representative of the continuously increasing use of advanced numerical analysis , supported by progress in computational mechanics , as a tool for practical design of engineering structures .
a novel fem model for biaxial non crimp fabric composite materials under tension . <eos> this paper presents a novel finite element based approach able to represent the complex architecture of the non crimp fabric ( ncf ) composite materials . by means of the stiffness averaging method , implemented in the research oriented fem ( finite element method ) code b2000 , the developed model is able to simulate the ncfs mechanical performances . applications to simple coupons loaded in tension are presented in order to demonstrate the capability and the effectiveness of the presented approach . nevertheless , the proposed methodology can be applied and extended to all ncf geometries . first , for validation purposes the numerical results detained for a specific configuration have been compared with experimental results available from literature . then , a parametric study has been carried out to investigate the influence of the bundle waviness on the tension stiffness . finally , due to the degradation of the in plane mechanical properties , the presence of the stitching has been investigated .
implementing polyinstantiation as a strategy for electronic commerce customer relationship management . <eos> polyinstantiation is the situation where multiple records sharing the same identifier value occur in one table . multi level secure ( mls ) data models manage and utilize polyinstantiation to provide a secure way of handling classified information . customer relationship management ( crm ) systems for e business can leverage the strategy of managed polyinstantiation by implementing mls technology to coordinate b2c interactions in order to build long term loyalty . this approach can be used to address some of the challenges faced by providers and adopters of e business crm technology solutions . a pilot study evaluated polyinstantiated information presentation strategy as a means of enhancing relationships between e businesses and their customers . the results support the idea that customers perceive the benefits of their special customer status as a function of how the relevant data are presented .
bayesian age period cohort modeling and prediction bamp . <eos> the software package b a m p provides a method of analyzing incidence or mortality data on the lexis diagram , using a bayesian version of an age period cohort model . a hierarchical model is assumed with a binomial model in the first stage . as smoothing priors for the age , period and cohort parameters random walks of first and second order , with and without an additional unstructured component are available . unstructured heterogeneity can also be included in the model . in order to evaluate the model fit , posterior deviance , dic and predictive deviances are computed . by projecting the random walk prior into the future , future death rates can be predicted .
knowledge based manufacturing system ( kbms ) . <eos> production management , in batch type manufacturing environment , is regarded by the current research community as a very complex task . this paper claims that the complexity is a result of the system approach where management performance relies on decisions made at a too early stage in the manufacturing process . decisions are made and stored in company databases by engineers who are neither economists nor production planner 's experts . this paper presents a new method where engineer 's task is not to make decisions but rather to prepare a knowledge based road map . the road map method does introduce flexibility and dynamics in the manufacturing process and thus simplifies the decision making process in production planning . each user will generate a routine that meets his her needs at the time of needs by using kbms capp . thereby this method increases dramatically manufacturing efficiency .
document retrieval by projection based frequency distribution . <eos> in document retrieval task , random projection ( rp ) is a useful technique of dimension reduction . it can be obtained very quickly yet the recalculation is not necessary to any changes . however , in lower dimension , random projection has instability by randomness in itself . in this investigation , we propose a new technique , called skewed projection ( sp ) , for dimension reduction based on term frequency distribution . by our experiments , we show that we can take advantages of local independence thus we can obtain efficient retrieval for documents which belong to specific application area . also we examine document size by which we can determine term distribution .
robust blind watermarking mechanism for motion data streams . <eos> the commercial reuse of 3d motion capture ( mocap ) data in animation and life sciences raises issues with respect to its copyright . in order to improvise content protection of mocap data , we devise a substitutive blind watermarking technique . this technique visualizes 3d mocap data as a series of non intersecting cluster of triangles . bits are encoded inside the triangles by using an extended substitutive bit encoder in spatial domain . the encoding supports watermark imperceptibility and develops robustness against affine transforms ( rotation , translation , scaling ) , noise addition , reordering and sample loss attacks . security of the scheme can be enhanced by adding secret embedding distances between clusters , which are based on a secret key used for watermarking purpose .
computational expressiveness of genetic systems . <eos> we introduce genetic systems , a formalism inspired by genetic regulatory networks and suitable for modelin g the interactions between the genes and the proteins , acting as regulatory products . the generation of new objects , representing proteins , is driven by genetic gates a new object is produced when all the activator objects are available in the system , and no inhibitor object is available . activators are not consumed by the application of such an evolution rule . objects disappear because of degradation each object is equipped with a lifetime , and the object decays when such a lifetime expires . we investigate the computational expressiveness of genetic systems we show that they are turing equivalent by providing encodings of random access machines in genetic systems . ( c ) <digit> elsevier b.v. all rights reserved .
identifying the nature of stomach diseases by ultrasonography based on genetic neural network . <eos> clinical features and ultrasound signs of <digit> subjects including <digit> health subjects are collected . all samples are divided into the training set ( <digit> samples ) and the test set ( <digit> samples ) based on mean vector similarity . each set contains <digit> benign and <digit> malignant . multiple linear regression model ( mlr model ) , back propagation neural network model ( bpn model ) and genetic algorithm based back propagation neural network model ( gabpn model ) are established for distinguishing malignant from benign stomach diseases and are trained using the training set . then three models are tested using the test set . the accuracy , the sensitivity and the specificity for the test set , gabpn model are 92.1 % , 89.5 % and 94.7 % , bpn model are 89.5 % , 89.5 % and 89.5 % , mlr model are 89.5 % , 84.2 % and 94.7 % . areas under curve of gabpn model , bpn model and mlr model are respectively 0.978 , 0.945 and 0.958 in roc analysis . these results confirm that color doppler ultrasound can be used as a tool for distinguishing benign from malignant stomach diseases ( p < 0.01 ) . genetic algorithm based neural network outperforms the multivariate linear regression and the back propagation neural network to establish a model identifying the nature of stomach diseases . ( c ) <digit> elsevier ltd. all rights reserved .
the selection and assemblage of approximation functions and disposal of its singularity in axisymmetric drbem for heat transfer problems . <eos> in this paper , a group of approximation functions f with summed form are developed in the dual reciprocity boundary element method ( drbem ) based on the three dimensional poisson type equation whose right hand side is radial basis function . the singularity on symmetrical axis of the function f is eliminated by integral averaging and selecting different assemblage of the functions f on the basis of the different characteristic of computational domains . by solving the transient heat conduction problems in solid cylinder and sphere , the laminar convective heat transfer problem in tube and the solidliquid phase change process of sphere , the feasibility of present selecting and assembling function f and its singularity processing are well proved .
a new coupled fluidstructure modeling methodology for running ductile fracture . <eos> a coupled fluidstructure modeling methodology for running ductile fracture in pressurized pipelines has been developed . the pipe material and fracture propagation have been modeled using the finite element method with a ductile fracture criterion . the finite volume method has been employed to simulate the fluid flow inside the pipe , and the resulting pressure profile was applied as a load in the finite element model . choked flow theory was used for calculating the flow through the pipe crack . a comparison to full scale tests of running ductile fracture in steel pipelines pressurized with hydrogen and with methane has been done , and very promising results have been obtained .
a dempster shafer theoretic framework for boosting based ensemble design . <eos> training set resampling based ensemble design techniques are successfully used to reduce the classification errors of the base classifiers . boosting is one of the techniques used for this purpose where each training set is obtained by drawing samples with replacement from the available training set according to a weighted distribution which is modified for each new classifier to be included in the ensemble . the weighted resampling results in a classifier set , each being accurate in different parts of the input space mainly specified the sample weights . in this study , a dynamic integration of boosting based ensembles is proposed so as to take into account the heterogeneity of the input sets . an evidence theoretic framework is developed for this purpose so as to take into account the weights and distances of the neighboring training samples in both training and testing boosting based ensembles . the effectiveness of the proposed technique is compared to the adaboost algorithm using three different base classifiers .
bpeldebugger an effective bpel specific fault localization framework . <eos> a formal fault localization framework for bpel programs . synthesization of existing fault localization techniques in the proposed framework . an empirical study that validated the feasibility of the proposed framework . significant improvements in fault localization effectiveness are observed .
development of a moving artificial compressibility solver on unified coordinates . <eos> based on the unified eulerian and lagrangian coordinate transformations , the unsteady incompressible navier stokes equations with artificial compressibility effects are developed . as we know , the eulerian coordinates cause excessive numerical diffusion across flow discontinuities , slip lines in particular . the lagrangian coordinates , on the other hand , can resolve slip lines sharply but cause severe grid deformation , resulting in large errors and even breakdown of the computation . recently , hui et al. ( j. comput . phys . 1999 153 596 637 ) have introduced a unified coordinate system that moves with velocity hq , q being the velocity of the fluid particle . it includes the eulerian system as a special case when h <digit> , and the lagrangian when h <digit> , and was shown for the two dimensional unsteady euler equations of compressible flow to be superior than both eulerian and lagrangian systems . in the framework of unified coordinates , our work will derive the unsteady incompressible flow equations and moving geometry equations , when hq equals and velocity in conservation form and is updated simultaneous during each time step . thus , the accurate estimation of geometry conservation and controlling the grid velocity or the h value based on the unified approach can keep numerical stability and avoid computation breakdown caused by moving body or boundary layers ( considered as slip lines in lagrangian coordinates ) . also , the existing high resolution riemann solver can be extended to discretize the current unified incompressible flow equations . our benchmark tests including the lid driven cavity flow and backward step flow , oscillating flat plat and pulsating stenotic tube are used to validate the computations . the results verify the accuracy and robustness of the unified artificial compressibility solver on the moving body simulation . copyright ( c ) <digit> john wiley sons , ltd .
single case study in rehabilitation with sam method ( sense and mind ) a proposal and analysis . <eos> the aim of the present case report is to describe a new rehabilitation approach for traumatic brain injury ( tbi ) .
enhanced and hierarchical structure algorithm for data imbalance problem in semantic extraction under massive video dataset . <eos> data imbalance problem often exists in our real life dataset , especial for massive video dataset , however , the balanced data distribution and the same misclassification cost are assumed in traditional machine learning algorithms , thus , it will be difficult for them to accurately describe the true data distribution , and resulting in misclassification . in this paper , the data imbalance problem in semantic extraction under massive video dataset is exploited , and enhanced and hierarchical structure ( called ehs ) algorithm is proposed . in proposed algorithm , data sampling , filtering and model training are considered and integrated together compactly via hierarchical structure algorithm , thus , the performance of model can be improved step by step , and is robust and stability with the change of features and datasets . experiments on trecvid2010 semantic indexing demonstrate that our proposed algorithm has much more powerful performance than that of traditional machine learning algorithms , and keeps stable and robust when different kinds of features are employed . extended experiments on trecvid2010 surveillance event detection also prove that our ehs algorithm is efficient and effective , and reaches top performance in four of seven events .
using equivalence checking to verify robustness to denial of service . <eos> in this paper , we introduce a new security property which intends to capture the ability of a cryptographic protocol being resistant to denial of service . this property , called impassivity , is formalised in the framework of a generic value passing process algebra , called security protocol process algebra , extended with local function calls , cryptographic primitives and special semantics features in order to cope with cryptographic protocols . impassivity is defined as an information flow property founded on bisimulation based non deterministic admissible interference . a sound and complete proof method , based on equivalence checking , for impassivity is also derived . the method extends results presented in a previous paper on admissible interference and its application to the analysis of cryptographic protocols . our equivalence checking method is illustrated throughout the paper on the tcp ip connection protocol and on the 1kp secure electronic payment protocol .
an interactive system for computer aided design of printed circuit boards . <eos> this paper describes an interactive system for the design of printed circuit boards . the system uses an interactive graphics terminal for solving the placement and routing problems . it can be used for designing two layer boards with integrated circuit modules and discrete components . a substantial portion of the wire routing is performed automatically and the remaining wires are routed manually in a single interactive session . the system provides significant savings in both design time and processing costs over batch processing and manual methods .
a class of composable and preemptible high level petri nets with an application to multi tasking systems . <eos> this paper presents an extension of an algebra of high level petri nets with operations for suspension and abortion . these operations are sound with respect to the semantics of preemption , and can be applied to the modelling of the semantics of high level parallel programming languages with preemption related features . as an illustration , the paper gives an application to the modelling of a multi tasking system in a parallel programming language , which is provided with a concurrent semantics based on petri nets and for which implemented tools can be used .
facial emotion and gesture reproduction method for substitute robot of remote person . <eos> ceos of big companies may travel frequently to give their philosophies and policies to the employees who arc working at world wide branches . video technology makes it possible to give their lectures anywhere and anytime in the world very easily . however , <digit> dimentional video systems lack the reality . if we can give natural realistic lectures through humanoid robots . ceos do not need to meet the employees in person . they can save their time and money for traveling . we propose a substitute robot of remote person . the substitute robot is a humanoid robot that can reproduce the lecturers ' facial expressions and body movements , and that can send the lecturers to everywhere in the world instantaneously with the feeling of being at a live performance . there arc two major tasks for the development they arc the facial expression recognition reproduction and the body language reproduction . for the former task , we proposed a facial expression recognition method based oil a neural network model . we recognized five emotions , or surprise , anger , sadness , happiness and no emotion , in real time . we also developed a facial robot to reproduce the recognized emotion on the robot face . through experiments , we showed that the robot could reproduce the speakers ' emotions with its face . for the latter task , we proposed it degradation control method to reproduce the natural movement of the lecturer even when a robot rotary joint fails . for the fundamental stage of our research for this sub system , we proposed a control method for the front view movement model , or <digit> dimentional model . ( c ) <digit> elsevier ltd. all rights reserved .
differential response of nnos knockout mice to mdma ( ecstasy ) and methamphetamine induced psychomotor sensitization and neurotoxicity . <eos> it has been shown that mice deficient in neuronal nitric oxide synthase ( nnos ) gene are resistant to cocaine induced psychomotor sensitization and methamphetamine ( meth ) induced dopaminergic neurotoxicity . the present study was undertaken to investigate the hypothesis that nnos has a major role in dopamine ( da ) but not serotonin ( <digit> hydroxytryptamine <digit> ht ) mediated effects of psychostimulants . the response of nnos knockout ( ko ) and wild type ( wt ) mice to the psychomotor stimulating and neurotoxic effects of 3,4 methylenedioxymethamphetamine ( mdma ecstasy ) and meth were investigated . repeated administration of mdma for <digit> days resulted in psychomotor sensitization in both wt and nnos ko mice , while repeated administration of meth caused psychomotor sensitization in wt but not in ko mice . sensitization to both mdma and meth was persistent for <digit> days in wt mice , but not in nnos ko mice . these findings suggest that the induction of psychomotor sensitization to mdma and meth is no independent and no dependent , respectively , while the persistence of sensitization to both drugs is no dependent . for the neurochemical studies , a high dose of mdma caused marked depletion of <digit> ht in several brain regions of both wt and ko mice , suggesting that the absence of the nnos gene did not afford protection against mdma induced depletion of <digit> ht . striatal dopaminergic neurotoxicity caused by high doses of mdma and meth in wt mice was partially prevented in ko mice administered with mdma , but it was fully precluded in ko mice administered with meth . the differential response of nnos ko mice to the behavioral and neurotoxic effects of mdma and meth suggests that the nnos gene is required for the expression and persistence of da mediated effects of meth and mdma , while <digit> ht mediated effects of mdma ( induction of sensitization and <digit> ht depletion ) are not dependent on nnos .
automatic programming methodologies for electronic hardware fault monitoring . <eos> this paper presents three variants of genetic programming ( gp ) approaches for intelligent online performance monitoring of electronic circuits and systems . reliability modeling of electronic circuits can be best performed by the stressor susceptibility interaction model . a circuit or a system is considered to be failed once the stressor has exceeded the susceptibility limits . for on line prediction , validated stressor vectors may be obtained by direct measurements or sensors , which after pre processing and standardization are fed into the gp models . empirical results are compared with artificial neural networks trained using backpropagation algorithm and classification and regression trees . the performance of the proposed method is evaluated by comparing the experiment results with the actual failure model values . the developed model reveals that gp could play an important role for future fault monitoring systems .
reproducing kernel hilbert spaces with odd kernels in price prediction . <eos> for time series of futures contract prices , the expected price change is modeled conditional on past price changes . the proposed model takes the form of regression in a reproducing kernel hilbert space with the constraint that the regression function must be odd . it is shown how the resulting constrained optimization problem can be reduced to an unconstrained one through appropriate modification of the kernel . in particular , it is shown how odd , even , and other similar kernels emerge naturally as the reproducing kernels of hilbert subspaces induced by respective symmetry constraints . to test the validity and practical usefulness of the oddness assumption , experiments are run with large real world datasets on four futures contracts , and it is demonstrated that using odd kernels results in a higher predictive accuracy and a reduced tendency to overfit .
computational simulation modelling of bioreactor configurations for regenerating human bladder . <eos> the objective of this study was to investigate a bioreactor suitable for human bladder regeneration . simulations were performed using the computational fluid dynamic tools . the thickness of the bladder scaffold was <digit> mm , similar to the human bladder , and overall hold up volume within the spherical shape scaffold was <digit> ml . all simulations were performed using ( i ) brinkman equation on porous regions using the properties of <digit> % chitosan <digit> % gelatin structures , ( ii ) michaelis menten type rate law nutrient consumption for smooth muscle cells ( smcs ) and ( iii ) mackie meares relationship for determining effective diffusivities . steady state simulations were performed using flow rates from 0.5 to <digit> ml min . two different inlet shapes ( i ) straight entry at the centre ( design <digit> ) and ( ii ) entry with an expansion ( design <digit> ) were simulated to evaluate shear stress distribution . also , mimicking bladder shape of two inlets ( design <digit> ) was tested . design <digit> provided the uniform shear stress at the inlet and nutrient distribution , which was further investigated for the effect of scaffold locations within the reactor ( i ) attached with a <digit> mm open channel ( design <digit> a ) , ( ii ) flow through with no open channel ( design <digit> b ) and ( iii ) porous structure suspended in the middle with 1.5 mm open channel on either side ( design <digit> c ) . in design <digit> a and <digit> c , fluid flow occurred by diffusion dominant mechanisms . furthermore , the designed bioreactor is suitable for increased cell density of smcs . these results showed that increasing the flow rate is necessary due to the decreased permeability at cell densities similar to the human bladder .
codes based on bck algebras . <eos> the notion of a bck valued function on a set is introduced , and related properties are investigated . codes generated by bck valued functions are established .
rule based management for simulation in agricultural decision support systems . <eos> rule based management systems can offer the farmer or consultant the opportunity to better approximate current or proposed management options , especially as they relate to dynamic conditions in farm fields and across the whole farm . most existing attempts at rule based management within simulation oriented agricultural decision support systems ( dss ) involve limited extension of fixed management dates in response to environmental conditions , or involve rules for implementing limited management events such as fertilizer applications or irrigations . a comprehensive rule based management system for agricultural dss was developed that allows simulated management events to occur in response to flexible producer defined rules and to weather and management induced changes in the soilcrop system over time and space . the system provides a simple , english based rules language , a rules development editor , and software to parse and interpret these rules and provide linkages to application dss software packages such as great plains framework for agricultural resource management ( gpfarm ) . rules can be quickly and easily developed that cover management activities for individual management units ( mus ) or groups of mus . feedback to the simulation package at each time step provides for generation of site and time specific management events across the application . tests of the rule based system on wheat ( triticum aestivum l. ) , corn ( zea mays l. ) , fallow and wheatfallow crop rotations used in eastern colorado showed that management events were simulated within the correct time windows and in the proper sequence . dates for simulated events varied as expected across each rotational cycle as a function of temporal conditions . additional work is anticipated to allow dynamic calculation of event attributes , capture and implementation of producer time priorities , and a simplified menu system for the rule editor .
estimation of predictive uncertainties in flood wave propagation in a river channel using adjoint sensitivity analysis . <eos> this paper applies adjoint sensitivity analysis to flash flood wave propagation in a river channel . a numerical model , based on the st venant equations and the corresponding adjoint equations , determines the sensitivities of predicted water levels to uncertainities in key controls such as inflow hydrograph , channel topography , frictional resistance and infiltration rate . sensitivities are calculated in terms of a measuring function that quantifies water levels greater than certain safe threshold levels along the channel . the adjoint model has been verified by means of an identical twin experiment . the method is applied to a simulated flash flood in a river channel . the sensitivities to key controls are evaluated and ranked and the effects of individual and combined uncertainties on the predicted flood impact are also quantified . copyright ( c ) <digit> john wiley sons , ltd .
identifying material properties of a dielectric motor . <eos> purpose the purpose of the paper is to define a methodology for identifying the electric conductivity and permittivity of lossy dielectric materials employed in a class of small dielectric motors , knowing the motor torque . reference is made to data measured on an existing prototype . design methodology approach the motor operates because of the interaction between a rotating electric field , generated by a three phase system of electrodes , and the charge density induced at the surface of the rotor , which lags with respect to the field . the stator and the rotor are hollow cylinders made of dielectric materials . a finite element model of the motor has been developed . for a given set of material properties , the time averaged value of starting torque acting on the rotor is evaluated by means of the maxwell stress tensor . findings a family of curves of starting torque vs conductivity for different values of rotor permittivity are obtained . each curve is well approximated by a lorentz distribution . originality value a field model of the motor was exploited to estimate the conductivity that gives rise to a prescribed value of the starting torque . the solution of the underlying inverse problem , which is ill posed , can give an helpful insight for maximizing the torque .
estimation of the principle curvatures of approximated surfaces . <eos> this paper presents a method for estimating curvature values of a surface , which is given only approximatively , e.g. , by measured data . the presented method requires estimates of the curvature of curves . these lead , along with the theorems from euler and meusnier , to the values of surface curvature . methods are presented , which converge with the different error order of o ( h ( <digit> ) ) and o ( h ( 2n ) ) . for the first of them explicit formulae are given . it is proved that the error order remains the same through all further calculations until the final estimation of surface curvature is found . ( c ) <digit> published by elsevier science b.v. all rights reserved .
performance analysis of an improved soft preemptive scheme for unevenly distributed traffic in optical wavelength division multiplexing networks . <eos> in optical wavelength division multiplexing ( wdm ) networks , traffic can be unevenly distributed across the network causing inefficient utilization of resources . to solve this problem , an improved soft preemptive ( sp ) scheme is proposed by considering dynamic resource distribution to deal with the uneven network utilization . a novel unevenly distributed traffic model in cross time zone networks is also presented to evaluate the efficiency of the new scheme . compared with other schemes such as normal shortest path first ( spf ) routing and wavelength conversion ( wc ) , the new proposed scheme results demonstrate significantly better performance with respect to the network utilization and overall network blocking probability .
an effective taint based software vulnerability miner . <eos> purpose the purpose of this paper is to propose an approach to detect indirect memory corruption exploit ( imce ) at runtime on binary code , which is often caused by integer conversion error . real world attacks were evaluated for experimentation . design methodology approach current dynamic analysis detects attacks by enforcing low level policy which can only detect control flow hijacking attack . the proposed approach detects imce with high level policy enforcement using dynamic taint analysis . unlike low level policy enforced on instruction level , the authors ' policy is imposed on memory operation routine . the authors implemented a fine grained taint analysis system with accurate taint propagation for detection . findings conversion errors are common and most of them are legitimate . taint analysis with high level policy can accurately block imce but have false positives . proper design of data structures to maintain taint tag can greatly improve overhead . originality value this paper proposes an approach to block imce with high level policy enforcement using taint analysis . it has very low false negatives , though still causes certain false positives . the authors made several implementation contributions to strengthen accuracy and performance .
cronoa code for the simulation of chemical weathering . <eos> we present a code crono for simulation of chemical weathering . dissolution kinetics is combined with the calculation of thermodynamic equilibrium . this algorithm simulates the regolith forming in time . this algorithm emulates low temperature systems that do not reach the equilibrium . the application of the code was illustrated by an example of weathering of basalts .
approximating minimum size weakly connected dominating sets for clustering mobile ad hoc networks . <eos> we present a series of approximation algorithms for finding a small weakly connected dominating set ( wcds ) in a given graph to be used in clustering mobile ad hoc networks . the structure of a graph can be simplified using wcds 's and made more succinct for routing in ad hoc networks . the theoretical performance ratio of these algorithms is o ( ln ) compared to the minimum size wcds , where is the maximum degree of the input graph . the first two algorithms are based on the centralized approximation algorithms of guha and khuller cite guha khuller <digit> for finding small connected dominating sets ( cds 's ) . the main contribution of this work is a completely distributed algorithm for finding small wcds 's and the performance of this algorithm is shown to be very close to that of the centralized approach . comparisons between our work and some previous work ( cds based ) are also given in terms of the size of resultant dominating sets and graph connectivity degradation .
incorporating partial matches within multiobjective pharmacophore identification . <eos> this paper describes the extension of our earlier multiobjective method for generating plausible pharmacophore hypotheses to incorporate partial matches . diverse sets of molecules rarely adopt exactly the same binding mode , and so allowing the identification of partial matches allows our program to be applied to larger and more diverse datasets . the method explores the conformational space of a series of ligands simultaneously with their alignment using a multiobjective genetic algorithm ( moga ) . the principles of pareto ranking are used to evolve a diverse set of pharmacophore hypotheses that are optimised on conformational energy of the ligands , the goodness of the overlay and the volume of the overlay . a partial match is defined as a pharmacophoric feature that is present in at least two , but not all , of the ligands in the set . the number of ligands that map to a given pharmacophore point is taken into account when evaluating an overlay . the method is applied to a number of test cases extracted from the protein data bank ( pdb ) where the true overlay is known .
dna sequence reconstruction based on genetic algorithm . <eos> it is becoming increasingly important to develop a novel process for determining the letters of our genetic code , known as dna sequencing . this task is performed by large datasets using the combination of heuristic methods with little mathematical calculation . in this paper , we present a new method for dna sequence reconstruction using genetic algorithm , which is able to predict the actual dna sequence . the performance of the genetic algorithm is evaluated with respect to previous methods in the literature . the results indicate that the proposed new method is superior to previous methods . finally we compare the results of the experiment and discuss the performance of the proposed method in the dna sequence reconstruction .
self compensating design for focus variation . <eos> process variations have become a bottleneck for predictable and high yielding ic design and fabrication . linewidth variation ( l ) due to defocus in a chip is largely systematic after the layout is completed , i.e. , dense lines smile through focus while isolated ( iso ) lines frown . in this paper , we propose a design flow that allows explicit compensation of focus variation , either within a cell ( self compensated cells ) or across cells in a critical path ( self compensated design ) . assuming that iso and dense variants are available for each library cell , we achieve designs that are more robust to focus variation . design with a self compensated cell library incurs <digit> <digit> % area penalty while compensating for focus variation . across cell optimization with a mix of dense and iso cell variants incurs <digit> <digit> % area overhead compared to the original cell library , while meeting timing constraints across a large range of focus variation ( from <digit> to 0.4 um ) . a combination of original and iso cells provides an even better self compensating design option , with only <digit> % area overhead . circuit delay distributions are tighter with self compensated cells and self compensated design than with a conventional design methodology .
a software only solution to use scratch pads for stack data . <eos> a dynamic scratch pad memory ( spm ) management scheme for program stack data with the objective of processor power reduction is presented . basic technique does not need the spm size at compile time , does not mandate any hardware changes , does not need profile information , and seamlessly integrates support for recursive functions . stack frames are managed using a software spm manager , integrated into the application binary , and shows average energy savings of <digit> % along with a performance improvement of <digit> % , on benchmarks from mibench . spm management can be further optimized and made pointer safe , by knowing the spm size .
a new algorithm for fixed point quantum search . <eos> the standard quantum search lacks a feature , enjoyed by many classical algorithms , of having a fixed point , i.e. monotonic convergence towards the solution . recently a fixed point quantum search algorithm has been discovered , referred to as the phase pi <digit> search algorithm , which gets around this limitation . while searching a database for a target state , this algorithm reduces the error probability from epsilon to epsilon ( 2q <digit> ) using q oracle queries , which has since been proved to be asymptotically optimal . a different algorithm is presented here , which has the same worst case behavior as the phase pi <digit> search algorithm but much better average case behavior . furthermore the new algorithm gives epsilon ( 2q <digit> ) convergence for all integral q , whereas the phase pi <digit> search algorithm requires q to be ( <digit> ( n ) <digit> ) <digit> with n a positive integer . in the new algorithm , the operations are controlled by two ancilla qubits , and fixed point behavior is achieved by irreversible measurement operations applied to these ancillas . it is an example of how measurement can allow us to bypass some restrictions imposed by unitarity on quantum computing .
test enrichment for path delay faults using multiple sets of target faults . <eos> test sets for path delay faults in circuits with large numbers of paths are typically generated for path delay faults associated with the longest circuit paths . it is shown that such test sets may not detect faults associated with the next to longest paths . this may lead to undetected failures since shorter paths may fail without any of the longest paths failing . in addition , paths that appear to be shorter may actually be longer than the longest paths if the procedure used for estimating path length is inaccurate . a test enrichment procedure is proposed that increases significantly the number of faults associated with the next to longest paths that are detected by a test set without increasing its size . this is achieved by targeting both types of faults , but ensuring that the test generation procedure would detect the faults associated with the longest paths , while allowing the procedure the flexibility of detecting or not detecting the faults associated with the next to longest paths . the proposed procedure thus improves the quality of the test set without increasing its size . the test enrichment procedure is built on top of a new and effective dynamic test compaction , procedure in order to demonstrate that test enrichment is effective even for compact test sets .
some problems of computer aided testing and interview like tests . <eos> computer based testing is an effective teacher 's tool , intended to optimize course goals and assessment techniques in a comparatively short time . however , this is accomplished only if we deal with high quality tests . it is strange , but despite the <digit> year history of testing theory ( see , anastasi , a. , urbina , s. ( <digit> ) . psychological testing . upper saddle river , nj prentice hall ) there still exist some misconceptions . modern wide spread systems for computer based course management and testing reveal a set of problems corresponding to certain features of testing methods . this article is devoted to some omissions typical to several course management systems ( e.g. , moodle and blackboard ) . these omissions and the ways of avoiding them are shown in a simple test intended to verify student knowledge . we suggest a special test description language dedicated to drawing your attention to the mathematical aspects of test quality . the language can also be realized in computer software . we provide an example of such software in this article . ( c ) <digit> elsevier ltd. all rights reserved .
numerical simulation of viscous flows with free surface around realistic hull forms with transom . <eos> this paper describes a method for simulation of viscous flows with a free surface around realistic hull forms with a transom , which has been developed based on a finflo rans solver with a moving mesh . a dry transom model is proposed and implemented for the treatment of flows off the transom . the bulk rans flow with the artificial compressibility is solved by a cell centred finite volume multigrid scheme and the free surface deformed by wave motions is tracked by satisfying the kinematic and dynamic free surface boundary conditions on the actual location of the surface . the effects of turbulence on flows are evaluated with the baldwin lomax turbulence model without a wall function . a test case is modern container ship model with a transom , the hamburg test case . the calculated results are validated and they agree well with the measured results in terms of the free surface waves and the total resistance coefficient . furthermore . the numerical solutions successfully captured many important features of the complicated interaction of the free surface with viscous flows around transom stern ships . in addition , the convergence performance and the grid refinement studies are also investigated . copyright ( c ) <digit> john wiley sons , ltd .
an analysis of language level support for self adaptive software . <eos> self adaptive software has become increasingly important to address the new challenges of complex computing systems . to achieve adaptation , software must be designed and implemented by following suitable criteria , methods , and strategies . past research has been mostly addressing adaptation by developing solutions at the software architecture level . this work , instead , focuses on finer grain programming language level solutions . we analyze three main linguistic approaches metaprogramming , aspect oriented programming , and context oriented programming . the first two are general purpose linguistic mechanisms , whereas the third is a specific and focused approach developed to support context aware applications . this paradigm provides specialized language level abstractions to implement dynamic adaptation and modularize behavioral variations in adaptive systems . the article shows how the three approaches can support the implementation of adaptive systems and compares the pros and cons offered by each solution .
a lattice based approach for updating access control policies in real time . <eos> real time update of access control policies , that is , updating policies while they are in effect and enforcing the changes immediately and automatically , is necessary for many dynamic environments . examples of such environments include disaster relief and war zone . in such situations , system resources may need re configuration or operational modes may change , necessitating a change of policies . for the system to continue functioning , the policies must be changed immediately and the modified policies automatically enforced . in this paper , we propose a solution to this problemwe consider real time update of access control policies in the context of a database system . in our model , a database consists of a set of objects that are read and updated through transactions . access to the data objects are controlled by access control policies which are stored in the form of policy objects . we consider an environment in which different kinds of transactions execute concurrently some of these may be transactions updating policy objects . updating policy objects while they are deployed can lead to potential security problems . we propose algorithms that not only prevent such security problems , but also ensure serializable execution of transactions . the algorithms differ on the degree of concurrency provided and the kinds of policies each can update .
one case , four theories . <eos> in this chapter , clinical material illustrates key theoretical concepts and underscores the value of heinz kohut 's radical approach to psychoanalysis . the psychodynamic treatment of matthew spans over a decade and traces the therapist 's immersion in four clinical modalities . the transition from kleinian and british object relations orientations to a therapeutic style informed by psychoanalytic self psychology and intersubjective systems theory broke through impasses generated in the earlier chapters of matthew 's therapy . the empathic listening stance , the impact of the analyst 's subjectivity on the treatment 's progress , and the vital role of selfobject experiences in the development , restoration , and maintenance of an individual 's sense of self constitute a few of the crucial and enduring curative elements brought to the field of psychoanalysis and psychodynamic therapy by kohut 's pioneering efforts .
<digit> sets cut decoding algorithms for random codes based on quasigroups . <eos> the decoding speed is the biggest problem of random codes based on quasigroups proposed elsewhere . these codes are a combination of cryptographic algorithms and error correcting codes . in our previous paper we proposed cut decoding algorithm which is 4.5 times faster than the original one for code ( 72,288 ) . in this paper , four new modifications ( so called <digit> sets cut decoding algorithms ) of this algorithm are proposed in order to obtain an improvement of the decoding speed . we present and analyze several experimental results obtained with different algorithms for random codes based on quasigroups . it is shown that using new algorithms , improvement of the efficiency and decoding speed is obtained . also , we derive the upper bound for packet error probability obtained with cut decoding and <digit> sets cut decoding algorithm . at the end , some methods for reducing the number of unsuccessful decodings in the new proposed algorithms are considered .
a robust method for handling low density regions in hybrid simulations for collisionless plasmas . <eos> a robust method to handle vacuum and near vacuum regions in hybrid simulations for space and astrophysical plasmas is presented . the conventional hybrid simulation model dealing with kinetic ions and a massless charge neutralizing electron fluid is known to be susceptible to numerical instability due to divergence of the whistler mode wave dispersion , as well as division by density operation in regions of low density . consequently , a pure vacuum region is not allowed to exist in the simulation domain unless some ad hoc technique is used . to resolve this difficulty , an alternative way to introduce finite electron inertia effect is proposed . contrary to the conventional method , the proposed one introduces a correction to the electric field rather than the magnetic field . it is shown that the generalized ohm 's law correctly reduces to laplace 's equation in a vacuum which therefore does not involve any numerical problems . in addition , a variable ion to electron mass ratio is introduced to reduce the phase velocity of high frequency whistler waves at low density regions so that the stability condition is always satisfied . it is demonstrated that the proposed model is able to handle near vacuum regions generated as a result of nonlinear self consistent development of the system , as well as pure vacuum regions set up at the initial condition , without losing the advantages of the standard hybrid code .
emergent cooperative goal satisfaction in large scale automated agent systems . <eos> cooperation among autonomous agents has been discussed in the dai community for several years . papers about cooperation ( conte et al. , <digit> rosenschein , <digit> ) , negotiation ( kraus and wilkenfeld , <digit> ) , distributed planning ( conry et al. , <digit> ) , and coalition formation ( ketchpel , <digit> sandholm and lesser , <digit> ) , have provided a variety of approaches and several algorithms and solutions to situations wherein cooperation is possible . however , the case of cooperation in large scale multi agent systems ( mas ) has not been thoroughly examined . therefore , in this paper we present a framework for cooperative goal satisfaction in large scale environments focusing on a low complexity physics oriented approach . the multi agent systems with which we deal are modeled by a physics oriented model . according to the model , mas inherit physical properties , and therefore the evolution of the computational systems is similar to the evolution of physical systems . to enable implementation of the model , we provide a detailed algorithm to be used by a single agent within the system . the model and the algorithm are appropriate for large scale , dynamic , distributed problem solver systems , in which agents try to increase the benefits of the whole system . the complexity is very low , and in some specific cases it is proved to be optimal . the analysis and assessment of the algorithm are performed via the well known behavior and properties of the modeling physical system . ( c ) <digit> elsevier science b.v. all rights reserved .
visualizing image collections using high entropy layout distributions . <eos> mechanisms for visualizing image collections are essential for browsing and exploring their content . this is especially true when metadata are ineffective in retrieving items due to the sparsity or esoteric nature of text . an obvious approach is to automatically lay out sets of images in ways that reflect relationships between the items . however , dimensionality reduction methods that map from high dimensional content based feature distributions to low dimensional layout spaces for visualization often result in displays in which many items are occluded whilst large regions are empty or only sparsely populated . furthermore , such methods do not consider the shape of the region of layout space to be populated . this paper proposes a method , high entropy layout distributions . that addresses these limitations . layout distributions with low differential entropy are penalized . an optimization strategy is presented that finds layouts that have high differential entropy and that reflect inter image similarities . efficient optimization is obtained using a step size constraint and an approximation to quadratic ( renyi ) entropy . two image archives of cultural and commercial importance are used to illustrate and evaluate the method . a comparison with related methods demonstrates its effectiveness .
hard problems in similarity searching . <eos> the closest substring problem is one of the most important problems in the field of computational biology . it is stated as follows given a set of t sequences s1 , s2 , , st over an alphabet , and two integers k , d with d k , can one find a string s of length k and , for all i 1,2 , , t , substrings oi of si , all of length k , such that d ( s , oi ) d ( for all i 1,2 , , t ) ( here , d ( . , . ) represents the hamming distance ) . closest substring was shown to be np hard ( proceedings of 10th soda , <digit> , pp. <digit> ) and w <digit> hard with respect to the number t of input sequences ( proceedings of stacs02 , lecture notes in computer science , vol . <digit> , <digit> , pp. <digit> ) recently , an important number of results concerning the parameterized computational complexity of closest substring has been added in evans et al. ( theoret . comput . sci . <digit> ( <digit> ) ( <digit> ) <digit> ) . in this paper we introduce and analyze two variants of the closest substring problem , obtained by imposing restrictions on the pairwise distances between the substrings oi the bounded hamming distance constraint asks that d ( oi , oj ) p , for all i , j 1,2 , , t ( where p < 2d is a given constant ) and yields the problem called bccs the sum of pairs constraint asks that <digit> i lt j td ( oi , oj ) p <digit> i lt j t d ( o i , o j ) p ( where p < dt ( t <digit> ) is a given constant ) and yields the problem called sccs .
a scalable farm skeleton for hybrid parallel and distributed programming . <eos> multi core processors and clusters of multi core processors are ubiquitous . they provide scalable performance yet introducing complex and low level programming models for shared and distributed memory programming . thus , fully exploiting the potential of shared and distributed memory parallelization can be a tedious and error prone task programmers must take care of low level threading and communication ( e.g. message passing ) details . in order to assist programmers in developing performant and reliable parallel applications algorithmic skeletons have been proposed . they encapsulate well defined , frequently recurring parallel and distributed programming patterns , thus shielding programmers from low level aspects of parallel and distributed programming . in this paper we take on the design and implementation of the well known farm skeleton . in order to address the hybrid architecture of multi core clusters we present a two tier implementation built on top of mpi and openmp . on the basis of three benchmark applications , including a simple ray tracer , an interacting particles system , and an application for calculating the mandelbrot set , we illustrate the advantages of both skeletal programming in general and this two tier approach in particular .
patterns from the sky satellite image analysis using pulse coupled neural networks for pre processing , segmentation and edge detection . <eos> in this work we attempt to distinguish land from water in satellite images , specifically images taken by the forte satellite . first , we successfully approximate areas hidden by stationary artefacts in the image . we then segment regions of land from water . finally , we determine the boundaries of the surrounding landmasses . ( c ) <digit> elsevier science b.v. all rights reserved .
dynamic influent pollutant disturbance scenario generation using a phenomenological modelling approach . <eos> activated sludge models are widely used for simulation based evaluation of wastewater treatment plant ( wwtp ) performance . however , due to the high workload and cost of a measuring campaign on a full scale wwtp , many simulation studies suffer from lack of sufficiently long influent flow rate and concentration time series representing realistic wastewater influent dynamics . in this paper , a simple phenomenological modelling approach is proposed as an alternative to generate dynamic influent pollutant disturbance scenarios . the presented set of models is constructed following the principles of parsimony ( limiting the number of parameters as much as possible ) , transparency ( using parameters with physical meaning where possible ) and flexibility ( easily extendable to other applications where long dynamic influent time series are needed ) . the proposed approach is sub divided in four main model blocks <digit> ) model block for flow rate generation , <digit> ) model block for pollutants generation ( carbon , nitrogen and phosphorus ) , <digit> ) model block for temperature generation and <digit> ) model block for transport of water and pollutants . the paper is illustrated with the results obtained during the development of the dynamic influent of the benchmark simulation model no. <digit> ( bsm2 ) . the series of simulations show that it is possible to generate a dry weather influent describing diurnal flow rate dynamics ( low rate at night , high rate during day time ) , weekend effects ( with different flow rate during weekends , compared to weekdays ) , holiday effects ( where the wastewater production is assumed to be different for a number of weeks ) and seasonal effects ( with variations in the infiltration and thus also the flow rate to the wwtp ) . in addition , the dry weather model can be extended with a rain and storm weather generator , where the proposed phenomenological model can also mimic the first flush effect from the sewer network and the influent dilution phenomena that are typically observed at full scale wwtps following a rain event . finally , the extension of the sewersystem can be incorporated in the influent dynamics as well the larger the simulated sewer network , the smoother the simulated diurnal flow rate and concentration variations . in the discussion , it is pointed out how the proposed phenomenological models can be expanded to other applications , for example to represent heavy metal or organic micro pollutant loads entering the treatment plant .
follow the money assessing the allocation of e rate funds . <eos> expanding the principles of the universal provision of telecommunications services to encompass elementary and secondary schools and libraries , lawmakers paved the way for the creation of the e rate program by passing the telecommunications act in <digit> . the e rate program aims to bridge the digital divide in advanced telecommunications by offering services to eligible schools and libraries for educational purposes at discounted rates . currently in its sixth cycle , the e rate program has provided nearly us <digit> billion in discounts to eligible organizations . the program has inspired much controversy and criticism , yet studies provide evidence that the program is effectively helping to enhance the provision of services to traditionally disadvantaged communities . this study challenges these claims and presents empirical evidence that suggests e rate resources are not systematically allocated to underprivileged , rural states where funds would most effectively help to bridge the digital gap in access to telecommunications technologies .
measurements and simulations of the heat transfer on end windings of an induction machine . <eos> purpose for an accurate simulation of the temperature distribution inside an electrical machine a method for deriving the convective heat transfer coefficient numerically would be desirable . the purpose of this paper is to present a reliable simulation setup , which is able to reproduce the measured , convective heat transfer coefficient at certain spots on the end windings of an electric machine . design methodology approach the heat flux density on certain spots on the end windings of an induction motor have been measured with heat flux sensors , in order to find out the convective heat transfer coefficient . to identify the air mass flow inside a cooling duct of an encapsulated cooling circuit during the operation of the motor , the pressure loss inside the duct has been measured . the measured data for temperature and air mass flow have been used as boundary conditions for the identification of the convective heat transfer coefficient with a commercial software for computational fluid dynamics ( cfd ) . findings the measured data for the local convective heat transfer coefficients have been compared to the results of the numerical simulation for various rotational velocities . the quality of the simulated convective heat transfer coefficient depending on the rotational velocity meets the measured values . owing to the used simplified model , the quantity of the measured values differ strongly around the simulated coefficient for the convective heat transfer . originality value the derivation of the convective heat transfer is a challenging subject in cfd but has become more reliable with the invention of the sst and the sas sst turbulence model . in the present work , measurements on the end windings have been compared to simulation results derived with the sas sst turbulence model .
local redundant polymorphism query elimination . <eos> dynamic polymorphism is a powerful yet costly feature of object oriented programming languages . to improve performance , several techniques have been developed to simplify or remove polymorphism operations when they are not needed . however , these techniques have little effect on sites that are actively polymorphic . in this paper we present an alternate approach to the optimization of operations such as virtual dispatches and type tests . rather than attempt to eliminate the polymorphism mechanism , we identify situations where resolved type and method information can be shared across multiple polymorphism operations . in short , we perform a partial redundancy elimination transform over the loads and tests that constitute polymorphism queries . we describe the realization of our technique in the jikes rvm and present its effects on the dacapo and specjvm98 benchmarks . rigorous measurements show a mean improvement , including several statistically significant results , over a configuration with no dispatch optimization and over one that employs guarded inlining . we underscore the potential of our approach by demonstrating speedups of up to <digit> % on a highly polymorphic benchmark . our results show the approach is both successful and complementary to techniques that focus on degenerate polymorphism .
acm international workshop on interactive multimedia on mobile and portable devices ( immpd ' <digit> ) . <eos> with the mobile and portable devices become ubiquitous for people 's daily life , how to design user interfaces of these products that enable natural , intuitive and fun interaction is one of the main challenges the multimedia community is facing . following several successful events , the acm international workshop on interactive multimedia on mobile and portable devices ( immpd ' <digit> ) aims to bring together researchers from both academia and industry in domains including computer vision , audio and speech processing , machine learning , pattern recognition , communications , human computer interaction , and media technology to share and discuss recent advances in interactive multimedia .
decision making under time pressure with different information sources and performance based financial incentives part <digit> . <eos> in part <digit> , we examine the viability of the new symbolic language that we described in part <digit> , in a specific setting . using an abstract classification task that involves decision making under time pressure , we study multiple measures of subject performance at this task using the new language vis a vis written and spoken english . initial experimental results suggest that , despite its relative novelty , the proposed language is at least as effective as the more traditional communication modes in the specific setting examined , while succinctly conveying what must be conveyed . ( c ) <digit> elsevier science b.v. all rights reserved .
air gap formation by uv assisted decomposition of cvd material . <eos> a sacrificial material deposited by cvd is used to demonstrate air gap formation in single damascene structures by uv assisted decomposition . the material is removed through a porous low k cap , after completion of the damascene scheme . the porosity of the low k cap is shown to be critical for efficient air gap formation . capacitance reduction of <digit> % is demonstrated using this technique compared to conventional sioc ( h ) interconnects and an effective dielectric constant of 1.7 is extrapolated .
recovering distributed objects . <eos> distributed multithreaded applications operating in shared nothing environments present challenges to classical fault tolerance mechanisms . the piecewise determinism assumption is lost ( due to multithreading ) , and data must be replicated ( because of the shared nothing environment ) . in this paper , we explore a systematic approach to providing fault tolerance , by considering data race free programs that have the benefits of piecewise determinism and yet allow multithreading . we base our logging and recovery algorithm on a logical ring structure that allows the underlying distributed system to migrate threads , migrate and replicate objects , and perform multi object transactions . ( c ) <digit> elsevier science b.v. all rights reserved .
bio and health informatics meets cloud biovlab as an example . <eos> the exponential increase of genomic data brought by the advent of the next or the third generation sequencing ( ngs ) technologies and the dramatic drop in sequencing cost have driven biological and medical sciences to data driven sciences . this revolutionary paradigm shift comes with challenges in terms of data transfer , storage , computation , and analysis of big bio medical data . cloud computing is a service model sharing a pool of configurable resources , which is a suitable workbench to address these challenges . from the medical or biological perspective , providing computing power and storage is the most attractive feature of cloud computing in handling the ever increasing biological data . as data increases in size , many research organizations start to experience the lack of computing power , which becomes a major hurdle in achieving research goals . in this paper , we review the features of publically available bio and health cloud systems in terms of graphical user interface , external data integration , security and extensibility of features . we then discuss about issues and limitations of current cloud systems and conclude with suggestion of a biological cloud environment concept , which can be defined as a total workbench environment assembling computational tools and databases for analyzing bio medical big data in particular application domains .
correlation and the time interval in multiple regression models . <eos> in this paper we investigate the time interval effect of multiple regression models in which some of the variables are additive and some are multiplicative . the effect on the partial regression and correlation coefficients is influenced by the selected time interval . we find that the partial regression and correlation coefficients between two additive variables approach one period values as n increases . when one of the variables is multiplicative , they will approach zero in the limit . we also show that the decreasing speed of the n period correlation coefficients between both multiplicative variables is faster than others , except that a one period correlation has a higher positive value . the results of this paper can be widely applied in various fields where regression or correlation analyses are employed .
linear time dependent constraints programming with msvl . <eos> this paper investigates specifying and solving linear time dependent constraints with an interval temporal logic programming language msvl . to this end , linear constraint statements involving linear equality and non strict inequality are first defined . further , the time dependent relations in the constraints are specified by temporal operators , such as . thus , linear time dependent constraints can be smoothly incorporated into msvl . moreover , to solve the linear constraints within msvl by means of reduction , the operational semantics for linear constraints is given . in particular , semantic equivalence rules and transition rules within a state are presented , which enable us to reduce linear equations , inequalities and optimization problems in a convenient way . besides , the operational semantics is proved to be sound . finally , a production scheduling application is provided to illustrate how our approach works in practice .
from business intelligence to semantic data stream management . <eos> evolution of business intelligence with emergence of big data technologies . new technologies and approaches the 3vs ( volume , velocity and variety ) of big data . stream reasoning over big data . summarizing data streams ( semantic and classic data ) . semantic data matching in stream context .
a roadmap towards sustainable self aware service systems . <eos> self awareness and self adaptation have become primary concerns in large scale systems as they have become too complex to be managed by human administrators alone , but rather require a new blend of coordination mechanisms between people and software services . this paper presents a roadmap to effective and efficient system adaptation through coupling self awareness of global level goals with sustainability constraints . sustainability of large scale systems challenges self adaptation approaches by its intrinsic characters of global and long lasting effects . we introduce five levels of awareness ( i ) event awareness , ( ii ) situation awareness , ( iii ) adaptability awareness , ( iv ) goal awareness , and ( v ) future awareness . within each level we introduce applicable principles and subsequently outline necessary models , algorithms , and protocols . the approach puts special focus on the interdependencies of human and service elements .
a method for recognizing overlapping elliptical bubbles in bubble image . <eos> direct imaging technology is an effective and convenient method for the estimation of bubble size distribution ( bsd ) . however , overlapping bubble has an influence on bsd when gas holdup is more than <digit> % . in this paper , we present a new method of overlapping elliptical bubble recognition to determine bubble size . the method mainly includes two steps contour segmentation and segment grouping . contour segmentation is on the assumption that the concave points in the dominant point sequence are always the connecting points , and segment grouping is mainly based on the average distance deviation criterion . both simulated images and real bubble images are used to evaluate this new method . the results show that it is effective in the recognition of overlapping elliptical bubbles and have a potential in other elliptical object recognition . in the last , two methods are used for bsd estimation . it is found that the bubble size ( such d10 or d32 ) estimated by the ignore method is slightly smaller than that estimated by the recognition method .
calculation by artificial compressibility method and virtual flux method on gpu . <eos> in this study , artificial compressibility method and virtual flux method were implemented on gpus . because gpus are recognized as massively parallel computers , dp lur was employed as time integration method . in spite of slow convergence characteristics of dp lur , calculation by the coupling of dp lur and gpu is about <digit> times faster in time than that of lu sgs and single cpu . virtual flux method , which enables to calculate flow around curved surface on the cartesian grid is also implemented to artificial compressibility method on gpu program , which was implemented in this study . gpu code of virtual flux method is about <digit> times faster than cpu code . ( c ) <digit> elsevier ltd. all rights reserved .
a synthetic view of belief revision with uncertain inputs in the framework of possibility theory . <eos> this paper discusses belief revision under uncertain inputs in the framework of possibility theory . this framework is flexible enough to allow for numerical and ordinal revision procedures . it is emphasized that revision under uncertain inputs can be understood in two different ways , depending on whether the input is viewed as a constraint to be enforced , or as an unreliable piece of information . two revision rules are proposed to implement these forms of revision . ir is shown that m. a. williams 's transmutations , originally defined in the setting of spohn 's functions , can be captured in possibility theory , as well as boutilier 's natural revision . the use of conditioning greatly simplifies the description of these belief change operations . lastly , preliminary results on implementing revision rules at the syntactic level are given . ( c ) <digit> elsevier science inc .
a regional approach to foot and ankle mri . <eos> this chapter presents a regional anatomic approach to mri applications in the foot and ankle . from a clinical perspective , patients often describe their symptoms in terms of the part of the foot that hurts and when and how it hurts . clinical questioning and physical diagnosis pursue this line as well , trying to narrow down the diagnostic possibilities . there are conditions that may blur the anatomic distinctions for forefoot , midfoot , rearfoot , and ankle involve more than one region of the foot simultaneously or occur in any area of the foot . the chapter also includes a separate section on the presentations of inflammatory arthritides in foot and ankle joints .
profiling the non users examination of life position indicators , sensation seeking , shyness , and loneliness among users and non users of social network sites . <eos> the aim of the current study is to explore if there are differences between users and non users of social network sites in terms of their sensation seeking , life position indicators , shyness , and loneliness . using data from a survey of adults <digit> years old , results revealed that compared to an average facebook user , a non user is significantly older and scores higher on shyness and loneliness , is less socially active , and less prone to sensation seeking activities . facebook is not a substitute channel of communication for those who are shy and lonely and lack face to face interactions . this study extends our knowledge of digital divide , uses and gratifications theory , and social enhancement hypothesis .
computing with coupled chaotic neuronal maps . <eos> chaos computing is a new paradigm of an unconventional computing that exploits the extreme non linearity of chaotic systems . we propose a stragey for chaos based computation in one way coupled chaotic neuronal maps . in the drive response unit , either the output state of the response system or the synchronization error between drive response systems are used to obtain basic logic operation .
robust f0 estimation based on complex lpc analysis for irs filtered noisy speech . <eos> this paper proposes a novel robust fundamental frequency ( f0 ) estimation algorithm based on complex valued speech analysis for an analytic speech signal . since analytic signal provides spectra only over positive frequencies , spectra can be accurately estimated in low frequencies . consequently , it is considered that f0 estimation using the residual signal extracted by complex valued speech analysis can perform better for f0 estimation than that for the residual signal extracted by conventional real valued lpc analysis . in this paper , the autocorrelation function weighted by amdf is adopted for the f0 estimation criterion and four signals speech signal , analytic speech signal , lpc residual and complex lpc residual , are evaluated for the f0 estimation . speech signals used in the experiments were an irs filtered speech corrupted by adding white gaussian noise or pink noise whose noise levels are <digit> , <digit> , <digit> , <digit> db . the experimental results demonstrate that the proposed algorithm based on complex lpc residual can perform better than other methods in noisy environment .
modeling wikipedia admin elections using multidimensional behavioral social networks . <eos> wikipedia admins are editors entrusted with special privileges and duties , responsible for the community management of wikipedia . they are elected using a special procedure defined by the wikipedia community , called request for adminship ( rfa ) . because of the growing amount of management work ( quality control , coordination , maintenance ) on the wikipedia , the importance of admins is growing . at the same time , there exists evidence that the admin community is growing more slowly than expected . we present an analysis of the rfa procedure in the polish language wikipedia , since the procedures introduction in <digit> . with the goal of discovering good candidates for new admins that could be accepted by the community , we model the admin elections using multidimensional behavioral social networks derived from the wikipedia edit history . we find that we can classify the votes in the rfa procedures using this model with an accuracy level that should be sufficient to recommend candidates . we also propose and verify interpretations of the dimensions of the social network . we find that one of the dimensions , based on discussion on wikipedia talk pages , can be validly interpreted as acquaintance among editors , and discuss the relevance of this dimension to the admin elections .
a computational model for ratbot locomotion based on cyborg intelligence . <eos> ratbots with electric stimulation in their brains possess not only their own biological sensation , perception , memory , and locomotion control abilities , but also machine visual sensation , memory and computing functionalities . with electrodes implanted in the medial forebrain bundle ( mfb ) , we propose here a hybrid bio machine locomotion system in the ratbots , processing the machine visual inputs , forming hybrid multiple memory system and outputting locomotion commands for navigation behaviors . to illustrate the enhanced performance of the ratbots theoretically , a computational model is presented to show how the multiple memory system affects the central pattern generator ( cpg ) generating the gait pattern and running velocity . compared with the extensive data from behavioral experiments , the results output from the proposed computational model fit the data of the decision accuracy and the relative velocity well , thus shown that the model provides a possible locomotion control mechanism innately in the cyborg systems .
strained ingaasp multi quantum well structures for inp based wide linewidth and polarization insensitive semiconductor optical amplifiers . <eos> we have calculated the band structure of 1.55 m ingaasp ingaasp multi quantum well structures using lttingerkohn hamiltonian taking into account the strain in the quantum wells ( qws ) and barriers , and the confinement in the quantum wells . using the calculated dispersion curves and oscillator strength between the different interband transitions , we have determined the optical gain in te and tm mode and the spontaneous amplified emission as a function of injected carrier density in devices composed of quantum wells with different thicknesses . we find that an optical gain linewidth larger than 130nm with a te tm polarization dependence lower than 1db can be obtained using a three quantum well in0 .53 ga0 .47 as0 .96 p0 .04 ingaasp active layer with quantum well thicknesses of <digit> , <digit> and 19nm .
buying and selling an asset over the finite time horizon a non parametric approach . <eos> we consider the problem of buying an asset and selling it later in the open market within a limited time period . in such a situation , it is usually assumed that the market prices are random observations from a known distribution . however , we propose in the paper the rank based trading strategy that does not require any distributional assumption . we only assume that the agent 's utility depends on the actual ranks of the purchase and selling prices of the asset . the non parametric trading policy , which maximizes the agent 's expected utility , can be stated with a sequence of critical ranks the agent must buy an asset at time j if the relative rank of its purchase price is larger than the pre specified critical rank at that time . likewise , the agent must sell the asset at time k if the relative rank of its selling price is less than the pre specified critical rank at time k. finally , we conduct a simulation experiment to analyze the effect of the auto correlation in market prices on the performance of the optimal trading policy .
a loop accelerator for low power embedded vliw processors . <eos> the high transistor density afforded by modern vlsi processes have enabled the design of embedded processors that use clustered execution units to deliver high levels of performance . however , delivering data to the execution resources in a timely manner remains a major problem that limits ilp . it is particularly significant for embedded systems where memory and power budgets are limited . a distributed address generation and loop acceleration architecture for vliw processors is presented . this decentralized on chip memory architecture uses multiple srams to provide high intra processor bandwidth . each sram has an associated stream address generator capable of implementing a variety of addressing modes in conjunction with a shared loop accelerator . the architecture is extremely useful for generating application specific embedded processors , particularly for processing input data which is organized as a stream . the idea is evaluated in the context of a fine grain vliwarchitecture executing complex perception algorithms such as speech and visual feature recognition . transistor level spice simulations are used to demonstrate a 159x improvement in the energy delay product when compared to conventional architectures executing the same applications .
knowledge propagation in large image databases using neighborhood information . <eos> the aim of this paper is to reduce to a minimum the level of human intervention in the semantic annotation process of images . ideally , only one copy of each object of interest would be labeled manually , and the labels would then be propagated automatically to all other occurrences of the objects in the database . to that end , we propose a neighbor based influence propagation approach kprop which builds a voting model and propagates the knowledge associated to some objects to similar objects . we show that kprop can perform efficiently through matrix computations and achieve better performance with fewer labeled examples per object .
a generalized model for scratch detection . <eos> this paper presents a generalization of kokaram 's model for scratch lines detection on digital film materials . it is based on the assumption that scratch is not purely additive on a given image but shows also a destroying effect . this result allows us to design a more efficacious scratch detector which performs on a hierarchical representation of a degraded image , i.e. , on its cross section local extrema . thanks to weber 's law , the proposed detector even works well on slight scratches resulting completely automatic , except for the scratch color ( black or white ) . the experimental results show that the proposed detector works better in terms of good detection and false alarms rejection with a lower computing time .
robust wavelet sliding mode control via time variant sliding function . <eos> in this paper , a new robust wavelet time variant sliding mode control ( rwtvsmc ) for an uncertain nonlinear system is presented . the proposed method is composed of two controllers , based on a time variant sliding equation . for this purpose a neural wavelet controller is designed to approximate an ideal controller based on the wavelet network approximation . also a robust controller is designed to achieve h ( infinity ) tracking performance . new terminologies , rejection parameter and rejection regulator , for filtering all un modeled frequencies are defined . a time variant sliding equation based on the time variant rejection parameter to achieve the best tracking performance is then presented . in addition , two theorems and one lemma which facilitate design of robust wavelet sliding mode control are proved . also , two simulation examples are presented to illustrate the performance and the advantages of the proposed method .
localising missing plants in squared grid patterns of discontinuous crops from remotely sensed imagery . <eos> the purpose of this work is to localise and characterise missing plants on very high resolution ( vhr ) aerial images of agricultural parcels , in the case of discontinuous crops like wine and olive tree , which are planted according to a squared grid pattern . it aims to establish an assisted , image processing system for remote sensed images , allowing the inventory the missing or withering plants , and the monitoring of their evolution during time . the global approach considers the planted parcel as a topological graph of vertices , whose reciprocal location conforms to a set of geometrical rules about orientation and length . the proposed system initiates the graph from the original image then it adds missing vertices and refines its knowledge of the spatial pattern on an iterative basis . quality indicators are assigned at each added vertex , and several stopping criteria are estimated for each iteration , permitting an automated use of the algorithm . test cases have been conducted on two data sets of three parcels each olive groves and goblet vineyards . the results are compared to validation data . they show an efficient reconstruction of the geometry and satisfactory omission commission errors they allow drawing up a typology of the major errors , and propose calibration parameters based on a sensitivity analysis . the main improvements include essentially the preprocessing , filtering step of the initial image . the process is being used for languedocian vineyards ( france ) , and may be potentially usable for other problematic with the same kind of spatial patterns . ( c ) <digit> elsevier ltd. all rights reserved .
applied analysis for improving rail network operations . <eos> crew scheduling component is analyzed by a column generation approach . results show that efficiency of crew schedules can be improved by more than <digit> % . additional driver training will increase the improvement to <digit> % . a detailed reliability analysis is demonstrated . a main causes for delays and unreliability problems is heavy passenger load .
saccade related remapping of target representations between topographic maps a neural network study . <eos> the goal of this study was to explore how a neural network could solve the updating task associated with the double saccade paradigm , where two targets are flashed in succession and the subject must make saccades to the remembered locations of both targets . because of the eye rotation of the saccade to the first target , the remembered retinal position of the second target must be updated if an accurate saccade to that target is to be made . we trained a three layer , feed forward neural network to solve this updating task using back propagation . the networks inputs were the initial retinal position of the second target represented by a hill of activation in a 2d topographic array of units , as well as the initial eye orientation and the motor error of the saccade to the first target , each represented as 3d vectors in brainstem coordinates . the output of the network was the updated retinal position of the second target , also represented in a 2d topographic array of units . the network was trained to perform this updating using the full 3d geometry of eye rotations , and was able to produce the updated second target position to within a <digit> rms accuracy for a set of test points that included saccades of up to <digit> . emergent properties in the network 's hidden layer included sigmoidal receptive fields whose orientations formed distinct clusters , and predictive remapping similar to that seen in brain areas associated with saccade generation . networks with the larger numbers of hidden layer units developed two distinct types of units with different transformation properties units that preferentially performed the linear remapping of vector subtraction , and units that performed the nonlinear elements of remapping that arise from initial eye orientation .
ship speed optimization concepts , models and combined speed routing scenarios . <eos> clarification of some important issues as regards ship speed optimization at the operational level . development of models that optimize ship speed for a spectrum of routing scenarios . clarification of concepts and misconceptions . incorporation of fundamental parameters that weigh heavily in speed and routing decisions . identification of properties of optimal solution .
fast and robust extraction of surrogate respiratory signal from intra operative liver ultrasound images . <eos> in model based respiratory motion estimation for the liver or other abdominal organs , the surrogate respiratory signal is usually obtained by using special tracking devices from skin or diaphragm , and subsequently applied to parameterize a 4d motion model for prediction or compensation . however , due to the intrinsic limits and economical costs of these tracking devices , the identification of the respiratory signal directly from intra operative ultrasound images is a more attractive alternative .
heuristic constraints enforcement for training of and rule extraction from a fuzzy neural architecture part ii implementation and application . <eos> this paper is the second of two companion papers . the foundations of the proposed method of heuristic constraint enforcement on membership functions for knowledge extraction from a fuzzy neural architecture was given in part i. part ii develops methods for forming constraint sets using the constraints and techniques for finding acceptable solutions that conform to all available a priori information . moreover , methods of integration of enforcement methods into the training of the fuzzy neural architecture are discussed . the proposed technique is illustrated on a fuzzy and classification problem and a motor fault detection problem . the results indicate that heuristic constraint enforcement on membership functions leads to extraction of heuristically acceptable membership functions in the input and output spaces . although the method is described on a specific fuzzy neural architecture , it is applicable to any realization of a fuzzy inference system , including adaptive and or static fuzzy inference systems .
j2ee server scalability through ejb replication . <eos> with the development of internet based business , web applications are becoming increasingly complex . the j2ee specification aims at enabling the design of such web application servers . these servers have to ensure scalability and availability of the supported applications . scalibility can be achieved using replication techniques or partitionning techniques . the aim of this paper is to compare these approaches . in a j2ee web application server , one important component is the ejb tier . in this context , the jonas web application server provides an example of ejb replication system called cmi ( cluster method invocation ) . in a first step , this paper presents a performance evaluation of cmi . it then introduces incrementally an alternative scheme based on partitionning and shows the performance benefits compared to cmi .
development graphs proof management for structured specifications . <eos> development graphs are a toot for dealing with structured specifications in a formal program development in order to ease the management of change and reusing proofs . in this work , we extend development graphs with hiding ( e.g. hidden operations ) . hiding is a particularly difficult to realize operation , since it does not admit such a good decomposition of the involved specifications as other structuring operations do . we develop both a semantics and proof rules for development graphs with hiding . the rules are proven to be sound , and also complete relative to an oracle for conservative extensions . we also show that an absolutely complete set of rules can not exist . the whole framework is developed in a way independent of the underlying logical system ( and thus also does not prescribe the nature of the parts of a specification that may be hidden ) . we also show how various other logic independent specification formalisms can be mapped into development graphs thus , development graphs can serve as a kernel formalism for management of proofs and of change . ( c ) <digit> elsevier inc. all rights reserved .
lattice boltzmann simulation of water and gas flow in porous gas diffusion layers in fuel cells reconstructed from micro tomography . <eos> the porous gas diffusion layers ( gdls ) are key components in hydrogen fuel cells . during their operation the cells produce water at the cathode , and to avoid flooding , the water has to be removed out of the cells . how to manage the water is therefore an important issue in fuel cell design . in this paper we investigated water flow in the gdls using a combination of the lattice boltzmann method and x ray computed tomography at the micron scale . water flow in the gdl depends on waterair surface tension and hydrophobicity . to correctly represent the watergas surface tension , the formations of water droplets in air were simulated , and the watergas surface tension was obtained by fitting the simulated results to the younglaplace formula . the hydrophobicity is represented by the watergas fabric contact angle . for a given watergas surface tension the value of the contact angle was determined by simulating the formations of water droplets on a solid surface with different hydrophobicity . we then applied the model to simulate water intrusion into initially dry gdls driven by a pressure gradient in attempts to understand the impact of hydrophobicity on water distribution in the gdls . the structures of the gdl were acquired by x ray micro tomography at a resolution of 1.7 microns . the simulated results revealed that with an increase in hydrophobicity , water transport in gdls changes from piston flow to channelled flow .
