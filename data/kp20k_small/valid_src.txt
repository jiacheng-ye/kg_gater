real time data aggregation in contention based wireless sensor networks . <eos> we investigate the problem of delay constrained maximal information collection for csma based wireless sensor networks . we study how to allocate the maximal allowable transmission delay at each node , such that the amount of information collected at the sink is maximized and the total delay for the data aggregation is within the given bound . we formulate the problem by using dynamic programming and propose an optimal algorithm for the optimal assignment of transmission attempts . based on the analysis of the optimal solution , we propose a distributed greedy algorithm . it is shown to have a similar performance as the optimal one .
word sense disambiguation for event trigger word detection . <eos> this paper describes a method for detecting event trigger words in biomedical text based on a word sense disambiguation ( wsd ) approach . we first investigate the applicability of existing wsd techniques to trigger word disambiguation in the bionlp <digit> shared task data , and find that we are able to outperform a traditional crf based approach for certain word types . on the basis of this finding , we combine the wsd approach with the crf , and obtain significant improvements over the standalone crf , gaining particularly in recall .
composing architectural aspects based on style semantics . <eos> the lack of architecturally significant mechanisms for aspectual composition might artificially hinder the specification of stable and reusable design aspects . current aspect oriented approaches at the architecture level tend to mimic programming language join point models while overlooking mainstream architectural concepts such as styles and their semantics . syntax based pointcuts are typically used to select join points based on the names of architectural elements , exposing architecture descriptions to pointcut fragility and reusability problems . this paper presents style based composition , a new flavor of aspect composition at the architectural level based on architectural styles . we propose style based join point models and provide a pointcut language that supports the selection of join points based on style constrained architectural models . stability and reusability assessments of the proposed style based composition model were carried out through three case studies involving different styles . the interplay of style based pointcuts and some style composition techniques is also discussed .
using pen based computers across the computer science curriculum . <eos> this paper describes our use of pen based electronic classrooms to enhance several computer science courses . after presenting our motivation for undertaking this work , and its relevance to the growing interest in using tablet pc 's in the classroom , we present an overview of our use of this technology to engage students during class . finally , we present the students ' reaction to the approach as measured through attitude surveys and a focus group .
on the syntactic and functional correspondence between hybrid ( or layered ) normalisers and abstract machines . <eos> we show how to connect the syntactic and the functional correspondence for normalisers and abstract machines implementing hybrid ( or layered ) reduction strategies , that is , strategies that depend on subsidiary sub strategies . many fundamental strategies in the literature are hybrid , in particular , many full reducing strategies , and many full reducing and complete strategies that deliver a fully reduced result when it exists . if we follow the standard program transformation steps the abstract machines obtained for hybrids after the syntactic correspondence can not be refunctionalised , and the junction with the functional correspondence is severed . however , a solution is possible based on establishing the shape invariant of well formed continuation stacks . we illustrate the problem and the solution with the derivation of substitution based normalisers for normal order , a hybrid , full reducing , and complete strategy of the pure lambda calculus . the machine we obtain is a substitution based , eval apply , open terms version of pierre crgut 's full reducing krivine machine kn .
a case based method for service oriented value chain and sustainable network design . <eos> the purpose of this research is to present a case based analytic method for a service oriented value chain and a sustainable network design considering customer , environmental and social values . enterprises can enhance competitive advantage by providing more values to all stakeholders in the network . our model employs a stylized database to identify successful cases of value chain application under similar company marketing conditions , illustrating potential value chains and sustainable networks as references . this work first identifies economic benefits , environmental friendliness and social contribution values based on prior studies . next , a search engine which is developed based on the rough set theory will search and map similarities to find similar or parallel cases in the database . finally , a visualized network mapping will be automatically generated to possible value chains . this study applies a case based methodology to assist enterprises in developing a service oriented value chain design . for decision makers , this can reduce survey time and inspire innovative works based on previous successful experience . besides , successful ideas from prior cases can be reused . in addition to customer values , this methodology incorporates environment and social values that may encourage a company to build their value chain in a more comprehensive and sustainable manner . this is a pilot study which attempts to utilize computer aided methodology to assist in service or value related design . the pertinent existing solutions can be filtered from an array of cases to engage the advantages from both product oriented and service oriented companies . finally , the visualized display of value network is formed to illustrate the results . a customized service oriented value chains which incorporates environment and social values can be designed according to different conditions . also , this system engages the advantages from both product oriented and service oriented companies to build a more comprehensive value network . apart from this , the system can be utilized as a benchmarking tool , and it could remind the decision makers to consider potential value from a more multifaceted perspective . this is the first paper that applied a computer aided method to design service oriented value chains . this work also can serve as a decision support and benchmarking system because decision makers can develop different value networks according to various emphasized values . finally , the visualized display of value network can improve the communication among stakeholders .
volume subdivision based hexahedral finite element meshing of domains with interior <digit> manifold boundaries . <eos> we present a subdivision based algorithm for multi resolution hexahedral meshing . the input is a bounding rectilinear domain with a set of embedded <digit> manifold boundaries of arbitrary genus and topology . the algorithm first constructs a simplified voronoi structure to partition the object into individual components that can be then meshed separately . we create a coarse hexahedral mesh for each voronoi cell giving us an initial hexahedral scaffold . recursive hexahedral subdivision of this hexahedral scaffold yields adaptive meshes . splitting and smoothing the boundary cells makes the mesh conform to the input <digit> manifolds . our choice of smoothing rules makes the resulting boundary surface of the hexahedral mesh as c <digit> continuous in the limit ( c <digit> at extra ordinary points ) , while also keeping a definite bound on the condition number of the jacobian of the hexahedral mesh elements . by modifying the crease smoothing rules , we can also guarantee that the sharp features in the data are captured . subdivision guarantees that we achieve a very good approximation for a given tolerance , with optimal mesh elements for each level of detail ( lod ) .
methods for evaluating and creating data quality . <eos> this paper provides a survey of two classes of methods that can be used in determining and improving the quality of individual files or groups of files . the first are edit imputation methods for maintaining business rules and for imputing for missing data . the second are methods of data cleaning for finding duplicates within files or across files .
nonlinear magnetostatic bem formulation using one unknown double layer charge . <eos> purpose the purpose of this paper is to solve generic magnetostatic problems by bem , by studying how to use a boundary integral equation ( bie ) with the double layer charge as unknown derived from the scalar potential . design methodology approach since the double layer charge produces only the potential gap without disturbing the normal magnetic flux density , the field is accurately formulated even by one bie with one unknown . once the double layer charge is determined , biot savart 's law gives easily the magnetic flux density . findings the bie using double layer charge is capable of treating robustly geometrical singularities at edges and corners . it is also capable of solving the problems with extremely high magnetic permeability . originality value the proposed bie contains only the double layer charge while the conventional equations derived from the scalar potential contain the single and double layer charges as unknowns . in the multiply connected problems , the excitation potential in the material is derived from the magnetomotive force to represent the circulating fields due to multiply connected exciting currents .
mathematical modeling of electrical activity of uterine muscle cells . <eos> the uterine electrical activity is an efficient parameter to study the uterine contractility . in order to understand the ionic mechanisms responsible for its generation , we aimed at building a mathematical model of the uterine cell electrical activity based upon the physiological mechanisms . first , based on the voltage clamp experiments found in the literature , we focus on the principal ionic channels and their cognate currents involved in the generation of this electrical activity . second , we provide the methodology of formulations of uterine ionic currents derived from a wide range of electrophysiological data . the model is validated step by step by comparing simulated voltage clamp results with the experimental ones . the model reproduces successfully the generation of single spikes or trains of action potentials that fit with the experimental data . it allows analyzing ionic channels implications . likewise , the calcium dependent conductance influences significantly the cellular oscillatory behavior .
sweep synchronization as a global propagation mechanism . <eos> this paper presents a new generic filtering algorithm which simultaneously considers n n conjunctions of constraints as well as those constraints mentioning some variables yk y k of the pairs x , y k ( <digit> k n ) occurring in these conjunctions . the main benefit of this new technique comes from the fact that , for adjusting the bounds of a variable x according to n conjunctions , we do not perform n sweeps in an independent way but rather synchronize them . we then specialize this technique to the non overlapping rectangles constraint where we consider the case where several rectangles of height one have the same x coordinate for their origin as well as the same length . for this specific constraint we come up with an incremental bipartite matching algorithm which is triggered while we sweep over the time axis . we illustrate the usefulness of this new pruning method on a timetabling problem , where each task can not be interrupted and requires the simultaneous availability of n distinct persons . in addition each person has his own periods of unavailability and can only perform one task at a time .
a structural approach to reversible computation . <eos> reversibility is a key issue in the interface between computation and physics , and of growing importance as miniaturization progresses towards its physical limits . most foundational work on reversible computing to date has focussed on simulations of low level machine models . by contrast , we develop a more structural approach . we show how high level functional programs can be mapped compositionally ( i.e. in a syntax directed fashion ) into a simple kind of automata which are immediately seen to be reversible . the size of the automaton is linear in the size of the functional term . in mathematical terms , we are building a concrete model of functional computation . this construction stems directly from ideas arising in geometry of interaction and linear logic but can be understood without any knowledge of these topics . in fact , it serves as an excellent introduction to them . at the same time , an interesting logical delineation between reversible and irreversible forms of computation emerges from our analysis . ( c ) <digit> elsevier b.v. all rights reserved .
ubl the dna of next generation e business . <eos> this paper introduces into the evolution of electronic data interchange ( edi ) and the universal business language ( ubl ) . an oasis standard to encode and customize business documents . it shows its peculiarities and also sets it into a broader picture showing where ubl is positioned in relationship to business processes and standards like bpel and bpmn .
k geni testbed deployment and federated meta operations experiment over geni and kreonet . <eos> the classical internet has confronted many drawbacks in terms of network security , scalability , and performance , although it has strongly influenced the development and evolution of diverse network technologies , applications , and services . therefore , new innovative research on the future internet has been performed to resolve the inherent weaknesses of the traditional internet , which , in turn , requires new at scale network testbeds and research infrastructure for large scale experiments . in this context , k geni has been developed as an international programmable future internet testbed in the geni spiral <digit> program , and it has been operational between the usa ( geni ) and korea ( kreonet ) since <digit> . the k geni testbed and the related collaborative efforts will be introduced with two major topics in this paper ( <digit> ) the design and deployment of the k geni testbed and ( <digit> ) the federated meta operations between the k geni and geni testbeds . regarding the second topic in particular , we will describe how meta operations are federated across k geni between gmoc ( geni meta operations center ) and dvnoc ( distributed virtual network operations center on kreonet k geni ) , which is the first trial of an international experiment on the federated network operations over geni .
conceptions and modeling for transmitted information evaluation by ann . <eos> in this paper , the main measure , an amount of information , of the information theory is analyzed and corrected . the three conceptions of the theory on the microstate , dissipation pathways , and self organization levels with a tight connection to the statistical physics are discussed . the concepts of restricted information were introduced as well as the proof of uniqueness of the entropy function , when the probabilities are rational numbers , is presented . the artificial neural network ( ann ) model for mapping the evaluation of transmitted information has been designed and experimentally approbated in the biological area .
fmesp framework for the modeling and evaluation of software processes . <eos> nowadays , organizations face with a very high competitiveness and for this reason they have to continuously improve their processes . two key aspects to be considered in the software processes management in order to promote their improvement are their effective modeling and evaluation . the integrated management of these key aspects is not a trivial task , the huge number and diversity of elements to take into account makes it complex the management of software processes . to ease and effectively support this management , in this paper we propose fmesp a framework for the integrated management of the modeling and measurement of software processes . fmesp incorporates the conceptual and technological elements necessary to ease the integrated management of the definition and evaluation of software processes . from the measurement perspective of the framework and in order to provide the support for the software process measurement at model level a set of representative measures have been defined and validated .
anticipated function synchronization with unknown parameters of discrete time chaotic systems . <eos> in this paper , firstly , the control problem for the chaos synchronization of discrete time chaotic ( hyperchaotic ) systems with unknown parameters are considered . next , back stepping control law is derived to make the error signals between drive 2d discrete time chaotic system and response 2d discrete time chaotic system with two uncertain parameters asymptotically synchronized . finally , the approach is extended to the synchronization problem for 3d discrete time chaotic system with two unknown parameters . numerical simulations are presented to show the effectiveness of the proposed chaos synchronization scheme .
methods of assessing spinal radiographs in scoliosis are functions of its geometry . <eos> the most important feature of scoliosis is the lateral curvature of the spine . it can be treated either conservatively or by surgery however , treatment choice depends mainly on curve progression which is determined by frequent curve assessment . this is a review of methods of curve measurement and proof of the relationship between them .
feedback control of natural convection . <eos> an applicable method is developed for the identification and feedback control of natural convection . the boussinesq equation is reduced to a small set of ordinary differential equations by means of the karhunenlove galerkin procedure int . j. heat mass transfer <digit> ( <digit> ) <digit> . based on this low dimensional dynamic model , a feedback control synthesis is constructed by first performing an extended kalman filter estimate of the velocity and temperature fields to treat the measurement errors and then developing the optimal feedback law by means of the linear quadratic regulator theory . the present method allows for the practical implementation of modern control concepts to many flow systems including natural convection .
demand assigned capacity management ( dacm ) in ip over optical ( ipo ) networks . <eos> the demand assigned capacity management ( dacm ) problem in ip over optical ( ipo ) network aims at devising efficient bandwidth replenishment schedules from the optical domain conditioned upon traffic evolution processes in the ip domain . a replenishment schedule specifies the location , sizing , and sequencing of link capacity expansions to support the growth of internet traffic demand in the ip network subject to economic considerations . a major distinction in the approach presented in this paper is the focus of attention on the economics of excess bandwidth in the ip domain , which can be viewed as an inventory system that is endowed with fixed and variable costs and depletes with increase in ip traffic demand requiring replenishment from the optical domain . we , develop mathematical models to address the dacm problem in ipo networks based on a class of inventory management replenishment methods . we apply the technique to ipo networks that implement capacity adaptive routing in the ip domain and networks without capacity adaptive routing . we analyze the performance characteristics under both scenarios , in terms of minimizing cumulative replenishment cost over an interval of time . for the non capacity adaptive routing scenario , we consider a shortest path approach in the ip domain , specifically ospf . for the capacity adaptive scenario , we use an online constraint based routing scheme . this study . represents an application of integrated traffic engineering which concerns collaborative decision making targeted towards network performance improvement that takes into consideration traffic demands , control capabilities , and network assets at different levels in the network hierarchy .
parallel algorithm for finding modules of large scale coherent fault trees . <eos> we propose a new parallel algorithm to find all modules of a large fault tree . an experiment is used to compare the linear time algorithm and parallel algorithm . the result shows that our method is efficient in handling large scale fault trees .
finding relevant clustering directions in high dimensional data using particle swarm optimization . <eos> a method based on particle swarm optimization ( pso ) is proposed and described for finding subspaces that carry meaningful information about the presence of groups in high dimensional data sets . the advantage of using pso is that not only the variables that are responsible for the main data structure are identified but also other subspaces corresponding to local optima . the characteristics of the method are shown on two simulated data sets and on a real matrix coming from the analysis of genomic microarrays . in all cases , pso allowed to explore different subspaces and to discover meaningful structures in the analyzed data . copyright ( c ) <digit> john wiley sons , ltd .
neural network learning of optimal kalman prediction and control . <eos> although there are many neural network ( nn ) algorithms for prediction and for control , and although methods for optimal estimation ( including filtering and prediction ) and for optimal control in linear systems were provided by kalman in <digit> ( with nonlinear extensions since then ) , there has been , to my knowledge , no nn algorithm that learns either kalman prediction or kalman control ( apart from the special case of stationary control ) . here we show how optimal kalman prediction and control ( kpc ) , as well as system identification , can be learned and executed by a recurrent neural network composed of linear response nodes , using as input only a stream of noisy measurement data . the requirements of kpc appear to impose significant constraints on the allowed nn circuitry and signal flows . the nn architecture implied by these constraints bears certain resemblances to the local circuit architecture of mammalian cerebral cortex . we discuss these resemblances , as well as caveats that limit our current ability to draw inferences for biological function . it has been suggested that the local cortical circuit ( lcc ) architecture may perform core functions ( as yet unknown ) that underlie sensory , motor , and other cortical processing . it is reasonable to conjecture that such functions may include prediction , the estimation or inference of missing or noisy sensory data , and the goal driven generation of control signals . the resemblances found between the kpc nn architecture and that of the lcc are consistent with this conjecture .
learning point to point movements on an elastic limb using dynamic movement primitives . <eos> expansion of the dmp approach for gravitation compensation in elastic robots . grid based mixture approach based on bilinear interpolation of learned trajectories . model free gravitation compensation in directed limb movements .
audio dual watermarking scheme for copyright protection and content authentication . <eos> we propose a new multipurpose audio watermarking scheme in which two watermarks are used . for intellectual property protection , audio clip is divided into frames and robust watermark is embedded . at the same time , the feature of each frame is extracted , and it is quantized as semi fragile watermark . then , the frame is cut into sections and the semi fragile watermark bits are embedded into these sections . for content authentication , the semi fragile watermark extracted from each frame is compared with the watermark generated from the same frame to judge whether the watermarked audio is tampered , and locate the tampered position . experimental results show that our scheme is inaudibility . the two watermark schemes are all robust to common signal processing operations such as additive noise , resampling , re quantization and low pass filtering , and the semi fragile watermark scheme can achieve tampered detection and location .
a comparison of path planning strategies for autonomous exploration and mapping of unknown environments . <eos> to date , a large number of algorithms to solve the problem of autonomous exploration and mapping has been presented . however , few efforts have been made to compare these techniques . in this paper , an extensive study of the most important methods for autonomous exploration and mapping of unknown environments is presented . furthermore , a representative subset of these techniques has been chosen to be analysed . this subset contains methods that differ in the level of multi robot coordination and in the grade of integration with the simultaneous localization and mapping ( slam ) algorithm . these exploration techniques were tested in simulation and compared using different criteria as exploration time or map quality . the results of this analysis are shown in this paper . the weaknesses and strengths of each strategy have been stated and the most appropriate algorithm for each application has been determined .
improving reliable multicast using active parity encoding services . <eos> we propose and evaluate novel reliable multicast protocols that combine active repair service ( a.k.a. local recovery ) and parity encoding ( a.k.a. forward error correction or fec ) techniques . we show that , compared to other repair service protocols , our protocols require less buffer inside the network , maintain the low bandwidth requirements of previously proposed repair service fec combination protocols , and reduce the amount of fec processing at repair servers , moving more of this processing to the end hosts . we also examine repair service fec combination protocols in an environment where loss rates differ across domains within the network . we find that repair services are more effective than fec at reducing bandwidth utilization in such environments . furthermore , we show that adding fec to a repair services protocol not only reduces buffer requirements at repair servers , but also reduces bandwidth utilization in domains with high loss , or in domains with large populations of receivers .
a comparative study on concept drift detectors . <eos> we evaluated eight different concept drift detectors . a 2k factorial design was used to indicate the best parameters for each method . tests compared accuracy , evaluation time , false alarm and miss detection rates . a mahalanobis distance is proposed as a metric to compare drift methods . ddm was the method that presented the best average results in all tested datasets .
automatic photo pop up . <eos> this paper presents a fully automatic method for creating a 3d model from a single photograph . the model is made up of several texture mapped planar billboards and has the complexity of a typical children 's pop up book illustration . our main insight is that instead of attempting to recover precise geometry , we statistically model geometric classes defined by their orientations in the scene . our algorithm labels regions of the input image into coarse categories ground , sky , and vertical . these labels are then used to cut and fold the image into a pop up model using a set of simple assumptions . because of the inherent ambiguity of the problem and the statistical nature of the approach , the algorithm is not expected to work on every image . however . it performs surprisingly well for a wide range of scenes taken from a typical person 's photo album .
h and h2 stabilisers via static output feedback based on coordinate transformations with free variables . <eos> this article designs h and h2 stabilisers , respectively , for linear time invariant systems via static output feedback ( sof ) . a state coordinate transformation of controlled system generates a dummy system with lower dimension , which can not be directly influenced by the sof stabiliser . then the h ( h2 ) stabiliser via sof may be obtained by solving proper linear matrix inequality ( lmi ) . this lmi is feasible only if the dummy system has a state feedback stabiliser with the same h ( h2 ) index . meanwhile , a free matrix variable in coordinate transformation can act as the state feedback gain matrix . hence after the design of dummy system , the sof stabiliser can be determined if certain lmi is feasible . this method does not concern any conservative reduction or enlargement of matrix inequalities . numerical examples show the validity of the proposed algorithms .
observation of linear systems with unknown inputs via high order sliding modes . <eos> a high order sliding mode observer is designed for linear time invariant systems with single output and unknown bounded single input . it provides for the global observation of the state and the output under sufficient and necessary conditions of strong observability or strong detectability . the observation is finite time convergent and exact in the strong observability case . the accuracy of the proposed observation and identification schemes is estimated via the sampling step or magnitude of deterministic noises . the results are extended to the multi input multi output case .
a mathematical model of penile vascular dysfunction and its application to a new diagnostic technique . <eos> a noninvasive diagnostic device was developed to assess the vascular origin and severity of penile dysfunction . it was designed and studied using both a mathematical model of penile hemodynamics and preliminary experiments on healthy young volunteers . the device is based on the application of an external pressure ( or vacuum ) perturbation to the penis following the induction of erection . the rate of volume change while the penis returns to its natural condition is measured using a noninvasive system that includes a volume measurement mechanism that has very low friction , thereby not affecting the measured system . the rate of volume change ( net flow ) is obtained and analyzed . simulations using a mathematical model show that the device is capable of differentiating between arterial insufficiency and venous leak and indicate the severity of each . in preliminary measurements on young healthy volunteers , the feasibility of the measurement has been demonstrated . more studies are required to confirm the diagnostic value of the measurements
the spectra of irreducible matrices over completed idempotent semifields . <eos> motivated by some spectral results in the characterization of concept lattices we investigate the spectra of reducible matrices over complete idempotent semifields in the framework of naturally ordered semirings , or dioids . we find non null eigenvectors for every non null element in the semifield and conclude that the notion of spectrum has to be refined to encompass that of the incomplete semifield case so as to include only those eigenvalues with eigenvectors that have finite coordinates . considering special sets of eigenvectors brings out finite complete lattices in the picture and we contend that such structure may be more important than standard eigenspaces for matrices over completed idempotent semifields .
interactive technologies for health special interest group . <eos> health and how to support it with interactive computer systems , networks , and devices is a global and , for many countries , an explicit national priority . significant interest in issues related to interactive systems for health has been demonstrated repeatedly within sigchi . a community focused on health started in <digit> , fostering collaboration and dissemination of research findings as well as bridging with practitioners . as part of this community 's on going efforts , we will hold a special interest group session during acm chi <digit> to discuss , prioritize , and promote some of these most pressing issues facing the community .
cyclic sequence alignments approximate versus optimal techniques . <eos> the problem of cyclic sequence alignment is considered . most existing optimal methods for comparing cyclic sequences are very time consuming . for applications where these alignments are intensively used , optimal methods axe seldom a feasible choice . the alternative to an exact and costly solution is to use a close to optimal but cheaper approach . in previous works , we have presented three suboptimal techniques inspired on the quadratic time suboptimal algorithm proposed by bunke and buhler . do these approximate approaches come sufficiently close to the optimal solution , with a considerable reduction in computing time is it thus worthwhile investigating these approximate methods this paper shows that approximate techniques are good alternatives to optimal methods .
direct auau bonding technology for high performance gaas algaas quantum cascade lasers . <eos> in this paper we investigate chip bonding technology of gaas algaas quantum cascade lasers ( qcls ) . its results have strong influence on final performance of devices and are essential for achieving room temperature operation . various solders were investigated and compared in terms of their thermal resistance and induced stress . the spatially resolved photoluminescence technique has been applied for a device thermal analysis . the soldering quality was also investigated by means of a scanning acoustic microscopy . the particular attention has been paid to auau die bonding , which seems to be a promising alternative to the choice between hard and soft solder bonding of gaas algaas qcls operating from cryogenic temperatures up to room temperatures . a good quality direct auau bonding was achieved for bonding parameters comparable with the ones typical for ausn eutectic bonding process . high performance room temperature operation of gaas algaas qcls has been achieved with the state of the art parameters .
harnessing cloud technologies for a virtualized distributed computing infrastructure . <eos> the intergrid system aims to provide an execution environment for running applications on top of interconnected infrastructures . the system uses virtual machines as building blocks to construct execution environments that span multiple computing sites . such environments can be extended to operate on cloud infrastructures , such as amazon ec2 . this article provides an abstract view of the proposed architecture and its implementation experiments show the scalability of an intergrid managed infrastructure and how the system can benefit from using the cloud .
a method of mpeg2 ts test stream generation for digital tv software . <eos> input of the digital tv software is a transport stream ( ts ) in mpeg ( moving picture expert group ) <digit> format , a standard specification for moving picture compression . we propose a method to thoroughly generate mpeg <digit> ts test data , namely , a test stream based on the black box test concept for digital tv software . we also introduce a tool to automate the test stream generation known as auto test data generator from protocol standard ( atep ) . this empirical study of the application of an atep derived test stream to an actual digital tv software settop box should benefit digital tv software developers as well as other testers .
unified representation of zipf distributions . <eos> certain discrete probability distributions , used independently from each other in linguistics and other sciences , can be considered as special cases of the distribution based on the lerch zeta function . we will list the probability functions for some of the most important cases . moments and estimators are derived for the general lerch distribution .
physical quantum algorithms . <eos> i review the differences between classical and quantum systems , emphasizing the connection between no hidden variable theorems and superior computational power of quantum computers . using quantum lattice gas automata as examples , i describe possibilities for efficient simulation of quantum and classical systems with a quantum computer . i conclude with a list of research directions . ( c ) <digit> published by elsevier science b.v.
a slack diversifying nonlinear fluctuation smoothing rule for job dispatching in a wafer fabrication factory . <eos> this study proposes a slack diversifying nonlinear fluctuation smoothing rule to reduce the average cycle time in a wafer fabrication factory . the slack diversifying nonlinear fluctuation smoothing rule is derived from the one factor tailored nonlinear fluctuation smoothing rule for cycle time variation ( 1f tnfsvct ) by dynamically maximizing the standard deviation of the slack , which has been shown to improve scheduling performance in several previous studies . the effectiveness of the proposed rule has been validated via using it with a simulated data set . based on the findings in this research we also derived several directions that can be exploited in the future .
linear and compact floating node voltage controlled variable resistor circuit . <eos> in this letter , my proposals for a floating node voltagecontrolled variable resistor circuit ( fvr ) are based upon its advantages as linear and compact . the performance of the proposed circuit was confirmed by pspice simulation . the simulation results are reported in this letter .
the management information systems ( mis ) job market late 1970s late 1990s . <eos> the rapidly changing information technology ( it ) environment continues to pose a challenging dilemma for both management information systems ( mis ) managers and mis educators at all levels , especially the collegiate level . this research examines the content of mis related job advertisements over a <digit> year period late 1970s late 1990s . it is the continuation of a study initially published in the journal of computer information systems ( <digit> ) and includes the data that represents the late 1990s timeframe . results trace the rise and fall in demand for certain it skills and knowledge and identify the growing strength or stability of others . the study clearly exposes the great diversity in the mis job market . this diversity is the root cause of the dilemma confronting mis managers and mis educators as they try to recruit workers from or prepare students for the changing it environment .
development of a ceramic bolus for the permanent electronic identification of sheep , goat and cattle . <eos> retention rate and digestive and performance effects of ceramic boluses ( <digit> mm , <digit> g ) enclosing passive transponders ( 32.53.8 mm ) were studied in three experiments . reading distances of transponders inside and outside the boluses ( n <digit> ) did not vary . in the first experiment , a total of <digit> boluses were applied to <digit> lambs and <digit> ewes , <digit> young and <digit> adult goats , <digit> calves and <digit> cows . plastic balling guns were used to insert the boluses and their effects were evaluated during <digit> years or until slaughter . time needed for application and recommended live weights ( lw ) depended on animal category ( sheep , <digit> s and > <digit> kg goats , <digit> s and > <digit> kg cattle , <digit> s and > <digit> kg ) . application in calves was possible during the first week of life . retention rates were <digit> , 98.8 and 99.7 % in sheep , goats and cattle , respectively . the location of boluses in the reticulum was checked with hand held readers and verified by x ray in a sample ( n <digit> ) of each animal category or directly in cannulated cows ( n <digit> ) . transceivers were interfaced with electronic scales for automatic weight recording . dynamic reading efficiency was <digit> % in race ways with a frame antenna ( <digit> cm ) . health and performances were not modified by boluses . an average of <digit> % of boluses were found in the reticulum at slaughter . recovery rates and times varied according to animal category ( lambs , <digit> % and <digit> s ewes and goats , <digit> % and <digit> s fattened calves , 91.3 % and <digit> s dairy cows , <digit> % and <digit> s ) . in the second experiment , two groups of adult ewes ( control , n <digit> bolus , n <digit> ) were housed in individual pens and fed forage ad libitum . mean forage intake and nutrient digestibility were not varied by the ceramic boluses . in the third experiment <digit> fattening male lambs ( <digit> kg lw ) and <digit> replacement ewe lambs ( <digit> kg lw ) were used . fattening lambs were divided into two groups and assigned to the treatments ( control , n <digit> bolus , n <digit> ) until slaughter ( <digit> kg lw ) . in spite of the difficulties observed in the force feeding of boluses in eight lambs ( <digit> % ) , average daily gain and reticulum rumen mucosa were not altered . ewe lambs were also assigned to the treatments in two groups ( control , n <digit> bolus , n <digit> ) and monitored until first lambing or <digit> year old . the weight , body condition score and reproductive performance were not affected by boluses . in conclusion , the use of the ceramic bolus is recommended as a safe and tamper proof method for electronic identification of ruminants once the animals have reached a weight where successful administration is possible . moreover , boluses proved to be useful for dynamic reading and automatic weight recording on farm conditions .
owl eu adding customised datatypes into owl . <eos> although owl is rather expressive , it has a very serious limitation on datatypes i.e. , it does not support customised datatypes . it has been pointed out that many potential users will not adopt owl unless this limitation is overcome , and the w3c semantic web best practices and development working group has set up a task force to address this issue . this paper makes the following two contributions ( i ) it provides a brief summary of owl related datatype formalisms , and ( ii ) it provides a decidable extension of owl dl , called owl eu , that supports customised datatypes . a detailed proof of the decidability of owl eu is presented .
turning a community into a market a practice perspective on information technology use in boundary spanning . <eos> this paper examines how information technology ( it ) transforrns relations across fields of practice within organizations . drawing on bourdieu 's practice theory , we argue that the production of any practice involves varying degrees of embodiment ( i.e. , relying on personal relationships ) and objectification ( i.e. , relying on the exchange of objects ) . we subsequently characterize boundary spanning practices according to their relative degrees of embodiment and objectification . we distinguish between market like boundary spanning practices , which rely primarily on an objectified mode of practice production , from community like practices , which involve mostly the embodied mode of practice production . it is then conceptualized as a medium for sharing objects in the production of practices . as such , it use allows for the sharing of objects without relying on embodied relationships . we use data from an in depth ethnographic case study to investigate how it was used to transform community like boundary spanning practices within an organization into market like ones . moreover , we demonstrate how , as it was used to support the exchange and combination of depersonalized objects , other aspects of the practice ( such as the roles of intermediaries and the nature of meetings ) also changed . the related changes in these diverse aspects of a boundary spanning practice supported the trend toward greater objectification . it use also increased visibility of the terms associated with object exchange . this increased visibility exposed the inequity of the exchange and encouraged the disadvantaged party to renegotiate the relationship .
integration of different risk assessment tools to improve stratification of patients with coronary artery disease . <eos> cardiovascular disease ( cvd ) causes unaffordable social and health costs that tend to increase as the european population ages . in this context , clinical guidelines recommend the use of risk scores to predict the risk of a cardiovascular disease event . some useful tools have been developed to predict the risk of occurrence of a cardiovascular disease event ( e.g. hospitalization or death ) . however , these tools present some drawbacks . these problems are addressed through two methodologies ( i ) combination of risk assessment tools fusion of nave bayes classifiers complemented with a genetic optimization algorithm and ( ii ) personalization of risk assessment subtractive clustering applied to a reduced dimensional space to create groups of patients . validation was performed based on two acs nstemi patient data sets . this work improved the performance in relation to current risk assessment tools , achieving maximum values of sensitivity , specificity , and geometric mean of , respectively , 79.8 , 83.8 , and 80.9 % . additionally , it assured clinical interpretability , ability to incorporate of new risk factors , higher capability to deal with missing risk factors and avoiding the selection of a standard cvd risk assessment tool to be applied in the clinical practice .
segmenting ideal morphologies of sewer pipe defects on cctv images for automated diagnosis . <eos> several literatures presented automated systems for detecting or classifying sewer pipe defects based on morphological features of pipe defects . in those automated systems , however , the morphologies of the darker center or some uncertain objects on cctv images are also segmented and become noises while morphology based pipe defect segmentation is implemented . in this paper , the morphology based pipe defect segmentation is proposed and discussed to be an improved approach for automated diagnosis of pipe defects on cctv images . the segmentation of pipe defect morphologies is first to implement an opening operation for gray level cctv images to distinguish pipe defects . then , otsu 's technique is used to segment pipe defects by determining the optimal thresholds for gray level cctv images of opening operation . based on the segmentation results of cctv images , the ideal morphologies of four typical pipe defects are defined . if the segmented cctv images match the definition of those ideal morphologies , the pipe defects on those cctv images can be successfully identified by a radial basis network ( rbn ) based diagnostic system . as for the rest cctv images failing to match the ideal morphologies , the failure causes was discussed so to suggest a regulation for imaging conditions , such as camera pose and light source , in order to obtain cctv images for successful segmentation . ( c ) <digit> elsevier ltd. all rights reserved .
a new fuzzy rule based classification system for word sense disambiguation . <eos> word sense disambiguation ( wsd ) can be thought of as the most challenging task in the process of machine translation . various supervised and unsupervised learning methods have already been proposed for this purpose . in this paper , we propose a new efficient fuzzy classification system in order to be applied for wsd . in order to optimize the generalization accuracy , we use rule weight as a simple mechanism to tune the classifier and propose a new learning method to iteratively adjust the weight of fuzzy rules . through computer simulations on twa data as a standard corpus , the proposed scheme shows a uniformly good behavior and achieves results which are comparable or better than other classification systems , proposed in the past .
on the hierarchy of conservation laws in a cellular automaton . <eos> conservation laws in cellular automata ( ca ) are studied as an abstraction of the conservation laws observed in nature . in addition to the usual real valued conservation laws we also consider more general group valued and semigroup valued conservation laws . the ( algebraic ) conservation laws in a ca form a hierarchy , based on the range of the interactions they take into account . the conservation laws with smaller interaction ranges are the homomorphic images of those with larger interaction ranges , and for each specific range there is a most general law that incorporates all those with that range . for one dimensional ca , such a most general conservation law haseven in the semigroup valued casean effectively constructible finite presentation , while for higher dimensional ca such effective construction exists only in the group valued case . it is even undecidable whether a given two dimensional ca conserves a given semigroup valued energy assignment . although the local properties of this hierarchy are tractable in the one dimensional case , its global properties turn out to be undecidable . in particular , we prove that it is undecidable whether this hierarchy is trivial or unbounded . we point out some interconnections between the structure of this hierarchy and the dynamical properties of the ca . in particular , we show that positively expansive ca do not have non trivial real valued conservation laws .
positive periodic solutions for the neutral ratio dependent predatorprey model . <eos> by using a continuation theorem based on coincidence degree theory , we obtain some new sufficient conditions for the existence of positive periodic solutions for the neutral ratio dependent predatorprey model with holling type ii functional response .
impact of dual k spacer on analog performance of underlap finfet . <eos> multigate structures have better short channel control than conventional bulk devices due to increased gate electrostatic control . finfet is a promising candidate among multigate structures due to its ease of manufacturability . the rf performance of finfet is affected by gate controlled parameters such as transconductance , output conductance and total gate capacitance . in this paper we have used dual k spacers in underlap finfets to improve the gate electrostatic integrity . the inner high k spacer helps in better screening out the gate sidewall fringing fields , thereby , increasing transconductance and reducing output conductance with increase in total gate capacitance . at 16nm gate lengths , we have observed that , the intrinsic gain of dual k spacer based finfet can be increased by more than <digit> % ( > 6db ) without affecting cutoff frequency and maximum oscillation frequency , as compared to conventional single spacer based finfet . improvement in cutoff frequency by <digit> % and maximum oscillation frequency by <digit> % can be achieved , when the gate lengths are scaled down to 12nm , in addition to 2.75 times ( 8.8 db ) increase in intrinsic gain .
3d depth estimation for visual inspection using in wavelet transform modulus maxima . <eos> a vision based approach for calculating accurate 3d models of the objects is presented . generally industrial visual inspection systems capable of accurate 3d depth estimation rely on extra hardware tools like laser scanners or light pattern projectors . these tools improve the accuracy of depth estimation but also make the vision system costly and cumbersome . in the proposed algorithm , depth and dimensional accuracy of the produced 3d depth model depends on the existing reference model instead of the information from extra hardware tools . the proposed algorithm is a simple and cost effective software based approach to achieve accurate 3d depth estimation with minimal hardware involvement . the matching process uses the well known coarse to fine strategy , involving the calculation of matching points at the coarsest level with consequent refinement up to the finest level . vector coefficients of the wavelet transform modulus are used as matching features , where wavelet transform modulus maxima defines the shift invariant high level features with phase pointing to the normal of the feature surface . the technique addresses the estimation of optimal corresponding points and the corresponding 2d disparity maps leading to the creation of accurate depth perception model . ( c ) <digit> elsevier ltd. all rights reserved .
adaptive fec based packet loss resilience scheme for supporting voice communication over ad hoc wireless networks . <eos> providing real time voice support over multihop ad hoc wireless networks ( awns ) is a challenging task . the standard retransmission based strategies proposed in the literature are poorly matched to voice applications because of timeliness and large overheads involved in transmitting small sized voice packets . to make a voice application feasible over awns , the perceived voice quality must be improved while not significantly increasing the packet overhead . we suggest packet level media dependent adaptive forward error correction ( fec ) at the application layer in tandem with multipath transport for improving the voice quality . since adaptive fec masks packet losses in the network , at the medium access control ( mac ) layer , we avoid retransmissions ( hence , no acknowledgments ) in order to reduce the control overhead and end to end delay . further , we exploit the combined strengths of layered coding and multiple description ( md ) coding for supporting error resilient voice communication in awns . we propose an efficient packetization scheme in which the important substream of the voice stream is protected adaptively with fec depending on the loss rate present in the network and is transmitted over two maximally node disjoint paths . the less important substream of the voice stream is encoded into two descriptions , which are then transmitted over two maximally node disjoint paths . the performance of our scheme ( packet level media dependent adaptive fec scheme ) is evaluated in terms of two parameters residual packet loss rate ( rplr , packet loss rate after fec recovery ) and average burst length ( abl , average length of consecutive packet losses after fec recovery ) of voice data after fec recovery . the sets of equations leading to the analytical formulation of both rplr and abl are first given for a renewal error process . the values of both these parameters depend on fec offset ( r , the distance between original voice frame and piggybacked redundant voice frame ) and loss rate present in the network . then , these parameters are computed for a gilbert elliott ( ge ) two state markov error model and compared with experimental data . our scheme adaptively selects the fec offset ( it chooses r that minimizes rplr and abl as much as possible ) based on the loss rate feedback obtained from the destination . the proposed scheme achieves significant gains in terms of reduced frame loss rate ( flr ) , reduced control overhead , and minimum end to end delay and almost doubles the perceived voice quality compared to the existing approaches .
introduction of a data schema to support a design repository . <eos> this paper presents the data schema required to capture fundamental elements of design information in a heterogeneous repository supporting design reuse . design information captured by the repository can be divided into seven main categories of artifact , function , failure , physical , performance , sensory and media related information types . each of the seven types of design information is described in detail . the repository schema is specific to a relational database system driving the implemented design repository however , the types of design information recorded are applicable to any implementation of a design repository . the aim of this paper is to fully describe the data schema such that it could be recreated or specialized for industrial or research applications . the result is a complete description of fundamental design knowledge to support design reuse and a data schema specification . the data schema has been vetted with the implemented design repository that contains design information for over <digit> consumer electro mechanical products .
distributivity in lattices of fuzzy subgroups . <eos> the main goal of this paper is to study the finite groups whose lattices of fuzzy subgroups are distributive . we obtain a characterization of these groups which is similar to a well known result of group theory . ( c ) <digit> elsevier inc. all rights reserved .
on delegation and workflow execution models . <eos> workflow systems have long been of interest to computer science researchers due to their practical relevance . supporting delegation mechanisms in workflow systems is receiving increasing research interest . in this paper , we conduct a comprehensive study of user delegation operations in computerized workflow systems . in a workflow system , the semantics of a delegation operation are largely based on three factors the underlying workflow execution model , task type and delegation type . we describe three different workflow execution models and examine the effect of various delegation operations in each workflow execution model . we then extend our workflow execution models to examine the effect of various delegation operations in different role based workflow execution models .
on line anomaly detection and resilience in classifier ensembles . <eos> detection of anomalies is a broad field of study , which is applied in different areas such as data monitoring , navigation , and pattern recognition . in this paper we propose two measures to detect anomalous behaviors in an ensemble of classifiers by monitoring their decisions one based on mahalanobis distance and another based on information theory . these approaches are useful when an ensemble of classifiers is used and a decision is made by ordinary classifier fusion methods , while each classifier is devoted to monitor part of the environment . upon detection of anomalous classifiers we propose a strategy that attempts to minimize adverse effects of faulty classifiers by excluding them from the ensemble . we applied this method to an artificial dataset and sensor based human activity datasets , with different sensor configurations and two types of noise ( additive and rotational on inertial sensors ) . we compared our method with two other well known approaches , generalized likelihood ratio ( glr ) and one class support vector machine ( ocsvm ) , which detect anomalies at data feature level . we found that our method is comparable with glr and ocsvm . the advantages of our method compared to them is that it avoids monitoring raw data or features and only takes into account the decisions that are made by their classifiers , therefore it is independent of sensor modality and nature of anomaly . on the other hand , we found that ocsvm is very sensitive to the chosen parameters and furthermore in different types of anomalies it may react differently . in this paper we discuss the application domains which benefit from our method .
scheduling unrelated parallel machines to minimize total weighted tardiness . <eos> this article considers the problem of scheduling a given set of independent jobs on unrelated parallel machines to minimize the total weighted tardiness . the problem is known to be np hard in the strong sense . efficient lower and upper bounds are developed . the lower bound is based on the solution of an assignment problem , while the upper bound is obtained by a two phase heuristic . a branch and bound algorithm that incorporates various dominance rules is presented . computational experiments are conducted to demonstrate the performance of the proposed algorithm . scope and purpose parallel machine scheduling models are important from both the theoretical and practical points of view . from the theoretical point of view , they generalize the single machine scheduling models . from the practical point of view , they are important because the occurrence of a bank of machines in parallel is common in industries . in this article , the unrelated parallel machine total weighted tardiness scheduling problem is examined . the tardiness criterion has many applications in real world . this problem is difficult to solve . a branch and bound algorithm that incorporates various dominance rules along with efficient lower and upper bounds is proposed to find an optimal solution .
lock free deques and doubly linked lists . <eos> we present a practical lock free shared data structure that efficiently implements the operations of a concurrent deque as well as a general doubly linked list . the implementation supports parallelism for disjoint accesses and uses atomic primitives which are available in modern computer systems . previously known lock free algorithms of doubly linked lists are either based on non available atomic synchronization primitives , only implement a subset of the functionality , or are not designed for disjoint accesses . our algorithm only requires single word compare and swap atomic primitives , supports fully dynamic list sizes , and allows traversal also through deleted nodes and thus avoids unnecessary operation retries . we have performed an empirical study of our new algorithm on two different multiprocessor platforms . results of the experiments performed under high contention show that the performance of our implementation scales linearly with increasing number of processors . considering deque implementations and systems with low concurrency , the algorithm by michael shows the best performance . however , as our algorithm is designed for disjoint accesses , it performs significantly better on systems with high concurrency and non uniform memory architecture .
an optimal deployable bandwidth aggregation system . <eos> the explosive increase in data demand coupled with the rapid deployment of various wireless access technologies have led to the increase of number of multi homed or multi interface enabled devices . fully exploiting these interfaces has motivated researchers to propose numerous solutions that aggregate their available bandwidths to increase overall throughput and satisfy the end users growing data demand . these solutions , however , do not utilize their interfaces to the maximum without network support , and more importantly , have faced a steep deployment barrier . in this paper , we propose an optimal deployable bandwidth aggregation system ( dbas ) for multi interface enabled devices . we present the dbas architecture that does not introduce any intermediate hardware , modify current operating systems , modify socket implementations , nor require changes to current applications or legacy servers . the dbas architecture is designed to automatically estimate the characteristics of applications and dynamically schedule various connections and or packets to different interfaces . we also formulate our optimal scheduler as a mixed integer programming problem yielding an efficient solution . we evaluate dbas via implementation on the windows os and further verify our results with simulations on ns2 . our evaluation shows that , with current internet characteristics , dbas reaches the throughput upper bound with no modifications to legacy servers . it also highlights the significant enhancements in the response time introduced by dbas , which directly enhances the user experience .
dynamic bandwidth allocation in heterogeneous wdm epons . <eos> several dynamic bandwidth allocation algorithms have been introduced to schedule upstream wavelength channels in wavelength division multiplexing ethernet passive optical networks ( wdm epons ) , but mostly for homogenous wdm epon networks with the same distance between each optical network unit ( onu ) and the optical line terminal . for wdm epon with heterogeneous round trip times ( rtts ) , we propose two algorithms for onu scheduling , called nearest first and early allocation ( ea ) and a wavelength assignment algorithm , called best fit ( bf ) . both onu scheduling algorithms take rtt dissimilarities into account , thus minimizing packet delay and packet drop ratio at onus . additionally , ea can relive the common drawback of offline scheduling , i.e. , channel idle time . on the other hand , the bf wavelength assignment is proposed that assigns the best wavelength to each onu in order to improve network performances in terms of packet queuing delay and packet drop ratio at onu sides .
a layered framework to study collaboration as a form of knowledge sharing and diffusion . <eos> collaboration is presented as a form of knowledge sharing and hence of knowledge diffusion . a layered framework for collaboration studies is proposed . the notions of relative and absolute proper essential node ( pen ) centrality are introduced as indicators of a node 's importance for diffusion of knowledge through collaboration .
classes of tree languages determined by classes of monoids . <eos> in this paper finite state recognizers are considered as unary tree recognizers with unary operational symbols . we introduce translation recognizers of a tree recognizer , which are finite state recognizers whose operations are the elementary translations of the underlying algebra of the considered tree recognizer . in terms of translation recognizers we give general conditions under which a class of recognizable tree languages with a given property can be determined by a class of monoids determining the class of string languages having the same property .
on a transfer theorem for the p not equal np conjecture . <eos> a model of computation is defined over the algebraic numbers and over number fields . this model is non uniform , and the cost of operations depends on the height of the operands and on the degree of the extension of the rational defined by those operands . a transfer theorem for the p not equal np conjecture is proved , namely p not equal np in this model over the real algebraic numbers if and only if p not equal np in the classical setting . ( c ) <digit> academic press .
the design and implementation of a future internet live tv system over 4g networks . <eos> the emerging 4g ( fourth generation ) networks featuring wider coverage , higher transmission bandwidth and easier deployment have a desirable potential to serve ubiquitous and pervasive multimedia applications in creating new user centric communication services . however , the practical implementation of 4g network to demonstrate such potential , especially for delivering real time and high quality video services , is scarce . this paper therefore provides the design and implementation of a future internet live tv system over 4g networks to achieve cost effectiveness , instead of using expensive satellite news gathering ( sng ) vehicle and costly satellite transmissions in traditional tv stations . to effectively provide live tv services , we apply not only the hybrid duplex modes but also the port based vlan on the deployed networks for maximizing bandwidth , minimizing signal interference , and guaranteeing qos of differentiated services . performance metrics are applied to demonstrate that the proposed solution is cost effective and is feasible for live tv services in future internet .
numerical study of a stochastic particle method for homogeneous gas phase reactions . <eos> in this paper , we study a stochastic particle system that describes homogeneous gasphase reactions of a number of chemical species . first , we introduce the system as a markov jump process and discuss how relevant physical quantities are represented in terms of appropriate random variables . then , we show how various deterministic equations , used in the literature , are derived from the stochastic system in the limit when the number of particles goes to infinity . finally , we apply the corresponding stochastic algorithm to a toy problem , a simple formal reaction mechanism , and a real combustion problem . this problem is given by the isothermal combustion of a homogeneous mixture of heptane and air modelled by a detailed reaction mechanism with <digit> chemical species and <digit> reversible reactions . heptane as described in this chemical mechanism serves as model fuel for different types of internal combustion engines . in particular , we study the order of convergence with respect to the number of simulation particles , and illustrate the limitations of the method . ( c ) <digit> elsevier science ltd. all rights reserved .
a nitsche stabilized finite element method for frictional sliding on embedded interfaces . part ii intersecting interfaces . <eos> developed a weighted nitsche stabilized method for embedded interfaces with junctions . provided an explicit expression for the method parameter for lower order elements in the presence of junctions . examples highlight the method capabilities in modeling grain boundary sliding behavior .
a class of aggregation functions encompassing two dimensional owa operators . <eos> in this paper we prove that , under suitable conditions , atanassovs k k operators , which act on intervals , provide the same numerical results as owa operators of dimension two . on one hand , this allows us to recover owa operators from k k operators . on the other hand , by analyzing the properties of atanassovs operators , we can generalize them . in this way , we introduce a class of aggregation functions the generalized atanassov operators that , in particular , include two dimensional owa operators . we investigate under which conditions these generalized atanassov operators satisfy some properties usually required for aggregation functions , such as bisymmetry , strictness , monotonicity , etc. we also show that if we apply these aggregation functions to interval valued fuzzy sets , we obtain an ordered family of fuzzy sets .
data structures on event graphs . <eos> we investigate the behavior of data structures when the input and operations are generated by an event graph . this model is inspired by markov chains . we are given a fixed graph g , whose nodes are annotated with operations of the type insert , delete , and query . the algorithm responds to the requests as it encounters them during a ( random or adversarial ) walk in g. we study the limit behavior of such a walk and give an efficient algorithm for recognizing which structures can be generated . we also give a near optimal algorithm for successor searching if the event graph is a cycle and the walk is adversarial . for a random walk , the algorithm becomes optimal .
radar detection algorithm for garch clutter model . <eos> we propose a garch model to represent the clutter in radar applications . we fit this model to real sea clutter data and we show that it represents adequately the statistics of the data . then , we develop a detection test based on this model . using synthetic and real radar data , we evaluate its performance and we show that the proposed detector offers higher probability of detection for a specified value of probability of false alarm than tests based on gaussian and weibull models , especially for low signal to clutter ratios .
particle based non photorealistic volume visualization . <eos> non photorealistic techniques are usually applied to produce stylistic renderings . in visualization , these techniques are often able to simplify data , producing clearer images than traditional visualization methods . we investigate the use of particle systems for visualizing volume datasets using non photorealistic techniques . in our volumeflies framework , user selectable rules affect particles to produce a variety of illustrative styles in a unified way . the techniques presented do not require the generation of explicit intermediary surfaces .
a framework for sequential multiblock component methods . <eos> multiblock or multiset methods are starting to be used in chemistry and biology to study complex data sets . in chemometrics , sequential multiblock methods are popular that is , methods that calculate one component at a time and use deflation for finding the next component . in this paper a framework is provided for sequential multiblock methods , including hierarchical pca ( hpca two versions ) , consensus pca ( cpca two versions ) and generalized pca ( gpca ) . properties of the methods are derived and characteristics of the methods are discussed . all this is illustrated with a real five block example from chromatography . the only methods with clear optimization criteria are gpca and one version of cpca . of these , gpca is shown to give inferior results compared with cpca . copyright ( c ) <digit> john wiley sons , ltd .
of the rich and the poor and other curious minds on open access and development . <eos> purpose the paper seeks to reconsider open access and its relation to issues of development by highlighting the ties the open access movement has with the hegemonic discourse of development and to question some of the assumptions about science and scientific communication upon which the open access debates are based . the paper also aims to bring out the conflict arising from the convergence of the hegemonic discourses of science and development with the contemporary discourse of openness . design methodology approach the paper takes the form of a critical reading of a range of published work on open access and the so called developing world as well as of various open access declarations . the argument is supported by insights from post development studies . findings open access is presented as an issue of moral concern beyond the narrow scope of scholarly communication . claims are made based on hegemonic discourses that are positioned as a priori and universal . the construction of open access as an issue of unquestionable moral necessity also impedes the problematisation of its own heritage . originality value this paper is intended to open up the view for open access 's less obvious alliances and conflicting discursive ties and thus to initiate a politisation , which is necessary in order to further the debate in a more fruitful way .
geometric surrogate based optimisation for permutation based problems . <eos> in continuous optimisation , surrogate models ( sms ) are used when tackling real world problems whose candidate solutions are expensive to evaluate . in previous work , we showed that a type of sms radial basis function networks ( rbfns ) can be rigorously generalised to encompass combinatorial spaces based in principle on any arbitrarily complex underlying solution representation by extending their natural geometric interpretation from continuous to general metric spaces . this direct approach to representations does not require a vector encoding of solutions , and allows us to use sms with the most natural representation for the problem at hand . in this work , we apply this framework to combinatorial problems using the permutation representation and report experimental results on the quadratic assignment problem .
computational approach to ensure the stability of the favorable atp binding site in e. coli hfq . <eos> bacterial hfq is a highly conserved thermostable protein of about 10kda . the hfq protein was discovered in <digit> as an e. coli host factor that was essential for replication of the bacteriophage q . it is now clear that hfq has many important physiological roles . in e. coli , hfq mutants show a multiple stress response related phenotypes . hfq is now known to regulate the translation of two major stress transcription factors rpos and rpoe in enterobacteria and mediates its plieotrophic effects through several mechanisms . it interacts with regulatory srna and facilitates their antisense interaction with their targets . it also acts independently to modulate mrna decay and in addition acts as a repressor of mrna translation . recent paper from arluison et al. <digit> provided the first evidence indicating that hfq is an atp binding protein . they determined a plausible atp binding site in hfq and tested hfq 's atp binding affinity and stoichiometry . experimental data suggest that the atp binding by the hfqrna complex results in its significant destabilization of the protein and the result also proves important role of tyr25 that flanks the cleft and stabilizes the adenine portion of atp , possibly via aromatic stacking . in our study , the atp molecule was docked into the predicted binding cleft using gold docking software . the binding nature of atp and its effect on hfqrna complex was studied using molecular dynamics simulations . importance of tyr25 residue was monitored and revealed using mutational study on the modeled systems . our data and the corresponding results point to one of hfq functional structural consequences due to atp binding and tyr25ala mutation .
the sdl pattern approach a reuse driven sdl design methodology . <eos> there are several sdl methodologies that offer full system life cycle support . only few of them consider software reuse , not to mention high level reuse of architecture and design . however , software reuse is a proven software engineering paradigm leading to high quality and reduced development effort . experience made it apparent that beyond the more traditional reuse of code especially high level reuse of architecture and design ( as in the case of design patterns or frameworks ) has the potential of achieving more systematic and widespread reuse . this paper presents the sdl pattern approach , a design methodology for distributed systems which integrates sdl based system development with the pattern paradigm . it supports reuse of design knowledge modeled as sdl patterns and concentrates on the design phase of sdl based system development . in order to get full life cycle support , the pattern based design process can be integrated within existing sdl methodologies .
database repairing using updates . <eos> repairing a database means bringing the database in accordance with a given set of integrity constraints by applying some minimal change . if a database can be repaired in more than one way , then the consistent answer to a query is defined as the intersection of the query answers on all repaired versions of the database . earlier approaches have confined the repair work to deletions and insertions of entire tuples . we propose a theoretical framework that also covers updates as a repair primitive . update based repairing is interesting in that it allows rectifying an error within a tuple without deleting the tuple , thereby preserving consistent values in the tuple . another novel idea is the construct of nucleus a single database that yields consistent answers to a class of queries , without the need for query rewriting . we show the construction of nuclei for full dependencies and conjunctive queries . consistent query answering and constructing nuclei is generally intractable under update based repairing . nevertheless , we also show some tractable cases of practical interest .
simulation of face hairstyle swapping in photographs with skin texture synthesis . <eos> the modern trend of diversification and personalization has encouraged people to boldly express their differentiation and uniqueness in many aspects , and one of the noticeable evidences is the wide variety of hairstyles that we could observe today . given the needs for hairstyle customization , approaches or systems , ranging from 2d to 3d , or from automatic to manual , have been proposed or developed to digitally facilitate the choice of hairstyles . however , nearly all existing approaches suffer from providing realistic hairstyle synthesis results . by assuming the inputs to be 2d photos , the vividness of a hairstyle re synthesis result relies heavily on the removal of the original hairstyle , because the co existence of the original hairstyle and the newly re synthesized hairstyle may lead to serious artifact on human perception . we resolve this issue by extending the active shape model to more precisely extract the entire facial contour , which can then be used to trim away the hair from the input photo . after hair removal , the facial skin of the revealed forehead needs to be recovered . since the skin texture is non stationary and there is little information left , the traditional texture synthesis and image inpainting approaches do not fit to solve this problem . our proposed method yields a more desired facial skin patch by first interpolating a base skin patch , and followed by a non stationary texture synthesis . in this paper , we also would like to reduce the user assistance during such a process as much as possible . we have devised a new and friendly facial contour and hairstyle adjusting mechanism that make it extremely easy to manipulate and fit a desired hairstyle onto a face . in addition , our system is also equipped with the functionality of extracting the hairstyle from a given photo , which makes our work more complete . moreover , by extracting the face from the input photo , our system allows users to exchange faces as well . in the end of this paper , our re synthesized results are shown , comparisons are made , and user studies are conducted as well to further demonstrate the usefulness of our system .
ubiquitous provision of context aware web services . <eos> providing context aware web services refers to an adaptive process of delivering contextually matched web services to meet service requesters needs at the moment . this article presents an ontology based context model that enables formal description and acquisition of contextual information pertaining to both service requesters and services . the context model is supported by context query and phased acquisition techniques . re also report two context aware web services built on top of our context model to demonstrate how the model can be used to facilitate web services discovery and web content adaptation . implementation details of the context elicitation system and the evaluation results of context aware services provision are also reported .
extracting representative information to enhance flexible data queries . <eos> extracting representative information is of great interest in data queries and web applications nowadays , where approximate match between attribute values records is an important issue in the extraction process . this paper proposes an approach to extracting representative tuples from data classes under an extended possibility based data model , and to introducing a measure ( namely , relation compactness ) based upon information entropy to reflect the degree that a relation is compact in light of information redundancy . theoretical analysis and data experiments show that the approach has desirable properties that <digit> ) the set of representative tuples has high degrees of compactness ( less redundancy ) and coverage ( rich content ) <digit> ) it provides a way to obtain data query outcomes of different sizes in a flexible manner according to user preference and <digit> ) the approach is also meaningful and applicable to web search applications .
points to analysis using bdds . <eos> this paper reports on a new approach to solving a subset based points to analysis for java using binary decision diagrams ( bdds ) . in the model checking community , bdds have been shown very effective for representing large sets and solving very large verification problems . our work shows that bdds can also be very effective for developing a points to analysis that is simple to implement and that scales well , in both space and time , to large programs.the paper first introduces bdds and operations on bdds using some simple points to examples . then , a complete subset based points to algorithm is presented , expressed completely using bdds and bdd operations . this algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally , in order to arrive at a very efficient algorithm . experimental results are given to justify the choice of variable ordering , to demonstrate the improvement due to incrementalization , and to compare the performance of the bdd based solver to an efficient hand coded graph based solver . finally , based on the results of the bdd based solver , a variety of bdd based queries are presented , including the points to query .
fuzzy q learning admission control for wcdma wlan heterogeneous networks with multimedia traffic . <eos> in this paper , admission control by a fuzzy q learning technique is proposed for wcdma wlan heterogeneous networks with multimedia traffic . the fuzzy q learning admission control ( fqac ) system is composed of a neural fuzzy inference system ( nfis ) admissibility estimator , an nfis dwelling estimator , and a decision maker . the nfis admissibility estimator takes essential system measures into account to judge how each reachable subnetwork can support the admission request 's required qos and then output admissibility costs . the nfis dwelling estimator considers the doppler shift and the power strength of the requested user to assess his her dwell time duration in each reachable subnetwork and then output dwelling costs . also , in order to minimize the expected maximal cost of the user 's admission request , a minimax theorem is applied in the decision maker to determine the most suitable subnetwork for the user request or to reject . simulation results show that fqac can always maintain the system qos requirement up to a traffic intensity of 1.1 because it can appropriately admit or reject the users ' admission requests . also , the fqac can achieve lower blocking probabilities than conventional jsac proposed in <digit> and can significantly reduce the handoff rate by <digit> <digit> percent .
a numerical study of three dimensional liquid sloshing in tanks . <eos> a numerical model newtank ( numerical wave tank ) has been developed to study three dimensional ( <digit> d ) non linear liquid sloshing with broken free surfaces . the numerical model solves the spatially averaged navierstokes equations , which are constructed on a non inertial reference frame having arbitrary six degree of freedom ( dof ) of motions , for two phase flows . the large eddy simulation ( les ) approach is adopted to model the turbulence effect by using the smagorinsky sub grid scale ( sgs ) closure model . the two step projection method is employed in the numerical solutions , aided by the bi cgstab technique to solve the pressure poisson equation for the filtered pressure field . the second order accurate volume of fluid ( vof ) method is used to track the distorted and broken free surface . laboratory experiments are conducted for both <digit> d and <digit> d non linear liquid sloshing in a rectangular tank . a linear analytical solution of <digit> d liquid sloshing under the coupled surge and sway excitation is also developed in this study . the numerical model is first validated against the available analytical solution and experimental data for <digit> d liquid sloshing of both inviscid and viscous fluids . the validation is further extended to <digit> d liquid sloshing . the numerical results match with the analytical solution when the excitation amplitude is small . when the excitation amplitude is large where sloshing becomes highly non linear , large discrepancies are developed between the numerical results and the analytical solutions , the former of which , however , agree well with the experimental data . finally , as a demonstration , a violent liquid sloshing with broken free surfaces under six dof excitations is simulated and discussed .
spike based vlsi modeling of the ild system in the echolocating bat . <eos> the azimuthal localization of objects by echolocating bats is based on the difference of echo intensity received at the two ears , known as the interaural level difference ( ild ) . mimicking the neural circuitry in the bat associated with the computation of ild , we have constructed a spike based vlsi model that can produce responses similar to those seen in the lateral superior olive ( lso ) and some parts of the inferior colliculus ( ic ) . we further explore some of the interesting computational consequences of the dynamics of both synapses and cellular mechanisms .
having expectations of information systems benefits that match received benefits does it really matter . <eos> a study was conducted to examine the effect of implementing a new system on its users , specifically , the relationship between pre implementation expectations and their perceived benefits based on post implementation experience . disconfirmation theory was used as the theoretical basis this predicts that unrealistically high expectations will result in lower levels of perceived benefit than those associated with realistic expectations ( i.e. where expectations match experience ) . support was found for this prediction , refuting the predictions of dissonance theory . in addition to examining expectations of system use generally , six expectation categories were examined to identify the critical categories where managers should keep expectations from becoming unrealistically high . significant relationships were found for three expectation categories system usefulness , ease of use , and information quality . the results indicate that creating and maintaining realistic expectations of future system benefits really does matter .
online updating belief rule based system for pipeline leak detection under expert intervention . <eos> a belief rule base inference methodology using the evidential reasoning approach ( rimer ) has been developed recently , where a new belief rule base ( brb ) is proposed to extend traditional if then rules and can capture more complicated causal relationships using different types of information with uncertainties , but these models are trained off line and it is very expensive to train and re train them . as such , recursive algorithms have been developed to update the brb systems online and their calculation speed is very high , which is very important , particularly for the systems that have a high level of real time requirement . the optimization models and recursive algorithms have been used for pipeline leak detection . however , because the proposed algorithms are both locally optimal and there may exist some noise in the real engineering systems , the trained or updated brb may violate some certain running patterns that the pipeline leak should follow . these patterns can be determined by human experts according to some basic physical principles and the historical information . therefore , this paper describes under expert intervention , how the recursive algorithm update the brb system so that the updated brb can not only be used for pipeline leak detection but also satisfy the given patterns . pipeline operations under different conditions are modeled by a brb using expert knowledge , which is then updated and fine tuned using the proposed recursive algorithm and pipeline operating data , and validated by testing data . all training and testing data are collected from a real pipeline . the study demonstrates that under expert intervention , the brb expert system is flexible , can be automatically tuned to represent complicated expert systems , and may be applied widely in engineering . it is also demonstrated that compared with other methods such as fuzzy neural networks ( fnns ) , the rimer has a special characteristic of allowing direct intervention of human experts in deciding the internal structure and the parameters of a brb expert system .
negative concord with polyadic quantifiers . <eos> in this paper we develop a syntax semantics of negative concord in romanian within a constraint based lexicalist framework . we show that n words in romanian are best treated as negative quantifiers which may combine by resumption to form polyadic negative quantifiers . optionality of resumption explains the existence of simple sentential negation readings alongside double negation readings . we solve the well known problem of defining general semantic composition rules for translations of natural language expressions in a logical language with polyadic quantifiers by integrating our higher order logical object language in lexical resource semantics ( lrs ) , whose constraint based composition mechanisms directly support a systematic syntax semantics for negative concord with polyadic quantification in head driven phrase structure grammar ( hpsg ) .
psychological reactance to online recommendation services . <eos> adoption of online recommendation services can improve the quality of decision making or it can pose threats to free choice . when people perceive that their freedom is reduced or threatened by others , they are likely to experience a psychological reactance where they attempt to restore the freedom . we performed an experimental study to determine whether users expectation of personalization increased their intention to use recommendation services , because their perception of expected threat to freedom caused by the recommendations reduced their intention to participate . an analysis based on subjects responses after using a hypothetical shopping website confirmed the two sided nature of personalized recommendations , suggesting that the approach and avoidance strategies in persuasive communications can be effectively applied to personalized recommendation services on the web . theoretical and practical implications are discussed .
enhanced ab initio protein folding simulations in poissonboltzmann molecular dynamics with self guiding forces . <eos> we have investigated the sampling efficiency in molecular dynamics with the pb implicit solvent when self guiding forces are added . compared with a high temperature dynamics simulation , the use of self guiding forces in room temperature dynamics is found to be rather efficient as measured by potential energy fluctuation , gyration radius fluctuation , backbone rmsd fluctuation , number of unique clusters , and distribution of low rmsd structures over simulation time . based on the enhanced sampling method , we have performed ab initio folding simulations of two small proteins , <digit> and villin headpiece . the preliminary data for the folding simulations is presented . it is found that <digit> folding proceeds by initiation of the turn and the helix . the hydrophobic collapse seems to be lagging behind or at most concurrent with the formation of the helix . the hairpin stability is weaker than the helix in our simulations . its role in the early folding events seems to be less important than the more stable helix . in contrast , villin headpiece folding proceeds first by hydrophobic collapse . the formation of helices is later than the collapse phase , different from the <digit> folding .
an implicit galerkin finite element rungekutta algorithm for shock structure investigations . <eos> this paper introduces an implicit high order galerkin finite element rungekutta algorithm for efficient computational investigations of shock structures . the algorithm induces no spatial discretization artificial diffusion , relies on cubic and higher degree elements for an accurate resolution of the steep shock gradients , uses an implicit time integration for swift convergence to steady states , and employs original neumann type outlet boundary conditions in the form of generalized rankinehugoniot conditions on normal stress and balance of heat flux and deviatoric stress work per unit time . the formulation automatically calculates the spatial extent of the shock and employs the single non dimensional ( 0,1 ) computational domain for the determination of any shock structure . since it is implicit , the algorithm rapidly generates steady shock structures , in at most <digit> time steps for any upstream mach number considered in this study . the finite element discretization is shown to be asymptotically convergent under progressive grid refinements , in respect of both the h0 h <digit> and h1 h <digit> error norms , with an h0 h <digit> accuracy order as high as <digit> and reduction of the discretization error to the round off error threshold of <digit> <digit> with just <digit> computational cells and 5th degree elements . for upstream mach numbers in the range 1.05 m 10.0 , the computational results satisfy the rankinehugoniot conditions and reflect independently published navierstokes results .
a contemporary view of organizational safety variability and interactions of organizational processes . <eos> studies of qualitative assessment of organizational processes ( e.g. , safety audits and performance indicators ) and their incorporation into risk models have been based on a normative view that decomposes organizations into separate processes that are likely to fail and lead to accidents . this paper discusses a control theoretic framework of organizational safety that views accidents as a result of performance variability of human behaviors and organizational processes whose complex interactions and coincidences lead to adverse events . safety related tasks managed by organizational processes are examined from the perspective of complexity and coupling . this allows safety analysts to look deeper into the complex interactions of organizational processes and how these may remain hidden or migrate toward unsafe boundaries . a taxonomy of variability of organizational processes is proposed and challenges in managing adaptability are discussed . the proposed framework can be used for studying interactions between organizational processes , changes of priorities over time , delays in effects , reinforcing influences , and long term changes of processes . these dynamic organizational interactions are visualized with the use of system dynamics . the framework can provide a new basis for modeling organizational factors in risk analysis , analyzing accidents and designing safety reporting systems .
robust image based visual servoing using invariant visual information . <eos> image based visual servoing from spherical projection . decoupled control using invariant features . a near linear behavior is obtained thanks to the proposed features . the sensitivity to image noise is taken into account .
a distributed numerical simulative algorithm for the analysis of large continuous time markov chains . <eos> a distributed algorithm is introduced for the analysis of large continuous time markov chains ( ctmcs ) by combining in some sense numerical solution techniques and simulation . ctmcs are described as a set of processes communicating via message passing . the state of a process is described by a probability distribution over a set of reachable states rather than by a single state . simulation is used to determine event times and messages types to be exchanged , whereas transitions are realized by vector matrix products as in iterative numerical analysis techniques . in this way , the state space explosion of numerical analysis is avoided , but it is still possible to determine more detailed results than with simulation . parallelization of the algorithm is realized applying a conservative synchronization scheme , which exploits the possibility of precomputing event times as already proposed for parallel simulation of ctmcs . in contrast to a pure simulation approach , the amount of computation is increased , whereas the amount of communication keeps constant . hence it is possible to achieve even on a workstation cluster a significant speedup .
properties of prediction sorting . <eos> one of the major sources of unwanted variation in an industrial process is the raw material quality . however , if the raw materials are sorted into more homogeneous groups before production , each group can be treated differently . in this way the raw materials can be better utilized and the stability of the end product may be improved . prediction sorting is a methodology for doing this . the procedure is founded on the fuzzy c means algorithm where the distance in the objective function is based on the predicted end product quality . usually empirical models such as linear regression are used for predicting the end product quality . by using simulations and bootstrapping , this paper investigates how the uncertainties connected with empirical models affect the optimization of the splitting and the corresponding process variables . the results indicate that the practical consequences of uncertainties in regression coefficients are small . copyright ( c ) <digit> john wiley sons , ltd .
interstitial fluid glucose time lag correction for real time continuous glucose monitoring . <eos> time lag between subcutaneous interstitial fluid and plasma glucose decreases the accuracy of real time continuous glucose monitors . however , inverse filters can be designed to correct time lag and attenuate noise enabling the bloodglucose profile to be reconstructed in real time from continuous measurements of the interstitial fluid glucose . we designed and tested a wiener filter using a set of <digit> sensor glucose tracings ( 30h each ) with a <digit> min sample interval . delays of <digit> min ( meansd ) were introduced into each signal with additive gaussian white noise ( snr 40db ) . performance of the filter was compared to conventional causal and non causal seventh order finite impulse response ( fir ) filters . time lags introduced an error of 5.32.7 % . the error increased in the presence of noise ( to 5.72.6 % ) and attempts to remove the noise with conventional low pass filtering increased the error still further ( to 7.03.5 % ) . in contrast , the wiener filter decreased the error attributed to time delay by <digit> % in the presence of noise ( from 5.7 % to 2.601.26 % ) and by <digit> % in the absence of noise ( 5.3 % to 1.31 % ) . introducing time lag correction without increasing sensitivity to noise can increase cgm accuracy .
fast data access and energy efficient protocol for wireless data broadcast . <eos> in wireless mobile computing environments , broadcasting is an effective and scalable technique to disseminate information to a massive number of clients , wherein the energy usage and responsiveness are considered major concerns . existing air indexing schemes for data broadcast have focused on energy efficiency ( reducing tuning time ) only . on the other hand , existing broadcast scheduling schemes have aimed at reducing access latency through nonflat data broadcast to improve responsiveness only . not much work has addressed the energy efficiency and responsiveness issues concurrently . in this paper , we propose a fast data access scheme concurrently supporting energy saving protocol that constructs the broadcast channels according to the access frequency of each type of message in order to improve energy efficiency in mobile devices ( mds ) . the pinwheel scheduling algorithm ( psa ) presented in this paper is used to organize all types of messages in the broadcast channel in the most symmetrical distribution in order to reduce both the tuning and access time . the performance of the proposed mechanism is analyzed , and the improvement over existing methods is demonstrated numerically . the results show that the proposed mechanism is capable of improving both the tuning and access time due to the presence of skewness in the access distribution among the disseminated messages . copyright ( c ) <digit> john wiley sons , ltd .
onoff retinal ganglion cells temporally encode off on sequence . <eos> while the functions of on and off retinal ganglion cells have been intensively investigated , that of onoff cells has not . in the present study , the temporal properties of spike trains emitted from on off cells in response to randomly flickering or multiphase ramp stimuli were examined in the japanese quail . the results indicate that the firing of on spikes was influenced by the recent firing of off spikes , and vice versa . as a result of this interaction , off on sequence of light intensity change was encoded with a spike pair with an interval of 20ms , indicating that temporal coding is utilized in the vertebrate visual system as early as the retina . thus , the present results suggest that retinal neuronal circuits may detect specific sequential features of stimuli .
noise reduction for magnetic resonance images via adaptive multiscale products thresholding . <eos> edge preserving denoising is of great interest in medical image processing . this paper presents a wavelet based multiscale products thresholding scheme for noise suppression of magnetic resonance images . a canny edge detector like dyadic wavelet transform is employed . this results in the significant features in images evolving with high magnitude across wavelet scales , while noise decays rapidly . to exploit the wavelet interscale dependencies we multiply the adjacent wavelet subbands to enhance edge structures while weakening noise . in the multiscale products , edges can be effectively distinguished from noise . thereafter , an adaptive threshold is calculated and imposed on the products , instead of on the wavelet coefficients , to identify important features . experiments show that the proposed scheme better suppresses noise and preserves edges than other wavelet thresholding denoising methods .
integration of semi fuzzy svdd and cc rule method for supplier selection . <eos> a model based on semi fuzzy support vector domain description ( semi fuzzy svdd ) is put forward to address multi classification problem involved in supplier selection . by preprocessing using semi fuzzy kernel clustering algorithm , original samples are divided into two subsets deterministic samples and fuzzy samples . only the fuzzy samples , rather than all original ones , require expert judgment to decide their categories and are selected as training samples to accomplish svdd specification . therefore , the samples preprocessing method can not only decrease experts working strength , but also achieve less computational consumption and better performance of the classifier . nevertheless , in order to accomplish practical decision making , another condition has to be met good explanations to the decision . a rule extraction method based on cooperative coevolution algorithm ( ccea ) , is introduced to achieve the target . to validate the proposed methodology , samples from real world were employed for experiments , with results compared with conventional multi classification support vector machine approaches and other artificial intelligence techniques . moreover , in terms of rule extraction , experiments on key parameters , different methods including decompositional and pedagogical ones etc. were also conducted .
a distributed disk layer for mass storage at desy . <eos> for the mass storage system at desy , a disk layer is under development . decoupling the client request queue and access to the mass storage system by means of migration , staging and prefetching it shall provide full utilization of robot and drive resources . by managing distributed disk resources in the heterogeneous computing environment of desy , optimized data access shall be given . ( c ) <digit> published by elsevier science b.v.
on the learning machine for three dimensional mapping . <eos> in this paper , we investigate the neural network with three dimensional parameters for applications like 3d image processing , interpretation of 3d transformations , and 3d object motion . a 3d vector represents a point in the 3d space , and an object might be represented with a set of these points . thus , it is desirable to have a 3d vector valued neural network , which deals with three signals as one cluster . in such a neural network , 3d signals are flowing through a network and are the unit of learning . this article also deals with a related 3d back propagation ( 3d bp ) learning algorithm , which is an extension of conventional back propagation algorithm in the single dimension . 3d bp has an inherent ability to learn and generalize the 3d motion . the computational experiments presented in this paper evaluate the performance of considered learning machine in generalization of 3d transformations and 3d pattern recognition .
a unified approach to the analysis of horton strahler parameters of binary tree structures . <eos> the horton strahler number naturally arose from problems in various fields , e.g. , geology , molecular biology and computer science . consequently , detailed investigations of related parameters for different classes of binary tree structures are of interest . this paper shows one possibility of how to perform a mathematical analysis for parameters related to the horton strahler number in a unified way such that only a single analysis is needed to obtain results for many different classes of trees . the method is explained by the examples of the expected horton strahler number and the related rth moments , the average number of critical nodes , and the expected distance between critical nodes . ( c ) <digit> wiley periodicals . inc .
visitors of two types of museums a segmentation study . <eos> market segmentation comprises a wide range of measurement tools that are useful for the sake of supporting marketing and promotional policies also in the sector of cultural economics . this paper aims to contribute to the literature on segmenting cultural visitors by using the bagged clustering method , as an alternative and effective strategy to conduct cluster analysis when binary variables are used . the technique is a combination of hierarchical and partitioning methods and presents several advantages with respect to more standard techniques , such as k means and lvq . for this purpose , two ad hoc surveys were conducted between june and september <digit> in the two principal museums of the two provinces of the trentino south tyrol region ( bolzano and trento ) , northern italy the south tyrol museum of archaeology in bolzano ( tzi ) , hosting the permanent exhibition of the iceman tzi , and the museum of modern and contemporaneous art of trento and rovereto ( mart ) . the segmentation analysis was conducted separately for the two kinds of museums in order to find similarities and differences in behaviour patterns and characteristics of visitors . the analysis identified three and two cluster segments respectively for the mart and tzi visitors , where two tzi clusters presented similar characteristics to two out of three mart groups . conclusions highlight marketing and managerial implications for a better direction of the museums .
grid resource availability prediction based scheduling and task replication . <eos> the frequent and volatile unavailability of volunteer based grid computing resources challenges grid schedulers to make effective job placements . the manner in which host resources become unavailable will have different effects on different jobs , depending on their runtime and their ability to be checkpointed or replicated . a multi state availability model can help improve scheduling performance by capturing the various ways a resource may be available or unavailable to the grid . this paper uses a multi state model and analyzes a machine availability trace in terms of that model . several prediction techniques then forecast resource transitions into the model 's states . we analyze the accuracy of our predictors , which outperform existing approaches . we also propose and study several classes of schedulers that utilize the predictions , and a method for combining scheduling factors . we characterize the inherent tradeoff between job makespan and the number of evictions due to failure , and demonstrate how our schedulers can navigate this tradeoff under various scenarios . lastly , we propose job replication techniques , which our schedulers utilize to replicate those jobs that are most likely to fail . our replication strategies outperform others , as measured by improved makespan and fewer redundant operations . in particular , we define a new metric for replication efficiency , and demonstrate that our multi state availability predictor can provide information that allows our schedulers to be more efficient than others that blindly replicate all jobs or some static percentage of jobs .
on the number of optimal identifying codes in a twin free graph . <eos> let g g be a simple , undirected graph with vertex set v v . for v v v v and r <digit> r <digit> , we denote by bg , r ( v ) b g , r ( v ) the ball of radius r r and centre v v . a set c v c v is said to be an r r identifying code in g g if the sets bg , r ( v ) c b g , r ( v ) c , v v v v , are all nonempty and distinct . a graph g g which admits an r r identifying code is called r r twin free or r r identifiable , and in this case the smallest size of an r r identifying code in g g is denoted by r i d ( g ) . we study the number of different optimal r r identifying codes c c , i.e. , such that c r i d ( g ) , that a graph g g can admit , and try to construct graphs having many such codes .
on the impact of triangle shapes for boundary layer problems using high order finite element discretization . <eos> the impact of triangle shapes , including angle sizes and aspect ratios , on accuracy and stiffness is investigated for simulations of highly anisotropic problems . the results indicate that for high order discretizations , large angles do not have an adverse impact on solution accuracy . however , a correct aspect ratio is critical for accuracy for both linear and high order discretizations . large angles are also found to be not problematic for the conditioning of the linear systems arising from the discretizations . further , when choosing preconditioning strategies , coupling strengths among elements rather than element angle sizes should be taken into account . with an appropriate preconditioner , solutions on meshes with and without large angles can be achieved within a comparable time .
tree walking automata can not be determinized . <eos> tree walking automata are a natural sequential model for recognizing languages of finite trees . such automata walk around the tree and may decide in the end to accept it . it is shown that deterministic tree walking automata are weaker than nondeterministic tree walking automata . ( c ) <digit> elsevier b.v. all rights reserved .
adaptable multi agent systems the case of the gaia methodology . <eos> changes and adaptations are always necessary after the deployment of a multi agent system ( mas ) , as well as of any other type of software systems . some of these changes may be simply perfective and have local impact only . however , adaptive changes to meet new situations in the operational environment of the mas may impact globally on the overall design . more specifically , those changes usually affect the organizational structure of the mas . in this paper we analyze the issue of design change adaptation in a mas organization , and the specific problem of how to properly model design a mas so as to make it ready for adaptation . special attention is paid to the gaia methodology , whose suitability in dealing with adaptable mas organizations is also discussed with the help of an illustrative application example .
on the multicriteria allocation problem . <eos> we consider multicriteria allocation problems with linear sum objectives . despite the fact that the single objective allocation problem is easily solvable , we show that already in the bicriteria case the problem becomes intractable , is np hard and has a non connected efficient set in general . using the equivalence to appropriately defined multiple criteria multiple choice knapsack problems , an algorithm is suggested that uses partial dominance conditions to save computational time . different types of enumeration schemes are discussed , for example , with respect to the number of necessary filtering operations and with regard to possible parallelizations of the procedure .
optimization and correction of the tool path of the five axis milling machine part <digit> rotations and setup . <eos> we present two new algorithms ( which supplement algorithms <digit> , <digit> and <digit> presented in part <digit> ) to optimize the tool path of the five axis numerically controlled milling machine . algorithm <digit> optimizes a set of feasible rotations . algorithm <digit> presents a least square optimization with regard to a setup of the machine ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
teaching about the risks of electronic voting technology . <eos> in these interesting times computer scientists are increasingly called upon to help concerned citizens understand the risks involved in the current generation of electronic voting machines . these risks and the concurrent escalation of legal challenges to the election system in the united states have shaken the confidence of many americans that a fair and accurate election is even possible . as computer science educators we have an opportunity to add breadth and depth to our curriculum by using these issues to show how existing concepts can be applied to new problems , and how new problems extend our field . in this paper we identify some of the main problems with e voting machines and vote counting technology and suggest ways that discussions of the risks and the attendant societal and ethical issues might be incorporated into the computer science curriculum .
effectiveness of bibliographic searches performed by paediatric residents and interns assisted by librarians . a randomised controlled trial . <eos> background considerable barriers still prevent paediatricians from successfully using information retrieval technology .
a genetic programming model for bankruptcy prediction empirical evidence from iran . <eos> prediction of corporate bankruptcy is a phenomenon of increasing interest to investors creditors , borrowing firms , and governments alike . timely identification of firms impending failure is indeed desirable . by this time , several methods have been used for predicting bankruptcy but some of them suffer from underlying shortcomings . in recent years , genetic programming ( gp ) has reached great attention in academic and empirical fields for efficient solving high complex problems . gp is a technique for programming computers by means of natural selection . it is a variant of the genetic algorithm , which is based on the concept of adaptive survival in natural organisms . in this study , we investigated application of gp for bankruptcy prediction modeling . gp was applied to classify <digit> bankrupt and non bankrupt iranian firms listed in tehran stock exchange ( tse ) . then a multiple discriminant analysis ( mda ) was used to benchmarking gp model . genetic model achieved <digit> % and <digit> % accuracy rates in training and holdout samples , respectively while mda model achieved only <digit> % and <digit> % accuracy rates in training and holdout samples , respectively . mcnemar test showed that gp approach outperforms mda to the problem of corporate bankruptcy prediction .
a queueing model for general group screening policies and dynamic item arrivals . <eos> classification of items as good or bad can often be achieved more economically by examining the items in groups rather than individually . if the result of a group test is good , all items within it can be classified as good , whereas one or more items are bad in the opposite case . whether it is necessary to identify the bad items or not , and if so , how , is described by the screening policy . in the course of time , a spectrum of group screening models has been studied , each including some policy . however , the majority ignores that items may arrive at random time epochs at the testing center in real life situations . this dynamic aspect leads to two decision variables the minimum and maximum group size . in this paper , we analyze a discrete time batch service queueing model with a general dependency between the service time of a batch and the number of items within it . we deduce several important quantities , by which the decision variables can be optimized . in addition , we highlight that every possible screening policy can , in principle , be studied , by defining the dependency between the service time of a batch and the number of items within it appropriately .
efficient character level taint tracking for java . <eos> over <digit> % of web services are vulnerable to attack , and much of the danger arises from command injection vulnerabilities . we present an efficient character level taint tracking system for java web applications and argue that it can be used to defend against command injection vulnerabilities . our approach involves modification only to java library classes and the implementation of the java servlets framework , so it requires only a one time modification to the server without any subsequent modifications to a web application 's bytecode or access to the web application 's source code . this makes it easy to deploy our technique and easy to secure legacy web software . our preliminary experiments with the jforum web application suggest that character level taint tracking adds <digit> <digit> % runtime overhead .
how accurate can block matches be in stereo vision . <eos> this article explores the subpixel accuracy attainable for the disparity computed from a rectified stereo pair of images with small baseline . in this framework we consider translations as the local deformation model between patches in the images . a mathematical study first shows how discrete block matching can be performed with arbitrary precision under shannon whittaker conditions . this study leads to the specification of a block matching algorithm which is able to refine disparities with subpixel accuracy . moreover , a formula for the variance of the disparity error caused by the noise is introduced and proved . several simulated and real experiments show a decent agreement between this theoretical error variance and the observed root mean squared error in stereo pairs with good signal to noise ratio and low baseline . a practical consequence is that under realistic sampling and noise conditions in optical imaging , the disparity map in stereo rectified images can be computed for the majority of pixels ( but only for those pixels with meaningful matches ) with a <digit> <digit> pixel precision .
the cognitive internet of things a unified perspective . <eos> in this article , we present a unified perspective on the cognitive internet of things ( ciot ) . it is noted that within the ciot design we observe the convergence of energy harvesting , cognitive spectrum access and mobile cloud computing technologies . we unify these distinct technologies into a ciot architecture which provides a flexible , dynamic , scalable and robust network design road map for large scale iot deployment . since the prime objective of the ciot network is to ensure connectivity between things , we identify key metrics which characterize the network design space . we revisit the definition of cognition in the context of iot networks and argue that both the energy efficiency and the spectrum efficiency are key design constraints . to this end , we define a new performance metric called the overall link success probability which encapsulates these constraints . the overall link success probability is characterized by both the self sustainablitiy of the link through energy harvesting and the availability of spectrum for transmissions . with the help of a reference scenario , we demonstrate that well known tools from stochastic geometry can be employed to investigate both the node and the network level performance . in particular , the reference scenario considers a large scale deployment of a ciot network empowered by solar energy harvesting deployed along with the centralized ciot device coordinators . it is assumed that ciot network is underlaid with a cellular network , i.e. , ciot nodes share spectrum with mobile users subject to a certain co existence constraint . considering the dynamics of both energy harvesting and spectrum sharing , the overall link success probability is then quantified . it is shown that both the self sustainability of the link , and the availability of transmission opportunites , are coupled through a common parameter , i.e. , the node level transmit power . furthermore , provided the co existence constraint is satisfied , the link level success in the presence of both the inter network and intra network interference is an increasing function of the transmit power . we demonstrate that the overall link level success probability can be maximized by employing a certain optimal transmit power . characterization of such an optimal operational point is presented . finally , we highlight some of the future directions which can benefit from the analytical framework developed in this paper .
lower bounds for modular counting by circuits with modular gates . <eos> we prove that constant depth circuits , with one layer of modm gates at the inputs , followed by a fixed number of layers of modp gates , where p is prime , require exponential size to compute the modq function , if q is a prime that divides neither p nor m.
on the physical implementation of logical transformations generalized l machines . <eos> any account of computation in a physical system , whether an artificial computing device or a natural system considered from a computational point of view , invokes some notion of the relationship between the abstract logical and concrete physical aspects of computation . in a recent paper , james ladyman explored this relationship using a hybrid physical logical entity the l machine and the general account of computation that it supports j. ladyman , what does it mean to say that a physical system implements a computation , theoretical computer science <digit> ( <digit> ) <digit> <digit> . the underlying l machine of ladyman 's analysis is , however , classical and highly idealized , and can not capture essential aspects of computation in important classes of physical systems ( e.g. emerging nanocomputing devices ) where logical states do not have faithful physical representations and where noise and quantum effects prevail . in this work we generalize the l machine to allow for generally unfaithful and noisy implementations of classical logical transformations in quantum mechanical systems . we provide a formal definition and physical information theoretic characterization of generalized quantum l machines ( qlms ) , identify important classes of qlms , and introduce new efficacy measures that quantify the faithfulness and fidelity with which logical transformations are implemented by these machines . two fundamental issues emphasized by ladyman realism about computation and the connection between logical and physical irreversibility are reconsidered within the more comprehensive account of computation that follows from our generalization of the l machine . ( c ) <digit> elsevier b.v. all rights reserved .
tangibles for learning a representational analysis of physical manipulation . <eos> manipulativesphysical learning materials such as cubes or tilesare prevalent in educational settings across cultures and have generated substantial research into how actions with physical objects may support childrens learning . the ability to integrate digital technology into physical objectsso called digital manipulativeshas generated excitement over the potential to create new educational materials . however , without a clear understanding of how actions with physical materials lead to learning , it is difficult to evaluate or inform designs in this area . this paper is intended to contribute to the development of effective tangible technologies for childrens learning by summarising key debates about the representational advantages of manipulatives under two key headings offloading cognitionwhere manipulatives may help children by freeing up valuable cognitive resources during problem solving , and conceptual metaphorswhere perceptual information or actions with objects have a structural correspondence with more symbolic concepts . the review also indicates possible limitations of physical objectsmost importantly that their symbolic significance is only granted by the context in which they are used . these arguments are then discussed in light of tangible designs drawing upon the authors current research into tangibles and young childrens understanding of number .
identifying spurious interactions and predicting missing interactions in the protein protein interaction networks via a generative network model . <eos> with the rapid development of high throughput experiment techniques for protein protein interaction ( ppi ) detection , a large amount of ppi network data are becoming available . however , the data produced by these techniques have high levels of spurious and missing interactions . this study assigns a new reliably indication for each protein pairs via the new generative network model ( rignm ) where the scale free property of the ppi network is considered to reliably identify both spurious and missing interactions in the observed high throughput ppi network . the experimental results show that the rignm is more effective and interpretable than the compared methods , which demonstrate that this approach has the potential to better describe the ppi networks and drive new discoveries .
similarities between powersets of terms . <eos> generalisation of the foundational basis for many valued logic programming builds upon generalised terms in the form of powersets of terms . a categorical approach involving set and term functors as monads allows for a study of monad compositions that provide variable substitutions and compositions thereof . in this paper , substitutions and unifiers appear as constructs in kleisli categories related to particular composed powerset term monads . specifically , we show that a frequently used similarity based approach to fuzzy unification is compatible with the categorical approach , and can be adequately extended in this setting also some examples are included in order to illuminate the definitions . ( c ) <digit> elsevier b.v. all rights reserved .
feedback vertex sets on restricted bipartite graphs . <eos> a feedback vertex set ( fvs ) in a graph is a subset of vertices whose complement induces a forest . finding a minimum fvs is n p complete on bipartite graphs , but tractable on convex bipartite graphs and on chordal bipartite graphs . a bipartite graph is called tree convex , if a tree is defined on one part of the vertices , such that for every vertex in the other part , its neighborhood induces a subtree . when the tree is a path , a triad or a star , the bipartite graph is called convex bipartite , triad convex bipartite or star convex bipartite , respectively . we show that ( i ) fvs is tractable on triad convex bipartite graphs ( <digit> ) fvs is n p complete on star convex bipartite graphs and on tree convex bipartite graphs where the maximum degree of vertices on the tree is at most three . ( c ) <digit> elsevier b.v. all rights reserved .
the extended delaunany tessellation . <eos> the extended delaunay tessellation ( edt ) is presented in this paper as the unique partition of a node set into polyhedral regions defined by nodes lying on the nearby voronoi spheres . until recently , all the fem mesh generators were limited to the generation of tetrahedral or hexahedral elements ( or triangular and quadrangular in 2d problems ) . the reason for this limitation was the lack of any acceptable shape function to be used in other kind of geometrical elements . nowadays , there are several acceptable shape functions for a very large class of polyhedra . these new shape functions , together with the edt , gives an optimal combination and a powerful tool to solve a large variety of physical problems by numerical methods . the domain partition into polyhedra presented here does not introduce any new node nor change any node position . this makes this process suitable for lagrangian problems and meshless methods in which only the connectivity information is used and there is no need for any expensive smoothing process .
interlaced euler scheme for stiff systems of stochastic differential equations . <eos> in deterministic as well as stochastic models , stiff systems , i.e. , systems with vastly different time scales where the fast scales are stable , are very common . it is well known that the implicit euler method is well suited for stiff deterministic equations ( modeled by odes ) while the explicit euler is not . in particular , once the fast transients are over , the implicit euler allows for the choice of time steps comparable to the slowest time scales of the system . in stochastic systems ( modeled by sdes ) the picture is more complex . while the implicit euler has better stability properties over the explicit euler , it underestimates the stationary variance . in general , one may not expect any method to work successfully by taking time steps of the order of the slowest time scale . we explore the idea of interlacing large implicit euler steps with a sequence of small explicit euler steps . in particular , we present our study of a linear test system of sdes and demonstrate that such interlacing could effectively deal with stiffness . we also discuss the uniform convergence of mean and variance .
inverse problem for wave propagation in a perturbed layered half space . <eos> this paper is concerned with the inverse medium scattering problem in a perturbed , layered , half space , which is a problem related to the seismologial investigation of inclusions inside the earth 's crust . a wave penetrable object is located in a layer where the refraction index is different from the other part of the half space . wave propagation in such a layered half space is different from that in a homogeneous half space . in a layered half space , a scattered wave consists of a free wave and a guided wave . in many cases , only the free wave far field or only the guided wave far field can be measured . we establish mathematical formulas for relations between the object , the incident wave and the scattered wave . in the ideal condition where exact data are given , we prove the uniqueness of the inverse problem . a numerical example is presented for the reconstruction of a penetrable object from simulated noise data . ( c ) <digit> elsevier ltd. all rights reserved .
an explorative analysis of user evaluation studies in information visualisation . <eos> this paper presents an analysis of user studies from a review of papers describing new visualisation applications and uses these to highlight various issues related to the evaluation of visualisations . we first consider some of the reasons why the process of evaluating visualisations is so difficult . we then dissect the problem by discussing the importance of recognising the nature of experimental design , datasets and participants as well as the statistical analysis of results . we propose explorative evaluation as a method of discovering new things about visualisation techniques , which may give us a better understanding of the mechanisms of visualisations . finally we give some practical guidance on how to do evaluation correctly .
an investigation the factors affecting mis student burnout in technical vocational college . <eos> management information system ( mis ) students are one of the most important information system ( is ) employee sources . however , the determinants of student 's burnout for mis major students have received little attentions , despite their importance as indicator in predicting professional burnout and their working intention after their graduation and becoming is professionals . this study explores the antecedents of student burnout for mis major at technical vocational college . self efficacy , social support , and sex role were considered as antecedents to mis student burnout . a questionnaire method by self administered technique was used in this study . multiple regression analysis was used to analyze the hypotheses . statistical results displayed that mis students with social support , self efficacy and femininity have predictive power over student burnout . mis students with social support and masculinity also have predictive power over self efficacy .
bem calculation of the complex thermal impedance of microelectronic devices . <eos> this paper presents a numerical method for modelling the dynamic thermal behaviour of microelectronic structures in the frequency domain . a boundary element method ( bem ) based on a green 's function solution is proposed for solving the 3d heat equation in phasor notation . the method is capable of calculating the ac temperature and heat flux distributions and complex thermal impedance for packages composed of an arbitrary number of bar shaped components . various types of boundary conditions , including thermal contact resistance and convective cooling , can be taken into account . a simple benchmark case is investigated and a good convergence towards the analytical solution is obtained . simulation results for a thin plate under convective cooling are compared with a theoretical model and an excellent agreement is observed . in a second example a more complicated three layer structure is investigated . the bem is used to analyse the thermal behaviour if delamination of the package occurs , and a physical explanation for the results is given .
directional texture transfer for video . <eos> texture transfer is a method that copies the texture of a reference image to a target image . this technique has an advantage in that various styles can be expressed according to the reference image , in a single framework . however , in this technique , it is not easy to control the effect of each style . in addition , when this technique is extended to processing video images , maintaining temporal coherence is very difficult . in this paper , we propose an algorithm that transfers the texture of a reference image to a target video while retaining the directionality of the target video . the algorithm maintains the temporal coherency of the transferred texture , and controls the style of the texture transfer .
wind parameters extraction from aircraft trajectories . <eos> high altitude wind parameters can be extracted from recorded aircraft positions . this method gives the best results when aircraft are stabilized ( no turn , no climb , no descend ) this method can extract wind dynamics , e.g. how wind parameters change over time . our method performs well thanks to a mix of automatic wind extraction and direct manipulation technique .
a three dimensional model of the human transglutaminase <digit> insights into the understanding of lamellar ichthyosis . <eos> the stratum corneum , the outer layer of the epidermis , serves as a protective barrier to isolate the skin from the external environment . keratinocyte transglutaminase <digit> ( tgase <digit> ) catalyzes amide crosslinking between glutamine and lysine residues on precursor proteins forming the impermeable layers of the epidermal cell envelopes ( ce ) , the highly insoluble membranous structures of the stratum corneum . patients with the autosomal recessive skin disorder lamellar ichthyosis ( li ) appear to have deficient cross linking of the cell envelope due to mutations identified in tgase <digit> , linking this enzyme to li . in the absence of a crystal structure , molecular modeling was used to generate the structure of tgase <digit> . we have mapped the known mutations of tgase <digit> from our survey obtained from a search of pubmed and successfully predicted the impact of these mutations on li . furthermore , we have identified ca ( <digit> ) binding sites and propose that ca ( <digit> ) induces a cis to trans isomerization in residues near the active site as part of the enzyme transamidation activation . docking experiments suggest that substrate binding subsequently induces the reverse cis to trans isomerization , which may be a significant part of the catalytic process . these results give an interpretation at the molecular level of previously reported mutations and lead to further insights into the structural model of tgase <digit> , providing a new basis for understanding li .
understanding continuance usage of mobile sites . <eos> purpose the purpose of this research is to draw on both perspectives of technological perceptions and flow experience to examine continuance usage of mobile sites . design methodology approach based on the valid responses collected from a. survey questionnaire , structural equation modeling technology was employed to examine the research model . findings the results indicated that both perspectives of technological perceptions and flow experience have effects on satisfaction , which in turn affects continuance usage . technological perceptions include system quality and information quality , whereas flow experience includes perceived enjoyment and attention focus . among them , perceived enjoyment has the largest effect on satisfaction . research limitations implications this research is conducted in china , where mobile internet is still in its early stage . thus , the results need to be generalized to other countries that had developed mobile internet . originality value previous research has focused on the effects of instrumental beliefs such as perceived usefulness on mobile user continuance . however , user behavior may be also affected by intrinsic motivations such as flow . this research tries to fill the gap .
interconnection analysis for standard cell layouts . <eos> we present an accurate model and procedures for predicting the common physical design characteristics of standard cell layouts ( i.e. , the interconnection length and the chip area ) . the predicted results are obtained from analysis of the net list only , that is , no prior knowledge of the functionality of the design is used , random and optimized placements , global routing , and detailed routing are each abstracted by procedural models that capture the important features of these processes , and closed form expressions that define these procedural models are presented . we have verified both the global characteristics ( total interconnection length and layout area ) and the detailed characteristics ( wire length and feedthrough distributions ) of the model , on the designs in our test suite , the estimates are very close to the actual layouts .
misep linear and nonlinear ica based on mutual information . <eos> linear independent components analysis ( ica ) has become an important signal processing and data analysis technique , the typical application being blind source separation in a wide range of signals , such as biomedical , acoustical and astrophysical ones . nonlinear ica is less developed , but has the potential to become at least as powerful . this paper presents misep , an ica technique for linear and nonlinear mixtures , which is based on the minimization of the mutual information of the estimated components . misep is a generalization of the popular infomax technique , which is extended in two ways ( <digit> ) to deal with nonlinear mixtures , and ( <digit> ) to be able to adapt to the actual statistical distributions of the sources , by dynamically estimating the nonlinearities to be used at the outputs . the resulting misep method optimizes a network with a specialized architecture , with a single objective function the output entropy . the paper also briefly discusses the issue of nonlinear source separation . examples of linear and nonlinear source separation performed by misep are presented .
beyond state of the art topology as normative ground for decison making systems . <eos> reviews experiments in design and urbanism , intervening in the development of transdisciplinary systems theory for decision making organizations . presents beyond state of the art phenomena , of a morphological and topological type ( out of architecture ) , and advocates harnessing such creativity power to problem solving in informatics .
fuzzy hybrid modelling of an ackerman steered electric vehicle . <eos> physical system modelling with known parameters together with <digit> d or high order look up tables ( obtained from experimental data ) , have been the preferred method for simulating electric vehicles . the non linear phenomena which are present at the vehicle tyre patch and ground interface have resulted in it quantitative understanding of this phenomena . however , nowadays , there is it requirement for a deeper understanding of the vehicle sub models which previously used look up tables . in this paper the hybrid modelling methodology used for electric vehicle systems offers a two stage advantage firstly , the vehicle model retains a comprehensive analytical formulation and secondly , the ' fuzzy ' element offers , in addition to the quantitative results , a qualitative understanding of specific vehicle sub models . in the literature several hybrid topologies are reported , sequential , auxiliary , and embedded . in this paper , the hybrid model topology selected is auxiliary and within the same hybrid model , the first paradigm used is the vehicle dynamics together with the actuator gearbox system . the second paradigm is the non linear fuzzy tyre model for each wheel . in particular , conventional physical system dynamic modelling has been combined with the fuzzy logic type ii or type iii methodology . the resulting hybrid fuzzy tyre models were estimated for a priori number of rules from experimental data . the physical system modelling required the available vehicle parameters such as the overall mass , wheel radius and chassis dimensions . the suggested synergetic fusion of the two methods , ( hybrid fuzzy ) , allowed the vehicle planar trajectories to be obtained prior to the hardware development of the entire vehicle . the strength of this methodology is that it requires localised system experimental data rather than global system data . the disadvantage in obtaining global experimental data is the requirement for comprehensive testing of it vehicle prototype which is both time consuming process and requires extensive resources . in this paper the authors have proposed the use of existing experimental rigs which are available from the leading automotive manufacturers . hence , for the ' hybrid ' modelling , localised data sets were used . in particular , wheel tyre experimental data were obtained from the university tyre rig experimental facilities . tyre forces acting on the tyre patch are mainly responsible for the overall electric vehicle motion . in addition , tyre measurement rigs are a well known method for obtaining localised data thus allowing the effective simulation of more detailed mathematical models . these include , firstly , physical system modelling ( conventional vehicle dynamics ) , secondly , fuzzy type ii or iii modelling ( for the tyre characteristics ) , and thirdly , electric drive modelling within the context of electric vehicles . the proposed hybrid model synthesis has resulted in simulation results which are similar to piece wise ' look up ' table solutions . in addition , the strength of the ' hybrid ' synthesis is that the analyst has a set of rules which clearly show the reasoning behind the complex development of the vehicle tyre forces . this is due to the inherent transparency of the type ii and type iii methodologies . finally , the authors discussed the reasons for selecting a type iii framework . the paper concludes with a plethora of simulation results . ( c ) <digit> elsevier inc. all rights reserved .
on the assessment and evaluation of voice hoarseness . <eos> this article presents a non invasive speech processing method for the assessment and evaluation of voice hoarseness . a technique based on time scale analysis of the voice signal is used to decompose the signal into a suitable number of high frequency details and extract the high frequency bands of the signal . a discriminating measure , which measures the roll off in power in the high frequency bands of the signal , with respect to the decomposition index , is developed . the measure reflects the presence and degree of severity of hoarseness in the analyzed voice signals . the discriminating measure is supported by frequency domain and time series analyses of the high frequency bands of normal and hoarse voice signals to provide a visual aid to the clinician or therapist . a database of sustained long vowels of normal and hoarse voices is created and used to assess the presence and degree of severity of hoarseness . the results obtained by the proposed method are compared to results obtained by perturbation analysis .
probabilistic models of computer systems . <eos> we develop a method based on diffusion approximations in order to compute , under some general conditions , the queue length distribution for a queue in a network . applications to computer networks and to time sharing systems are presented .
integrating synchronization with priority into a kronecker representation . <eos> the compositional representation of a markov chain using kronecker algebra , according to a compositional model representation as a superposed generalized stochastic petri net or a stochastic automata network , has been studied for a while . in this paper we describe a kronecker expression and associated data structures , that allows to handle nets with synchronization over activities of different levels of priority . new algorithms for these structures are provided to perform an iterative solution method of jacobi or gaussseidel type . these algorithms are implemented in the apnn toolbox . we use this implementation in combination with greatspn and exercise an example that illustrates characteristics of the presented algorithms .
non monotone trust region methods for nonlinear equality constrained optimization without a penalty function . <eos> we propose and analyze a class of penalty function free nonmonotone trust region methods for nonlinear equality constrained optimization problems . the algorithmic framework yields global convergence without using a merit function and allows nonmonotonicity independently for both , the constraint violation and the value of the lagrangian function . similar to the byrd omojokun class of algorithms , each step is composed of a quasi normal and a tangential step . both steps are required to satisfy a decrease condition for their respective trust region subproblems . the proposed mechanism for accepting steps combines nonmonotone decrease conditions on the constraint violation and or the lagrangian function , which leads to a flexibility and acceptance behavior comparable to filter based methods . we establish the global convergence of the method . furthermore , transition to quadratic local convergence is proved . numerical tests are presented that confirm the robustness and efficiency of the approach .
numerical computation of the helical chandrasekhar kendall modes . <eos> a new formulation is presented for numerically computing the helical chandrasekhar kendall modes in an axisymmetric torus . it explicitly imposes del . b <digit> and yields a standard matrix eigenvalue problem , which can then be solved by standard matrix eigenvalue techniques . numerical implementation and computational results are shown for an axisymmetric torus typical of reversed field pinch and spherical tokamak . ( c ) <digit> elsevier inc. all rights reserved .
fuzzy adaptive neural network approach to path loss prediction in urban areas at gsm <digit> band . <eos> this paper presents the results of the adaptive network based fuzzy inference system ( anfis ) for the prediction of path loss in a specific urban environment . a new algorithm based anfis for tuning the path loss model is introduced in this work . the performance of the path loss model which is obtained from proposed algorithm is compared to the bertoni walfisch model , which is one of the best studied for propagation analysis involving buildings . this comparison is based on the mean square error between predicted and measured values . according to the indicated error criterion , the errors related to the predictions that are obtained from the algorithm are less than the errors that are obtained from the bertoni walfisch model . in this study , propagation measurements were carried out in the <digit> mhz band in the city of istanbul , turkey .
optimization of microwave devices combining topology gradient and genetic algorithm . <eos> topology optimization can be seen as optimizing a distribution of small topological elements within a domain with respect to given specifications . a numerical topology gradient ( tg ) algorithm is applied in the context of electromagnetism for optimizing microwave devices , computing the sensitivity on adding or removing small metallic elements . this method leads to an optimum topology with very little initial information in acceptable time consumption . the method is applied to the design of a microstrip component in which the topology gradient is directly used as a direction of descent . however , in some ill behavior problems , topology gradient is not sufficient to converge to the global optimum . in the latter case , the basic tg is coupled with a genetic algorithm ( g.a ) to make a more suitable algorithm for solving local optima problems . ( c ) <digit> wiley periodicals . inc .
a layout algorithm for undirected compound graphs . <eos> we present an algorithm for the layout of undirected compound graphs , relaxing restrictions of previously known algorithms in regards to topology and geometry . the algorithm is based on the traditional force directed layout scheme with extensions to handle multi level nesting , edges between nodes of arbitrary nesting levels , varying node sizes , and other possible application specific constraints . experimental results show that the execution time and quality of the produced drawings with respect to commonly accepted layout criteria are quite satisfactory . the algorithm has also been successfully implemented as part of a pathway integration and analysis toolkit named patika , for drawing complicated biological pathways with compartmental constraints and arbitrary nesting relations to represent molecular complexes and various types of pathway abstractions . ( c ) <digit> elsevier inc. all rights reserved .
a knowledge based scheduling system for emergency departments . <eos> a knowledge based reactive scheduling system is proposed to answer the requirements of emergency departments ( eds ) . the algorithm includes detailed patient priority , arrival time , flow time and doctor load . the main aim is to determine the patients who have higher priorities initially , and then minimize their waiting times . to achieve this aim , physicians and the other related workers can use an interactive system . in this study , we evaluated the existing system by comparing the proposed system . also , reactive scheduling cases were evaluated for some items such as decreasing the number of doctors , changing durations and entering of an urgent patient to the system . all experiments were performed with proposed algorithm and right shift rescheduling approach .
specifying and proving properties of timed i o automata using tempo . <eos> timed i o automata ( tioa ) is a mathematical framework for modeling and verification of distributed systems that involve discrete and continuous dynamics . tioa can be used for example , to model a real time software component controlling a physical process . the tioa model is sufficiently general to subsume other models in use for timed systems . the tempo toolset , currently under development , is aimed at supporting system development based on tioa specifications . the tempo toolset is an extension of the ioa toolkit , which provides a specification simulator , a code generator , and both model checking and theorem proving support for analyzing specifications . this paper focuses on the modeling of timed systems and their properties with tioa and on the use of tame4tioa , the tame ( timed automata modeling environment ) based theorem proving support provided in tempo , for proving system properties , including timing properties . several examples are provided by way of illustration .
distributed virtual backbone construction in sensor networks with asymmetric links . <eos> in this paper , we study the problem of distributed virtual backbone construction in sensor networks , where the coverage area of nodes are disks with different radii . this problem is modeled by the construction of a minimum connected dominating set ( mcds ) in geometric k disk graphs . we derive the size relationship of any maximal independent set ( mis ) and mcds in geometric k disk graphs , and apply it to analyze the performances of two distributed connected dominating set ( cds ) algorithms we propose in this paper . these algorithms have bounded performance ratio and low communication overhead . to the best of our knowledge , the results reported in this paper represent the state of the art . copyright ( c ) <digit> john wiley sons , ltd .
a real time java chip multiprocessor . <eos> chip multiprocessors are an emerging trend for embedded systems . in this article , we introduce a real time java multiprocessor called jopcmp . it is a symmetric shared memory multiprocessor , and consists of up to eight java optimized processor ( jop ) cores , an arbitration control device , and a shared memory . all components are interconnected via a system on chip bus . the arbiter synchronizes the access of multiple cpus to the shared main memory . in this article , three different arbitration policies are presented , evaluated , and compared with respect to their real time and average case performance a fixed priority , a fair based , and a time sliced arbiter . tasks running on different cpus of a chip multiprocessor ( cmp ) influence each others ' execution times when accessing a shared memory . therefore , the system needs an arbiter that is able to limit the worst case execution time of a task running on a cpu , even though tasks executing simultaneously on other cpus access the main memory . our research shows that timing analysis is in fact possible for homogeneous multiprocessor systems with a shared memory . the timing analysis of tasks , executing on the cmp using time sliced memory arbitration , leads to viable worst case execution time bounds . the time sliced arbiter divides the memory access time into equal time slots , one time slot for each cpu . this memory arbitration scheme allows for a calculation of upper bounds of java application worst case execution times , depending on the number of cpus , the time slot size , and the memory access time . examples of worst case execution time calculation are presented , and the analyzed results of a real world application task are compared to measured execution time results . finally , we evaluate the tradeoffs when using a time predictable solution compared to using average case optimized chip multiprocessors , applying three different benchmarks . these experiments are carried out by executing the programs on the cmp prototype .
a new finite element to represent prismatic joint constraints in mechanisms . <eos> among existing kinematic analysis methods of mechanisms , the techniques based on finite elements represent a generally applicable alternative which enable a wide variety of problems to be solved , including linear ( velocities , accelerations , jerk , ) and non linear ones ( position ) . to modelize a mechanism via these techniques , the link element may be used to introduce a distance constraint between two points . the stiffness matrix assembly of these link elements enables stiffness matrix construction from the model , from which the kinematic behaviour of the mechanism may be extracted . normally kinematic link conditions introduced directly into the system stiffness matrix are used to introduce point to line constraints like those originated by prismatic joints . a new finite element is presented in this paper , defined by its stiffness or geometric matrix , capable of alternatively modelizing the constraints imposed by the prismatic joint . this new element offers numerous advantages against the procedure based on anterior link conditions , particularly in the case of non linear problems .
image thresholding based on the em algorithm and the generalized gaussian distribution . <eos> in this paper , a novel parametric and global image histogram thresholding method is presented . it is based on the estimation of the statistical parameters of object and background classes by the expectationmaximization ( em ) algorithm , under the assumption that these two classes follow a generalized gaussian ( gg ) distribution . the adoption of such a statistical model as an alternative to the more common gaussian model is motivated by its attractive capability to approximate a broad variety of statistical behaviors with a small number of parameters . since the quality of the solution provided by the iterative em algorithm is strongly affected by initial conditions ( which , if inappropriately set , may lead to unreliable estimation ) , a robust initialization strategy based on genetic algorithms ( gas ) is proposed . experimental results obtained on simulated and real images confirm the effectiveness of the proposed method .
fast fractal image coding based on lmse analysis and subblock feature . <eos> in this paper , we propose a fast fractal image coding based on lmse ( least mean square error ) analysis and subblock feature . the proposed method focuses on efficient search of contrast scaling , position of its matched domain block , and isometric transform for a range block . the contrast scaling and the domain block position are searched using a cost function that comes from the lmse analysis of the range block and its fractal approximated block . the isometric transform is searched using <digit> x <digit> blocks formed with the averages of subblocks of range block and domain block . experimental results show that the encoding time of a conventional fractal image coding with our search method is 25.6 39.7 times faster than that with full search method at the same bit rate while giving psnr decrement of 0.2 0.7 db with negligible deterioration in subjective quality . it is also shown that the encoding time of a conventional fractal image coding with our search method is 3.4 4.2 times faster than jacquin 's fractal image coding and is superior by maximum 0.8 db in psnr . it also yields reconstructed images of better quality .
finite element solution to passive scalar transport behind line sources under neutral and unstable stratification . <eos> this study employed a direct numerical simulation ( dns ) technique to contrast the plume behaviours and mixing of passive scalar emitted from line sources ( aligned with the spanwise direction ) in neutrally and unstably stratified open channel flows . the dns model was developed using the galerkin finite element method ( fem ) employing trilinear brick elements with equal order interpolating polynomials that solved the momentum and continuity equations , together with conservation of energy and mass equations in incompressible flow . the second order accurate fractional step method was used to handle the implicit velocity pressure coupling in incompressible flow . it also segregated the solution to the advection and diffusion terms , which were then integrated in time , respectively , by the explicit third order accurate runge kutta method and the implicit second order accurate crank nicolson method . the buoyancy term under unstable stratification was integrated in time explicitly by the first order accurate euler method . the dns fem model calculated the scalar plume development and the mean plume path . in particular , it calculated the plume meandering in the wall normal direction under unstable stratification that agreed well with the laboratory and field measurements . as well as previous modelling results available in literature . copyright ( c ) <digit> john wiley sons , ltd .
the effect of radius height ratio on truss optimization . <eos> the optimal topology of a michell 's truss is being considered as a benchmark problem . it has been observed that this optimal topology is only applicable up to a particular ratio of distance between the loading point to the line joining the supports and the span of the supports . once the ratio exceeds this critical ratio , the optimum topology of the michell 's truss changes . it has been observed from the studies that it is possible to demarcate the region of two different types of optimum topologies by a linear relation . extending this problem to a <digit> d , similar type of observation of different optimum topologies has been observed above and below the critical ratio of height to radius ratio . this critical ratio , similar to a <digit> d case , follows almost a linear relation .
exploratory evaluations of a computer game supporting cognitive behavioural therapy for adolescents . <eos> the need to provide effective mental health treatments for adolescents has been described as a global public health challenge <digit> . in this paper we discuss the exploratory evaluations of the first adolescent intervention to fully integrate a computer game implementing cognitive behavioural therapy . three distinct studies are presented a detailed evaluation in which therapists independent of the design team used the game with <digit> adolescents experiencing clinical anxiety disorders a study in which a member of the design team used the game with <digit> adolescents and finally a study assessing the acceptability of the game and intervention with <digit> practicing therapists . findings are presented within the context of a framework for the design and evaluation of complex health interventions . the paper provides an in depth insight into the use of therapeutic games to support adolescent interventions and provides stronger evidence than previously available for both their effectiveness and acceptability to stakeholders .
efficient high order waveguide mode solvers based on boundary integral equations . <eos> for optical waveguides with high index contrast and sharp corners , high order full vectorial mode solvers are difficult to develop , due to the field singularities at the corners . a recently developed method ( the so called bie ntd method ) based on boundary integral equations ( bies ) and neumann to dirichlet ( ntd ) maps achieves high order of accuracy for dielectric waveguides . in this paper , we develop two new bie mode solvers , including an improved version of the bie ntd method and a new bie dtn method based on dirichlet to neumann ( dtn ) maps . for homogeneous domains with sharp corners , we propose better bies to compute the dtn and ntd maps , and new kernel splitting techniques to discretize hypersingular operators . numerical results indicate that the new methods are more efficient and more accurate , and work very well for metallic waveguides and waveguides with extended mode profiles .
caches for multimedia workloads power and energy tradeoffs . <eos> one of the significant workloads in current generation desktop processors and mobile devices is multimedia processing . large on chip caches are common in modern processors , but large caches will result in increased power consumption and increased access delays . regular data access patterns in streaming multimedia applications and video processing applications can provide high hit rates , but due to issues associated with access time , power and energy , caches can not be made very large . characterizing and optimizing the memory system is conducive for designing power and performance efficient multimedia application processors . performance tradeoffs for multimedia applications have been studied in the past , however , power and energy tradeoffs for caches for multimedia processing have not been adequately studied in the past . in this paper , we characterize multimedia applications for i cache and d cache power and energy using a multilevel cache hierarchy . both dynamic and static power increase with increasing cache sizes , however , the increase in dynamic power is small . the increase in static power is significant , and becomes increasingly relevant for smaller feature sizes . there is significant static power dissipation , similar to <digit> % , in l1 l2 caches at <digit> ram technology sizes , emphasizing the fact that future multimedia systems must be designed by taking leakage power reduction techniques into account . the energy consumption of on chip l2 caches is seen to be very sensitive to cache size variations . sizes larger than <digit> k for i caches and <digit> k for d caches will not be efficient choices to maintain power and performance balance . since multimedia applications spend significant amounts of time in integer operations , to improve the performance , we propose implementing low power full adders and hybrid multipliers in the data path , which results in <digit> % to <digit> % savings in the overall power consumption .
methodology for modeling and analysis of supply networks . <eos> the analysis and modeling of business processes are the basis on which management methodologies , simulation models and information systems are developed . the goal of this paper is to point out the possibility of establishing relationships between processes in supply networks and functioning of the whole system . in this integrated system , all relevant factors for supply network management , both at the global level and at the single process level , could be observed . the idea is to form a process library of the supply network , which would contain process description , inputs , outputs , and the way the process is realized . every record in the library presents the single instance of that process . the relationships of one process with another depend on process structure and the way of its realization . every instance of a process represents its realization . the assembly of mutual compatible instances of all processes represents one realization of the supply network . the key problem , triggering the process realization , is solved by specific production expert system . process realization is very similar to a real system , because the environment influence , uncertainty , and available resources are taken into consideration . as the output , the aggregate of relevant parameters for the evaluation of model functioning are derived . this concept presents the basis of virtual framework for supply network simulation .
quantifying information leaks in software . <eos> leakage of confidential information represents a serious security risk . despite a number of novel , theoretical advances , it has been unclear if and how quantitative approaches to measuring leakage of confidential information could be applied to substantial , real world programs . this is mostly due to the high complexity of computing precise leakage quantities . in this paper , we introduce a technique which makes it possible to decide if a program conforms to a quantitative policy which scales to large state spaces with the help of bounded model checking . our technique is applied to a number of officially reported information leak vulnerabilities in the linux kernel . additionally , we also analysed authentication routines in the secure remote password suite and of a internet message support protocol implementation . our technique shows when there is unacceptable leakage the same technique is also used to verify , for the first time , that the applied software patches indeed plug the information leaks . this is the first demonstration of quantitative information flow addressing security concerns of real world industrial programs .
four dimensional radiotherapeutic dose calculation using biomechanical respiratory motion description . <eos> organ motion due to patient breathing introduces a technical challenge for dosimetry and lung tumor treatment by hadron therapy . accurate dose distribution estimation requires patient specific information on tumor position , size , and shape as well as information regarding the material density and stopping power of the media along the beam path . a new 4d dosimetry method was developed , which can be coupled to any motion estimation method . as an illustration , the new method was implemented and tested with a biomechanical model and clinical data .
fast dynamic organization without short term synaptic plasticity a new view on hebb 's dynamical assemblies . <eos> hebb postulated cell assemblies as the basic computational elements for understanding cortical processing . he defined them as temporary associations of neurons that organize fast and flexibly into functional units , using correlation based short term synaptic plasticity . based on the properties of spiking neurons , we implement dynamical assemblies that organize completely without synaptic plasticity . instead , we find varying effective connection strengths that reflect the organizational process . we propose that this dynamic reorganization capabilities ocurring on a fast temporal scale may be a central element of cortical processing .
state space analysis of petri nets with relation algebraic methods . <eos> a large variety of systems can be modelled by petri nets . their formal semantics are based on linear algebra which in particular allows the calculation of a petri net 's state space . since state space explosion is still a serious problem , efficiently calculating , representing , and analysing the state space is mandatory . we propose a formal semantics of petri nets based on executable relation algebraic specifications . thereupon , we suggest how to calculate the markings reachable from a given one simultaneously . we provide an efficient representation of reachability graphs and show in a correct by construction approach how to efficiently analyse their properties . therewith we cover two aspects modelling and model checking systems by means of one and the same logic based approach . on a practical side , we explore the power and limits of relation algebraic concepts for concurrent system analysis . ( c ) <digit> elsevier ltd. all rights reserved .
longitudinal image registration with temporally dependent image similarity measure . <eos> longitudinal imaging studies are frequently used to investigate temporal changes in brain morphology and often require spatial correspondence between images achieved through image registration . beside morphological changes , image intensity may also change over time , for example when studying brain maturation . however , such intensity changes are not accounted for in image similarity measures for standard image registration methods . hence , <digit> ) local similarity measures , <digit> ) methods estimating intensity transformations between images , and <digit> ) metamorphosis approaches have been developed to either achieve robustness with respect to intensity changes or to simultaneously capture spatial and intensity changes . for these methods , longitudinal intensity changes are not explicitly modeled and images are treated as independent static samples . here , we propose a model based image similarity measure for longitudinal image registration that estimates a temporal model of intensity change using all available images simultaneously .
simultaneous optimization of phase balancing and reconfiguration in distribution networks using bfnm algorithm . <eos> rephasing strategy is one of the main methods used for phase balancing and neutral current reduction in electrical distribution networks and the reconfiguration technique is an effective method for network loss reduction . in this paper , a new method for the simultaneous implementation of reconfiguration and phase balancing strategies is presented as a combinational strategy . in order to solve the proposed optimization problem , nelder mead algorithm combined with a bacterial foraging algorithm ( bfnm ) is used based on a fuzzy multi objective function . the proposed method allows for the simultaneous execution of reconfiguration and phase balancing while minimizing the interruption cost of rephasing in addition to eliminating network unbalancing and reducing neutral current and network losses . to demonstrate the efficiency of the bfnm algorithm , its performance is compared with bacterial foraging ( bf ) , particle swarm optimization ( pso ) , genetic and immune algorithms ( ga and ia ) . the proposed method is applied to the ieee <digit> bus test network for evaluation . the simulation results confirm the efficiency of the method in reducing the system costs and network phase balancing .
vowel onset point detection for noisy speech using spectral energy at formant frequencies . <eos> in this paper , we propose a method for robust detection of the vowel onset points ( vops ) from noisy speech . the proposed vop detection method exploits the spectral energy at formant frequencies of the speech segments present in glottal closure region . in this work , formants are extracted by using group delay function , and glottal closure instants are extracted by using zero frequency filter based method . performance of the proposed vop detection method is compared with the existing method , which uses the combination of evidence from excitation source , spectral peaks energy and modulation spectrum . speech data from timit database and noise samples from noisex database are used for analyzing the performance of the vop detection methods . significant improvement in the performance of vop detection is observed by using proposed method compared to existing method .
permutable fuzzy consequence and interior operators and their connection with fuzzy relations . <eos> fuzzy operators are an essential tool in many fields and the operation of composition is often needed . in general , composition is not a commutative operation . however , it is very useful to have operators for which the order of composition does not affect the result . in this paper , we analyze when permutability appears . that is , when the order of application of the operators does not change the outcome . we characterize permutability in the case of the composition of fuzzy consequence operators and the dual case of fuzzy interior operators . we prove that for these cases , permutability is completely connected to the preservation of the operator type . we also study the particular case of fuzzy operators induced by fuzzy relations through zadehs compositional rule and the inf composition . for this cases , we connect permutability of the fuzzy relations ( using the sup composition ) with permutability of the induced operators . special attention is paid to the cases of operators induced by fuzzy preorders and similarities . finally , we use these results to relate the operator induced by the transitive closure of the composition of two reflexive fuzzy relations with the closure of the operator this composition induces .
text mining , names and security . <eos> a process query system , a new approach to representing and querying multiple hypotheses , is proposed for cross document co reference and linking based on existing entity extraction , co reference and database name matching technologies . a crucial component of linking entities across documents is the ability to recognize when different name strings are potential references to the same entity . given the extraordinary range of variation international names can take when rendered in the roman alphabet , this is a daunting task . the extension of name variant matching to free text will add important text mining functionality for intelligence and security informatics ' toolkits .
carbenic vs. ionic mechanistic pathway in reaction of cyclohexanone with bromoform . <eos> the extensive computation study was done to elucidate the mechanism of formation dibromoepoxide from cyclohexanone and bromoform . in this reaction , the formation of dihaloepoxide <digit> is postulated as a key step that determines the distribution and stereochemistry of products . two mechanistic paths of reaction were investigated the addition of dibromocarbene to carbonyl group of ketone , and the addition of tribromomethyl carbanion to the same ( c o ) group . the mechanisms for the addition reactions of dibromocarbenes and tribromomethyl carbanions with cyclohexanone have been investigated using ab initio hf <digit> <digit> g and mp2 <digit> <digit> g level of theory . solvent effects on these reactions have been explored by calculations which included a continuum polarizable conductor model ( cpcm ) for the solvent ( h2o ) . the calculations showed that both mechanisms are possible and are exothermic , but have markedly different activation energies .
query operations for moving objects database systems . <eos> geographical information systems were originally intended to deal with snapshots representing a single state of some reality but there are more and more applications requiring the representation and querying of time varying information . this work addresses the representation of moving objects on gis . the continuous nature of movement raises problems for representation in information systems due to the limited capacity of storage systems and the inherently discrete nature of measurement instruments . the stored information has therefore to be partial and does not allow an exact inference of the real world object 's behavior . to cope with this , query operations must take uncertainty into consideration in their semantics in order to give accurate answers to the users . the paper proposes a set of operations to be included in a gis or a spatial database to make it able to answer queries on the spatio temporal behavior of moving objects . the operations have been selected according to the requirements of real applications and their semantics with respect to uncertainty is specified . a collection of examples from a case study is included to illustrate the expressiveness of the proposed operations .
on the relationship between the traceability properties of reed solomon codes . <eos> fingerprinting codes are used to prevent dishonest users ( traitors ) from redistributing digital contents . in this context , codes with the traceability ( ta ) property and codes with the identifiable parent property ( ipp ) allow the unambiguous identification of traitors . the existence conditions for ipp codes are less strict than those for ta codes . in contrast , ipp codes do not have an efficient decoding algorithm in the general case . other codes that have been widely studied but possess weaker identification capabilities are separating codes . it is a well known result that a ta code is an ipp code , and an ipp code is a separating code . the converse is in general false . however , it has been conjectured that for reed solomon codes all three properties are equivalent . in this paper we investigate this equivalence , providing a positive answer when the number of traitors divides the size of the ground field .
occlusion handling based on sub blobbing in automated video surveillance system . <eos> object tracking with occlusion handling is a challenging problem in automated video surveillance . in particular , occlusion handling and tracking have been often considered as separate modules . this paper proposes a tracking method in the context of video surveillance , where occlusions are automatically detected and handled to solve ambiguities . hence , the tracking process can continue to track the different moving objects correctly . the proposed approach is based on sub blobbing , that is , blobs representing moving objects are segmented into sections whenever occlusions occur . these sub blobs are then treated as blobs with the occluded ones ignored . by doing so , the tracking of objects has become more accurate and less sensitive to occlusions . we have also used a feature based framework for identifying the tracked objects , where several flexible attributes were involved . experiments on several videos have clearly demonstrated the success of the proposed method .
towards the formalization of interaction semantics . <eos> with the advent of web 2.0 and the emergence of improved technologies to enhance ui , the importance of user experience and intuitiveness of web interfaces led to the growth and success of interaction design . web designers often turn to pre defined and well founded design patterns and user interaction paradigms to build novel and more effective web interfaces . the rational behind interaction design patterns is based on user behavior and web navigation studies . the semantics of user interaction is therefore a rich and interesting area that is worth exploring in association with traditional semantic web approaches . in this paper , we present our first attempts of an ontological formalization of interaction patterns and its implications . to prove our concept , we illustrate the mapping approach we employed to put in relation that interaction formalization with data specific ontologies , to create web interfaces to browse and navigate that specialized kind of information the aforementioned ontologies and mapping rules are the basis of the internal operation of a semantic web application framework called star chart , leveraged to build the service finder portal finally , we present our evaluation results .
college students academic motivation , media engagement and fear of missing out . <eos> possible links between fomo , social media engagement , and three motivational constructs were examined . a new scale was designed to measure the extent to which students used social media tools in the classroom . the links between social media engagement and motivational factors were mediated by fomo .
a hybrid collaborative filtering method for multiple interests and multiple content recommendation in e commerce . <eos> recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for products or services during a live interaction . these systems , especially collaborative filtering based on user , are achieving widespread success on the web . the tremendous growth in the amount of available information and the kinds of commodity to web sites in recent years poses some key challenges for recommender systems . one of these challenges is ability of recommender systems to be adaptive to environment where users have many completely different interests or items have completely different content ( we called it as multiple interests and multiple content problem ) . unfortunately , the traditional collaborative filtering systems can not make accurate recommendation for the two cases because the predicted item for active user is not consist with the common interests of his neighbor users . to address this issue we have explored a hybrid collaborative filtering method , collaborative filtering based on item and user techniques , by combining collaborative filtering based on item and collaborative filtering based on user together . collaborative filtering based on item and user analyze the user item matrix to identify similarity of target item to other items , generate similar items of target item , and determine neighbor users of active user for target item according to similarity of other users to active user based on similar items of target item . in this paper we firstly analyze limitation of collaborative filtering based on user and collaborative filtering based on item algorithms respectively and emphatically make explain why collaborative filtering based on user is not adaptive to multiple interests and multiplecontent recommendation . based on analysis , we present collaborative filtering based on item and user for multiple interests and multiple content recommendation . finally , we experimentally evaluate the results and compare them with collaborative filtering based on user and collaborative filtering based on item , respectively . the experiments suggest that collaborative filtering based on item and user provide better recommendation quality than collaborative filtering based on user and collaborative filtering based on item dramatically . ( c ) <digit> published by elsevier ltd .
new approaches to covering and packing problems . <eos> covering and packing integer programs model a large family of combinatorial optimization problems . the current best approximation algorithms for these are an instance of the basic probabilistic method showing that a certain randomized approach produces a good approximation with positive probability . this approach seems inherently sequential by employing the method of alteration we present the first rnc and nc approximation algorithms that match the best sequential guarantees . extending our approach , we get the first rnc and nc approximation algorithms for certain multi criteria versions of these problems . we also present the first nc algorithms for two packing and covering problems that are not subsumed by the above result finding large independent sets in graphs , and rounding fractional group steiner solutions on trees .
novel phenotype issues raised in cross national epidemiological research on drug dependence . <eos> stage transition models based on the american diagnostic and statistical manual ( dsm ) generally are applied in epidemiology and genetics research on drug dependence syndromes associated with cannabis , cocaine , and other internationally regulated drugs ( irds ) . difficulties with dsm stage transition models have surfaced during cross national research intended to provide a truly global perspective , such as the work of the world mental health surveys consortium . alternative simpler dependence related phenotypes are possible , including population level count process models for steps early and before coalescence of clinical features into a coherent syndrome ( e.g. , zero inflated poisson zip regression ) . selected findings are reviewed , based on zip modeling of alcohol , tobacco , and ird count processes , with an illustration that may stimulate new research on genetic susceptibility traits . the annual national surveys on drug use and health ( nsduh ) can be readily modified for this purpose , along the lines of a truly anonymous research approach that can help make nsduh type cross national epidemiological surveys more useful in the context of subsequent genomewide association ( gwas ) research and post gwas investigations with a truly global health perspective .
tcad study on gate all around cylindrical ( gaac ) transistor for cmos scaling to the end of the roadmap . <eos> in this paper , we report tcad study on gate all around cylindrical ( gaac ) transistor for sub <digit> nm scaling . the gaac transistor device physics , tcad simulation , and proposed fabrication procedure have been discussed . among all other novel fin field effect transistor ( finfet ) devices , the gate all around cylindrical device can be particularly used for reducing the problems of conventional multi gate finfet , improving device performance , and scaling down capabilities . with gate all around cylindrical architecture , the transistor is controlled essentially by infinite number of gates surrounding the entire cylinder shaped channel . electrical integrity within the channel is improved by reducing the leakage current due to the non symmetrical field accumulation such as the corner effect . our proposed fabrication procedure for making devices having the gate all around cylindrical ( gaac ) device architecture is also discussed .
a rule engine to process acceleration data on small sensor nodes . <eos> in this paper , we propose a compact rule processing engine to process acceleration data on a small sensor device . our proposed engine enables us to develop applications using acceleration data on the small device with a quite simple and short description . we describe the outline of both our proposed rule engine and an implementation on our developed sensor device called the mo co mi chip .
mug1 an incremental compiler compiler . <eos> mug1 is a compiler generating system developed and implemented at the technical university of munich . the structure of the system and the concepts used in the compiler description are presented . special emphasis is laid on the use of mug1 as a tool for the incremental design of programming languages and the construction of their compilers in parallel .
parallel and distributed local search in comet . <eos> the availability of commodity multiprocessors and high speed networks of workstations offer significant opportunities for addressing the increasing computational requirements of optimization applications . to leverage these potential benefits , it is important , however , to make parallel and distributed processing easily accessible to a wide audience of optimization programmers . this paper addresses this challenge by proposing parallel and distributed programming abstractions that keep the distance from sequential local search algorithms as small as possible . the abstractions , including parallel loops , interruptions , thread pools , and shared objects , are compositional and cleanly separate the optimization program and the parallel instructions . they have been evaluated experimentally on a variety of applications , including warehouse location and coloring , for which they provide significant speedups .
causal relationship from exposure to chemicals in oil refining and chemical industries and malignant melanoma . <eos> malignant melanoma has been thought to be related mainly to exposure to the sun or radiation . a review of the scientific literature reveals many significant correlations between benzene and benzene containing solvents in the workplace and the occurrence of malignant melanoma , particularly in sites that have never been exposed to sunlight . a comparison of positive correlations between such exposure and malignant melanoma by independent investigators and negative findings by investigators with industry affiliations reveals that this difference , at least in part , may account for the discrepant findings . based on independent studies , it is reasonable to conclude that malignant melanoma is causally related to employment related chemical exposures in the petroleum refining industry
control of a wind turbine cluster based on squirrel cage induction generators connected to a single vsc power converter . <eos> a control procedure for wind farms connected to a unique converter is presented . the control method is based on vector control , providing high performance . the system operates in the maximum efficiency area due to the use of a mppt . a power reduction method used in case of an electrical contingency is described . the proposed wind farm layout claims to improve the efficiency and the reliability .
semantic inference of user 's reputation and expertise to improve collaborative recommendations . <eos> collaborative recommender systems select potentially interesting items for each user based on the preferences of like minded individuals . particularly , e commerce has become a major domain in these research field due to its business interest , since identifying the products the users may like or find useful can boost consumption . during the last years , a great number of works in the literature have focused in the improvement of these tools . expertise , trust and reputation models are incorporated in collaborative recommender systems to increase their accuracy and reliability . however , current approaches require extra data from the users that is not often available . in this paper , we present two contributions that apply a semantic approach to improve recommendation results transparently to the users . on the one hand , we automatically build implicit trust networks in order to incorporate trust and reputation in the selection of the set of like minded users that will drive the recommendation . on the other hand , we propose a measure of practical expertise by exploiting the data available in any e commerce recommender system the consumption histories of the users . ( c ) <digit> elsevier ltd. all rights reserved .
two tier image annotation model based on a multi label classifier and fuzzy knowledge representation scheme . <eos> multi label classification and knowledge based approach to image annotation . the definition of the fuzzy knowledge representation scheme based on fpn . novel data driven algorithms for automatic acquisition of fuzzy knowledge . novel inference based algorithms for annotation refinement and scene recognition . a comparison of inference based scene classification with an ordinary approach .
betting system for formative code review in educational competitions . <eos> grading systems based on competition ranking usually limit the grade distribution . we propose a methodology based on a betting system to relax the ranking restrictions . betting assesses the skill to critically analyze source code . a case study in a video game development course validates our proposal .
combinatorial construction of locally testable codes . <eos> an error correcting code is said to be locally testable if there is a test that checks whether a given string is a codeword , or rather far from the code , by reading only a constant number of symbols of the string . while the best known construction of locally testable codes ( ltcs ) by ben sasson and sudan siam j. comput. , <digit> ( <digit> ) , pp. <digit> <digit> and dinur j. acm , <digit> ( <digit> ) , article <digit> achieves very efficient parameters , it relies heavily on algebraic tools and on probabilistically checkable proof ( pcp ) machinery . in this work we present a new and arguably simpler construction of ltcs that is purely combinatorial , does not rely on pcp machinery , and matches the parameters of the best known construction . however , unlike the latter construction , our construction is not entirely explicit .
deterministic and probabilistic multi modal analysis of slope stability . <eos> traditional slope stability analysis involves predicting the location of the critical slip surface for a given slope and computing a safety factor at that location . however , for some slopes with complicated stratigraphy several distinct critical slip surfaces can exist . furthermore , the global minimum safety factor in some cases can be less important than potential failure zones when rehabilitating or reinforcing a slope . existing search techniques used in slope stability analysis can not find all areas of concern , but instead converge exclusively on the critical slip surface . this paper therefore proposes the use of a holistic multi modal optimisation technique which is able to locate and converge to multiple failure modes simultaneously . the search technique has been demonstrated on a number of benchmark examples using both deterministic and probabilistic analysis to find all possible failure mechanisms , and their respective factors of safety and reliability indices . the results from both the deterministic and probabilistic models show that the search technique is effective in locating the known critical slip surface while also establishing the locations of any other distinct critical slip surfaces within the slope . the approach is of particular relevance for investigating the stability of large slopes with complicated stratigraphy , as these slopes are likely to contain multiple failure mechanisms .
epithelial tight junctions in intestinal inflammation . <eos> the epithelium in inflamed intestinal segments of patients with crohn 's disease is characterized by a reduction of tight junction strands , strand breaks , and alterations of tight junction protein content and composition . in ulcerative colitis , epithelial leaks appear early due to micro erosions resulting from upregulated epithelial apoptosis and in addition to a prominent increase of claudin <digit> . th1 cytokine effects by interferon in combination with tnf are important for epithelial damage in crohn 's disease , while interleukin <digit> ( il <digit> ) is the key effector cytokine in ulcerative colitis stimulating apoptosis and upregulation of claudin <digit> expression . focal lesions caused by apoptotic epithelial cells contribute to barrier disturbance in ibd by their own conductivity and by confluence toward apoptotic foci or erosions . another type of intestinal barrier defect can arise from hemolysin harboring e. coli strains among the physiological flora , which can gain pathologic relevance in combination with proinflammatory cytokines under inflammatory conditions . on the other hand , intestinal barrier impairment can also result from transcellular antigen translocation via an initial endocytotic uptake into early endosomes , and this is intensified by proinflammatory cytokines as interferon and may thus play a relevant role in the onset of ibd . taken together , barrier defects contribute to diarrhea by a leak flux mechanism ( e.g. , in ibd ) and can cause mucosal inflammation by luminal antigen uptake . immune regulation of epithelial functions by cytokines may cause barrier dysfunction not only by tight junction impairments but also by apoptotic leaks , transcytotic mechanisms , and mucosal gross lesions .
fusion of perceptual cues for robust tracking of head pose and position . <eos> the paradigm of perceptual fusion provides robust solutions to computer vision problems . by combining the outputs of multiple vision modules , the assumptions and constraints of each module are factored out to result in a more robust system overall . the integration of different modules can be regarded as a form of data fusion . to this end , we propose a framework for fusing different information sources through estimation of covariance from observations . the framework is demonstrated in a face and 3d pose tracking system that fuses similarity to prototypes measures and skin colour to track head pose and face position . the use of data fusion through covariance introduces constraints that allow the tracker to robustly estimate head pose and track face position simultaneously . ( c ) <digit> pattern recognition society . published by elsevier science ltd. all rights reserved .
encoding multiple orientations in a recurrent network . <eos> models containing recurrent connections amongst the cells within a population can account for a range of empirical data on orientation selectivity in striate cortex . however , existing recurrent models are unable to veridically encode more than one orientation at a time . underlying this inability is an inherent limitation in the variety of activity profiles that can be stably maintained . we propose a new recurrent model that can form a broader range of stable population activity patterns . we demonstrate that these patterns preserve information about multiple orientations present in the population inputs . this preservation has significant computational consequences when information encoded in several populations must be integrated to perform behavioral tasks , such as visual discrimination .
an automated design and assembly of interference free modular fixture setup . <eos> this paper describes an automated modular fixture design system developed using a cad based methodology and implemented on a <digit> d cad cam software package . the developed automated fixture design ( afd ) system automates the fixturing points determination and is integrated on top of the previously developed interactive and semi automated fixture design systems . the determination of fixturing points is implemented in compliance with the fixturing principles that are formulated as heuristics rules to generate candidate list of points and then select the exact points from the list . apart from determining the fixturing points automatically , the system is capable of producing cutting tool collision free fixture design using its machining interference detection sub module . the machining interference detection is accomplished through the use of cutter swept solid based on cutter swept volume approach . therefore , using the developed afd , an interference free fixture design and assembly can be achieved in the possible shortest design lead time .
pseudorandomness and average case complexity via uniform reductions . <eos> impagliazzo and wigderson ( <digit> ) gave the first construction of pseudorandom generators from a uniform complexity assumption on exp ( namely exp not equal bpp ) . unlike results in the nonuniform setting , their result does not provide a continuous trade off between worst case hardness and pseudorandomness , nor does it explicitly establish an average case hardness result . in this paper we obtain an optimal worst case to average case connection for exp if exp not subset of bptime ( t ( n ) ) , then exp has problems that can not be solved on a fraction <digit> <digit> <digit> t ' ( n ) of the inputs by bptime ( t ' ( n ) ) algorithms , for t ' t ( omega ( <digit> ) ) . we exhibit a pspace complete self correctible and downward self reducible problem . this slightly simplifies and strengthens the proof of impagliazzo and wigderson , which used a p complete problem with these properties . we argue that the results of lmpagliazzo and wigderson , and the ones in this paper , can not be proved via black box uniform reductions .
galectins in acute and chronic inflammation . <eos> galectins are animal lectins that bind to galactosides , such as lactose and n acetyllactosamine , in free form or contained in glycoproteins or glycolipids . they are located intracellularly or extracellularly . in the latter they exhibit bivalent or multivalent interactions with glycans on cell surfaces and induce various cellular responses , including production of cytokines and other inflammatory mediators , cell adhesion , migration , and apoptosis . furthermore , they can form lattices with membrane glycoprotein receptors and modulate receptor properties . intracellular galectins can participate in signaling pathways and alter biological responses , including apoptosis , cell differentiation , and cell motility . current evidence indicates that galectins play important roles in acute and chronic inflammatory responses , as well as other diverse pathological processes . galectin involvement in some processes in vivo has been discovered , or confirmed , through studies of genetically engineered mouse strains , each deficient in a given galectin . current evidence also suggests that galectins may be therapeutic targets or employed as therapeutic agents for these inflammatory responses .
hierarchical analysis of power distribution networks . <eos> careful design and verification of the power distribution network of a chip are of critical importance to ensure its reliable performance . with the increasing number of transistors on a chip , the size of the power network has grown so large as to make the verification task very challenging . the available computational power and memory resources impose limitations on the size of networks that can be analyzed using currently known techniques . many of today 's designs have power networks that are too large to be analyzed in the traditional way as flat networks . in this paper , we propose a hierarchical analysis technique to overcome the aforesaid capacity limitation . we present a new technique for analyzing a power grid using macromodels that are created for a set of partitions of the grid . efficient numerical techniques for the computation and sparsification of the port admittance matrices of the macromodels are presented . a novel sparsification technique using a <digit> <digit> integer linear programming formulation is proposed to achieve superior sparsification for a specified error . the run time and memory efficiency of the proposed method are illustrated on industrial designs . it is shown that even for a <digit> million node power grid , our approach allows for an efficient analysis , whereas previous approaches have been unable to handle power grids of such size .
on the interference of ultra wide band systems on point to point links and fixed wireless access systems . <eos> ultra wide bandwidth ( uwb ) spread spectrum techniques will play a key role in short range wireless connectivity supporting high bit rates availability and low power consumption . uwb can be used in the design of wireless local and personal area networks providing advanced integrated multimedia services to nomadic users within hot spot areas . thus the assessment of the possible interference caused by uwb devices on already existing narrowband and wideband systems is fundamental to ensure nonconflicting coexistence and , therefore , to guarantee acceptance of uwb technology worldwide . in this paper , we study the coexistence issues between an indoor uwb based system ( hot spot ) and outdoor point to point ( pp ) links and fixed wireless access ( fwa ) systems operating in the 3.5 5.0 ghz frequency range . we consider a realistic uwb master slave system architecture and we show through computer simulation , that in all practical cases uwb system can coexist with pp and fwa without causing any dangerous interference .
scheduling multimedia services in a low power mac for wireless and mobile atm networks . <eos> this paper describes the design and analysis of the scheduling algorithm for energy conserving medium access control ( ec mac ) , which is a low power medium access control ( mac ) protocol for wireless and mobile atm networks . we evaluate the scheduling algorithms that have been proposed for traditional atm networks . based on the structure of ec mac and the characteristics of wireless channel , we propose a new algorithm that can deal with the burst errors and the location dependent errors . most scheduling algorithms proposed for either wired or wireless networks were analyzed with homogeneous traffic or multimedia services with simplified traffic models . we analyze our scheduling algorithm with more realistic multimedia traffic models based on h. <digit> video traces and self similar data traffic . one of the key goals of the scheduling algorithms is simplicity and fast implementation . unlike the time stamped based algorithms , our algorithm does not need to sort the virtual time , and thus , the complexity of the algorithm is reduced significantly .
solving capacitated arc routing problems using a transformation to the cvrp . <eos> a well known transformation by pearn , assad and golden reduces a capacitated arc routing problem ( carp ) into an equivalent capacitated vehicle routing problem ( cvrp ) . however , that transformation is regarded as unpractical , since an original instance with r required edges is turned into a cvrp over a complete graph with 3r <digit> <digit> r <digit> vertices . we propose a similar transformation that reduces this graph to 2r <digit> <digit> r <digit> vertices , with the additional restriction that a previously known set of r pairwise disconnected edges must belong to every solution . using a recent branch and cut and price algorithm for the cvrp , we observed that it yields an effective way of attacking the carp , being significantly better than the exact methods created specifically for that problem . computational experiments obtained improved lower bounds for almost all open instances from the literature . several such instances could be solved to optimality . scope and purpose the scope of this paper is transforming arc routing problems into node routing problems . the paper shows that this approach can be effective and , in particular , that the original instances may generate node routing instances that behave as if the size is not increased . this result is obtained by slightly modifying the well known transformation by pearn , assad and golden from capacitated arc routing problem ( carp ) to the capacitated vehicle routing problem ( cvrp ) , that is regarded as unpractical . the paper provides a computational experience using a recent branch and cut and price algorithm for the cvrp . the results are significantly better than the exact methods created specifically for that problem , improving lower bounds for almost all open instances from the literature . several such instances could be solved to optimality .
combining two pheromone structures for solving the car sequencing problem with ant colony optimization . <eos> the car sequencing problem involves scheduling cars along an assembly line while satisfying capacity constraints . in this paper , we describe an ant colony optimization ( aco ) algorithm for solving this problem , and we introduce two different pheromone structures for this algorithm the first pheromone structure aims at learning for good sequences of cars , whereas the second pheromone structure aims at learning for critical cars . we experimentally compare these two pheromone structures , that have complementary performances , and show that their combination allows ants to solve very quickly most instances .
calibrating information users ' views on relevance a social representations approach . <eos> the purpose of this study is to investigate how information users view the concept of relevance and make their judgement ( s ) on relevant information through the framework of social representations theory . more specifically , this study attempts to address the questions of what users view as the constituent concepts of relevance , what are core and peripheral concepts of relevance , and how these concepts are structured by applying a structural analysis approach of social representations theory . we employ a free word association method for data collection . two hundred and forty four information users of public and academic libraries responded to questionnaires on their relevance judgement criteria . collected data were content analysed and assessed using weighted frequency , similarity measure , and core periphery measurements to identify key elements of relevance and to differentiate core and periphery elements of relevance . results show that four out of <digit> emerged elements ( concepts ) are core and <digit> are periphery elements of the concept of relevance . the findings of this study provide a quantitative measure of weighing various elements of relevance and the internal structure of the concept of relevance from users ' perspectives providing enhancements for search algorithms with quantitative metadata support .
constructing fault tolerant communication trees in hypercubes . <eos> a communication tree is a binomial tree embedded in a hypercube , whose communication direction is from its leaves to its root . if a problem to be solved is first divided into independent subproblems , then each subproblem can be solved by one of the hypercube processors , and all the subresults can be merged into the final results through tree communication . this paper uses two random search techniques , the genetic algorithm ( ga ) and simulated annealing ( sa ) , to construct fault tolerant communication trees with the minimum data transmission time . experimental evaluation shows that , with reasonably low search time , the proposed ga and sa approaches are able to find more desirable communication trees ( i.e. , trees with less data transmission time ) than the minimal cost approach can . a distributed approach which applies parallel search to communication subtrees in disjoint subcubes is also provided to reduce the search time of the proposed approaches .
the firekeepers aging considered as a resource . <eos> technology can improve the quality of life for elderly persons by supporting and facilitating the unique leadership roles that elderly play in groups , communities , and other organizations . elderly people are often organizational firekeepers . they maintain community memory , pass on organizational practices , and ensure social continuity . this paper reports studies of several essential community roles played by elderly community membersincluding the role of volunteer community webmasterand describes two positive design projects that investigated how technology can support new kinds of social endeavors and contributions to society by elderly citizens . finally , the paper speculates on the utility of intergenerational teams in strengthening societys workforce .
limited error based event localizing temporal decomposition and its application to variable rate speech coding . <eos> this paper proposes a novel algorithm for temporal decomposition ( td ) of speech , called limited error based event localizing temporal decomposition ( lebel td ) , and its application to variable rate speech coding . in previous work with td , td analysis was usually performed on each speech segment of about 200300ms or more , making it impractical for online applications . in this present work , the event localization is determined based on a limited error criterion and a local optimization strategy , which results in an average algorithmic delay of 65ms . simulation results show that an average log spectral distortion of about 1.5 db can be achievable at an event rate of 20events s. also , lebel td uses neither the computationally costly singular value decomposition routine nor the event refinement process , thus reducing significantly the computational cost of td . further , a method for variable rate speech coding an average rate of around 1.8 kbps based on straight ( speech transformation and representation using adaptive interpolation of weighted spectrum ) , which is a high quality speech analysissynthesis framework , using lebel td is also realized . subjective test results indicate that the performance of the proposed speech coding method is comparable to that of the 4.8 kbps fs <digit> celp coder .
online and offline social ties of social network website users an exploratory study in eleven societies . <eos> this study presents results of a survey about social network website ( snw ) usage that was administered to university students in china , egypt , france , israel , india , korea , macao , sweden , thailand , turkey , and the united states . the offline and online social ties of snw users were examined by nationality , levels of individualism collectivism ( i c ) , gender , snw usage , age , and access location . contrary to existing literature , we found no differences in the number of offline friends between individualist and collectivist nations . similarly , there was not a difference in the number of online social ties between individualist and collectivist nations . however , members of collectivist nations had significantly more online social ties never met in person . heavy snw users in individualist nations maintained significantly higher numbers of offline social ties however , heavy snw users in collectivist nations did not have higher numbers of offline social ties . related implications and recommendations are provided .
pair wise path key establishment in wireless sensor networks . <eos> when sensor networks deployed in unattended and hostile environments , for securing communication between sensors , secret keys must be established between them . many key establishment schemes have been proposed for large scale sensor networks . in these schemes , each sensor shares a secret key with its neighbors via preinstalled keys . but it may occur that two end nodes which do not share a key with each other could use a secure path to share a secret key between them . however during the transmission of the secret key , the secret key will be revealed to each node along the secure path . several researchers proposed a multi path key establishment to prevent a few compromised sensors from knowing the secret key , but it is vulnerable to stop forwarding or byzantine attacks . to counter these attacks , we propose a hop by hop authentication scheme for path key establishment to prevent byzantine attacks . compared to conventional protocols , our proposed scheme can mitigate the impact of malicious nodes from doing a byzantine attack and sensor nodes can identify the malicious nodes . in addition , our scheme can save energy since it can detect and filter false data not beyond two hops . ( c ) <digit> elsevier b.v. all rights reserved .
affect aware behaviour modelling and control inside an intelligent environment . <eos> the evidence suggests that human actions are supported by emotional elements that complement logic inference in our decision making processes . in this paper an exploratory study is presented providing initial evidence of the positive effects of emotional information on the ability of intelligent agents to create better models of user actions inside smart homes . preliminary results suggest that an agent incorporating valence based emotional data into its input array can model user behaviour in a more accurate way than agents using no emotion based data or raw data based on physiological changes .
a new dsmt combination rule in open frame of discernment and its application . <eos> a new combination rule based on dezert smarandache theory ( dsmt ) is proposed to deal with the conflict evidence resulting from the non exhaustivity of the discernment frame . a two dimensional measure factor in dempster shafer theory ( dst ) is extended to dsmt to judge the conflict degree between evidence . the original dsmt combination rule or new dsmt combination rule can be selected for fusion according to this degree . finally , some examples in simultaneous fault diagnosis of motor rotor are given to illustrate the effectiveness of the proposed combination rule .
trading off computation for error in providing immersive voice communications for mobile gaming . <eos> the interactive experiences of players in networked games can be enhanced with the provision of an immersive voice communication service . game players are immersed in their voice communication experience as they exchange live voice streams which are rendered in real time with directional and distance cues corresponding to the users ' positions in the virtual game world . in particular , we propose a mobile immersive communication environment ( mice ) which targets mobile game players using platforms such as sony psp and nintendo ds . a computation reduction scheme was proposed in our previous work for the scalable delivery of mice from a central server . on the basis of that computation reduction scheme , this paper identifies what factors , and to what extent , affect the unacceptable voice rendering error incurred when providing mice . in the first experimental scenario , we investigate the level of unacceptable voice rendering error incurred in mice for different avatar densities or avatar population sizes , with a fixed level of processing limit . in the second experimental scenario , we studied the level of unacceptable voice rendering error incurred in mice for different processing resource limits , with a fixed avatar population size or avatar density . our findings provide important insights into the planning and dimensioning of processing resources for the support of mice , with due considerations to the impact on the unacceptable voice rendering error incurred .
a scalable heuristic for evacuation planning in large road network . <eos> evacuation planning is of critical importance for civil authorities to prepare for natural disasters , but efficient evacuation planning in large city is computationally challenging due to the large number of evacuees and the huge size of transportation networks . one recently proposed algorithm capacity constrained route planner ( ccrp ) can give sub optimal solution with good accuracy in less time and use less memory compared to previous approaches . however , it still can not scale to large networks . in this paper , we analyze the overhead of ccrp and come to a new heuristic ccrp that scalable to large network . our algorithm can reuse search results in previous iterations and avoid the repetitive global shortest path expansion in ccrp . we conducted extensive experiments with real world road networks and different evacuation parameter settings . the result shows it can gives great speed up without loosing the optimality .
battery energy storage system for frequency support in microgrids and with enhanced control features for uninterruptible supply of local loads . <eos> a battery energy storage system to support the frequency in autonomous microgrids . original frequency controller to better damp the frequency oscillations . the frequency controller covers the main two control levels , namely primary and secondary . enhanced control functions to ensure uninterruptible power supply to local sensitive loads . simulations and experimental results validate the proposed control solution .
extended average magnitude difference function based pitch detection . <eos> this paper presents a new extended average magnitude difference function for noise robust pitch detection . average magnitude difference function based algorithms are suitable for real time operations , but suffer from incorrect pitch detection in noisy conditions . the proposed new extended average magnitude difference function involves in sufficient number of averaging for all lag values compared to the original average magnitude difference function , and thereby eliminates the falling tendency of the average magnitude difference function without emphasizing pitch harmonics at higher lags , which is a severe limitation of other existing improvements of the average magnitude difference function . a noise robust post processing that explores the contribution of each frequency channel is also presented . experimental results on keele pitch database in different noise level , both with white and color noise , shows the superiority of the proposed extended average magnitude difference function based pitch detection method over other methods based on average magnitude difference function .
multi core demands multi interfaces . <eos> the challenge for the microarchitect has always been ( with very few notable domain specific exceptions ) how to translate the continually increasing processing power provided by moore 's law into increased performance , or more recently into similar performance at lower cost in energy . the mechanisms in the past ( almost entirely ) kept the interface intact and used the increase in transistor count to improve the performance of the microarchitecture of the uniprocessor . when that became too hard , we went to larger and larger on chip caches . both are consistent with the notion that abstractions are good . at some point , we got overwhelmed with too many transistors predictably , multi core was born . as the transistor count continues to skyrocket , we are faced with two questions what should be on the chip , and how should the software interface to it . if we expect to continue to take advantage of what process technology is providing , i think we need to do several things , starting with rethinking the notion of abstraction and providing multiple interfaces for the programmer .
decisions , decisions , decisions transfer and specificity of decision making skill between sports . <eos> the concept of transfer of learning holds that previous practice or experience in one task or domain will enable successful performance in another related task or domain . in contrast , specificity of learning holds that previous practice or experience in one task or domain does not transfer to other related tasks or domains . the aim of the current study is to examine whether decision making skill transfers between sports that share similar elements , or whether it is specific to a sport . participants ( n <digit> ) completed a video based temporal occlusion decision making test in which they were required to decide on which action to execute across a series of <digit> versus <digit> soccer game situations . a sport engagement questionnaire was used to identify <digit> soccer players , <digit> other invasion sport players and <digit> other sport players . positive transfer of decision making skill occurred between soccer and other invasion sports , which are related and have similar elements , but not from volleyball , supporting the concept of transfer of learning .
an approach of creative application evolution on cloud computing platform . <eos> cloud computing is a paradigm that focuses on sharing data and computing resources over a scalable network of nodes , so it is becoming a preferred environment for those applications with large scalability , dynamic collaboration and elastic resource requirements . creative computing is an emerging research field in these applications , which can be considered as the study of computer science and related technologies and how they are applied to support creativity , take part in creative processes , and solve creativity related problems . however , it is a very hard work to develop such applications from the very beginning under new environment , while it is a big waste for legacy systems under existing environment . now software evolution plays an important role . in this paper , we introduced creative computing firstly , including definition , properties and requirements . then the advantages of cloud computing platform for supporting creative computing were analysed . next , a private cloud as experimental environment was built . finally , the process of creative application evolution was illustrated . our work is about research and application of software evolution methodology , also is an exploratory try to do creative computing research under cloud environment .
a sensitivity based approach to analyzing signal delay uncertainty of coupled interconnects . <eos> performance optimization is a critical step in the design of integrated circuits . rapid advances in very large scale integration ( vlsi ) technology have enabled shrinking feature sizes , wire widths , and wire spacings , making the effects of coupling capacitance more apparent . as signals switch faster , noise due to coupling between neighboring wires becomes more pronounced . changing the relative signal arrival times ( rsats ) alters the victim line delay due to the varying coupling noise on the victim line . the authors propose a sensitivity based method to analyze delay uncertainties of coupled interconnects due to uncertain signal arrival times at its inputs . compared to existing methods of analyzing delay uncertainties of coupled interconnects , the simulation results show that the proposed method strikes a good balance between model accuracy and complexity compared to the existing approaches .
visual estimation of pointed targets for robot guidance via fusion of face pose and hand orientation . <eos> problem formulation given a number of possible pointed targets , compute the target that the user points to . estimate head pose by visually tracking the off plane rotations of the face . recognize two different hand pointing gestures ( point left and point right ) . model the problem using the dempstershafer theory of evidence . use demspsters rule of combination to fuse information and derive the pointed target .
rtn distribution comparison for bulk , fdsoi and finfets devices . <eos> in this paper we investigate the sensitivity of rtn noise spectra to statistical variability alone and in combination with variability in the traps properties , such as trap level and trap activation energy . by means of 3d statistical simulation , we demonstrate the latter to be mostly responsible for noise density spectra dispersion , due to its large impact on the rtn characteristic time . as a result finfets devices are shown to be slightly more sensitive to rtn than fdsoi devices . in comparison bulk mosfets are strongly disadvantaged by the statistical variability associated with high channel doping .
learning similarity matching in multimedia content based retrieval . <eos> many multimedia content based retrieval systems allow query formulation with user setting of relative importance of features ( e.g. , color , texture , shape , etc ) to mimic the user 's perception of similarity . however , the systems do not modify their similarity matching functions , which are defined during the system development . in this paper , we present a neural network based learning algorithm for adapting similarity matching function toward the user 's query preference based on his her relevance feedback . the relevance feedback is given as ranking errors ( misranks ) between the retrieved and desired lists of multimedia objects . the algorithm is demonstrated for facial image retrieval using the nist mugshot identification database with encouraging results .
self organization of decentralized swarm agents based on modified particle swarm algorithm . <eos> in this paper , an attempt has been made by incorporating some special features in the conventional particle swarm optimization ( pso ) technique for decentralized swarm agents . the modified particle swarm algorithm ( mpsa ) for the self organization of decentralized swarm agents is proposed and studied . in the mpsa , the update rule of the best agent in swarm is based on a proportional control concept and the objective value of each agent is evaluated on line . in this scheme , each agent self organizes to flock to the best agent in swarm and migrate to a moving target while avoiding collision between the agent and the nearest obstacle agent . to analyze the dynamics of the mpsa , stability analysis is carried out on the basis of the eigenvalue analysis for the time varying discrete system . moreover , a guideline about how to tune the mpsa 's parameters is proposed . the simulation results have shown that the proposed scheme effectively constructs a self organized swarm system in the capability of flocking and migration .
computing large deformation metric mappings via geodesic flows of diffeomorphisms . <eos> this paper examine the euler lagrange equations for the solution of the large deformation diffeomorphic metric mapping problem studied in dupuis et al. ( <digit> ) and trouve ( <digit> ) in which two images <digit> , <digit> , are given and connected via the diffeomorphic change of coordinates i <digit> o phi ( <digit> ) i , where p <digit> is the end point at t <digit> of curve phi ( t ) , t is an element of <digit> , <digit> satisfying ( phi ) over dot ( t ) v ( t ) ( phi ( t ) ) , t is an element of <digit> , <digit> with phi ( <digit> ) id . the variational problem takes the form graphics where parallel tov ( t ) parallel to ( v ) is an appropriate sobolev norm on the velocity field v ( t ) ( . ) , and the second term enforces matching of the images with parallel to.parallel to ( l2 ) representing the squared error norm . in this paper we derive the euler lagrange equations characterizing the minimizing vector fields vt ( , ) t is an element of <digit> , <digit> assuming sufficient smoothness of the norm to guarantee existence of solutions in the space of diffeomorphisms . we describe the implementation of the euler equations using semi lagrangian method of computing particle flows and show the solutions for various examples . as well , we compute the metric distance on several anatomical configurations as measured by integral ( <digit> ) ( <digit> ) parallel tov ( t ) parallel to ( v ) dt on the geodesic shortest paths .
arvo cl the opencl version of the arvo package an efficient tool for computing the accessible surface area and the excluded volume of proteins via analytical equations . <eos> introduction of graphical processing units ( gpus ) and computing using gpus in recent years opened possibilities for simple parallelization of programs . in this update , we present the modernized version of program arvo j. busa , j. dzurina , e. hayryan , s. hayryan , c k. hu , j. plavka , i. pokorny , j. skivanek , m. c. wu , comput . phys . comm . <digit> ( <digit> ) <digit> . the whole package has been rewritten in the c language and parallelized using opencl . some new tricks have been added to the algorithm in order to save memory much needed for efficient usage of graphical cards . a new tool called ' input_structure ' was added for conversion of pdb files into files suitable for work with the c and opencl version of arvo . new version program summary program title arvo cl catalog identifier adul_v2_0 program summary url http cpc.cs.qub.ac.uk summaries adul_v2_0.html program obtainable from cpc program library , queen 's university , belfast , n. ireland licensing provisions standard cpc licence , http cpc.cs.qub.ac.uk licence licence.html no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language c. opencl . computer pc pentium spp ' <digit> . operating system all opencl capable systems . has the code been vectorized or parallelized parallelized using gpus . a serial version ( non gpu ) is also included in the package . classification <digit> . external routines cl.hpp ( http www.khronos.org registry cl api 1.1 cl.hpp ) catalog identifier of previous version adul_v1_0 journal reference of previous version comput . phys . comm . <digit> ( <digit> ) <digit> does the new version supercede the previous version yes
three dimensional near field mimo array imaging using range migration techniques . <eos> this paper presents a <digit> d near field imaging algorithm that is formulated for <digit> d wideband multiple input multiple output ( mimo ) imaging array topology . the proposed mimo range migration technique performs the image reconstruction procedure in the frequency wavenumber domain . the algorithm is able to completely compensate the curvature of the wavefront in the near field through a specifically defined interpolation process and provides extremely high computational efficiency by the application of the fast fourier transform . the implementation aspects of the algorithm and the sampling criteria of a mimo aperture are discussed . the image reconstruction performance and computational efficiency of the algorithm are demonstrated both with numerical simulations and measurements using <digit> d mimo arrays . real time <digit> d near field imaging can be achieved with a real aperture array by applying the proposed mimo range migration techniques .
imprint lithography for flexible transparent plastic substrates . <eos> a novel imprinting process has been developed for the use of resist pattern transfer on flexible transparent plastic substrates . the polymer resist was first spin coated on the mold , which was treated with a release agent . after softbaking , the resist layer was attached to a plastic substrate coated with an adhesive . the patterns were completely transferred to the substrate after removing the mold . using this method , we were able to obtain the desired patterns on the plastic substrate without heating the substrate , which could deform the substrate .
efficient routing of subspace skyline queries over highly distributed data . <eos> data generation increases at highly dynamic rates , making its storage , processing , and update costs at one central location excessive . the p2p paradigm emerges as a powerful model for organizing and searching large data repositories distributed over independent sources . advanced query operators , such as skyline queries , are necessary in order to help users handle the huge amount of available data . a skyline query retrieves the set of nondominated data points in a multidimensional data set . skyline query processing in p2p networks poses inherent challenges and demands nontraditional techniques , due to the distribution of content and the lack of global knowledge . relying on a superpeer architecture , we propose a threshold based algorithm , called skypeer and its variants , for efficient computation of skyline points in arbitrary subspaces , while reducing both computational time and volume of transmitted data . furthermore , we address the problem of routing skyline queries over the superpeer network and we propose an efficient routing mechanism , namely skypeer ( ) , which further improves the performance by reducing the number of contacted superpeers . finally , we provide an extensive experimental evaluation showing that our approach performs efficiently and provides a viable solution when a large degree of distribution is required .
parallel processing in regional climatology the parallel version of the karlsruhe atmospheric mesoscale model ( kamm ) . <eos> simultaneously to improvements of computer performance and of availability of memory not only the resolution of meteorological models of atmospheric currents has been refined but also the accuracy of the necessary physical approximations has been improved more and more . now full elastic models are developed which describe also sound waves , although sound processes are not supposed to be relevant for atmospheric flow phenomena . but the full set of the elastic navier stokes equations has a quite simple structure in comparison to sound proved systems like anelastically approximated models , so that the corresponding numerical models can be implemented on parallel computer systems without too much efforts . this has been considered by the redesign of the karlsruhe atmospheric mesoscale model ( kamm ) for parallel processing . the new full elastic version of this model is written in fortran <digit> . the necessary communication operations are gathered into few functions of a communication library , which is designed for different computer architectures , for massive parallel systems , for parallel vector computers requiring long vectors , but also for mono processors . ( c ) <digit> elsevier science b.v. all rights reserved .
x ray scattering processes and chemometrics for differentiating complex samples using conventional edxrf equipment . <eos> mild variations in organic matrices , which are investigated in this work , are caused by alterations in x ray raman scattering . the multivariate approaches , principal component analysis ( pca ) and hierarchical cluster analysis ( hca ) , are applied to visualize these effects . conventional energy dispersive x ray fluorescence equipment is used , where organic compounds produce intense scattering of the x ray source . x ray raman processes , before obtained only for solid samples using synchrotron radiation , are indirectly visualized here through pca scores and hca cluster analysis , since they alter the compton and rayleigh scattering . as a result , their influences can be seen in known sample characteristics , as those associated with gender and melanin in dog hairs , and the differentiation in coconut varieties . chemometrics has shown that , despite their complexity , natural samples can be easily classified . ( c ) <digit> elsevier b.v. all rights reserved .
a deterministic time algorithm for the reeb graph . <eos> we present a deterministic algorithm to compute the reeb graph of a pl real valued function on a simplicial complex in time , where is the size of the <digit> skeleton . the problem can be solved using dynamic graph connectivity . we obtain the running time by using offline graph connectivity which assumes that the deletion time of every arc inserted is known at the time of insertion . the algorithm is implemented and experimental results are given . in addition , we reduce the offline graph connectivity problem to computing the reeb graph .
advances in pcb routing . <eos> due to rapid increases in printed circuit board ( pcb ) complexity and lack of research progresses in pcb routing algorithms over the years , routing has become a bottleneck in overall circuit board design time . today , a high end pcb typically takes significant tedious manual efforts to complete the wiring and this problem will only get worse for future generations of pcbs . in this talk , we present some of our recent research results on this problem .
a test of two models of value creation in virtual communities . <eos> does a firm get any extra value from investing resources in sponsoring its own virtual community above and beyond the value that could be created for the firm , indirectly , via customer initiated communities if so , what explains the extra value derived from a firm sponsored virtual community and how might this understanding inform managers about appropriate strategies for leveraging virtual communities as part of a value creating strategy for the firm we test two models of virtual community to help shed light on the answers to these questions . we hypothesize that in customer initiated virtual communities , three attributes of member generated information ( mgi ) drive value , while in firm sponsored virtual communities , a sponsoring firm 's efforts , as well as mgi , drive value . drawing on information search and processing theories , and developing new measures of three attributes of mgi ( consensus , consistency , and distinctiveness ) , we surveyed <digit> consumers across numerous communities . we find that value can emerge via both models , but that in a firm sponsored model , a sponsor 's efforts are more powerful than mgi and have a positive , direct effect on the trust building process . our results suggest a continuum of value creation whereby firms extract greater value as they migrate toward the firm sponsored model .
performance optimization and modeling of blocked sparse kernels . <eos> we present a method for automatically selecting optimal implementations of sparse matrix vector operations . our software accels ( accelerated compress storage elements for linear solvers ) involves a setup phase that probes machine characteristics , and a run time phase where stored characteristics are combined with a measure of the actual sparse matrix to find the optimal kernel implementation . we present a performance model that is shown to be accurate over a large range of matrices .
parameter estimation of two level nonlinear mixed effects models using first order conditional linearization and the em algorithm . <eos> multi level nonlinear mixed effects ( ml nlme ) models have received a great deal of attention in recent years because of the flexibility they offer in handling the repeated measures data arising from various disciplines . in this study , we propose both maximum likelihood and restricted maximum likelihood estimations of ml nlme models with two level random effects , using first order conditional expansion ( foce ) and the expectationmaximization ( em ) algorithm . the foceem algorithm was compared with the most popular lindstrom and bates ( lb ) method in terms of computational and statistical properties . basal area growth series data measured from chinese fir ( cunninghamia lanceolata ) experimental stands and simulated data were used for evaluation . the foceem and lb algorithms given the same parameter estimates and fit statistics for models that converged by both . however , foceem converged for all the models , while lb did not , especially for the models in which two level random effects are simultaneously considered in several base parameters to account for between group variation . we recommend the use of foceem in ml nlme models , particularly when convergence is a concern in model selection .
multi scale modelling of sandwich structures using the arlequin method part i linear modelling . <eos> the paper presents an arlequin based multi scale method for studying problems related to the mechanical behaviour of sandwich composite structures . towards this end , different models are mixed and glued to each other . several coupling operators are tested in order to assess the usefulness of the proposed approach . a new coupling operator is proposed and tested on the different glued arlequin zones . a freeclamped sandwich beam with soft core undergoing a concentrated effort on the free edge is used as a typical example ( benchmark ) in the validation procedure . numerical simulations were conducted as the preliminary evaluation of the various coupling operators and the discrepancies between local and global models in the gluing zone have been addressed with sufficient care .
determinates of eis acceptance . <eos> the large number of organizations developing executive information systems ( eiss ) highlights the importance of understanding why executives use these systems . this survey investigated how ease of use , the number of features , and support staff characteristics are related to eis acceptance . acceptance was measured by the percentage of the targeted users who incorporate the eis into their daily routine . high usage was not associated with ease of use , a large number of features , or the staff being physically close to the users . however , rapid development time was positively correlated with acceptance . higher numbers of available features were associated with larger support staffs and larger user groups . the number of users was positively correlated with both staff size and eis age . existing eiss place a stronger emphasis on reporting internal rather than external data .
sift based non blind watermarking robust to non linear geometrical distortions . <eos> this paper presents a non blind watermarking technique that is robust to non linear geometric distortion attacks . this is one of the most challenging problems for copyright protection of digital content because it is difficult to estimate the distortion parameters for the embedded blocks . in our proposed scheme , the location of the blocks are recorded by the translation parameters from multiple scale invariant feature transform ( sift ) feature points . this method is based on two assumptions sift features are robust to non linear geometric distortion and even such non linear distortion can be regarded as linear distortion in local regions . we conducted experiments using 149,800 images ( <digit> standard images and <digit> images downloaded from flickr , <digit> different messages , <digit> different embedding block patterns , and <digit> attacks ) . the results show that the watermark detection performance is drastically improved , while the baseline method can achieve only chance level accuracy .
zoom navigation exploring large information and application spaces . <eos> we present the concept of zoom navigation , a new interaction paradigm to cope with visualization and navigation problems as found in large information and application spaces . it is based on the pluggable zoom , an object oriented component derived from the variable zoom fisheye algorithm.working with a limited screen space we apply a degree of interest ( doi ) function to guide the level of detail used in presenting information . furthermore we determine the user 's information and navigation needs by analysing the interaction history . this leads to the definition of the aspect of interest ( aoi ) function . the aoi is evaluated in order to choose one of the several information aspects , under which an item can be studied . this allows us to change navigational affordance and thereby enhance navigation.in this paper we describe the ideas behind the pluggable zoom and the definition of doi and aoi functions . the application of these functions is demonstrated within two case studies , the zoom illustrator and the zoom navigator . we discuss our experience with these implemented systems .
fits a finite time reputation system for cooperation in wireless ad hoc networks . <eos> a wireless ad hoc network does not have an infrastructure , and thus , needs the cooperation of nodes in forwarding other nodes ' packets . reputation system is an effective approach to give nodes incentives to cooperate in packet forwarding . however , existing reputation systems either lack rigorous analysis , or have analysis in unrealistic models . in this paper , we propose fits , the first reputation system that has rigorous analysis and guaranteed incentive compatibility in a practical model . fits has two schemes the first scheme is very simple , but needs a perceived probability assumption ( ppa ) the second scheme uses more sophisticated techniques to remove the need for ppa . we show that both of these two fits schemes have a subgame perfect nash equilibrium in which the packet forwarding probability of every node is one . experimental results verify that fits provides strong incentives for nodes to cooperate .
developing a verbal protocol method for collecting and analysing reports of workers thoughts during manual handling tasks . <eos> concurrent and retrospective verbal protocol methods were used to collect thoughts from <digit> participants during a manual handling task involving the repeated transfer of loads between locations at two tables . the effectiveness of qualitative and quantitative methods of analysing the reported information was tested in the study . a simple taxonomy was developed to investigate the content of the reports ( including reports on postures and loads ) and determine how the participants approached the task ( whether they made plans , described actions or evaluated their completion of the task ) . references to posture were obtained in the verbal protocol reports , indicating that the participants had some awareness of their postures during parts of the task . there were similarities in the content of the concurrent and retrospective reports , but there were differences in the amount of detail between the methods and differences in the way the reports were constructed . there could be some scope for developing the quantitative analysis of the frequencies of references to classes of information , though this can only be recommended for concurrent reports on tasks of short duration . the analyses of qualitative data gave a deeper insight into the reports , such as identifying factors that can be important when planning to handle a load , or illustrating how participants can change their focus of attention periodically throughout the task . the relative strengths of the concurrent and retrospective methods are described , along with ideas for improving the quality of information collected in future studies . a number of potential problems with the interpretation of the reported information are explained .
fast and flexible instruction selection with on demand tree parsing automata . <eos> tree parsing as supported by code generator generators like beg , burg , iburg , lburg and ml burg is a popular instruction selection method . there are two existing approaches for implementing tree parsing dynamic programming , and tree parsing automata each approach has its advantages and disadvantages . we propose a new implementation approach that combines the advantages of both existing approaches we start out with dynamic programming at compile time , but at every step we generate a state for a tree parsing automaton , which is used the next time a tree matching the state is found , turning the instruction selector into a fast tree parsing automaton . we have implemented this approach in the gforth code generator . the implementation required little effort and reduced the startup time of gforth by up to a factor of 2.5 .
challenges on preserving scientific data with data grids . <eos> the emerging context of e science imposes new scenarios and requirements for digital preservation . in particular , the data must be reliably stored , for which redundancy is a key strategy . but managing redundancy must take into account the potential failure of component . considering that correlated failures can affect multiple components and potentially cause a complete loss of data , we propose an innovative solution to manage redundancy strategies in heterogeneous environments such as data grids . this solution comprises a simulator that can be used to evaluate redundancy strategies according to preservation requirements and supports the process to design the best architecture to be deployed , which can latter be used as an observer of the deployed system , supporting its monitoring and management .
automated test order generation for software component integration testing . <eos> the order in which software components are tested can have a significant impact on the number of stubs required during component integration testing . this paper presents an efficient approach that applies heuristics based on a given software component test dependency graph to automatically generate a test order that requires a ( near ) minimal number of test stubs . thus , the approach reduces testing effort and cost . the paper describes the proposed approach , analyses its complexity and illustrates its use . comparison with three well known graph based approaches , for a real world software application , shows that only the classic le traon et al. s approach and ours give an optimal number of stubs . however , experiments on randomly simulated dependency models with <digit> to 10,000 components show that our approach has a significant performance advantage with a reduction in the average running time of 96.01 % .
inhomogeneous and self organized temperature in schelling ising model . <eos> the schelling model of <digit> is a complicated version of a square lattice ising model at zero temperature , to explain urban segregation , based on the neighbor preferences of the residents , without external reasons . various versions between ising and schelling models give about the same results . inhomogeneous temperatures t do not change the results much , while a feedback between segregation and t leads to a self organization of an average t.
an exploratory study of architectural effects on requirements decisions . <eos> the question of the manner in which an existing software architecture affects requirements decision making is considered important in the research community however , to our knowledge , this issue has not been scientifically explored . we do not know , for example , the characteristics of such architectural effects . this paper describes an exploratory study on this question . specific types of architectural effects on requirements decisions are identified , as are different aspects of the architecture together with the extent of their effects . this paper gives quantitative measures and qualitative interpretation of the findings . the understanding gained from this study has several implications in the areas of project planning and risk management , requirements engineering ( re ) and software architecture ( sa ) technology , architecture evolution , tighter integration of re and sa processes , and middleware in architectures . furthermore , we describe several new hypotheses that have emerged from this study , that provide grounds for future empirical work . this study involved six re teams ( of university students ) , whose task was to elicit new requirements for upgrading a pre existing banking software infrastructure . the data collected was based on a new meta model for requirements decisions , which is a bi product of this study . ( c ) <digit> elsevier inc. all rights reserved .
a direction of arrival estimation method for spatial optical beam forming network . <eos> spatial optical beam forming network ( obfn ) is a superior structure than traditional ones in bandwidth adaptability , system complexity , and so forth . compared with conventional beam forming network , the output signal model of obfn is different , making the previous direction of arrival ( doa ) estimation methods improper for this structure . at present , doa estimation method for this structure has not been sufficiently explored and there is no efficient algorithm . in this paper , the observation model of the network is established first , and then a new doa estimation method is proposed . the new method makes use of the amplitude distribution of the fiber array to achieve direction finding . sufficient numerical simulations are carried out to demonstrate the feasibility and efficiency of the proposed algorithm .
on central algorithms of approximation under fuzzy information . <eos> we consider the problem of approximation of an operator by information described by n real characteristics in the case when this information is fuzzy . we develop the well known idea of an optimal error method of approximation for this case . it is a method whose error is the infimum of the errors of all methods for a given problem characterized by fuzzy numbers in this case . we generalize the concept of central algorithms , which are always optimal error algorithms and in the crisp case are useful both in practice and in theory . in order to do this we define the centre of an l fuzzy subset of a normed space . the introduced concepts allow us to describe optimal methods of approximation for linear problems using balanced fuzzy information . ( c ) <digit> elsevier b.v. all rights reserved .
prediction of currents and sea surface elevation in the gulf of california from tidal to seasonal scales . <eos> a web tool that provides currents and or sea surface elevation in the gulf of california is presented . the above variables are reconstructed from harmonic constants obtained from harmonic analyses of time series produced by a 3d baroclinic numerical model of the gulf . the numerical model was forced ( <digit> ) at the gulf 's mouth by the tides and the hydrographic variability of the pacific ocean ( at semiannual and annual frequencies ) , and ( <digit> ) at the gulf 's surface by winds , heat and fresh water fluxes ( also at the semiannual and annual frequencies ) . the response to these forcings results in motions with time scales limited to semidiurnal and diurnal , fortnightly and monthly ( due to nonlinear interactions of the tidal components ) , and semiannual and annual frequencies ( due to the nontidal forcing ) .
surpassing the fractional derivative concept of the memory dependent derivative . <eos> enlightened by the caputo type of fractional derivative , here we bring forth a concept of memory dependent derivative , which is simply defined in an integral form of a common derivative with a kernel function on a slipping interval . in case the time delay tends to zero it tends to the common derivative . high order derivatives also accord with the first order one . comparatively , the form of kernel function for the fractional type is fixed , yet that of the memory dependent type can be chosen freely according to the necessity of applications . so this kind of definition is better than the fractional one for reflecting the memory effect ( instantaneous change rate depends on the past state ) . its definition is more intuitionistic for understanding the physical meaning and the corresponding memory dependent differential equation has more expressive force .
deriving robust counterparts of nonlinear uncertain inequalities . <eos> in this paper we provide a systematic way to construct the robust counterpart of a nonlinear uncertain inequality that is concave in the uncertain parameters . we use convex analysis ( support functions , conjugate functions , fenchel duality ) and conic duality in order to convert the robust counterpart into an explicit and computationally tractable set of constraints . it turns out that to do so one has to calculate the support function of the uncertainty set and the concave conjugate of the nonlinear constraint function . conveniently , these two computations are completely independent . this approach has several advantages . first , it provides an easy structured way to construct the robust counterpart both for linear and nonlinear inequalities . second , it shows that for new classes of uncertainty regions and for new classes of nonlinear optimization problems tractable counterparts can be derived . we also study some cases where the inequality is nonconcave in the uncertain parameters .
on the weight distribution of terminated convolutional codes . <eos> in this correspondence , the low weight terms of the weight distribution of the block code obtained by terminating a convolutional code after x information blocks are expressed as a function of x. it is shown that this function is linear in x for codes with noncatastrophic encoders , but quadratic in x for codes with catastrophic encoders , these results are useful to explain the poor performance of convolutional codes with a catastrophic encoder at low to medium signal to noise ratios .
common data model for natural language processing based on two existing standard information models cda graf . <eos> an increasing need for collaboration and resources sharing in the natural language processing ( nlp ) research and development community motivates efforts to create and share a common data model and a common terminology for all information annotated and extracted from clinical text . we have combined two existing standards the hl7 clinical document architecture ( cda ) , and the iso graph annotation format ( graf in development ) , to develop such a data model entitled cda graf . we experimented with several methods to combine these existing standards , and eventually selected a method wrapping separate cda and graf parts in a common standoff annotation ( i.e. , separate from the annotated text ) xml document . two use cases , clinical document sections , and the <digit> i2b2 va nlp challenge ( i.e. , problems , tests , and treatments , with their assertions and relations ) , were used to create examples of such standoff annotation documents , and were successfully validated with the xml schemata provided with both standards . we developed a tool to automatically translate annotation documents from the <digit> i2b2 va nlp challenge format to graf , and automatically generated <digit> annotation documents using this tool , all successfully validated . finally , we adapted the xsl stylesheet provided with hl7 cda to allow viewing annotation xml documents in a web browser , and plan to adapt existing tools for translating annotation documents between cda graf and the uima and gate frameworks . this common data model may ease directly comparing nlp tools and applications , combining their output , transforming and translating annotations between different nlp applications , and eventually plug and play of different modules in nlp applications .
labelings and encoders with the uniform bit error property with applications to serially concatenated trellis codes . <eos> the well known uniform error property for signal constellations and codes is extended to encompass information bits . we introduce a class of binary labelings for signal constellations , called bit geometrically uniform ( bgu ) labelings , for which the uniform bit error property holds , i.e. , the bit error probability does not depend on the transmitted signal . strong connections between the symmetries of constellations and binary hamming spaces are involved . for block coded modulation ( bcm ) and trellis coded modulation ( tcm ) euclidean space codes , bgu encoders are introduced and studied . the properties of bgu encoders prove quite useful for the analysis and design of codes aimed at minimizing the bit , rather than symbol , error probability . applications to the analysis and the design of serially concatenated trellis codes are presented , together with a case study which realizes a spectral efficiency of <digit> b s hz .
stochastic analysis of packet pair probing for network bandwidth estimation . <eos> in this paper , we perform a stochastic analysis of the packet pair technique , which is a widely used method for estimating the network bandwidth in an end to end manner . there has been no explicit delay model of the packet pair technique primarily because the stochastic behavior of a packet pair has not been fully understood . our analysis is based on a novel insight that the transient analysis of the g d <digit> system can accurately describe the behavior of a packet pair , providing an explicit stochastic model . we first investigate a single hop case and derive an analytical relationship between the input and the output probing gaps of a packet pair . using this single hop model , we provide a multi hop model under an assumption of a single tight link . our model shows the following two important features of the packet pair technique ( i ) the difference between the proposed model and the previous fluid model becomes significant when the input probing gap is around the characteristic value . ( ii ) the available bandwidth of any link after the tight link is not observable . we verify our model via ns <digit> simulations and empirical results . we give a discussion on recent packet pair models in relation to the proposed model and show that most of them can be regarded as special cases of the proposed model .
development of a teleoperation system for agricultural vehicles . <eos> a teleoperation system for a hydro static transmission ( hst ) drive crawler type robotic vehicle is described in this paper . the system was developed to satisfy the needs of various farm operations and teleoperation in unknown agricultural fields . the controller has a layered architecture and supports two degrees of cooperation between the operator and robot , direct and supervisory control . the vehicle can travel autonomously by using an rtk gps and a fiber optic gyroscope during supervisory control , and the operator interface also provides a field navigator based on google map technology . the vehicle 's position and heading direction was capable of 1hz update using precise satellite image maps . the results of field tests using direct control showed that it is difficult for the operator to control the movement of the vehicle along the target lines . on the other hand , the vehicle could travel in a straight line with a maximum lateral error of 0.3 m by using supervisory control .
a general audio classifier based on human perception motivated model . <eos> the audio channel conveys rich clues for content based multimedia indexing . interesting audio analysis includes , besides widely known speech recognition and speaker identification problems , speech music segmentation , speaker gender detection , special effect recognition such as gun shots or car pursuit , and so on . all these problems can be considered as an audio classification problem which needs to generate a label from low audio signal analysis . while most audio analysis techniques in the literature are problem specific , we propose in this paper a general framework for audio classification . the proposed technique uses a perceptually motivated model of the human perception of audio classes in the sense that it makes a judicious use of certain psychophysical results and relies on a neural network for classification . in order to assess the effectiveness of the proposed approach , large experiments on several audio classification problems have been carried out , including speech music discrimination in radio tv programs , gender recognition on a subset of the switchboard database , highlights detection in sports videos , and musical genre recognition . the classification accuracies of the proposed technique are comparable to those obtained by problem specific techniques while offering the basis of a general approach for audio classification .
stability routing with constrained path length for improved routability in dynamic manets . <eos> quality of service ( qos ) routing is known to be an np hard problem in case of two or more additive constraints , and several exact algorithms and heuristics have been proposed to address this issue . in this paper , we consider a particular two constrained quality of service routing problem maximizing path stability with a limited path length in the quest of improving routability in dynamic multi hop mobile wireless ad hoc networks . first , we propose a novel exact algorithm to solve the optimal weight constrained path problem . we instantiate our algorithm to solve the most stable path not exceeding a certain number of hops , in polynomial time . this algorithm is then applied to the practical case of proactive routing in dynamic multi hop wireless ad hoc networks . in these networks , an adequate compromise between route stability and its length in hops is essential for appropriately mitigating the impact of the network dynamics on the validity of established routes . secondly , we set up a common framework for the comparison between three families of proactive routing the shortest path based routing , the most stable path based routing and our proposed most stable constrained path routing . we show then through extensive simulations that routing based on our proposed algorithm selects appropriate stable paths yielding a very high routability with an average path length just above that of the shortest paths .
sar complex image data compression based on quadtree and zerotree coding in discrete wavelet transform domain a comparative study . <eos> sar complex image data compression based on wavelet quadtree is proposed . qc dwt has achieved the state of the art performance . qc dwt has achieved higher performance compared with wavelet zerotree .
on the power of tree walking automata . <eos> tree walking automata ( twas ) recently received new attention in the fields of formal languages and databases . to achieve a better understanding of their expressiveness , we characterize them in terms of transitive closure logic formulas in normal form . it is conjectured by engelfriet and hoogeboom that twas can not define all regular tree languages , or equivalently , all of monadic second order logic . we prove this conjecture for a restricted , but powerful , class of twas . in particular , we show that <digit> bounded twas , that is twas that are only allowed to traverse every edge of the input tree at most once in every direction , can not define all regular languages . we then extend this result to a class of twas that can simulate first order logic ( fo ) and is capable of expressing properties not definable in fo extended with regular path expressions the latter logic being a valid abstraction of current query languages for xml and semistructured data .
authenticity by tagging and typing . <eos> we propose a type and effect system for authentication protocols built upon a tagging scheme that formalizes the intended semantics of ciphertexts . the main result is that the validation of each component in isolation is provably sound and fully compositional if all the protocol participants are independently validated , then the protocol as a whole guarantees authentication in the presence of dolev yao intruders . the highly compositional nature of the analysis makes it suitable for multi protocol systems , where different protocols might be executed concurrently .
the conformational behavior , geometry and energy parameters of menshutkin like reaction of o isopropylidene protected glycofuranoid mesylates in view of dft calculations . <eos> the reaction of three mesylates of furanoderivatives in pyridine is presented at the dft . all the structures were fully optimized in the gas phase , in chloroform and water . the calculations revealed the barrier height increasing order as follows <digit> > <digit> > <digit> . mpw1k <digit> <digit> g level activation barriers are higher than those from b3lyp <digit> <digit> g . the furanoid ring conformations are close to e0 or 0e .
translating update operations from relational to object oriented databases . <eos> in migrating a legacy relational database system to the object oriented ( oo ) platform , when database migration completes , application modules are to be migrated , where embedded relational database operations are mapped into their oo correspondents . in this paper we study mapping relational update operations to their oo equivalents , which include update1 , insert and delete operations . relational update operation translation from relational to oo faces the touchy problem of transformation from a value based relationship model to a reference based model and maintaining the relational integrity constraints . moreover , with a relational database where inheritance is expressed as attribute value subset relationship , changing of some attribute values may lead to the change of the position of an object in the class inheritance hierarchy , which we call object migration . considering all these aspects , algorithms are given mapping relational update , insert and delete operations to their oo correspondents . our work emphasize in examining the differences in the representation of the source schema 's semantics resulting from the translation process , as well as differences in the inherent semantics of the two models .
secure threshold multi authority attribute based encryption without a central authority . <eos> an attribute based encryption scheme ( abe ) is a cryptographic primitive in which every user is identified by a set of attributes , and some function of these attributes is used to determine the ability to decrypt each ciphertext . chase proposed the first multi authority abe scheme which requires a fully trusted central authority who has the ability to decrypt each ciphertext in the system . this central authority would endanger the whole system if it is corrupted . this paper provides a threshold multi authority fuzzy identity based encryption ( ma fibe ) scheme without a central authority for the first time . an encrypter can encrypt a message such that a user could only decrypt if he has at least d ( k ) of the given attributes about the message for at least t <digit> , t < n <digit> honest authorities of all the n attribute authorities in the proposed scheme . this paper considers a stronger adversary model in the sense that the corrupted authorities are allowed to distribute incorrect secret keys to the users . the security proof is based on the secrecy of the underlying distributed key generation protocol and joint zero secret sharing protocol and the standard decisional bilinear diffie hellman assumption . the proposed ma fibe could be extended to the threshold multi authority attribute based encryption ( ma abe ) scheme , and both key policy based and ciphertext policy based ma abe schemes without a central authority are presented in this paper . moreover , several other extensions , such as a proactive large universe ma abe scheme , are also provided in this paper . ( c ) <digit> elsevier inc. all rights reserved .
segmentation according to natural examples learning static segmentation from motion segmentation . <eos> the segmentation according to natural examples ( sane ) algorithm learns to segment objects in static images from video training data . sane uses background subtraction to find the segmentation of moving objects in videos . this provides object segmentation information for each video frame . the collection of frames and segmentations forms a training set that sane uses to learn the image and shape properties of the observed motion boundaries . when presented with new static images , the trained model infers segmentations similar to the observed motion segmentations . sane is a general method for learning environment specific segmentation models . because it can automatically generate training data from video , it can adapt to a new environment and new objects with relative ease , an advantage over untrained segmentation methods or those that require human labeled training data . by using the local shape information in the training data , it outperforms a trained local boundary detector . its performance is competitive with a trained top down segmentation algorithm that uses global shape . the shape information it learns from one class of objects can assist the segmentation of other classes .
new delay dependent conditions on robust stability and stabilisation for discrete time systems with time delay . <eos> this article is concerned with new robust stability conditions and robust stabilisation method for a discrete time system with time delay and time varying structured uncertainties that come into state and input matrices . an improved approach to obtain new robust stability conditions is proposed . our approach employs a generalised lyapunov functional combined with the parameterised model transformation method and the generalised free weighting matrix method . these generalisations lead to generalised robust stability conditions that are given in terms of linear matrix inequalities . moreover , based on new robust stability conditions , a robust stabilisation method for uncertain discrete time systems with time delay is given . numerical examples compare our robust stability conditions with some existing conditions to show the effectiveness of our approach and also illustrate the improvement of our robust stabilisation method .
link testa statistical method for finding prostate cancer biomarkers . <eos> we present a new method , link test , to select prostate cancer biomarkers from seldi mass spectrometry and microarray data sets . biomarkers selected by link test are supported by data sets from both mrna and protein levels , and therefore results in improved robustness . link test determines the level of significance of the association between a microarray marker and a specific mass spectrum marker by constructing background mass spectra distributions estimated by all human protein sequences in the swiss prot database . the data set consist of both microarray and mass spectrometry data from prostate cancer patients and healthy controls . a list of statistically justified prostate cancer biomarkers is reported by link test . cross validation results show high prediction accuracy using the identified biomarker panel . we also employ a text mining approach with omim database to validate the cancer biomarkers . the study with link test represents one of the first cross platform studies of cancer biomarkers .
a low leakage 9t sram cell for ultra low power operation . <eos> this paper presents the design and evaluation of a new sram cell made of nine transistors ( 9t ) . the proposed 9t cell utilizes a scheme with separate read and write wordlines it is shown that the 9t cell achieves improvements in power dissipation , performance and stability compared with previous designs ( that require 10t and 8t ) for low power operation . the 9t scheme is amenable to small feature sizes as encountered in the deep sub micron nano ranges of cmos technology .
bacteria hunt evaluating multi paradigm bci interaction . <eos> the multimodal , multi paradigm brain computer interfacing ( bci ) game bacteria hunt was used to evaluate two aspects of bci interaction in a gaming context . one goal was to examine the effect of feedback on the ability of the user to manipulate his mental state of relaxation . this was done by having one condition in which the subject played the game with real feedback , and another with sham feedback . the feedback did not seem to affect the game experience ( such as sense of control and tension ) or the objective indicators of relaxation , alpha activity and heart rate . the results are discussed with regard to clinical neurofeedback studies . the second goal was to look into possible interactions between the two bci paradigms used in the game steady state visually evoked potentials ( ssvep ) as an indicator of concentration , and alpha activity as a measure of relaxation . ssvep stimulation activates the cortex and can thus block the alpha rhythm . despite this effect , subjects were able to keep their alpha power up , in compliance with the instructed relaxation task . in addition to the main goals , a new ssvep detection algorithm was developed and evaluated .
metamodels and emergent behaviour in models of conflict . <eos> in this paper , we develop a simplified mathematical model ( a metamodel ) of a simulation model of conflict , based on ideas drawn from the analysis of more general physical systems , such as found in fluid dynamics modelling . we show that there is evidence from the analysis of historical conflicts to support the kind of emergent behaviour implied by this approach . we then apply this approach to the development of a metamodel of a particular complexity based simulation model of conflict ( isaac ) , developed for the us marine corps . the approach we have illustrated here is very generic , and is applicable to any simulation model which has complex interactions similar to those found in fluid dynamic modelling , or in simulating the emergent behaviour of large numbers of simple systems which interact with each other locally .
topological implications of selfish neighbor selection in unstructured peer to peer networks . <eos> current peer to peer ( p2p ) systems often suffer from a large fraction of freeriders not contributing any resources to the network . various mechanisms have been designed to overcome this problem . however , the selfish behavior of peers has aspects which go beyond resource sharing . this paper studies the effects on the topology of a p2p network if peers selfishly select the peers to connect to . in our model , a peer exploits locality properties in order to minimize the latency ( or response times ) of its lookup operations . at the same time , the peer aims at not having to maintain links to too many other peers in the system . by giving tight bounds on the price of anarchy , we show that the resulting topologies can be much worse than if peers collaborated . moreover , the network may never stabilize , even in the absence of churn . finally , we establish the complexity of nash equilibria in our game theoretic model of p2p networks . specifically , we prove that it is np hard to decide whether our game has a nash equilibrium and can stabilize .
a readable tcp in the prolac protocol language . <eos> prolac is a new statically typed , object oriented language for network protocol implementation . it is designed for readability , extensibility , and real world implementation most previous protocol languages , in contrast , have been based on hard to implement theoretical models and have focused on verification . we present a working prolac tcp implementation directly derived from 4.4 bsd . our implementation is modular protocol processing is logically divided into minimally interacting pieces readable prolac encourages top down structure and naming intermediate computations and extensible subclassing cleanly separates protocol extensions like delayed acknowledgements and slow start . the prolac compiler uses simple global analysis to remove expensive language features like dynamic dispatch , resulting in end to end performance comparable to an unmodified linux 2.0 tcp .
a wearable document reader for the visually impaired dewarping and segmentation . <eos> while reading devices for the visually impaired have been available for many years , they are often expensive and difficult to use . the image processing required to enable the reading task is a composition of several important sub tasks , such as image capture , image stabilization , image enhancement and page curl dewarping region segmentation , regions grouping , and word recognition in this paper we deal with some of these sub tasks in an effort to prototype a device ( tyflos reader ) that will read a document for a person with a visual impairment and respond to voice commands for control . initial experimental results on a set of textbook and newspaper pages are also presented .
top k structural diversity search in large networks . <eos> social contagion depicts a process of information ( e.g. , fads , opinions , news ) diffusion in the online social networks . a recent study reports that in a social contagion process , the probability of contagion is tightly controlled by the number of connected components in an individuals neighborhood . such a number is termed structural diversity of an individual , and it is shown to be a key predictor in the social contagion process . based on this , a fundamental issue in a social network is to find top ( k ) users with the highest structural diversities . in this paper , we , for the first time , study the top ( k ) structural diversity search problem in a large network . specifically , we study two types of structural diversity measures , namely , component based structural diversity measure and core based structural diversity measure . for component based structural diversity , we develop an effective upper bound of structural diversity for pruning the search space . the upper bound can be incrementally refined in the search process . based on such upper bound , we propose an efficient framework for top ( k ) structural diversity search . to further speed up the structural diversity evaluation in the search process , several carefully devised search strategies are proposed . we also design efficient techniques to handle frequent updates in dynamic networks and maintain the top ( k ) results . we further show how the techniques proposed in component based structural diversity measure can be extended to handle the core based structural diversity measure . extensive experimental studies are conducted in real world large networks and synthetic graphs , and the results demonstrate the efficiency and effectiveness of the proposed methods .
a unified approach for detecting and eliminating selfish nodes in manets using tbut . <eos> recent years have witnessed the increasing efforts toward making architecture standardization for the secured wireless mobile ad hoc networks . in this scenario when a node actively utilizes the other node resources for communicating and refuses to help other nodes in their transmission or reception of data , it is called a selfish node . as the entire mobile ad hoc network ( manets ) depends on cooperation from neighboring nodes , it is very important to detect and eliminate selfish nodes from being part of the network . in this paper , token based umpiring technique ( tbut ) is proposed , where every node needs a token to participate in the network and the neighboring nodes act as umpire . this proposed tbut is found to be very efficient with a reduced detection time and less overhead . the security analysis and experimental results have shown that tbut is feasible for enhancing the security and network performance of real applications .
multiversion join index for multiversion data warehouse . <eos> the data warehouse ( dw ) technology is developed in order to support the integration of external data sources ( edss ) for the purpose of advanced data analysis by on line analytical processing ( olap ) applications . since contents and structures of integrated edss may evolve in time , the content and schema of a dw must evolve too in order to correctly reflect the evolution of edss . in order to manage a dw evolution , we developed the multiversion data warehouse ( mvdw ) approach . in this approach , different states of a dw are represented by the sequence of persistent dw versions that correspond either to the real world state or to a simulation scenario . typically , olap applications execute star queries that join multiple fact and dimension tables . an important optimization technique for this kind of queries is based on join indexes . since in the mvdw fact and dimension data are physically distributed among multiple dw versions , standard join indexes need extensions . in this paper we present the concept of a multiversion join index ( mvji ) applicable to indexing dimension and fact tables in the mvdw . the mvji has a two level structure , where an upper level is used for indexing attributes and a lower level is used for indexing dw versions . the paper also presents the theoretical upper bound ( pessimistic ) analysis of the mvji performance characteristic with respect to i o operations . the analysis is followed by experimental evaluation . it shows that the mvji increases a system performance for queries addressing multiple dw versions with exact match and range predicates . ( c ) <digit> elsevier b.v. all rights reserved .
reactive column profile map topology continuous distillation column with non reversible kinetics . <eos> in this paper we present a topologically based approach to the analysis and synthesis of reactive distillation columns . we extend the definition of tapp et al. tapp , m. , holland , s. , glasser , d. , hildebrandt , d. ( <digit> ) . column profile maps part a derivation and interpretation . industrial and engineering chemistry research , <digit> , <digit> <digit> of a column section in non reactive distillation column to a reactive column section ( rcs ) in a reactive distillation column . a rcs is defined as a section of a reactive distillation column in which there is no addition or removal of material or energy . we introduce the concept of a reactive column profile map ( rcpm ) in which the profiles in the rcpm correspond to the liquid composition profiles in the rcs . by looking at the singular points in the rcpm , it is demonstrated that for a single chemical reaction with no net change in the total number of moles , the bifurcation of the singular points depends on both the difference point as introduced by hauan et al. hauan , s. , ciric , a. r. , westerberg , a. w. , lien , k. m. ( <digit> ) . difference points in extractive and reactive cascades i basic properties and analysis . chemical engineering science , <digit> , <digit> <digit> as well as the direction of the stoichiometric vector . these two vectors combine to define what we call the reactive difference point composition . we show that there only certain feasible topologies of the rcpm and these depend only on the position of the reactive difference point composition . we look at a simple example where the vapour liquid equilibrium ( vle ) is ideal and show that we can classify regions of reactive difference point compositions that result in similar topology of the rcpm . thus , by understanding the feasible topologies of the rcpm , one is able to identify profiles in the rcpm that are desirable and hence one is able to synthesize a reactive distillation column by combining rcs that correspond to the desired profile in the rcpm . we believe that this tool will help understand how and when reaction could introduce unexpected behaviors and this can be used as a complementary tool to existing methods used for synthesis of reactive distillation columns . ( c ) <digit> elsevier ltd. all rights reserved .
fault tolerant hamiltonian laceability of hypercubes . <eos> it is known that every hypercube q ( n ) is a bipartite graph . assume that n greater than or equal to <digit> and f is a subset of edges with f less than or equal to n <digit> . we prove that there exists a hamiltonian path in q ( n ) f between any two vertices of different partite sets . moreover , there exists a path of length <digit> ( n ) <digit> between any two vertices of the same partite set . assume that n greater than or equal to <digit> and f is a subset of edges with f less than or equal to n <digit> . we prove that there exists a hamiltonian path in q ( n ) v f between any two vertices in the partite set without v. furthermore , all bounds are tight . ( c ) <digit> elsevier science b.v. all rights reserved .
incorporating user control into recommender systems based on naive bayesian classification . <eos> recommender systems are increasingly being employed to personalize services , such as on the web , but also in electronics devices , such as personal video recorders . these recommenders learn a user profile , based on rating feedback from the user on , e.g. , books , songs , or tv programs , and use machine learning techniques to infer the ratings of new items . the techniques commonly used are collaborative filtering and naive bayesian classification , and they are known to have several problems , in particular the cold start problem and its slow adaptivity to changing user preferences . these problems can be mitigated by allowing the user to set up or manipulate his profile . in this paper , we propose an extension to the naive bayesian classifier that enhances user control . we do this by maintaining and flexibly integrating two profiles for a user , one learned by rating feedback , and one created by the user . we in particular show how the cold start problem is mitigated .
population variation in genetic programming . <eos> a new population variation approach is proposed , whereby the size of the population is systematically varied during the execution of the genetic programming process with the aim of reducing the computational effort compared with standard genetic programming ( sgp ) . various schemes for altering population size under this proposal are investigated using a comprehensive range of standard problems to determine whether the nature of the population variation , i.e. the way the population is varied during the search , has any significant impact on gp performance . the initial population size is varied in relation to the initial population size of the sgp such that the worst case computational effort is never greater than that of the sgp . it is subsequently shown that the proposed population variation schemes do have the capacity to provide solutions at a lower computational cost compared with the sgp .
the need for power debugging in the multi core environment . <eos> debugging an application for power has a wide array of benefits ranging from minimizing the thermal hotspots to reducing the likelihood of cpu malfunction . in this work , we justify the need for power debugging , and show that performance debugging of a parallel application does not automatically guarantee power balance across multiple cores . we perform experiments and show our results using two case study benchmarks , volrend from splash <digit> and bodytrack from parsec 1.0 .
human resource assignment system for distribution centers . <eos> information technology and its wide range of applications have begun to make their presence in a new generation of logistic and distribution service industry . a more flexible breed of application packages is emerging by the application of fourth generation language ( 4gl ) technologies , which are able to provide foundations for true enterprise resource planning ( erp ) . there are many good reasons for adopting enterprise wide resource planning systems . this research , however , focuses on the development of a human resource assignment module ( hr module ) , usually considered as an essential part of an erp system . this module provides crucial human resource data and supports decisions in human resource utilization in distribution center operations . we detail the crucial algorithm for the hr module , which provides efficient and effective manpower management for key logistic distribution center operations .
quantized circulation in dilute bose einstein condensates . <eos> we compute using a microscopic mean field theory the structure and the quasiparticle excitation spectrum of a dilute . trapped bose einstein condensate penetrated by an axisymmetric vortex line . the gross pitaevskii equation for the condensate and the coupled hartree fock bogoliubov popov equations describing the elementary excitations are solved self consistently using finite difference methods . we find locally stable vortex configurations at all temperatures below t c. ( c ) <digit> elsevier science b.v. all rights reserved .
the design and evaluation of a high performance soft keyboard . <eos> the design and evaluation of a high performance soft keyboard for mobile systems are described . using a model to predict the upper bound text entry rate for soft keyboards , we designed a keyboard layout with a predicted upper bound entry rate of 58.2 wpm . this is about <digit> % faster than the predicted rate for a qwerty layout . we compared our design ( opti ) with a qwerty layout in a longitudinal evaluation using five participants and <digit> <digit> minute sessions of text entry . average entry rates for opt1 increased from 17.0 wpm initially to 44.3 wpm at session <digit> . the average rates exceeded those for the qwerty layout after the <digit> session ( about <digit> hours of practice ) . a regression equation ( r .997 ) in the form of the power law of learning predicts that our upper bound prediction would be reach at about session <digit> .
stereoscopic video coding and disparity estimation for low bitrate applications based on mpeg <digit> multiple auxiliary components . <eos> using an mpeg <digit> mac ( multiple auxiliary component ) system is a good way to encode stereoscopic video with existing standard codecs , especially when it comes to low bitrate applications . in this paper , we discuss the properties and problems of mac systems when encoding stereoscopic video , and propose an mac based stereoscopic video coder and disparity estimation scheme to solve those problems . we used a reconstructed disparity map during the disparity compensation process and took that disparity map into account while estimating the base view sequence motion vectors . moreover , we proposed a search range finding and illumination imbalance decision system . we also proposed a block based disparity map regularization process as well as block splitting in the object boundary and occlusion regions ( to reduce the number of bits to encode in both the disparity map and the residual image ) . last , we compensated for the imbalance between two cameras with a novel system that used mac characteristics . experimental results indicate that the proposed mac system outperformed conventional stereo coding systems by a maximum of 3.5 db in terms of the psnr and <digit> % in terms of bitsaving , especially in low bitrate applications .
robust recovery of subspace structures by low rank representation . <eos> in this paper , we address the subspace clustering problem . given a set of data samples ( vectors ) approximately drawn from a union of multiple subspaces , our goal is to cluster the samples into their respective subspaces and remove possible outliers as well . to this end , we propose a novel objective function named low rank representation ( lrr ) , which seeks the lowest rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary . it is shown that the convex program associated with lrr solves the subspace clustering problem in the following sense when the data is clean , we prove that lrr exactly recovers the true subspace structures when the data are contaminated by outliers , we prove that under certain conditions lrr can exactly recover the row space of the original data and detect the outlier as well for data corrupted by arbitrary sparse errors , lrr can also approximately recover the row space with theoretical guarantees . since the subspace membership is provably determined by the row space , these further imply that lrr can perform robust subspace clustering and error correction in an efficient and effective way .
on an efficient cad implementation of the distance term in pelgrom 's mismatch model . <eos> in <digit> , pelgrom et al published a mismatch model for mos transistors , where the variation of parameter mismatch between two identical transistors is given by two independent terms a size dependent term and a distance dependent term . some cad tools based on a nonphysical interpretation of pelgrom 's distance term result in excessive computationally expensive algorithms , which become nonviable even for circuits with a reduced number of transistors . furthermore , some researchers are reporting new variations on the original nonphysically interpreted algorithms , which may render false results . the purpose of this paper is to clarify the physical interpretation of the distance term of pelgrom et al. and indicate how to model it efficiently in prospective cad tools .
vertex ordering characterizations of graphs of bounded asteroidal number . <eos> asteroidal triple free ( at free ) graphs have received considerable attention due to their inclusion of various important graphs families , such as interval and cocomparability graphs . the asteroidal number of a graph is the size of a largest subset of vertices such that the removal of the closed neighborhood of any vertex in the set leaves the remaining vertices of the set in the same connected component . ( at free graphs have asteroidal number at most <digit> . ) in this article , we characterize graphs of bounded asteroidal number by means of a vertex elimination ordering , thereby solving a long standing open question in algorithmic graph theory . similar characterizations are known for chordal , interval , and cocomparability graphs .
clustering validity checking methods part ii . <eos> clustering results validation is an important topic in the context of pattern recognition . we review approaches and systems in this context . in the first part of this paper we presented clustering validity checking approaches based on internal and external criteria . in the second , current part , we present a review of clustering validity approaches based on relative criteria . also we discuss the results of an experimental study based on widely known validity indices . finally the paper illustrates the issues that are under addressed by the recent approaches and proposes the research directions in the field .
a training software model of an interrupt system . <eos> the paper justifies the necessity to introduce the students from the ' computer systems and technologies ' degree course to the structuce and way of operation of the interrupt system one of the important components of the processor . analysis of the basic funcionality of an example interrupt system is presented , an existing interrupt system is selected as a prototype of the training model and the arguments for its selection are proposed . the paper also describes the implemented model and its features . the work with the model will enable students to comprehend the way of operation of the interrupt system and it will be also used to check and assess their knowledge .
efficient network qos provisioning based on per node traffic shaping . <eos> this paper addresses the problem of providing per connection end to end delay guarantees in a high speed network , we assume that the network is connection oriented and enforces some admission control which ensures that the source traffic conforms to specified traffic characteristics . we concentrate on the class of rate controlled service ( rcs ) disciplines , in which traffic from each connection is reshaped at every hop , and develop end to end delay bounds for the general case where different reshapers are used at each hop . in addition , we establish that these bounds can also be achieved when the shapers at each hop have the same '' minimal '' envelope . the main disadvantage of this class of service discipline is that the end to end delay guarantees are obtained as the sum of the worst case delays at each node , but we show that this problem can be alleviated through '' proper '' reshaping of the traffic , we illustrate the impact of this reshaping by demonstrating its use in designing rcs disciplines that outperform service disciplines that are based on generalized processor sharing ( gps ) , furthermore , we show that we fan restrict the space of '' good '' shapers to a family which is characterized by only one parameter , we also describe extensions to the service discipline that make it work conserving and as a result reduce the average end to end delays .
as level source routing for multi provider connection oriented services . <eos> in this paper , we study the inter domain autonomous system ( as ) level routing problem within an alliance of ass . we first describe the framework of our work , based on the introduction of a service plane for automatic multi domain service provisioning . we adopt an abstract representation of domain relationships by means of directional metrics which are applied to a triplet ( ingress point , transit as , egress point ) where the ingress and egress points can be ass or routers . then , we focus on the point to point and multipoint as level routing problems that arise in such an architecture . we propose an original approach that reaches near optimal solutions with tractable computation times . a further contribution of this paper is that a heavy step in the proposed heuristic can be precomputed , independently of the service demands . moreover , we describe how in this context as level path diversity can be considered , and present the related extension of our heuristic . by extensive tests on as graphs derived from the internet , we show that our heuristic is often equal or a few percent close to the optimal , and that , in the case of precomputation , its time consumption can be much lower than with other well known algorithms .
parallel and distributed simulation of sediment dynamics in shallow water using particle decomposition approach . <eos> this paper describes the parallel simulation of sediment dynamics in shallow water . by using a lagrangian model , the problem is transformed to one in which a large number of independent particles must be tracked . this results in a technique that can be parallelised with high efficiency . we have developed a sediment transport model using three different sediment suspension methods . the first method uses a modified mean for the poisson distribution function to determine the expected number of the suspended particles in each particular grid cell of the domain over all available processors . the second method determines the number of particles to suspend with the aid of the poisson distribution function only in those grid cells which are assigned to that processor . the third method is based on the technique of using a synchronised pseudo random number generator to generate identical numbers of suspended particles in all valid grid cells for each processor . parallel simulation experiments are performed in order to investigate the efficiency of these three methods . also the parallel performance of the implementations is analysed . we conclude that the second method is the best method on distributed computing systems ( e.g. , a beowulf cluster ) , whereas the third maintains the best load distribution .
a numerical technique to predict periodic and quasi periodic response of nonlinear dynamic systems . <eos> a frequency domain based algorithm using fourier approximation and galerkin error minimization has been used to obtain the periodic orbits of large order nonlinear dynamic systems . the stability of these periodic response is determined through a bifurcation analysis using floquet theory . this technique is applicable to dynamic systems having both analytic and nonanalytic nonlinearities . this technique is compared with numerical time integration and is found to be much faster in predicting the steady state periodic response .
rough fuzzy approximations on two universes of discourse . <eos> in rough set theory , the lower and upper approximation operators can be constructed via a variety of approaches . various fuzzy generalizations of rough approximation operators have been made over the years . this paper presents a framework for the study of rough fuzzy sets on two universes of discourse . by means of a binary relation between two universes of discourse , a covering and three relations are induced to a single universe of discourse . based on the induced notions , four pairs of rough fuzzy approximation operators are proposed . these models guarantee that the approximating sets and the approximated sets are on the same universes of discourse . furthermore , the relationship between the new approximation operators and the existing rough fuzzy approximation operators on two universes of discourse are scrutinized , and some interesting properties are investigated . finally , the connections of these approximation operators are made , and conditions under which some of these approximation operators are equivalent are obtained .
identification of different stages of diabetic retinopathy using retinal optical images . <eos> diabetes is a disease which occurs when the pancreas does not secrete enough insulin or the body is unable to process it properly . this disease affects slowly the circulatory system including that of the retina . as diabetes progresses , the vision of a patient may start to deteriorate and lead to diabetic retinopathy . in this study on different stages of diabetic retinopathy , <digit> retinal photographs were analyzed . as a result , four groups were identified , viz. , normal retina , moderate non proliferative diabetic retinopathy , severe non proliferative diabetic retinopathy and proliferative diabetic retinopathy . classification of the four eye diseases was achieved using a three layer feedforward neural network . the features are extracted from the raw images using the image processing techniques and fed to the classifier for classification . we demonstrate a sensitivity of more than <digit> % for the classifier with the specificity of <digit> % .
constrained zip code segmentation by a pcnn based thinning algorithm . <eos> this paper proposes a novel thinning algorithm and applies it to automatic constrained zip code segmentation . the segmentation method consists of two main stages removal of rectangle boxes and location of zip code digits . both the two stages are implemented on the skeleton of boxes , which is extracted by the proposed pulse coupled neural network ( pcnn ) based thinning algorithm . this algorithm is specially designed to merely skeletonize the boxes . at the second stage , a projection method is employed to segment zip code image into its constituent digits . experimental results show that the proposed method is very efficient in segmenting zip code images even with noise .
multimodal authentication based on random projections and source coding . <eos> in this paper , we consider an authentication framework for independent modalities based on binary hypothesis testing using source coding jointly with the random projections . the source coding ensures the multimodal signals reconstruction at the decoder based on the authentication data . the random projections are used to cope with the security , privacy , robustness and complexity issues . finally , the authentication performance is investigated for both direct and random projections domains . the asymptotic performance approximation is derived and compared with the exact solutions . the impact of modality fusion on the authentication system performance is demonstrated .
effects of additive elements on the phase formation and morphological stability of nickel monosilicide films . <eos> alloying elements can substantially affect the formation and morphological stability of nickel monosilicide . a comprehensive study of phase formation was performed on <digit> ni alloys with varying concentrations of alloying elements . silicide films have been used for more than <digit> years to contact the source , drain and gate of state of the art complementary metal oxide semiconductor ( cmos ) devices . in the past , the addition of alloying elements was shown to improve the transformation from the high resistivity c49 to the low resistivity c54 tisi2 phase and to allow for the control of surface and interface roughness of cosi2 films as well as produce significant improvements with respect to agglomeration of the films . using simultaneous time resolved x ray diffraction ( xrd ) , resistance and light scattering measurements , we follow the formation of the silicide phases in real time during rapid thermal annealing . additions to the nisi system lead to modifications in the phase formation sequence at low temperatures ( metal rich phases ) , to variations in the formation temperatures of nisi and nisi2 , and to changes in the agglomeration behavior of the films formed . of the <digit> elements studied , additions of mo , re , ta and w are amongst the most efficient to retard agglomeration while elements such as pd , pt and rh are most efficient to retard the formation of nisi2 .
high order multiple mode and transadmittance mode ota c universal filters . <eos> this paper presents two new high order ota c universal filters . the first proposed filter structure employs n <digit> operational transconductance amplifiers ( otas ) and n grounded capacitors , which can realize nth order multiple mode ( including voltage , current , transadmittance , and transimpedance modes ) universal filtering responses ( lowpass , highpass , bandpass , bandreject , and allpass ) from the same topology . since the ota has high input and output impedances , it is very suitable for transadmittance mode circuit applications . therefore , a new high order transadmittance mode ota c universal filter structure using the minimum components is introduced . the second proposed filter structure uses only n <digit> otas and n grounded capacitors , which are the minimum components necessary for realizing nth order transadmittance mode universal filtering responses ( lowpass , highpass , bandpass , bandreject , and allpass ) from the same topology . this represents the attractive feature from chip area and power consumption point of view . moreover , the two new ota c universal filters still enjoy many important advantages no need of extra inverting or double type amplifiers for special input signals , using only n grounded capacitors , no need of any resistors , cascadably connecting the former voltage mode stage and the latter current mode stage , and low sensitivity performance . h spice simulations with tsmc 0.35 mu m process and 1.65 v supply voltages are included and confirm the theoretical predictions .
random fuzzy fractional integral equations theoretical foundations . <eos> this paper presents mathematical foundations for studies of random fuzzy fractional integral equations which involve a fuzzy integral of fractional order . we consider two different kinds of such equations . their solutions have different geometrical properties . the equations of the first kind possess solutions with trajectories of nondecreasing diameter of their consecutive values . on the other hand , the solutions to equations of the second kind have trajectories with nonincreasing diameter of their consecutive values . firstly , the existence and uniqueness of solutions is investigated . this is showed by using a method of successive approximations . an estimation of error of nth approximation is given . also a boundedness of the solution is indicated . to show well posedness of the considered theory , we prove that solutions depend continuously on the data of the equations . some concrete examples of random fuzzy fractional integral equations are solved explicitly .
efficient opportunistic routing in utility based ad hoc networks . <eos> due to resource scarcity , a paramount concern in ad hoc networks is utilizing limited resources efficiently . the self organized nature of ad hoc networks makes the network utility based approach an efficient way to allocate limited resources . however , the effect of link instability has not yet been adequately addressed in literature . to efficiently address the routing problem in ad hoc networks , we integrate the cost and stability into a network utility metric , and adopt the metric to evaluate the routing optimality in a unified , opportunistic routing model . based on this model , an efficient algorithm is designed , both centralized and distributed implementations are presented , and extensive simulations on ns <digit> are conducted to verify our results .
a computation to integrate the analysis of genetic variations occurring within regulatory elements and their possible effects . <eos> single nucleotide polymorphisms ( snps ) and short tandem repeats ( strs ) are the most common genetic variations , are widespread within genomes , and form the diversity within species . these genetic variations affect many regulatory elements such as transcription factor binding sites ( tfbss ) , dna methylation sites on cpg islands , and microrna target sites these elements have been found to play major as well as indirect roles in regulating gene expression . currently , systems are available to display such genetic variation occurring within regulatory elements . to understand and display all the potential variation described above , we have developed a web based system tool , the regulatory element and genetic variation viewer ( regv viewer regv ) , which provides a friendly web interface for users and shows genetic variation information within regulatory elements by either inputting a gene list or selecting a chromosome by name . moreover , our tool not only supports logic operation queries , but after a query is submitted , it also shows a high throughput simulation , including combined data , statistical graphs , and graphical views of the genetic variants and regulatory elements . additionally , when the snp variation occurs within tfbss and if the snp allele frequency and tfbs position weight matrices ( pwms ) are available , our system will show the new putative tfbss resulting from the snp variation .
programs as visual , interactive documents . <eos> we present a novel approach to combined textual and visual programming by allowing visual , interactive objects to be embedded within textual source code and segments of source code to be further embedded within those objects . we retain the strengths of text based source code , while enabling visual programming where it is beneficial . additionally , embedded objects and code provide a simple object oriented approach to adding a visual form of lisp style macros to a language . the ability to freely combine source code and visual , interactive objects with one another allows for the construction of interactive programming tools and experimentation with novel programming language extensions . our visual programming system is supported by a type coercion based presentation protocol that displays normal java and python objects in a visual , interactive form . we have implemented our system within a prototype interactive programming environment called the larch environment . copyright <digit> john wiley sons , ltd .
the equilibrium limit of a constitutive model for two phase granular mixtures and its numerical approximation . <eos> in this paper we analyze the equilibrium limit of the constitutive model for two phase granular mixtures introduced in papalexandris ( <digit> ) <digit> , and develop an algorithm for its numerical approximation . at , equilibrium , the constitutive model reduces to a strongly coupled , overdetermined system of quasilinear elliptic partial differential equations with respect to the pressure and the volume fraction of the solid granular phase . first we carry a perturbation analysis based on standard hydrostatic type scaling arguments which reduces the complexity of the coupling of the equations . the perturbed system is then supplemented by an appropriate compatibility condition which arises from the properties of the gradient operator . further , based on the helmholtz decomposition and ladyzhenskayas decomposition theorem , we develop a projection type , successive over relaxation numerical method . this method is general enough and can be applied to a variety of continuum models of complex mixtures and mixtures with micro structure . we also prove that this method is both stable and consistent hence , under standard assumptions , convergent . the paper concludes with the presentation of representative numerical results .
analysis of sonar targets by teager huang transform ( tht ) . <eos> in this paper , an approach for sonar targets analysis based on a new energy time frequency representation , called teager huang transform ( tht ) , is presented . the tht is the combination of the empirical mode decomposition of huang and the teager kaiser signal demodulation method . the tht is free of interferences and does not requires basis functions for signals decomposition . the analysis is carried out , in free field , from the impulse responses of sonar targets . we compare the analysis results of impulse responses of spherical and cylindrical targets given by tht to those of the smoothed wigner ville transformation .
international standard development for knowledge based engineering services for product lifecycle management . <eos> in september <digit> , the international information technology standard body object management group ( omg ) published a request for proposal ( rfp ) for an international standard for knowledge based engineering ( kbe ) services for product lifecycle management ( plm ) . the standard aims to facilitate the integration of kbe applications in a plm environment . kbe has been used in key engineering industry to deliver significant business benefits and has been a catalyst for changes in engineering processes . in recent years , mainstream cad vendors begin to incorporate kbe functionalities in their solutions . plm is evolving from the platform to manage engineering data to the repository of complete enterprise knowledge . as cad becomes more knowledge based , the convergence of kbe and plm is expected to happen soon . the omg standard rfp is an action to accelerate this convergence . the rfp is the result of an international effort with a team that includes engineering end users , software vendors and researchers . this paper presents the essence and the development process of the rfp to widen the engagement with the engineering research community .
an image topic model for image denoising . <eos> topic model is a powerful tool for the basic document or image processing tasks . in this study we introduce a novel image topic model , called latent patch model ( lpm ) , which is a generative bayesian model and assumes that the image and pixels are connected by a latent patch layer . based on the lpm , we further propose an image denoising algorithm namely multiple estimate lpm ( melpm ) . unlike other works , the proposed denoising framework is totally implemented on the latent patch layer , and it is effective for both gaussian white noises and impulse noises . experimental results demonstrate that lpm performs well in representing images . and its application in image denoising achieves competitive psnr and visual quality with conventional algorithms .
improving test case generation for web applications using automated interface discovery . <eos> with the growing complexity of web applications , identifying web interfaces that can be used for testing such applications has become increasingly challenging . many techniques that work effectively when applied to simple web applications are insufficient when used on modern , dynamic web applications , and may ultimately result in inadequate testing of the applications ' functionality . to address this issue , we present a technique for automatically discovering web application interfaces based on a novel static analysis algorithm . we also report the results of an empirical evaluation in which we compare our technique against a traditional approach . the results of the comparison show that our technique can ( <digit> ) discover a higher number of interfaces and ( <digit> ) help generate test inputs that achieve higher coverage .
numerical study of stream function formulation governing flows in multiply connected domains by integrated rbfs and cartesian grids . <eos> this paper describes a new numerical procedure , based on point collocation , integrated multiquadric functions and cartesian grids , for the discretisation of the stream function formulation for flows of a newtonian fluid in multiply connected domains . three particular issues , namely ( i ) the derivation of the stream function values on separate boundaries , ( ii ) the implementation of cross derivatives in irregular regions , and ( iii ) the treatment of double boundary conditions , are studied in the context of cartesian grids and approximants based on integrated multiquadric functions in one dimension . several test problems , i.e. steady flows between a rotating circular cylinder and a fixed square cylinder and also between eccentric cylinders maintained at different temperatures , are investigated . results obtained are compared well with numerical data available in the literature . crown copyright ( c ) <digit> published by elsevier ltd. all rights reserved .
linear techniques to correct for temperature induced spectral variation in multivariate calibration . <eos> the influence of external physical variation such as temperature fluctuations on near infrared ( nir ) spectra and their effect on the predictive power of calibration models such as pls have been studied . different methods to correct for the temperature effect by explicitly including the temperature in a calibration model have been tested . the results are compared to the implicit inclusion , which takes the temperature into account only through the calibration design . two data sets are used , one well designed data set measured in the laboratory and one industrial data set consisting of measurements for process samples . for both data sets , the explicit inclusion of the temperature in the calibration models did not result in an improvement of the prediction accuracy compared to implicit inclusion . ( c ) <digit> elsevier science b.v. all rights reserved .
overlapping b trees an implementation of a transaction time access method . <eos> a new variation of overlapping b trees is presented , which provides efficient indexing of transaction time and keys in a two dimensional key time space . modification operations ( i.e. insertions , deletions and updates ) are allowed at the current version , whereas queries are allowed to any temporal version , i.e. either in the current or in past versions . using this structure , snapshot and range timeslice queries can be answered optimally . however , the fundamental objective of the proposed method is to deliver efficient performance in case of a general pure key query ( i.e. ' history of a key ' ) . the trade off is a small increase in time cost for version operations and storage requirements . ( c ) <digit> elsevier science b.v. all rights reserved .
using a strategy aligned fuzzy competitive analysis approach for market segment evaluation and selection . <eos> this study applies five forces analysis to evaluate and select market segments for international business using a strategy aligned fuzzy approach . an illustration segment evaluation procedure is used to demonstrate that our procedure is an effective quantification approach for integrating five forces , generic strategies and marketing information in a group decision making process . the final decision maker ( dm ) synthesizes the total crisp scores of individual alternatives by choosing judgmental coefficients based on individual attitude towards core business competitiveness and market risks to accommodate differences among market segments to the specific environment with a better understanding of the decision problem and individual decision making behavior . in the illustration presented here , the final solution is then obtained by identifying the best market segment for further development and negotiation .
using adaboost classifiers in a hierarchical framework for classifying surface images of marble slabs . <eos> in this paper , a new hierarchical classification method based on the use of various types of adaboost classification algorithms is proposed for automatic classification of marble slab images according to their quality . at first , features are extracted using the sum and difference histograms method and , at the second stage , different versions of the adaboost algorithms are used as classifiers together with those extracted features in a proposed hierarchical fashion . performance of the proposed method is compared against performances of different types of neural network classifiers and a support vector machine ( svm ) classifier . computational results show that the proposed hierarchical structure employing adaboost algorithms performs superior to neural networks and the svm classifier for classifying marble slab images in our large and diversified data set . ( c ) <digit> elsevier ltd. all rights reserved .
on bridging the gap between stochastic integer programming and mip solver technologies . <eos> stochastic integer programs ( sips ) represent a very difficult class of optimization problems arising from the presence of both uncertainty and discreteness in planning and decision problems . although applications of sips are abundant , nothing is available by way of computational software . on the other hand , commercial software packages for solving deterministic integer programs have been around for quite a few years , and more recently , a package for solving stochastic linear programs has been released . in this paper , we describe how these software tools can be integrated and exploited for the effective solution of general purpose sips . we demonstrate these ideas on four problem classes from the literature and show significant computational advantages .
historiographic mapping of knowledge domains literature . <eos> to better understand the topic of this colloquium , we have created a series of databases related to knowledge domains ( dynamic systems small world milgram , information visualization tufte , co citation small , bibliographic coupling kessler , and scientometrics scientometrics ) . i have used a software package called histcite ( tm ) which generates chronological maps of subject ( topical ) collections resulting from searches of the isi web of science ( r ) or isi citation indexes ( sci , ssci , and or ahci ) on cd rom . when a marked list is created on wos , an export file is created which contains all cited references for each source document captured . these bibliographic collections , saved as ascii files , are processed by histcite in order to generate chronological and other tables as well as historiographs which highlight the most cited works in and outside the collection . histcite also includes a module for detecting and editing errors or variations in cited references as well as a vocabulary analyzer which generates both ranked word lists and word pairs used in the collection . ideally the system will be used to help the searcher quickly identify the most significant work on a topic and trace its year by year historical development . in addition to the collections mentioned above , historiographs based on collections of papers that cite the watson crick <digit> classic paper identifying the helical structure of dna were created . both year by year as well as month by month displays of papers from <digit> to <digit> were necessary to highlight the publication activity of those years .
bot detection evasion a case study on local host alert correlation bot detection methods . <eos> botnets have continuously evolved since their inception as a malicious entity . attackers come up with new botnet designs that exploit the weaknesses in existing defense mechanisms and continue to evade detection . it is necessary to analyze the weaknesses of existing defense mechanisms to find out the lacunae in them . this research exposes a weakness found in an existing bot detection method ( bdm ) by implementing a specialized p2p botnet model and carrying out experiments on it . weaknesses that are found and validated can be used to predict the development path of botnets , and as a result , detection and mitigation measures can be implemented in a proactive fashion . the main contribution of this work is to demonstrate the exploitation pattern of an inherent weakness in local host alert correlation ( lhac ) based methods and to assert that current lhac implementations could allow pockets of cooperative bots to hide in an enterprise size network . this work suggests that additional monitoring capabilities must be added to current lhac based methods in order for them to remain a viable bot detection mechanism . copyright ( c ) <digit> john wiley sons , ltd .
new plating bath for electroless copper deposition on sputtered barrier layers . <eos> a new copper plating bath for electroless deposition directly on conductive copper diffusion barrier layers has been developed . this plating bath can be operated at temperatures between <digit> and 50c and has good stability . high temperature processing allows for increased deposition rates and decreased specific resistivity values for the deposited copper films . electroless cu films deposited from this bath showed a conformal step coverage in high aspect ratio trenches and , therefore , are promising as seed layers for copper electroplating . the effect of the bath composition , activation procedure and processing temperature on the plating rate and morphology of the deposited copper has been studied and is presented here .
an adaptive finite element procedure for fully coupled point contact elastohydrodynamic lubrication problems . <eos> this paper presents an automatic locally adaptive finite element solver for the fully coupled ehl point contact problems . the proposed algorithm uses a posteriori error estimation in the stress in order to control adaptivity in both the elasticity and lubrication domains . the implementation is based on the fact that the solution of the linear elasticity equation exhibits large variations close to the fluid domain on which the reynolds equation is solved . thus the local refinement in such region not only improves the accuracy of the elastic deformation solution significantly but also yield an improved accuracy in the pressure profile due to increase in the spatial resolution of fluid domain . thus , the improved traction boundary conditions lead to even better approximation of the elastic deformation . hence , a simple and an effective way to develop an adaptive procedure for the fully coupled ehl problem is to apply the local refinement to the linear elasticity mesh . the proposed algorithm also seeks to improve the quality of refined meshes to ensure the best overall accuracy . it is shown that the adaptive procedure effectively refines the elements in the region ( s ) showing the largest local error in their solution , and reduces the overall error with optimal computational cost for a variety of ehl cases . specifically , the computational cost of proposed adaptive algorithm is shown to be linear with respect to problem size as the number of refinement levels grows .
allowing each node to communicate only once in a distributed system shared whiteboard models . <eos> in this paper we study distributed algorithms on massive graphs where links represent a particular relationship between nodes ( for instance , nodes may represent phone numbers and links may indicate telephone calls ) . since such graphs are massive they need to be processed in a distributed way . when computing graph theoretic properties , nodes become natural units for distributed computation . links do not necessarily represent communication channels between the computing units and therefore do not restrict the communication flow . our goal is to model and analyze the computational power of such distributed systems where one computing unit is assigned to each node . communication takes place on a whiteboard where each node is allowed to write at most one message . every node can read the contents of the whiteboard and , when activated , can write one small message based on its local knowledge . when the protocol terminates its output is computed from the final contents of the whiteboard . we describe four synchronization models for accessing the whiteboard . we show that message size and synchronization power constitute two orthogonal hierarchies for these systems . we exhibit problems that separate these models , i.e. , that can be solved in one model but not in a weaker one , even with increased message size . these problems are related to maximal independent set and connectivity . we also exhibit problems that require a given message size independently of the synchronization model .
a dual spinal cord lesion paradigm to study spinal locomotor plasticity in the cat . <eos> after a complete spinal cord injury ( sci ) at the lowest thoracic level ( t13 ) , adult cats trained to walk on a treadmill can recover hindlimb locomotion within <digit> weeks , resulting from the activity of a spinal circuitry termed the central pattern generator ( cpg ) . the role of this spinal circuitry in the recovery of locomotion after partial scis , when part of descending pathways can still access the cpg , is not yet fully understood . using a dual spinal lesion paradigm ( first hemisection at t10 followed three weeks after by a complete spinalization at t13 ) , we showed that major changes occurred in this locomotor spinal circuitry . these plastic changes at the spinal cord level could participate in the recovery of locomotion after partial sci . this short review describes the main findings of this paradigm in adult cats .
analysis of a sparse hypermatrix cholesky with fixed sized blocking . <eos> we present the way in which we have constructed an implementation of a sparse cholesky factorization based on a hypermatrix data structure . this data structure is a storage scheme which produces a recursive 2d partitioning of a sparse matrix . it can be useful on some large sparse matrices . subblocks are stored as dense matrices . thus , efficient blas3 routines can be used . however , since we are dealing with sparse matrices some zeros may be stored in those dense blocks . the overhead introduced by the operations on zeros can become large and considerably degrade performance . we present the ways in which we deal with this overhead . using matrices from different areas ( interior point methods of linear programming and finite element methods ) , we evaluate our sequential in core hypermatrix sparse cholesky implementation . we compare its performance with several other codes and analyze the results . in spite of using a simple fixed size partitioning of the matrix our code obtains competitive performance .
atomic precision patterning on si an opportunity for a digitized process . <eos> h depassivation lithography is a process by which a monolayer of h absorbed on a si ( <digit> ) <digit> surface may be patterned by the removal of h atoms using a scanning tunneling microscope . this process can achieve atomic resolution where individual atoms are targeted and removed . this paper suggests that such a patterning process can be carried out as a digital process , where the pixels of the pattern are the individual h atoms . the goal is digital fabrication rather than digital information processing . the margins for the read and write operators appear to be sufficient for a digital process , and the tolerance for physical addressing of the atoms is technologically feasible . a digital fabrication process would enjoy some of the same advantages of digital computation namely high reliability , error checking and correction , and the creation of complex systems .
automated worst case execution time analysis based on program modes . <eos> a program mode is a regular trajectory of the execution of a program that is determined by the values of its input variables . by exploiting program modes , we may make worst case execution time ( wcet ) analysis more precise . this paper presents a novel method to automatically find program modes and calculate the wcet estimates of programs . first , the modes of a program will be identified automatically by mode relevant program slicing , and the precondition will be calculated for each mode using a path wise test data generation method . then , for each feasible mode , we show how to calculate its wcet estimate for modern reduced instruction set computer ( risc ) processors with caches and pipelines and for traditional complex instruction set computer ( cisc ) processors . we also present a method to obtain the symbolic expression for each mode for cisc processors . the experimental results show the effectiveness of the method .
non agricultural databases and thesauri retrieval of subject headings and non controlled terms in relation to agriculture . <eos> purpose the paper aims to assess the utility of non agriculture specific information systems , databases , and respective controlled vocabularies ( thesauri ) in organising and retrieving agricultural information . the purpose is to identify thesaurus linked tree structures , controlled subject headings terms ( heading words , descriptors ) , and principal database dependent characteristics and assess how controlled terms improve retrieval results ( recall ) in relation to free text uncontrolled terms in abstracts and document titles . design methodology approach several different hosts ( interfaces , platforms , portals ) and databases were used csa illumina . ( eric , lisa ) , ebscohost ( academic search complete , medline , political science complete ) , ei engineering village ( compendex , inspec ) , ovid ( psycinfo ) , proquest ( abi inform global ) . the search terms agriculture and agricultural and truncated word stem agricultur were employed . permuted ( rotated index ) search fields were used to retrieve terms from thesauri . subject heading search was assessed in relation to free text search , based on abstracts and document titles . findings all thesauri contain agriculture based headings however , associative , hierarchical and synonymous relationships show important inter database differences . using subject headings along with abstracts and titles in search syntax ( query ) sometimes improves retrieval by up to <digit> per cent . retrieval depends on search fields and database specifics , such as autostemming ( lemmatization ) , explode function , word indexing , or phrase indexing . research limitations implications inter database and host comparison , on consistent principles , can be limited because of some particular host and database specifics . practical implications end users may exploit databases more competently and thus achieve better retrieval results in searching for agriculture related information . originality value the function of as many as ten databases in different disciplines in providing information relevant to subject matter that is not a topical focus of databases is assessed .
efficient packet classification with a hybrid algorithm . <eos> packet classification categorizes incoming packets into multiple forwarding classes based on pre defined filters . this categorization makes information accessible for quality of service or security handling in the network . in this paper , we propose a scheme which combines the aggregate bit vector algorithm and the pruned tuple space search algorithm to improve the performance of packet classification in terms of speed and storage . we also present the procedures of incremental update . our scheme is evaluated with filter databases of varying sizes and characteristics . the experimental results demonstrate that our scheme is feasible and scalable .
the intra americas sea low level jet . <eos> a relevant climate feature of the intra americas sea ( ias ) is the low level jet ( iallj ) dominating the ias circulation , both in summer and winter and yet it is practically unknown with regard to its nature , structure , interactions with mid latitude and tropical phenomena , and its role in regional weather and climate . this paper updates iallj current knowledge and its contribution to ias circulationprecipitation patterns and presents recent findings about the iallj based on first in situ observations during phase <digit> of the experimento climtico en las albercas de agua clida ( ecac ) , an international field campaign to study iallj dynamics during july <digit> . nonhydrostatic fifth generation pennsylvania state university national center for atmospheric research mesoscale model ( mm5 ) simulations were compared with observations and reanalysis . large scale circulation patterns of the iallj northern hemisphere summer and winter components suggest that trades , and so the iallj , are responding to landocean thermal contrasts during the summer season of each continent . the iallj is a natural component of the american monsoons as a result of the continent 's approximate northsouth land distribution . during warm ( cold ) el niosouthern oscillation phases , winds associated with the iallj core ( ialljc ) are stronger ( weaker ) than normal , so precipitation anomalies are positive ( negative ) in the western caribbean near central america and negative ( positive ) in the central ias . during the ecac phase <digit> , strong surface winds associated with the iallj induced upwelling , cooling down the sea surface temperature by <digit> c. the atmospheric mixed layer height reached <digit> km near the surface wind maximum below the ialljc . observations indicate that primary water vapor advection takes place in a shallow layer between the ialljc and the ocean surface . latent heat flux peaked below the ialljc . neither the reanalysis nor mm5 captured the observed thermodynamic and kinematic iallj structure . so far , iallj knowledge is based on either dynamically initialized data or simulations of global ( regional ) models , which implies that a more systematic and scientific approach is needed to improve it . the intra americas study of climate processes is a great regional opportunity to address trough field work , modeling , and process studies , many of the iallj unknown features .
operational knowledge representation for practical decision making . <eos> for the design of an intelligent assistant system aimed at supporting operators ' decision in subway control , we modeled operators ' activity and know how . as a result , we introduce the notion of a contextual graph , which appears as a simple solution to describe and manage operational decision making .
organizational structure satisfactory social law determination in multiagent workflow systems . <eos> the multiagent workflow systems can be formalized from an organizational structure viewpoint , which includes three parts the interaction structure among agents , the temporal flow of activities , and the critical resource sharing relations among activities . while agents execute activities , they should decide their strategies to satisfy the constraints brought by the organizational structure of multiagent workflow system . to avoid collisions in the multiagent workflow system , this paper presents a method to determine social laws in the system to restrict the strategies of agents and activities the determined social laws can satisfy the characteristics of organization structures so as to minimize the conflicts among agents and activities . moreover , we also deal with the social law adjustment mechanism for the alternations of interaction relations , temporal flows , and critical resource sharing relations . it is proved that our model can produce useful social laws for organizational structure of multiagent workflow systems , i.e. , the conflicts brought by the constraints of organization structure can be minimized .
content aware retargeting for soccer video adaptation . <eos> a content aware retargeting method is proposed for adapting soccer video to heterogeneous terminals . according to domain specific knowledge , ball , player and player 's face are defined as user interested objects ( uios ) in different view types . the uios are extracted by semantic analysis on soccer video , and then a region of interest ( roi ) of each shot is determined jointly by three factors terminal size , scaling factor and aspect ratio . the proposed method optimizes the retargeted region to contain more semantic content while adapting the constraint of terminal screen . the simulation results prove that the proposed car system wins better viewing experiences than the traditional methods such as resizing in a letter box mechanism or cropping directly .
ribosome kinetics and aa trna competition determine rate and fidelity of peptide synthesis . <eos> it is generally accepted that the translation rate depends on the availability of cognate aa trnas . in this study it is shown that the key factor that determines translation rate is the competition between near cognate and cognate aa trnas . the transport mechanism in the cytoplasm is diffusion , thus the competition between cognate , near cognate and non cognate aa trnas to bind to the ribosome is a stochastic process . two competition measures are introduced c ( i ) and r ( i ) ( i <digit> , <digit> ) are quotients of the arrival frequencies of near cognates vs. cognates and non cognates vs. cognates , respectively . furthermore , the reaction rates of bound cognates differ from those of bound near cognates . if a near cognate aa trna binds to the a site of the ribosome , it may be rejected at the anti codon recognition step or proofreading step or it may be accepted . regardless of its fate , the near cognates and non cognates have caused delays of varying duration to the observed rate of translation . rate constants have been measured at a temperature of 20c by ( gromadski , k.b. , rodnina , m.v. , <digit> . kinetic determinants of high fidelity trna discrimination on the ribosome . mol . cell <digit> , <digit> ) . these rate constants have been re evaluated at 37c , using experimental data at 24.5 c and 37c ( varenne , s. , et al. , <digit> . translation in a non uniform process effect of trna availability on the rate of elongation of nascent polypeptide chains . j. mol . biol . <digit> , <digit> ) . the key results of the study are ( i ) the average time ( at 37c ) to add an amino acid , as defined by the ith codon , to the nascent peptide chain is ( i ) 9.06 1.445 10.48 c ( i ) 0.5 r ( i ) ( in ms ) ( ii ) the misreading frequency is directly proportional to the near cognate competition , e ( i ) 0.0009 c ( i ) ( iii ) the competition from near cognates , and not the availability of cognate aa trnas , is the most important factor that determines the translation rate the four codons with highest near cognate competition ( in the case of e. coli ) are gcc > cgg > agg > gga , which overlap only partially with the rarest codons agg < cca < gcc < cac ( iv ) based on the kinetic rates at 37c , the average time to insert a cognate amino acid is 9.06 ms and the average delay to process a near cognate aa trna is 10.45 ms and ( vii ) the model also provides estimates of the vacancy times of the a site of the ribosome an important factor in frameshifting .
an efficient and spectrally accurate numerical method for computing dynamics of rotating bose einstein condensates . <eos> in this paper , we propose an efficient and spectrally accurate numerical method for computing the dynamics of rotating bose einstein condensates ( bec ) in two dimensions ( 2d ) and 3d based on the gross pitaevskii equation ( gpe ) with an angular momentum rotation term . by applying a time splitting technique for decoupling the nonlinearity and properly using the alternating direction implicit ( adi ) technique for the coupling in the angular momentum rotation term in the gpe , at every time step , the gpe in rotational frame is decoupled into a nonlinear ordinary differential equation ( ode ) and two partial differential equations with constant coefficients . this allows us to develop new time splitting spectral methods for computing the dynamics of bec in a rotational frame . the new numerical method is explicit , unconditionally stable , and of spectral accuracy in space and second order accuracy in time . moreover , it is time reversible and time transverse invariant , and conserves the position density in the discretized level if the gpe does . extensive numerical results are presented to confirm the above properties of the new numerical method for rotating bec in 2d and 3d . ( c ) <digit> elsevier inc. all rights reserved .
the entropy of traces in parallel computation . <eos> the following problem arises in thtr context of parallel computation how many bits of information are required to specify any one element from an arbitrary ( nonempty ) x subset of a set we characterize optimal coding techniques for this problem . we calculate the asymptotic behavior of the amount of information necessary , and construct an algorithm that specifies an element from a subset in an optimal manner .
proteomic technologies to study diseases of the lymphatic vascular system . <eos> now that the human genome has been mapped , a new challenge has emerged
leveraging eclipse for integrated model based engineering of web service compositions . <eos> in this paper we detail the design and implementation of an eclipse plug in for an integrated , model based approach , to the engineering of web service compositions . the plug in allows a designer to specify a service 's obligations for coordinated web service compositions in the form of message sequence charts ( mscs ) and then generate policies in the form of ws cdl and services in the form of bpel4ws . the approach uses finite state machine representations of web service compositions and service choreography rules , and assigns semantics to the distributed process interactions . the move towards implementing web service choreography requires design time verification of these service interactions to ensure that service implementations fulfill requirements for multiple interested partners before such compositions and choreographies are deployed . the plug in provides a tool for integrated specification , formal modeling , animation and providing verification results from choreographed web service interactions . the ltsa eclipse ( for web services ) plug in is publicly available , along with other plug ins , at http www.doc.ic.ac.uk ltsa .
targeted recruitment of histone modifications in humans predicted by genomic sequences . <eos> histone modifications are important epigenetic regulators and play a critical role in development . the targeting mechanism for histone modifications is complex and still incompletely understood . here we applied a computational approach to predict genome scale histone modification targets in humans by the genomic dna sequences using a set of recent chip seq data . we found that a number of histone modification marks could be predicted with high accuracy . on the other hand , the impact of dna sequences for each mark is intrinsically different dependent upon the target and tissue specificity . diverse patterns are associated with different repetitive elements . unexpectedly , we found that non overlapping , functionally opposite histone modification marks could share similar sequence features . we propose that these marks may target a common set of loci but are mutually exclusive and that the competition may be important for developmental control . taken together , we show that our computational approach has provided new insights into the targeting mechanism of histone modifications .
normal vector voting crease detection and curvature estimation on large , noisy meshes . <eos> this paper describes a robust method for crease detection and curvature estimation on large , noisy triangle meshes . we assume that these meshes are approximations of piecewise smooth surfaces derived from range or medical imaging systems and thus may exhibit measurement or even registration noise . the proposed algorithm , which we call normal vector voting , uses an ensemble of triangles in the geodesic neighborhood of a vertex instead of its simple umbrella neighborhood to estimate the orientation and curvature of the original surface at that point . with the orientation information , we designate a vertex as either lying on a smooth surface , following a crease discontinuity , or having no preferred orientation . for vertices on a smooth surface , the curvature estimation yields both principal curvatures and principal directions while for vertices on a discontinuity we estimate only the curvature along the crease . the last case for no preferred orientation occurs when three or more surfaces meet to form a corner or when surface noise is too large and sampling density is insufficient to determine orientation accurately . to demonstrate the capabilities of the method , we present results for both synthetic and real data and compare these results to the g. taubin ( <digit> , in proceedings of the fifth international conference on computer vision , pp. <digit> <digit> ) algorithm . additionally , we show practical results for several large mesh data sets that are the motivation for this algorithm . ( c ) <digit> elsevier science ( usa ) .
acoustic super models for large scale video event detection . <eos> given the exponential growth of videos published on the internet , mechanisms for clustering , searching , and browsing large numbers of videos have become a major research area . more importantly , there is a demand for event detectors that go beyond the simple finding of objects but rather detect more abstract concepts , such as feeding an animal or a wedding ceremony . this article presents an approach for event classification that enables searching for arbitrary events , including more abstract concepts , in found video collections based on the analysis of the audio track . the approach does not rely on speech processing , and is language indepent , instead it generates models for a set of example query videos using a mixture of two types of audio features linear frequency cepstral coefficients and modulation spectrogram features . this approach can be used in complement with video analysis and requires no domain specific tagging . application of the approach to the trecvid med <digit> development set , which consists of more than <digit> random wild videos from the internet , has shown a detection accuracy of <digit> % including those videos which do not contain an audio track .
morphological influence of the beam overlap in focused ion beam induced deposition using raster scan . <eos> material addition using focused ion beam induced deposition ( fibid ) is a well established local deposition technology in microelectronic engineering . we investigated fibid characteristics as a function of beam overlap using phenanthrene molecules . to initiate the localization of gas molecules , we irradiated the ion beams using a raster scan . we varied the beam overlap between <digit> % and <digit> % by adjusting the pixel size from 300nm to 15nm . we discuss the changes in surface morphologies and deposition rates due to delocalization by the range effect of excited surface atoms , the divided structure by continuous effect from the raster scan , enhanced localization by discrete effect from replenished gas molecules , the competition between deposition and sputtering processes , and the change in processing time with scan speed ( smaller overlap case ) .
content aware rate allocation for efficient video streaming via dynamic network utility maximization . <eos> nowadays it is vital to design robust mechanisms to provide qos for multimedia applications as an integral part of the network traffic . the main goal of this paper is to provide an efficient rate control scheme to support content aware video transmission mechanism with buffer underflow avoidance at the receiver in congested networks . towards this , we introduce a content aware time varying utility function , in which the quality impact of video content is incorporated into its mathematical expression . moreover , we analytically model the buffer requirements of video sources in two ways first as constraints of the optimization problem to guarantee a minimum rate demand for each source , and second as a penalty function embedded as part of the objective function attempting to achieve the highest possible rate for each source . then , using the proposed analytical model , we formulate a dynamic network utility maximization problem , which aims to maximize the aggregate hybrid objective function of sources subject to capacity and buffer constraints . finally , using primaldual method , we solve dnum problem and propose a distributed algorithm called ca dnum that optimally allocates the shared bandwidth to video streams . the experimental results demonstrate the efficacy and performance improvement of the proposed content aware rate allocation algorithm for video sources in different scenarios .
inverse problem for fricatives . <eos> articulatory parameters , vocal tract shape and cross sectional area function were determined from fricative spectra . a model of fricative generation was used for providing acoustical constraints for an optimization procedure with muscles work as the criterion of optimality . a distance between spectra was measured with the use of the cauchy bounjakovsky non equality . a proper initial approximation of articulatory parameters is required to obtain an accurate and stable solution of the inverse problem .
efficient web usage mining process for sequential patterns . <eos> the tremendous growth in volume of web usage data results in the boost of web mining research with focus on discovering potentially useful knowledge from web usage data . this paper presents a new web usage mining process for finding sequential patterns in web usage data which can be used for predicting the possible next move in browsing sessions for web personalization . this process consists of three main stages preprocessing web access sequences from the web server log , mining preprocessed web log access sequences by a tree based algorithm , and predicting web access sequences by using a dynamic clustering based model . it is designed based on the integration of the dynamic clustering based markov model with the pre order linked wap tree mining ( plwap ) algorithm to enhance mining performance . the proposed mining process is verified by experiments with promising results .
experimenting with an organic metaphor and hypervisual links for the interface of a video collection . <eos> in this paper we describe the prototype of an archive of short movies . the project proposes two original solutions for implementing the interface of this archive an organic metaphor and a hypervisual navigation mechanism .
a randomized exhaustive propositionalization approach for molecule classification . <eos> drug discovery is the process of designing compounds that have desirable properties , such as activity and nontoxicity . molecule classification techniques are used along with this process to predict the properties of the compounds to expedite their testing . ideally , the classification rules found should be accurate and reveal novel chemical properties , but current molecule representation techniques lead to less than adequate accuracy and knowledge discovery . this work extends the propositionalization approach recently proposed for multirelational data mining in two ways it generates expressive attributes exhaustively , and it uses randomization to sample a limited set of complex ( deep ) attributes . our experimental tests show that the procedure is able to generate meaningful and interpretable attributes from molecular structural data , and that these features are effective for classification purposes .
error propagation analysis for underwater cooperative multi hop communications . <eos> the potential gains of cooperative communication and multi hopping in underwater acoustic communication channels is examined . in particular , performance of such systems is compared to a comparable single hop system ( direct transmission ) with a common transmission distance . the effects of error propagation with decode and forward at each relay are explicitly treated and it is shown that strong gains can be achieved by multi hopping ( an effective snr gain ) is well as cooperation , which contributes to a diversity gain . we observe that cooperative diversity gains are retained even when considering error propagation . the analysis is done via a markov chain analysis for both regular linear and grid networks . our initial analysis is for single path channels the effects of inter symbol interference as well as multi . user interference are examined . it is found that due to the strong decay of signal power as a function of transmission distance , multi user interference is not as significant as inter symbol interference . in both cases , cooperative and multi hopping gains are observed . ( c ) <digit> published by elsevier b.v.
evolutionary algorithms for reasoning in fuzzy description logics with fuzzy quantifiers . <eos> the task of reasoning with fuzzy description logics with fuzzy quantification is approached by means of an evolutionary algorithm . an essential ingredient of the proposed method is a heuristic , implemented as an intelligent mutation operator , which observes the evolutionary process and uses the information gathered to guess at the mutations most likely to bring about an improvement of the solutions . the viability of the method is demonstrated by applying it to reasoning on a resource sheduling problem .
deadline based scheduling in support of real time data delivery . <eos> the use of deadline based scheduling in support of real time delivery of application data units ( adus ) in a packet switched network is investigated . of interest is priority scheduling where a packet with a smaller ratio of t h ( time until delivery deadline over number of hops remaining ) is given a higher priority . we refer to this scheduling algorithm as the t h algorithm . t h has time complexity of o ( logn ) for a backlog of n packets and was shown to achieve good performance in terms of the percentage of adus that are delivered on time . we develop a new and efficient algorithm , called t h p , that has o ( <digit> ) time complexity . the performance difference of t h , t h p and fcfs are evaluated by simulation . implementations of t h and t h p in high speed routers are also discussed . we show through simulation that t h p is superior to fcfs but not as good as t h. in view of the constant time complexity , t h p is a good candidate for high speed routers when both performance and implementation cost are taken into consideration .
space hierarchy theorem revised . <eos> we show that , for an arbitrary function h ( n ) and each recursive function e ( n ) , that are separated by a nondeterministically fully space constructible g ( n ) , such that h ( n ) e q ( g ( n ) ) but l ( n ) q ( g ( n ) ) , there exists a unary language l in nspace ( h ( n ) ) that is not contained in nspace ( l ( n ) ) . the same holds for the deterministic case . the main contribution to the well known space hierarchy theorem is that ( i ) the language l separating the two space classes is unary ( tally ) , ( ii ) the hierarchy is independent of whether h ( n ) or l ( n ) are in omega ( log n ) or in o ( log n ) , ( iii ) the functions h ( n ) or l ( n ) themselves need not be space constructible nor monotone increasing , ( iv ) the hierarchy is established both for strong and weak space complexity classes . this allows us to present unary languages in such complexity classes as , for example , nspace ( log log n ( . ) log n ) nspace ( log log n ) , using a plain diagonalization . ( c ) <digit> elsevier science b.v. all rights reserved .
non dominated sorting genetic algorithm with decomposition to solve constrained optimisation problems . <eos> pareto domination was adopted to handle not only trade off between objective and constraints but also trade off between convergence and diversity on solving a constrained optimisation problem ( cop ) in this paper like many other researchers . but there are some differences . this paper converts a cop into an equivalent dynamic constrained multi objective optimisation problem ( dcmop ) first , then dynamic version of non dominated sorting genetic algorithm with decomposition ( nsga d ) is designed to solve the equivalent dcmop , consequently solve the cop . a key issue for the nsga d working effectively is that the environmental change should not destroy the feasibility of the population . with a feasible population , the nsga d could solve well the dcmop just as a moea usually can solve well an unconstrained mop . experimental results show that the nsga d outperforms or performs similarly to other state of the art algorithms referred to in this paper , especially in global search .
selecting for evolvable representations . <eos> evolutionary algorithms tend to produce solutions that are not evolvable although current fitness may be high , further search is impeded as the effects of mutation and crossover become increasingly detrimental . in nature , in addition to having high fitness , organisms have evolvable genomes phenotypic variation resulting from random mutation is structured and robust . evolvability is important because it allows the population to produce meaningful variation , leading to efficient search . however , because evolvability does not improve immediate fitness , it must be selected for indirectly . one way to establish such a selection pressure is to change the fitness function systematically . under such conditions , evolvability emerges only if the representation allows manipulating how genotypic variation maps onto phenotypic variation and if such manipulations lead to detectable changes in fitness . this research forms a framework for understanding how fitness function and representation interact to produce evolvability . ultimately evolvable encodings may lead to evolutionary algorithms that exhibit the structured complexity and robustness found in nature .
modeling biocomplexity actors , landscapes and alternative futures . <eos> increasingly , models ( and modelers ) are being asked to address the interactions between human influences , ecological processes , and landscape dynamics that impact many diverse aspects of managing complex coupled human and natural systems . these systems may be profoundly influenced by human decisions at multiple spatial and temporal scales , and the limitations of traditional process level ecosystems modeling approaches for representing the richness of factors shaping landscape dynamics in these coupled systems has resulted in the need for new analysis approaches . new tools in the areas of spatial data management and analysis , multicriteria decision making , individual based modeling , and complexity science have all begun to impact how we approach modeling these systems . the term biocomplexity has emerged as a descriptor of the rich patterns of interactions and behaviors in human and natural systems , and the challenges of analyzing biocomplex behavior is resulting in a convergence of approaches leading to new ways of understanding these systems . important questions related to system vulnerability and resilience , adaptation , feedback processing , cycling , non linearities and other complex behaviors are being addressed using models employing new representational approaches to analysis . the complexity inherent in these systems challenges the modeling community to provide tools that capture sufficiently the richness of human and ecosystem processes and interactions in ways that are computationally tractable and understandable . we examine one such tool , evoland , which uses an actor based approach to conduct alternative futures analyses in the willamette basin , oregon .
computerised video tracking , movement analysis and behaviour recognition in insects . <eos> the need for automating behavioural observations and the evolution of systems developed for that purpose are outlined . automatic video tracking systems enable behaviour to be studied in a reliable and consistent way , and over longer time periods than if it is manually recorded . to overcome limitations of currently available systems and to meet researchers ' needs as these have been identified , we have developed an integrated system ( ethovision ) for automatic recording of activity , movement and interactions of insects . the system is described here , with special emphasis on file management , experiment design , arena and zone definition , object detection , experiment control , visualisation of tracks and calculation of analysis parameters . a review of studies using our system is presented , to demonstrate its use in a variety of entomological applications . this includes research on beetles , fruit flies , soil insects , parasitic wasps , predatory mites , ticks , and spiders . finally , possible future directions for development are discussed .
a novel similarity measure on intuitionistic fuzzy sets with its applications . <eos> the intuitionistic fuzzy set , as a generation of zadeh fuzzy set , can express and process uncertainty much better , by introducing hesitation degree . similarity measures between intuitionistic fuzzy sets ( ifss ) are used to indicate the similarity degree between the information carried by ifss . although several similarity measures for intuitionistic fuzzy sets have been proposed in previous studies , some of those can not satisfy the axioms of similarity , or provide counter intuitive cases . in this paper , we first review several widely used similarity measures and then propose new similarity measures . as the consistency of two ifss , the proposed similarity measure is defined by the direct operation on the membership function , non membership function , hesitation function and the upper bound of membership function of two ifs , rather than based on the distance measure or the relationship of membership and non membership functions . it proves that the proposed similarity measures satisfy the properties of the axiomatic definition for similarity measures . comparison between the previous similarity measures and the proposed similarity measure indicates that the proposed similarity measure does not provide any counter intuitive cases . moreover , it is demonstrated that the proposed similarity measure is capable of discriminating the difference between patterns .
an object oriented simulation framework for real time control of automated flexible manufacturing systems . <eos> this paper describes an object oriented simulation approach for the design of a flexable , manufacturing system that allows the implementation of control logic during the system design phase . the object oriented design approach is built around the formal theory of supervisory control based on finite automata . the formalism is used to capture inter object relationships that are difficult to identify in the object oriented design approach . the system resources are modeled as object classes based on the events that have to be monitored for real time control . real time control issues including deadlock resolution , resource failures in various modes of operation and recovery from failures while sustaining desirable logical system properties are integrated into the logical design for simulating the supervisory controller . ( c ) <digit> elsevier ltd. all rights reserved .
a fully distributed architecture for large scale workflow enactment . <eos> standard client server workflow management systems are usually designed as client server systems . the central server is responsible for the coordination of the workflow execution and , in some cases , may manage the activities database . this centralized control architecture may represent a single point of failure , which compromises the availability of the system . we propose a fully distributed and configurable architecture for workflow management systems . it is based on the idea that the activities of a case ( an instance of the process ) migrate from host to host , executing the workflow tasks , following a process plan . this core architecture is improved with the addition of other distributed components so that other requirements for workflow management systems , besides scalability , are also addressed . the components of the architecture were tested in different distributed and centralized configurations . the ability to configure the location of components and the use of dynamic allocation of tasks were effective for the implementation of load balancing policies .
finite difference model for one dimensional electro osmotic consolidation . <eos> small strain consolidation theories treat soil properties as being constant and uniform in the course of consolidation , which is not true in the case of electro osmosis induced consolidation practices . electro osmotic consolidation leads to large strain , which physically and electro chemically affects to a non negligible extent the nonlinear changes of the soil properties . for the nonlinear changes , iterative computations provide a mathematical approximation of the soil consolidation when the time steps and spatial geometry are intensively meshed . in this context , this paper presents a finite difference model , ec1 , for one dimensional electro osmotic consolidation , and this model is developed based on a fixed eulerian co ordinate system and uses a piecewise linear approximation . the model is able to account for the large strain induced nonlinear changes of the physical and electro chemical properties in a compressible mass subjected to electro osmotic consolidation and to predict the consolidation characteristics of the compressible mass . ec1 is verified against exact analytical solutions and test results obtained from an experimental program . example problems are illustrated with respect to the numerical solutions of large strain electro osmotic consolidation .
nonlinear characteristics of on chip spiral inductors under high rf power . <eos> this paper explores silicon cmos on chip spiral inductors performance degradation under high rf power . a novel methodology to calibrate and characterize on chip spiral inductor with large signal inputs ( high medium power ) is presented . experiments showed <digit> % degradation of quality factor in a particular inductor design when 34dbm rf power was applied . the degradation of quality factor of inductor can be attributed to a local self heating effect . thermal imaging of such an inductor under high rf power validates the hypothesis .
on d multiplicative secret sharing . <eos> a multiplicative secret sharing scheme allows players to multiply two secret shared field elements by locally converting their shares of the two secrets into an additive sharing of their product . multiplicative secret sharing serves as a central building block in protocols for secure multiparty computation ( mpc ) . motivated by open problems in the area of mpc , we introduce the more general notion of d multiplicative secret sharing , allowing to locally multiply d shared secrets , and study the type of access structures for which such secret sharing schemes exist . while it is easy to show that d multiplicative schemes exist if no d unauthorized sets of players cover the whole set of players , the converse direction is less obvious for da parts per thousand yen3 . our main result is a proof of this converse direction , namely that d multiplicative schemes do not exist if the set of players is covered by d unauthorized sets . in particular , t private d multiplicative secret sharing among k players is possible if and only if k > dt . our negative result holds for arbitrary ( possibly inefficient or even nonlinear ) secret sharing schemes and implies a limitation on the usefulness of secret sharing in the context of mpc . its proof relies on a quantitative argument inspired by communication complexity lower bounds .
shortest path amidst disc obstacles is computable . <eos> an open question in exact geometric computation is whether there re transcendental computations that can be made geometrically exact . perhaps the simplest such problem in computational geometry is that of computing the shortest obstacle avoiding path between two points p , q in the plane , where the obstacles re collection of n discs.this problem can be solved in o ( n <digit> log n ) time in the real ram model , but nothing was known about its computability in the standard ( turing ) model of computation . we first show the turing computability of this problem , provided the radii of the discs are rationally related . we make the usual assumption that the numerical input data are real algebraic numbers . by appealing to effective bounds from transcendental number theory , we further show single exponential time upper bound when the input numbers are rational.our result ppears to be the first example of non algebraic combinatorial problem which is shown computable . it is also rare example of transcendental number theory yielding positive computational results .
a uniform approach to constraint solving for lists , multisets , compact lists , and sets . <eos> lists , multisets , and sets are well known data structures whose usefulness is widely recognized in various areas of computer science . they have been analyzed from an axiomatic point of view with a parametric approach in dovier et al. <digit> , where the relevant unification algorithms have been developed . in this article , we extend these results considering more general constraints , namely , equality and membership constraints and their negative counterparts .
a competency framework for the stakeholders of a software process improvement initiative . <eos> the competencies ( a set of specific knowledges , skills , attitudes and behaviors e.g. stress handling , commitment , collaboration and identification of conflicts ) of the employees of software organizations are a fundamental element for the success of a software process improvement ( spi ) initiative . we performed three case studies to identify the competencies required for the stakeholders in an spi initiative . to identify these competencies , we observed the activities that each stakeholder performs and the interactions among them . we also identified the competencies that are required to perform those activities . we performed a classification of the identified competencies and integrated them into a framework . this framework defines the competencies for seven roles involved in an spi initiative and defines the level of expertise required by each role for each competency . to evaluate the framework , we performed ten interviews and two empirical tests . preliminary results show that this framework is relevant in spi initiatives , the use of this framework can raise the awareness about the competencies , and it can support some spi activities .
remote information concentration via ( w ) state reverse of ancilla free phase covariant telecloning . <eos> in this paper , we investigate generalized remote information concentration as the reverse process of ancilla free phase covariant telecloning ( afpct ) which is different from the reverse process of optimal universal telecloning . it is shown that the quantum information via ( <digit> rightarrow <digit> ) aepct procedure can be remotely concentrated back to a single qubit with a certain probability by utilizing ( non ) maximally entangled ( w ) states as quantum channels . our protocols are the generalization of wangs scheme ( open j microphys <digit> <digit> . doi <digit> . <digit> ojm . <digit> . <digit> , <digit> ) . and von neumann measure and positive operator valued measurement are performed in the maximal and non maximal cases respectively . relatively the former , the dimension of measurement space in the latter is greatly reduced . it makes the physical realization easier and suitable .
simulation of continuous fibre reinforced thermoplastic forming using a shell finite element with transverse stress . <eos> a shell finite element with transverse stress is presented in this paper in order to simulate the forming of thermoplastic composites reinforced with continuous fibres . it is shown by an experimental work that many porosities occurs through the thickness of the composite during the heating and the forming process . consequently the reconsolidation i.e. the porosity removing by applying a compressive stress through the thickness is a main point of the process . the presented shell finite element keeps the five degrees of freedom of the standard shell elements and adds a sixth one which is the variation in thickness . a locking phenomenon is avoided by uncoupling bending and pinching in the material law . a set of classical validation tests will prove the efficiency of this approach . finally a forming process is simulated . it shows that the computed transverse stresses are in good agreement with porosity removing in the experiments .
twiki a collaboration tool for the lhc . <eos> at the european laboratory for high energy physics , cern <digit> , the large hadron collider ( lhc ) <digit> accelerator is colliding beams of protons at energies of 3.5 tev , recreating conditions close to those at the origin of the universe . the four main lhc experiments , alice , atlas , cms and lhcb are complex detectors with millions of output channels . these experiment detectors , large as cathedrals , have been designed , built and are now operated by collaborations of physicists from universities and research institutes spread across the world . wikis are a perfect match to the collaborative nature of cern experiments and since twiki <digit> was installed at cern in <digit> it has grown in popularity and the statistics from april <digit> show nearly <digit> registered editors and about <digit> topics ( figure <digit> ) . since the start up of the lhc more and more users are accessing twiki requiring better server performance as well as finer control for read and write access and more features . this paper discusses the evolution of the use of twiki at cern .
a glimpse of symbolic statistical modeling by prism . <eos> we give a brief overview of a logic based symbolic modeling language prism which provides a unified approach to generative probabilistic models including bayesian networks , hidden markov models and probabilistic context free grammars . we include some experimental result with a probabilistic context free grammar extracted from the penn treebank . we also show em learning of a probabilistic context free graph grammar as an example of exploring a new area .
improving validity and reliability in longitudinal case study timelines . <eos> management information systems researchers rely on longitudinal case studies to investigate a variety of phenomena such as systems development , system implementation , and information systems related organizational change . however , insufficient attention has been spent on understanding the unique validity and reliability issues related to the timeline that is either explicitly or implicitly required in a longitudinal case study . in this paper , we address three forms of longitudinal timeline validity time unit validity ( which deals with the question of how to segment the timeline weeks , months , years , etc. ) , time boundaries validity ( which deals with the question of how long the timeline should be ) , and time period validity ( which deals with the issue of which periods should be in the timeline ) . we also examine timeline reliability , which deals with the question of whether another judge would have assigned the same events to the same sequence , categories , and periods . techniques to address these forms of longitudinal timeline validity include matching the unit of time to the pace of change to address time unit validity , use of member checks and formal case study protocol to address time boundaries validity , analysis of archival data to address both time unit and time boundary validity , and the use of triangulation to address timeline reliability . the techniques should be used to design , conduct , and report longitudinal case studies that contain valid and reliable conclusions . european journal of information systems ( <digit> ) <digit> , <digit> <digit> . doi 10.1057 ejis .2011.53 published online <digit> december <digit>
information privacy measuring individuals ' concerns about organizational practices . <eos> information privacy has been called one of the most important ethical issues of the information age . public opinion polls show rising levels of concern about privacy among americans . against this backdrop , research into issues associated with information privacy is increasing . based on a number of preliminary studies , it has become apparent that organizational practices , individuals ' perceptions of these practices , and societal responses are inextricably linked in many ways . theories regarding these relationships are slowly emerging . unfortunately , researchers attempting to examine such relationships through confirmatory empirical approaches may be impeded by the lack of validated instruments for measuring individuals ' concerns about organizational information privacy practices . to enable future studies in the information privacy research stream , we developed and validated an instrument that identifies and measures the primary dimensions of individuals ' concerns about organizational information privacy practices . the development process included examinations of privacy literature experience surveys and focus groups and the use of expert judges . the result was a parsimonious <digit> item instrument with four subscales tapping into dimensions of individuals ' concerns about organizational information privacy practices . the instrument was rigorously tested and validated across several heterogenous populations , providing a high degree of confidence in the scales ' validity , reliability , and generalizability .
using genetic programming and simulation to learn how to dynamically adapt the number of cards in reactive pull systems . <eos> we show how to learn dynamically to adapt the number of cards in real time in token based pull systems . we propose a simulation based genetic programming approach which does not need training sets . we illustrate how the approach can be implemented using arena and gp . a reactive conwip example show the efficiency of the approach and of the knowledge extracted . the resulting decision tree can be used online by production managers or for self adaptation issues .
a development and verification framework for the segbus platform . <eos> we describe the creation of a development framework for a platform based design approach , in the context of the segbus platform . the work intends to provide automated procedures for platform build up and application mapping . the solution is based on a model based process and heavily employs the uml . we develop a domain specific language to support the platform modeling . an emulator is consequently introduced to allow an as much as possible accurate performance estimation of the solution , at high abstraction levels . automated execution schedule generation is also featured . the resulting framework is applied to build actual design solutions for a mp3 decoder application .
a neuro fuzzy controller for speed control of a permanent magnet synchronous motor drive . <eos> this paper introduces a neuro fuzzy controller ( nfc ) for the speed control of a pmsm . a four layer neural network ( nn ) is used to adjust input and output parameters of membership functions in a fuzzy logic controller ( flc ) . the back propagation learning algorithm is used for training this network . the performance of the proposed controller is verified by both simulations and experiments . the hardware implementation of the controllers is made using a tms320f240 dsp . the results are compared with the results obtain from a proportional integral ( pi ) controller . simulation and experimental results indicate that the proposed nfc is reliable and effective for the speed control of the pmsm over a wide range of operations of the pmsm drive .
on the sorting complexity of suffix tree construction . <eos> the suffix tree of a string is the fundamental data structure of combinatorial pattern matching . we present a recursive technique for building suffix trees that yields optimal algorithms in different computational models . sorting is an inherent bottleneck in building suffix trees and our algorithms match the sorting lower bound . specifically , we present the following results . ( <digit> ) weiner <digit> , who introduced the data structure , gave an optimal o ( n ) time algorithm for building the suffix tree of an n character string drawn from a constant size alphabet . in the comparison model , there is a trivial n ( n log n ) time lower bound based on sorting , and weiner 's algorithm matches this bound . for integer alphabets , the fastest known algorithm is the o ( n log n ) time comparison based algorithm , but no super linear lower bound is known . closing this gap is the main open question in stringology . we settle this open problem by giving a linear time reduction to sorting for building suffix trees . since sorting is a lower bound for building suffix trees , this algorithm is time optimal in every alphabet model , in particular , for an alphabet consisting of integers in a polynomial range we get the first known linear time algorithm . ( <digit> ) all previously known algorithms for building suffix trees exhibit a marked absence of locality of reference , and thus they tend to elicit many page faults ( i os ) when indexing very long strings . they are therefore unsuitable for building suffix trees in secondary storage devices , where i os dominate the overall computational cost . we give a linear i o reduction to sorting for suffix tree construction . since sorting is a trivial i o lower bound for building suffix trees , our algorithm is i o optimal .
quasi birthdeath processes , tree like qbds , probabilistic <digit> counter automata , and pushdown systems . <eos> we begin by observing that ( discrete time ) quasi birthdeath processes ( qbds ) are equivalent , in a precise sense , to probabilistic <digit> counter automata ( p1cas ) , and both tree like qbds ( tl qbds ) and tree structured qbds ( ts qbds ) are equivalent to both probabilistic pushdown systems ( ppdss ) and recursive markov chains ( rmcs ) . we then proceed to exploit these connections to obtain a number of new algorithmic upper and lower bounds for central computational problems about these models . our main result is this for an arbitrary qbd , we can approximate its termination probabilities ( i.e. , its g g matrix ) to within i i bits of precision ( i.e. , within additive error <digit> 2i <digit> <digit> i ) , in time polynomial in both the encoding size of the qbd and in i i , in the unit cost rational arithmetic ram model of computation . specifically , we show that a decomposed newtons method can be used to achieve this . we emphasize that this bound is very different from the well known linear quadratic convergence of numerical analysis , known for qbds and tl qbds , which typically gives no constructive bound in terms of the encoding size of the system being solved . in fact , we observe ( based on recent results ) that for the more general tl qbds such a polynomial upper bound on newtons method fails badly . our upper bound proof for qbds combines several ingredients a detailed analysis of the structure of <digit> counter automata , an iterative application of a classic condition number bound for errors in linear systems , and a very recent constructive bound on the performance of newtons method for strongly connected monotone systems of polynomial equations . we show that the quantitative termination decision problem for qbds ( namely , is gu , v <digit> <digit> g u , v <digit> <digit> ) is at least as hard as long standing open problems in the complexity of exact numerical computation , specifically the square root sum problem . on the other hand , it follows from our earlier results for rmcs that any non trivial approximation of termination probabilities for tl qbds is sqrt root sum hard .
composite kernel learning . <eos> the support vector machine is an acknowledged powerful tool for building classifiers , but it lacks flexibility , in the sense that the kernel is chosen prior to learning . multiple kernel learning enables to learn the kernel , from an ensemble of basis kernels , whose combination is optimized in the learning process . here , we propose composite kernel learning to address the situation where distinct components give rise to a group structure among kernels . our formulation of the learning problem encompasses several setups , putting more or less emphasis on the group structure . we characterize the convexity of the learning problem , and provide a general wrapper algorithm for computing solutions . finally , we illustrate the behavior of our method on multi channel data where groups correspond to channels .
recent progress in linear algebra and lattice basis reduction . <eos> a general goal concerning fundamental linear algebra problems is to reduce the complexity estimates to essentially the same as that of multiplying two matrices ( plus possibly a cost related to the input and output sizes ) . among the bottlenecks one usually finds the questions of designing a recursive approach and mastering the sizes of the intermediately computed data . in this talk we are interested in two special cases of lattice basis reduction . we consider bases given by square matrices over k x or z , with , respectively , the notion of reduced form and lll reduction . our purpose is to introduce basic tools for understanding how to generalize the lehmer and knuth schnhage gcd algorithms for basis reduction . over k x this generalization is a key ingredient for giving a basis reduction algorithm whose complexity estimate is essentially that of multiplying two polynomial matrices . such a problem relation between integer basis reduction and integer matrix multiplication is not known . the topic receives a lot of attention , and recent results on the subject show that there might be room for progressing on the question .
quantifying content consistency improvements through opportunistic contacts . <eos> contacts between mobile users provide opportunities for data updates that supplement infrastructure based mechanisms . while the benefits of such opportunistic sharing are intuitive , quantifying the capacity increase they give rise to is challenging because both contact rates and contact graphs depend on the structure of the social networks users belong to . furthermore , social connectivity influences not only users ' interests , i.e. , the content they own , but also their willingness to share data with others . all these factors can have a significant effect on the capacity gains achievable through opportunistic contacts . this paper 's main contribution is in developing a tractable model for estimating such gains in a content update system , where content originates from a server along multiple channels , with blocks of information in each channel updated at a certain rate , and users differ in their contact graphs , interests , and willingness to share content , e.g. , only to the members of their own social networks . we establish that the added capacity available to improve content consistency through opportunistic sharing can be obtained by solving a convex optimization problem . the resulting optimal policy is evaluated using traces reflecting contact graphs in different social settings and compared to heuristic policies . the evaluation demonstrates the capacity gains achievable through opportunistic sharing , and the impact on those gains of the structure of the underlying social network .
study of the communication distance of a mems pressure sensor integrated in a rfid passive tag . <eos> the performance of a mems ( micro electro mechanical systems ) sensor in a rfid system has been calculated , simulated and analyzed . it documents the viability from the power consumption point of view of integrating a mems sensor in a passive tag maintaining its long range . the wide variety of sensors let us specify as many applications as the imagination is able to create . the sensor tag works without battery , and it is remotely powered through a commercial reader accomplishing the epc standard class <digit> gen <digit> . the key point is the integration in the tag of a very low power consumption pressure mems sensor . the power consumption of the sensor is 12.5 mu w. the specifically developed rfid cmos passive module , with an integrated temperature sensor , is able to communicate up to 2.4 meters . adding the pressure mems sensor an input capacity , a maximum range of <digit> meters can be achieved between the rfid sensor tag and a commercial reader ( typical reported range for passive pressure sensors are in the range of a few centimeters ) . the rfid module has been fabricated with a cmos process compatible with a bulk micromachining mems process . so , the feasibility of a single chip is presented .
a hardware architecture for real time image compression using a searchless fractal image coding method . <eos> in this paper we present a novel hardware architecture for real time image compression implementing a fast , searchless iterated function system ( sifs ) fractal coding method . in the proposed method and corresponding hardware architecture , domain blocks are fixed to a spatially neighboring area of range blocks in a manner similar to that given by furao and hasegawa . a quadtree structure , covering from <digit> blocks down to <digit> blocks , and even to single pixels , is used for partitioning . coding of <digit> blocks and single pixels is unique among current fractal coders . the hardware architecture contains units for domain construction , zig zag transforms , range and domain mean computation , and a parallel domain range match capable of concurrently generating a fractal code for all quadtree levels . with this efficient , parallel hardware architecture , the fractal encoding speed is improved dramatically . additionally , attained compression performance remains comparable to traditional search based and other searchless methods . experimental results , with the proposed hardware architecture implemented on an altera apex20k fpga , show that the fractal encoder can encode a <digit> image in approximately 8.36 ms operating at 32.05 mhz . therefore , this architecture is seen as a feasible solution to real time fractal image compression .
distinguishing views in symmetric networks a tight lower bound . <eos> the view of a node in a port labeled network is an infinite tree encoding all walks in the network originating from this node . we prove that for any integers n d <digit> n d <digit> , there exists a port labeled network with at most n nodes and diameter at most d which contains a pair of nodes whose ( infinite ) views are different , but whose views truncated to depth ( dlog ( n d ) ) ( d log ( n d ) ) are identical .
cache injection for parallel applications . <eos> for two decades , the memory wall has affected many applications in their ability to benefit from improvements in processor speed . cache injection addresses this disparity for i o by writing data into a processor 's cache directly from the i o bus . this technique reduces data latency and , unlike data prefetching , improves memory bandwidth utilization . these improvements are significant for data intensive applications whose performance is dominated by compulsory cache misses . we present an empirical evaluation of three injection policies and their effect on the performance of two parallel applications and several collective micro benchmarks . we demonstrate that the effectiveness of cache injection on performance is a function of the communication characteristics of applications , the injection policy , the target cache , and the severity of the memory wall . for example , we show that injecting message payloads to the l3 cache can improve the performance of network bandwidth limited applications . in addition , we show that cache injection improves the performance of several collective operations , but not all to all operations ( implementation dependent ) . our study shows negligible pollution to the target caches .
cryptanalysis and improvement of an access control in user hierarchy based on elliptic curve cryptosystem . <eos> in a key management scheme for hierarchy based access control , each security class having higher clearance can derive the cryptographic secret keys of its other security classes having lower clearances . in <digit> , chung et al. proposed an efficient scheme on access control in user hierarchy based on elliptic curve cryptosystem information sciences <digit> ( <digit> ) ( <digit> ) <digit> . their scheme provides solution of key management efficiently for dynamic access problems . however , in this paper , we propose an attack on chung et al. s scheme to show that chung et al. s scheme is insecure against the exterior root finding attack . we show that under this attack , an attacker ( adversary ) who is not a user in any security class in a user hierarchy attempts to derive the secret key of a security class by using the root finding algorithm . in order to remedy this attack , we further propose a simple improvement on chung et al. s scheme . overall , the main theme of this paper is very simple a security flaw is presented on chung et al. s scheme and then a fix is provided in order to remedy the security flaw found in chung et al. s scheme .
use of molecular modeling , docking , and 3d qsar studies for the determination of the binding mode of benzofuran <digit> yl ( indol <digit> yl ) maleimides as gsk <digit> beta inhibitors . <eos> molecular modeling and docking studies along with three dimensional quantitative structure relationships ( 3d qsar ) studies have been used to determine the correct binding mode of glycogen synthase kinase <digit> beta ( gsk <digit> beta ) inhibitors . the approaches of comparative molecular field analysis ( comfa ) and comparative molecular similarity index analysis ( comsia ) are used for the 3d qsar of <digit> substituted benzofuran <digit> yl ( indol <digit> yl ) maleimides as gsk <digit> beta inhibitors . two binding modes of the inhibitors to the binding site of gsk <digit> beta are investigated . the binding mode <digit> yielded better 3d qsar correlations using both comfa and comsia methodologies . the three component comfa model from the steric and electrostatic fields for the experimentally determined pic ( <digit> ) values has the following statistics r ( <digit> ) ( cv ) 0.386 nd se ( cv ) 0.854 for the cross validation , and r ( <digit> ) 0.811 and se 0.474 for the fitted correlation . f ( 3,47 ) 67.034 , and probability of r ( <digit> ) <digit> ( 3,47 ) 0.000 . the binding mode suggested by the results of this study is consistent with the preliminary results of x ray crystal structures of inhibitor bound gsk <digit> beta . the 3d qsar models were used for the estimation of the inhibitory potency of two additional compounds .
on the numerical solution of some semilinear elliptic problems ii . <eos> in the earlier paper <digit> , a galerkin method was proposed and analyzed for the numerical solution of a dirichlet problem for a semi linear elliptic boundary value problem of the form delta u f ( ( . ) , u ) . this was converted to a problem on a standard domain and then converted to an equivalent integral equation . galerkina 's method was used to solve the integral equation , with the eigenfunctions of the laplacian operator on the standard domain d as the basis functions . in this paper we consider the implementing of this scheme , and we illustrate it for some standard domains d.
supporting mobile work processes in logistics with wearable computing . <eos> logistics is a very dynamic and heterogeneous application area which generates complex requirements regarding the development of information and communication technologies ( ict ) . for this area , it is a challenge to support mobile workers on site in an unobtrusive manner . in this contribution , wearable computing technologies are investigated as basis for a mobile worker supporting system for tasks at an automobile terminal . the features of wearable computing technologies are checked against the requirements of the application area to come to an usable and acceptable mobile solution in an user centred design process .
a nonlinear domain decomposition formulation with application to granular dynamics . <eos> simulation of granular media undergoing dynamic evolution involves nonsmooth problems when grains are modeled as rigid bodies . with dense samples , this nonsmoothness occurs everywhere in the studied domain , and large sized systems lead to computationally intensive simulations . in this article , we combine domain decomposition approaches and nonsmooth contact dynamics . unlike the smooth continuum media case , a coarse space problem does not trivially increase the convergence rate , as it is exemplified in this article , with semi analytical examples and real size numerical simulations . nevertheless , the description of an underlying force network in the samples may guide the analysis for new approximation schemes or algorithms .
minimum cut linear arrangement of p q dags for vlsi layout of adder trees . <eos> two algorithms for minimum cut linear arrangement of a class of graphs called p q dags are proposed . a p q dag represents the connection scheme of an adder tree , such as wallace tree , and the vlsi layout problem of a bit slice of an adder tree is treated as the minimum cut linear arrangement problem of its corresponding p q dag . one of the two algorithms is based on dynamic programming . it calculates an exact minimum solution within n ( o ( <digit> ) ) time and space , where n is the size of a given graph . the other algorithm is an approximation algorithm which calculates a solution with o ( log n ) cutwidth . it requires o ( n log n ) time .
optimization of illumination environmental factors based on orthogonal test . <eos> this paper presents a comparative research of nine different combinations of imaging environmental factors using orthogonal test approach to gain optimal illumination in an image acquisition device which was self designed . the effect of four different environmental factors such as shoot distance , lamp number , lamp height , lamp side distance have been investigated on the key parameters . experimental results based on l ( <digit> ) ( <digit> ) ( <digit> ) orthogonal test design shows that under different combination of environmental factors , there are obvious differences between illumination intensity and illumination uniformity of images and which are mainly affected by the shoot distance and lamp number . based on these experiments , we get two preferable combinations . attention is concentrated on finding the best . through further analysis and discussion , the best combination is identified . our experimental results indicate that orthogonal test here is very suitable for gaining optimal environmental factors .
linear b cell epitope prediction for epitope vaccine design against meningococcal disease and their computational validations through physicochemical properties . <eos> neisseria meningitidis serogroup b is predominantly known for its leading role in bacterial meningitis and septicemia worldwide . although , polysaccharide conjugate vaccines have been developed and used successfully against many of the serogroups of n. meningitidis , such strategy has proved ineffective against group b meningococci . here , we proposed to develop peptide epitope based vaccine candidates from outer membrane ( om ) protein contained in the outer membrane vesicles ( omv ) based on our in silico analysis . in omv , a total of <digit> proteins were identified , only <digit> ( 6.4 % ) of which were predicted to be located in the outer membrane . for the preparation of specific monoclonal antibodies against pathogenic bacterial protein , identification and selection of b cell epitopes that act as a vaccine target are required . we selected <digit> outer membrane proteins from omv proteins while taking into consideration the removal of cross reactivity . epitopia web server was used for the prediction of b cell epitopes . epitopes are distinguished from non epitopes by properties such as amino acid preference on the basis of amino acid composition , secondary structure composition , and evolutionary conservation . predicted results were subject to verification with experimental data and we performed string based search through iedb . our finding shows that epitopes have general preference for charged and polar amino acids epitopes are enriched with loop as a secondary structure element that renders them flexible and also exposes another view of antibodyantigen interaction .
object recognition using proportion based prior information application to fisheries acoustics . <eos> this paper addresses the inference of probabilistic classification models using weakly supervised learning . the main contribution of this work is the development of learning methods for training datasets consisting of groups of objects with known relative class priors . this can be regarded as a generalization of the situation addressed by bishop and ulusoy ( <digit> ) , where training information is given as the presence or absence of object classes in each set . generative and discriminative classification methods are conceived and compared for weakly supervised learning , as well as a non linear version of the probabilistic discriminative models . the considered models are evaluated on standard datasets and an application to fisheries acoustics is reported . the proposed proportion based training is demonstrated to outperform model learning based on presence absence information and the potential of the non linear discriminative model is shown .
hes frequencyamplitude formulation for nonlinear oscillators with an irrational force . <eos> in this paper , hes frequencyamplitude formulation is applied to determine the periodic solution for a nonlinear oscillator system with an irrational force . comparison with the exact solution shows that the result obtained is of high accuracy .
the effectiveness of a training method using self modeling webcam photos for reducing musculoskeletal risk among office workers using computers . <eos> an intervention study was conducted to examine the effectiveness of an innovative self modeling photo training method for reducing musculoskeletal risk among office workers using computers . sixty workers were randomly assigned to either <digit> ) a control group <digit> ) an office training group that received personal , ergonomic training and workstation adjustments or <digit> ) a photo training group that received both office training and an automatic frequent feedback system that displayed on the computer screen a photo of the workers current sitting posture together with the correct posture photo taken earlier during office training . musculoskeletal risk was evaluated using the rapid upper limb assessment ( rula ) method before , during and after the six weeks intervention . both training methods provided effective short term posture improvement however , sustained improvement was only attained with the photo training method . both interventions had a greater effect on older workers and on workers suffering more musculoskeletal pain . the photo training method had a greater positive effect on women than on men .
the influence of feedback on egocentric distance judgments in real and virtual environments . <eos> a number of investigators have reported that distance judgments in virtual environments ( ves ) are systematically smaller than distance judgments made in comparably sized real environments . many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression . one approach to this problem that has implications for both ve applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user . most generally , we asked whether feedback within a virtual environment would lead to more accurate estimations of distance . next , given the prediction that some change in behavior would be observed , we asked whether specific adaptation effects would generalize to other indications of distance . finally , we asked whether these effects would transfer from the ve to the real world . all distance judgments in the head mounted display ( hmd ) became near accurate after three different forms of feedback were given within the hmd . however , not all feedback sessions within the hmd altered real world distance judgments . these results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for ve applications .
synchronous versus asynchronous collaboration in situated multi agent systems . <eos> according to the taxonomy for agent activity , proposed by v. parunak , a collaboration is an interaction between agents of a multi agent system ( mas ) whereby the agents explicitly coordinate their actions before they cooperate . we discuss two sub types of collaboration in the context of situated mass , namely asynchronous and synchronous collaboration . after setting up collaboration , the interaction between the agents in an asynchronous collaboration happens indirectly through the environment . agents direct their actions via the perceived state change of their environment . on the other hand , during a synchronous collaboration agents have to act simultaneously and this requires an additional agreement about which actions should be executed . although they both fit the characteristics of collaboration , the requirements for their implementation is quite different . whereas agents in an asynchronous collaboration can be implemented as separate processes that act directly into the environment , the implementation of synchronous collaboration is more complex since it requires support for simultaneous actions . in the paper we give examples of both kinds of collaborations and outline the necessary support for their implementation .
web services compositions modelling and choreographies analysis . <eos> in rouached et al. ( <digit> ) and rouached and godart ( <digit> ) the authors described the semantics of wsbpel by way of mapping each of the wsbpel ( arkin et al. , <digit> ) constructs to the ec algebra and building a model of the process behaviour . with these mapping rules , the authors describe a modelling approach of a process defined for a single web service composition . however , this modelling is limited to a local view and can only be used to model the behaviour of a single process . the authors further the semantic mapping to include web service composition interactions through modelling web service conversations and their choreography . this paper elaborates the models to support a view of interacting web service compositions extending the mapping from wsbpel to ec , and including web service interfaces ( wsdl ) for use in modelling between services . the verification and validation techniques are also exposed while automated induction based theorem prover is used as verification back end .
pnp problem revisited . <eos> perspective n point camera pose determination , or the pnp problem , has attracted much attention in the literature . this paper gives a systematic investigation on the pnp problem from both geometric and algebraic standpoints , and has the following contributions firstly , we rigorously prove that the pnp problem under distance based definition is equivalent to the pnp problem under orthogonal transformation based definition when n > <digit> , and equivalent to the pnp problem under rotation transformation based definition when n <digit> . secondly , we obtain the upper bounds of the number of solutions for the pnp problem under different definitions . in particular , we show that for any three non collinear control points , we can always find out a location of optical center such that the p3p problem formed by these three control points and the optical center can have <digit> solutions , its upper bound . additionally a geometric way is provided to construct these <digit> solutions . thirdly , we introduce a depth ratio based approach to represent the solutions of the whole pnp problem . this approach is shown to be advantageous over the traditional elimination techniques . lastly , degenerated cases for coplanar or collinear control points are also discussed . surprisingly enough , it is shown that if all the control points are collinear , the pnp problem under distance based definition has a unique solution , but the pnp problem under transformation based definition is only determined up to one free parameter .
example driven animation synthesis . <eos> we introduce an easy and intuitive approach to create animations by assembling existing animations . using our system , the user needs only to simply scribble regions of interest and select the example animations that he she wants to apply . our system will then synthesize a transformation for each triangle and solve an optimization problem to compute the new animation for this target mesh . like playing a jigsaw puzzle game , even a novice can explore his her creativity by using our system without learning complicated routines , but just using a few simple operations to achieve the goal .
macroblock and motion feature analysis to h. <digit> avc fast inter mode decision . <eos> one fast inter mode decision algorithm is proposed in this paper . the whole algorithm is divided into two stages . in the pre stage , by exploiting spatial and temporal information of encoded macrobocks ( mbs ) , a skip mode early detection scheme is proposed . the homogeneity of current mb is also analyzed to filter out small inter modes in this stage . secondly , during the block matching stage , a motion feature based inter mode decision scheme is introduced by analyzing the motion vector predictor 's accuracy , the block overlapping situation and the smoothness of sad ( sum of absolute difference ) value . moreover , the rate distortion cost is checked in an early stage and we set some constraints to speed up the whole decision flow . experiments show that our algorithm can achieve a speed up factor of up to 53.4 % for sequences with different motion type . the overall bit increment and quality degradation is negligible compared with existing works .
the design of parsers for incremental language processors . <eos> an incremental language processor is one that accepts as input a sequence of substrings of the source language and maps them independently onto fragments in some object code . the ordered sequence of these object code fragments are then either compiled , in which case we have an incremental compiler , or interpreted . in the first case the advantage resulting is that subsequent changes in the source program entail only reprocessing the source fragments affected and recompiling the updated collection of object code fragments . in an environment where small changes are made frequently to large programs , e.g. debugging , the curtailment of reprocessing is attractive . in the second case the object code fragments are the actual run time program representation , and hence inter fragment relations are transiently evaluated as needed in the process of execution , with no long term preservation of these relationships beyond the scope of their immediate need in execution time . this permits the possibility of program recomposition in the midst of execution , one of the principal characteristics of conversational computing . many conversational language processors execute a program representation functionally analogous to parse trees , i.e. the syntax analysis of a fragment , insofar as it is possible , is done at fragment load time . this representation choice is popular because many of the expensive aspects of interpretation , including character string scanning , symbol table lookup , and parsing , are performed once only and do not contribute to the execution overhead . this paper is devoted to examining the question of the construction of such a parser in a general manner for an arbitrary source language .
on the computation of all supported efficient solutions in multi objective integer network flow problems . <eos> this paper presents a new algorithm for identifying all supported non dominated vectors ( or outcomes ) in the objective space , as well as the corresponding efficient solutions in the decision space , for multi objective integer network flow problems . identifying the set of supported non dominated vectors is of the utmost importance for obtaining a first approximation of the whole set of non dominated vectors . this approximation is crucial , for example , in two phase methods that first compute the supported non dominated vectors and then the unsupported non dominated ones . our approach is based on a negative cycle algorithm used in single objective minimum cost flow problems , applied to a sequence of parametric problems . the proposed approach uses the connectedness property of the set of supported non dominated vectors efficient solutions to find all integer solutions in maximal non dominated efficient facets .
two objective method for crisp and fuzzy interval comparison in optimization . <eos> in real optimization we always meet two main groups of criteria requirements of useful outcomes increasing or expenses decreasing and demands of lower uncertainty or , in other words , risk minimization . therefore , it seems advisable to formulate optimization problem under conditions of uncertainty , at least , two objective on the basis of local criteria of outcomes increasing or expenses reduction and risk minimization . generally , risk may be treated as the uncertainty of obtained result . in the considered situation , the degree of risk ( uncertainty ) may be defined in a natural way through the width of final interval objective function at the point of optimum achieved . to solve the given problem , the two objective interval comparison technique has been developed taking into account the probability of supremacy of one interval over the other one and relation of compared widths of intervals . to illustrate the efficiency of the proposed method , the simple examples of minimization of interval double extreme discontinuous cost function and fuzzy extension of rosenbrock 's test function are presented .
development of head detection and tracking systems for visual surveillance . <eos> this paper proposes a technique for the detection of head nod and shake gestures based on eye tracking and head motion decision . the eye tracking step is divided into face detection and eye location . here , we apply a motion segmentation algorithm that examines differences in moving peoples faces . this system utilizes a hidden markov model based head detection module that carries out complete detection in the input images , followed by the eye tracking module that refines the search based on a candidate list provided by the preprocessing module . the novelty of this paper is derived from differences in real time input images , preprocessing to remove noises ( morphological operators and so on ) , detecting edge lines and restoration , finding the face area , and cutting the head candidate . moreover , we adopt a k means algorithm for finding the head region . real time eye tracking extracts the location of eyes from the detected face region and is performed at close to a pair of eyes . after eye tracking , the coordinates of the detected eyes are transformed into a normalized vector of x coordinate and y coordinate . head nod and shake detector uses three hidden markov models ( hmms ) . hmm representation of the head detection can estimate the underlying hmm states from a sequence of face images . head nod and shake can be detected by three hmms that are adapted by a directional vector . the directional vector represents the direction of the head movement . the vector is hmms for determining neutral as well as head nod and shake . these techniques are implemented on images , and notable success is notified .
scheduling of printed circuit board ( pcb ) assembly systems with heterogeneous processors using simulation based intelligent optimization methods . <eos> the complexity of printed circuit boards ( pcbs ) , as an important sector of the electronics manufacturing industry , has increased over the last three decades . this paper focuses on a practical application observed at a pcb assembly line of electronics manufacturing facility . it is shown that this problem is equivalent to a flowshop scheduling with multiple heterogeneous batch processors where processors can perform multiple tasks as long as the sizes of jobs in a batch do not violate the processors capacity . the equivalent problem is mathematically formulated as a mixed integer programming model . then , a monte carlo simulation is incorporated into high level genetic algorithm based intelligent optimization techniques to assess the performance of makespan oriented system under uncertain processing times . at each iteration of algorithm , the output of simulator is used by optimizers to provide online feedbacks on the progress of the search and direct the search toward a promising solution zone . furthermore , various parameters and operators of the algorithm are discussed and calibrated by means of taguchi statistical technique . the result of extensive computational experiments shows that the solution approach gives high quality solutions in reasonable computational time .
multi agent systems with reinforcement hierarchical neuro fuzzy models . <eos> this paper introduces a new multi agent model for intelligent agents , called reinforcement learning hierarchical neuro fuzzy multi agent system . this class of model uses a hierarchical partitioning of the input space with a reinforcement learning algorithm to overcome limitations of previous rl methods . the main contribution of the new system is to provide a flexible and generic model for multi agent environments . the proposed generic model can be used in several applications , including competitive and cooperative problems , with the autonomous capacity to create fuzzy rules and expand their own rule structures , extracting knowledge from the direct interaction between the agents and the environment , without any use of supervised algorithms . the proposed model was tested in three different case studies , with promising results . the tests demonstrated that the developed system attained good capacity of convergence and coordination among the autonomous intelligent agents .
possibilistic meanvariance models and efficient frontiers for portfolio selection problem . <eos> in this paper , it is assumed that the rates of return on assets can be expressed by possibility distributions rather than probability distributions . we propose two kinds of portfolio selection models based on lower and upper possibilistic means and possibilistic variances , respectively , and introduce the notions of lower and upper possibilistic efficient portfolios . we also present an algorithm which can derive the explicit expression of the possibilistic efficient frontier for the possibilistic mean variance portfolio selection problem dealing with lower bounds on asset holdings .
millennials among the professional workforce in academic libraries their perspective on leadership . <eos> this study explores possible leadership perceptions of millennials working in academic libraries , specifically their definition , the attributes they associate with leadership , whether they want to assume formal leadership roles , whether they perceive themselves as leaders , and whether they perceive leadership opportunities within their organizations and lis professional associations . an online survey was utilized to gather the responses and the study participants comprised of millennials ( born <digit> or after ) currently working full time in libraries that were a member of the committee on institutional cooperation ( cic ) , a consortium of the big ten universities and the university of chicago in <digit> .
mesh fusion using functional blending on topologically incompatible sections . <eos> three dimensional mesh fusion provides an easy and fast way to create new mesh models from existing ones . we introduce a novel approach of mesh fusion in this paper based on functional blending . our method has no restriction of disk like topology or one ring opening on the meshes to be merged . first of all , sections with boundaries of the under fusing meshes are converted into implicit representations . an implicit transition surface , which joins the sections together while keeping smoothness at the boundaries , is then created based on cubic hermite functional blending . finally , the implicit surface is tessellated to form the resultant mesh . our scheme is both efficient and simple , and with it users can easily construct interesting , complex 3d models .
imaging spectrometry and asphalt road surveys . <eos> this study integrates ground spectrometry , imaging spectrometry , and in situ pavement condition surveys for asphalt road assessment . field spectra showed that asphalt aging and deterioration produce measurable changes in spectra as these surfaces undergo a transition from hydrocarbon dominated new roads to mineral dominated older roads . several spectral measures derived from field and image spectra correlated well with pavement quality indicators . spectral confusion between pavement material aging and asphalt mix erosion on the one hand , and structural road damages ( e.g. cracking ) on the other , poses some limits to remote sensing based mapping . both the common practice methods ( pavement management system pms , in situ vehicle inspections ) , and analysis using imaging spectrometry are effective in identifying roads in good and very good condition . variance and uncertainty in all survey data ( pms , in situ vehicle inspections , remote sensing ) increases for road surfaces in poor condition and clear determination of specific ( and expensive ) surface treatment decisions remains problematic from these methods .
conceptions of learning versus conceptions of web based learning the differences revealed by college students . <eos> past research has shown the variations of students ' conceptions of learning , but little has been especially undertaken to address students ' conceptions of web based learning and to make comparisons between students ' conceptions of learning in general and their conceptions of web based learning in particular . by interviewing <digit> taiwanese college students with some web based learning experiences , this study attempted to investigate the students ' conceptions of learning , conceptions of web based learning , and the differences between these conceptions . using the phenomenographic method of analyzing student interview transcripts , several categories of conceptions of learning and of web based learning were revealed . the analyses of interview results suggested that the conceptions of web based learning were often more sophisticated than those of learning . for example , much more students conceptualized learning in web based context as pursuing real understanding and seeing in a new way than those for learning in general . this implies that the implementation of web based instruction may be a potential avenue for promoting students ' conceptions of learning . by gathering questionnaire responses from the students , this study further found that the sophistication of the conceptions toward web based learning was associated with better searching strategies as well as higher self efficacy for web based learning . ( c ) <digit> elsevier ltd. all rights reserved .
on the behaviour of weighted multi recombination evolution strategies optimising noisy cigar functions . <eos> cigar functions are convex quadratic functions that are characterised by the presence of only two distinct eigenvalues of their hessian , the smaller one of which occurs with multiplicity one . their ridge like topology makes them a useful test case for optimisation strategies . this paper extends previous work on modelling the behaviour of evolution strategies with isotropically distributed mutations optimising cigar functions by considering weighted recombination as well as the effects of noise on optimisation performance . it is found that the same weights that have previously been seen to be optimal for the sphere and parabolic ridge functions are optimal for cigar functions as well . the influence of the presence of noise on optimisation performance depends qualitatively on the trajectory of the search point , which in turn is determined by the strategy 's mutation strength as well as its population size and recombination weights . analytical results are obtained for the case of cumulative step length adaptation .
human emotion recognition from videos using spatio temporal and audio features . <eos> in this paper , we present human emotion recognition systems based on audio and spatio temporal visual features . the proposed system has been tested on audio visual emotion data set with different subjects for both genders . the mel frequency cepstral coefficient ( mfcc ) and prosodic features are first identified and then extracted from emotional speech . for facial expressions spatio temporal features are extracted from visual streams . principal component analysis ( pca ) is applied for dimensionality reduction of the visual features and capturing <digit> % of variances . codebook is constructed for both audio and visual features using euclidean space . then occurrences of the histograms are employed as input to the state of the art svm classifier to realize the judgment of each classifier . moreover , the judgments from each classifier are combined using bayes sum rule ( bsr ) as a final decision step . the proposed system is tested on public data set to recognize the human emotions . experimental results and simulations proved that using visual features only yields on average 74.15 % accuracy , while using audio features only gives recognition average accuracy of 67.39 % . whereas by combining both audio and visual features , the overall system accuracy has been significantly improved up to 80.27 % .
fortification of rice technologies and nutrients . <eos> this article provides a comprehensive review of the currently available technologies for vitamin and mineral rice fortification . it covers currently used technologies , such as coating , dusting , and the various extrusion technologies , with the main focus being on cold , warm , and hot extrusion technologies , including process flow , required facilities , and sizes of operation . the advantages and disadvantages of the various processing methods are covered , including a discussion on micronutrients with respect to their technical feasibility during processing , storage , washing , and various cooking methods and their physiological importance . the microstructure of fortified rice kernels and their properties , such as visual appearance , sensory perception , and the impact of different micronutrient formulations , are discussed . finally , the article covers recommendations for quality control and provides a summary of clinical trials .
a perceptual mapping procedure for analysis of proximity data to determine common and unique product market structures . <eos> traditional techniques of perceptual mapping hypothesize that products are differentiated in a common perceptual space of attributes . this paper suggests that each product is differentiated not only in a common perceptual space , but also a unique perceptual space consisting of as many dimensions as the number of products . it provides a model and estimation procedure based on alternating least squares for estimating the model parameters .
on the design and prototype implementation of a multimodal situation aware system . <eos> in this paper we describe the design concepts and prototype implementation of a situation aware ubiquitous computing system using multiple modalities such as national marine electronics association ( nmea ) data from global positioning system ( gps ) receivers , text , speech , environmental audio , and handwriting inputs . while most mobile and communication devices know where and who they are , by accessing context information primarily in the form of location , time stamps , and user identity , the concept of sharing of this information in a reliable and intelligent fashion is crucial in many scenarios . a framework which takes the concept of context aware computing to the level of situation aware computing by intelligent information exchange between context aware devices is designed and implemented in this work . four sensual modes of contextual information like text , speech , environmental audio , and handwriting are augmented to conventional contextual information sources like location from gps , user identity based on ip addresses ( ipa ) , and time stamps . each device derives its context not necessarily using the same criteria or parameters but by employing selective fusion and fission of multiple modalities . the processing of each individual modality takes place at the client device followed by the summarization of context as a text file . exchange of dynamic context information between devices is enabled in real time to create multimodal situation aware devices . a central repository of all user context profiles is also created to enable self learning devices in the future . based on the results of simulated situations and real field deployments it is shown that the use of multiple modalities like speech , environmental audio , and handwriting inputs along with conventional modalities can create devices with enhanced situational awareness .
neural association of microcalcification patterns for their reliable classification in digital mammography . <eos> breast cancer continues to be the most common cause of cancer deaths in women . early detection of breast cancer is significant for better prognosis . digital mammography currently offers the best control strategy for the early detection of breast cancer . the research work in this paper investigates the significance of neural association of microcalcification patterns for their reliable classification in digital mammograms . the proposed technique explores the auto associative abilities of a neural network approach to regenerate the composite of its learned patterns most consistent with the new information , thus the regenerated patterns can uniquely signify each input class and improve the overall classification . two types of features computer extracted ( gray level based statistical ) features and human extracted ( radiologists ' interpretation ) features are used for the classification of calcification type of breast abnormalities . the proposed technique attained the highest 90.5 % classification rate on the calcification testing dataset .
modeling hemodynamics in an unoccluded and partially occluded inferior vena cava under rest and exercise conditions . <eos> pulmonary embolism is the third leading cause of death in hospitalized patients in the us . vena cava filters are medical devices inserted into the inferior vena cava ( ivc ) and are designed to trap thrombi before they reach the lungs . once trapped in a filter , however , thrombi disturb otherwise natural flow patterns , which may be clinically significant . the goal of this work is to use computational modeling to study the hemodynamics of an unoccluded and partially occluded ivc under rest and exercise conditions . a realistic , three dimensional model of the ivc , iliac , and renal veins represents the vessel geometry and spherical clots represent thombi trapped by several conical filter designs . inflow rates correspond to rest and exercise conditions , and a transitional turbulence model captures transitional flow features , if they are present . the flow equations are discretized and solved using a second order finite volume method . no significant regions of transitional flow are observed . nonetheless , the volume of stagnant and recirculating flow increases with partial occlusion and exercise . for the partially occluded vessel , large wall shear stresses are observed on the ivc and on the model thrombus , especially under exercise conditions . these large wall shear stresses may have mixed clinical implications thrombotic like behavior may initiate on the vessel wall , which is undesirable and thrombolysis may be accelerated , which is desirable .
recent advances in direct methods for solving unsymmetric sparse systems of linear equations . <eos> during the past few years , algorithmic improvements alone have reduced the time required for the direct solution of unsymmetric sparse systems of linear equations by almost an order of magnitude . this paper compares the performance of some well known software packages for solving general sparse systems . in particular , it demonstrates the consistently high level of performance achieved by wsmp the most recent of such solvers . it compares the various algorithmic components of these solvers and discusses their impact on solver performance . our experiments show that the algorithmic choices made in wsmp enable it to run more than twice as fast as the best among similar solvers and that wsmp can factor some of the largest sparse matrices available from real applications in only a few seconds on a <digit> cpu workstation . thus , the combination of advances in hardware and algorithms makes it possible to solve those general sparse linear systems quickly and easily that might have been considered too large until recently .
a linear goal programming approach to determining the relative importance weights of customer requirements in quality function deployment . <eos> quality function deployment ( qfd ) is a planning tool used in new product development and quality management . it aims at achieving maximum customer satisfaction by listening to the voice of customers . to implement qfd , customer requirements ( crs ) should be identified and assessed first . the current paper proposes a linear goal programming ( lgp ) approach to assess the relative importance weights of crs . the lgp approach enables customers to express their preferences on the relative importance weights of crs in their preferred or familiar formats , which may differ from one customer to another but have no need to be transformed into the same format , thus avoiding information loss or distortion . a numerical example is tested with the lgp approach to demonstrate its validity , effectiveness and potential applications in qfd practice . ( c ) <digit> elsevier inc. all rights reserved .
local , deformable precomputed radiance transfer . <eos> precomputed radiance transfer ( prt ) captures realistic lighting effects from distant , low frequency environmental lighting but has been limited to static models or precomputed sequences . we focus on prt for local effects such as bumps , wrinkles , or other detailed features , but extend it to arbitrarily deformable models . our approach applies zonal harmonics ( zh ) which approximate spherical functions as sums of circularly symmetric legendre polynomials around different axes . by spatially varying both the axes and coefficients of these basis functions , we can fit to spatially varying transfer signals . compared to the spherical harmonic ( sh ) basis , the zh basis yields a more compact approximation . more important , it can be trivially rotated whereas sh rotation is expensive and unsuited for dense per vertex or per pixel evaluation . this property allows , for the first time , prt to be mapped onto deforming models which re orient the local coordinate frame . we generate zh transfer models by fitting to prt signals simulated on meshes or simple parametric models for thin membranes and wrinkles . we show how shading with zh transfer can be significantly accelerated by specializing to a given lighting environment . finally , we demonstrate real time rendering results with soft shadows , inter reflections , and subsurface scatter on deforming models .
a mimetic mass , momentum and energy conserving discretization for the shallow water equations . <eos> a spatial semi discretization is developed for the two dimensional depth averaged shallow water equations on a non equidistant structured and staggered grid . the vector identities required for energy conservation in the continuous case are identified . discrete analogues are developed , which lead to a finite volume semi discretisation which conserves mass , momentum , and energy simultaneously . the key to discrete energy conservation for the shallow water equations is to numerically distinguish storage of momentum from advective transport of momentum . simulation of a large amplitude wave in a basin confirms the conservative properties of the new scheme , and demonstrates the enhanced robustness resulting from the compatibility of continuity and momentum equations . the scheme can be used as a building block for constructing fully conservative curvilinear , higher order , variable density , and non hydrostatic discretizations . ( c ) <digit> elsevier ltd. all rights reserved .
3d object search through semantic component . <eos> in this paper , we present a novel concept named semantic component for 3d object search which describes a key component that semantically defines a 3d object . in most cases , the semantic component is intra category stable and therefore can be used to construct an efficient 3d object retrieval scheme . by segmenting an object into segments and learning the similar segments shared by all the objects in the same category , we can summarise what human uses for object recognition , from the analysis of which we develop a method to find the semantic component of an object . in our experiments , the proposed method is justified and the effectiveness of our algorithm is also demonstrated .
formal semantics and efficient analysis of timed rebeca in real time maude . <eos> provides an executable formal real time maude semantics for timed rebeca . integrates real time maude analysis into the rebeca toolchain . provides an efficient semantics using partial order reduction like techniques . shows the performance gained by this optimization .
a new bonding tool solution to improve stitch bondability . <eos> a new bonding tool solution is proposed to improve stitch bondability by creating a new surface morphology on the tip surface of a wire bonding tool ( capillary ) . the surface has relatively deep lines with no fixed directions . this new capillary has less slipping between the wire and the capillary tip surface and provides better coupling effect between them . experiments of wire bonding on unstable lead frames substrates , alloyed wire ( 2n gold wire ) bonding , and copper wire bonding were carried out to confirm the effect of the new capillary on the stitch bondability . the experimental results are promising and have proved that the use of the new capillary could improve the bondability of the stitch bond and minimize the occurrence of short tail defects and non sticking on lead during bonding .
optimal instruction scheduling using integer programming . <eos> this paper presents a new approach to local instruction scheduling based on integer programming that produces optimal instruction schedules in a reasonable time , even for very large basic blocks . the new approach first uses a set of graph transformations to simplify the data dependency graph while preserving the optimality of the final schedule . the simplified graph results in a simplified integer program which can be solved much faster . a new integer programming formulation is then applied to the simplified graph . various techniques are used to simplify the formulation , resulting in fewer integer program variables , fewer integer program constraints and fewer terms in some of the remaining constraints , thus reducing integer program solution time . the new formulation also uses certain adaptively added constraints ( cuts ) to reduce solution time . the proposed optimal instruction scheduler is built within the gnu compiler collection ( gcc ) and is evaluated experimentally using the spec95 floating point benchmarks . although optimal scheduling for the target processor is considered intractable , all of the benchmarks ' basic blocks are optimally scheduled , including blocks with up to <digit> instructions , while total compile time increases by only <digit> % .
web services discovery with rough sets . <eos> web services are emerging as a major technology for building service oriented distributed systems . potentially , various resources on the internet can be virtualized as web services for a wider use by their communities . service discovery becomes an issue of vital importance for web services applications . this article presents rosse , a rough sets based search engine for web service discovery . one salient feature of rosse lies in its capability to deal with uncertainty of service properties when matching services . a use case is presented to demonstrate the use of rosse for discovery of car services . rosse is evaluated in terms of its accuracy and efficiency in service discovery .
stability optimized time adjustment for a networked computer clock . <eos> we propose an optimal time adjustment method from the viewpoint of frequency stability , which is defined as the allan deviation . when time adjustment is needed for a clock in a networked computer , it is made over a period called a time adjustment period . the proposed method optimizes frequency stability for a given time adjustment period . this method has been evaluated and compared with the adjtime ( ) system call in unix systems in terms of frequency stability and duration of time adjustment period needed for achieving particular values of frequency stability . for time intervals from <digit> to <digit> <digit> s , the frequency stability achieved by the proposed method was about 0.01 0.5 of that achieved by the adjtime ( ) system call . the evaluation also showed that the duration of a time adjustment period needed for achieving the frequency stability of 1.0 x <digit> ( <digit> ) in the proposed method was less than <digit> <digit> ( <digit> <digit> ) that of the period in the adjtime ( ) system call when we optimized frequency stability for a <digit> ( 3,600 ) s time interval under the condition that the duration of the time adjustment period was <digit> h.
improving mobile services design a qfd approach . <eos> the quality of mobile services in the mobile and wireless world is ultimately judged in terms of customer satisfaction . this is particularly true for the third generation ( 3g ) and beyond multimedia mobile services which should meet or exceed customer expectations . in this study quality function deployment ( qfd ) is used for the first time as a quality improvement approach for building customers ' requirements into mobile services . traditionally qfd approach is adopted in product and manufacturing industries . in this paper qfd is extended to mobile service industry which is such a promising industry in today 's information society . this paper proposes a generic framework based on qfd concepts and practices to improve mobile service design and development . an example is presented to illustrate the use of qfd for mobile e learning services for university students and lecturers . the data transmission speed is found to be the most critical requirement in mobile e learning services . by the use of qfd the developed mobile services can best meet customers ' requirements or even exceed their expectations . at the end of this paper some benefits as well as further improvements regarding qfd approach are discussed and concluded .
international research collaborations of asean nations in economics , <digit> . <eos> this study examines the research performance and international research collaborations ( irc ) of asean nations in the area of economics . over the last <digit> decades international collaborated papers have increased in the region , while locally co authored papers have declined . singapore towered among asean nations in research efficiency based on geographical area , population and gdp . vietnam performed relatively better in research efficiency than research productivity ( number of papers produced ) , while indonesia performed poorly . overall , internationally co authored papers were cited twice as often as locally authored papers except that both the philippines and indonesia exhibited almost no difference in how their local and internationally co authored papers were cited . the study also examined irc from the perspective of social networks . centrality had a strong correlation with research performance however , vertex tie strength ( a result of repeat collaboration ) showed maximum correlation with research performance . while malaysia emerged as the nation with the highest betweenness centrality or bridging power , the us emerged as the most favoured international partner of asean nations . however , collaboration between asean countries accounted for just <digit> % of all international collaborations . increased academic mobility and more joint scientific works are suggestions to consider to boost educational co operation among the asean nations .
automated assistants for analyzing team behaviors . <eos> multi agent teamwork is critical in a large number of agent applications , including training , education , virtual enterprises and collective robotics . the complex interactions of agents in a team as well as with other agents make it extremely difficult for human developers to understand and analyze agent teambehavior . it has thus become increasingly important to develop tools that can help humans analyze , evaluate , and understand team behaviors . however , the problem of automated team analysis is largely unaddressed in previous work . in this article , we identify several key constraints faced by team analysts . most fundamentally , multiple types of models of team behavior are necessary to analyze different granularities of team events , including agent actions , interactions , and global performance . in addition , effective ways of presenting the analysis to humans is critical and the presentation techniques depend on the model being presented . finally , analysis should be independent of underlying team architecture and implementation . we also demonstrate an approach to addressing these constraints by building an automated team analyst called isaac for post hoc , off line agent team analysis . isaac acquires multiple , heterogeneous team models via machine learning over teams ' external behavior traces , where the specific learning techniques are tailored to the particular model learned . additionally , isaac employs multiple presentation techniques that can aid human understanding of the analyses . isaac also provides feedback on team improvement in two novel ways ( i ) it supports principled what if '' reasoning about possible agent improvements ( ii ) it allows the user to compare different teams based on their patterns of interactions . this paper presents isaac 's general conceptual framework , motivating its design , as well as its concrete application in two domains ( i ) robocup soccer ( ii ) software agent teams participating in a simulated evacuation scenario . in the robocup domain , isaac was used prior to and during the robocup '99 tournament , and was awarded the robocup scientific challenge award . in the evacuation domain , isaac was used to analyze patterns of message exchanges among software agents , illustrating the generality of isaac 's techniques . we present detailed algorithms and experimental results from isaac 's application .
adaptive clustering in wireless sensor networks by mining sensor energy data . <eos> clustering has been well received as one of the effective solutions to enhance energy efficiency and scalability of large scale wireless sensor networks . the goal of clustering is to identify a subset of nodes in a wireless sensor network , then all the other nodes communicate with the network sink via these selected nodes . however , many current clustering algorithms are tightly coupled with exact sensor locations derived through either triangulation methods or extra hardware such as gps equipment . however , in practice , it is very difficult to know sensor location coordinates accurately due to various factors such as random deployment and low power , low cost sensing devices . therefore , how to develop an adaptive clustering algorithm without relying on exact sensor location information is a very important yet challenging problem . in this paper , we try to address this problem by proposing a new adaptive clustering algorithm for energy efficiency of wireless sensor networks . compared with other work having been done in this area , our proposed adaptive clustering algorithm is original because of its capability to infer the location information by mining wireless sensor energy data . furthermore , based on the inferred location information and the remaining ( residual ) energy level of each node , the proposed clustering algorithm will dynamically change cluster heads for energy efficacy . simulation results show that the proposed adaptive clustering algorithm is efficient and effective for energy saving in wireless sensor networks . ( c ) <digit> elsevier b.v. all rights reserved .
to feature space and back identifying top weighted features in polynomial support vector machine models . <eos> polynomial support vector machine models of degree d are linear functions in a feature space of monomials of at most degree d. however , the actual representation is stored in the form of support vectors and lagrange multipliers that is unsuitable for human understanding . an efficient , heuristic method for searching the feature space of a polynomial support vector machine model for those features with the largest absolute weights is presented . the time complexity of this method is theta ( dms ( <digit> ) sdp ) , where m is the number of variables , d the degree of the kernel , s the number of support vectors , and p the number of features the algorithm is allowed to search . in contrast , the brute force approach of constructing all weights and then selecting the largest weights has complexity theta ( sd ( ( m d ) ( d ) ) ) . the method is shown to be effective in identifying the top weighted features on several simulated data sets , where the true weight vector is known . additionally , the method is run on several high dimensional , real world data sets where the features returned may be used to construct classifiers with classification performances similar to models built with all or subsets of variables returned by variable selection methods . this algorithm provides a new ability to understand , conceptualize , visualize , and communicate polynomial svm models and has implications for feature construction , dimensionality reduction , and variable selection .
sound direction estimation using an artificial ear for robots . <eos> we propose a novel design of an artificial robot ear for sound direction estimation using two measured outputs only . the spectral features in the interaural transfer functions ( itfs ) of the proposed artificial ears are distinctive and move monotonically according to the sound direction . thus , these features provide effective sound cues to estimate sound direction using the measured two output signals . bilateral asymmetry of microphone positions can enhance the estimation performance even in the median plane where interaural differences vanish . we propose a localization method to estimate the lateral and vertical angles simultaneously . the lateral angle is estimated using interaural time difference and woodworth and schlosberg 's formula , and the front back discrimination is achieved by finding the spectral features in the itf estimated from two measured outputs . the vertical angle of a sound source in the frontal region is estimated by comparing the spectral features in the estimated itf with those in the database built in an anechoic chamber . the feasibility of the designed artificial ear and the estimation method were verified in a real environment . in the experiment , it was shown that both the front back discrimination and the sound direction estimation in the frontal region can be achieved with reasonable accuracy . thus , we expect that robots with the proposed artificial ear can estimate the direction of speaker from two output signals only . crown copyright ( c ) <digit> published by elsevier b.v. all rights reserved .
alcohol , type of alcohol , and all cause and coronary heart disease mortality . <eos> many studies from a variety of countries have shown a u or j shaped relation between alcohol intake and mortality from all causes . it is now quite well documented from epidemiologic as well as clinical and experimental studies that the descending leg of the curve results from a decreased risk of cardiovascular disease among those with light to moderate alcohol consumption . the findings that wine drinkers are at a decreased risk of mortality from cardiovascular disease compared to non wine drinkers suggest that substances present in wine are responsible for a beneficial effect on the outcome , in addition to that from a light intake of ethanol . several potential confounding factors still remain to be excluded , however .
rigorous development of an embedded fault tolerant system based on coordinated atomic actions . <eos> this paper describes our experience using coordinated atomic ( ca ) actions as a system structuring tool to design and validate a sophisticated and embedded control system for a complex industrial application that has high reliability and safety requirements . our study is based on an extended production cell model , the specification and simulator for which were defined and developed by fzi ( forschungszentrum informatik , germany ) . this fault tolerant production cell represents a manufacturing process involving redundant mechanical devices ( provided in order to enable continued production in the presence of machine faults ) . the challenge posed by the model specification is to design a control system that maintains specified safety and liveness properties even in the presence of a large number and variety of device and sensor failures . based on an analysis of such failures , we provide in this paper details of <digit> ) a design for a control program that uses ca actions to deal with both safety related and fault tolerance concerns and <digit> ) the formal verification of this design based on the use of model checking . we found that ca action structuring facilitated both the design and verification tasks by enabling the various safety problems ( involving possible clashes of moving machinery ) to be treated independently . even complex situations involving the concurrent occurrence of any pairs of the many possible mechanical and sensor failures can be handled simply yet appropriately . the formal verification activity was performed in parallel with the design activity and the interaction between them resulted in a combined exercise in design for validation formal verification was very valuable in identifying some very subtle residual bugs in early versions of our design which would have been difficult to detect otherwise .
ventricular shape visualization using selective volume rendering of cardiac datasets . <eos> in this paper , we present a novel technique of improving volume rendering quality and speed by integrating original volume data and global model information attained by segmentation . the segmentation information prevents object occlusions that may appear when volume rendering is based on local image features only . thus the presented visualization technique provides meaningful visual results that enable a clear understanding of complex anatomical structures . in the first part , we describe a segmentation technique for extracting the region of interest based on an active contour model . in the second part , we propose a volume rendering method for visualizing the selected portions of fuzzy surfaces extracted by local image processing methods . we show the results of selective volume rendering of left and right ventricle based on cardiac datasets from clinical routines . our method offers an accelerated technique to accurately visualize the surfaces of segmented objects .
mutual information for lucas kanade tracking ( milk ) an inverse compositional formulation . <eos> mutual information ( mi ) is popular for registration via function optimization . this work proposes an inverse compositional formulation of mi for levenberg marquardt optimization . this yields a constant hessian , which may be precomputed . speed improvements of <digit> percent were obtained , with convergence accuracies similar those of the standard formulation .
a comparative study of software model checkers as unit testing tools an industrial case study . <eos> conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems . this deficiency incurs significant development and support maintenance cost for the manufacturers . model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses . whereas conventional model checkers require manual effort to create an abstract target model , modern software model checkers remove this overhead by directly analyzing a target c program , and can be utilized as unit testing tools . however , since software model checkers are not fully mature yet , they have limitations according to the underlying technologies and tool implementations , potentially critical issues when applied in industrial projects . this paper reports our experience in applying blast and cbmc to testing the components of a storage platform software for flash memory . through this project , we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real world industrial application counterexample guided abstraction refinement with predicate abstraction and sat based bounded analysis .
cmaterdb1 a database of unconstrained handwritten bangla and bangla english mixed script document image . <eos> in this paper , we have described the preparation of a benchmark database for research on off line optical character recognition ( ocr ) of document images of handwritten bangla text and bangla text mixed with english words . this is the first handwritten database in this area , as mentioned above , available as an open source document . as india is a multi lingual country and has a colonial past , so multi script document pages are very much common . the database contains <digit> handwritten document pages , among which <digit> pages are written purely in bangla script and rests of the <digit> pages are written in bangla text mixed with english words . this database for off line handwritten scripts is collected from different data sources . after collecting the document pages , all the documents have been preprocessed and distributed into two groups , i.e. , cmaterdb1 .1.1 , containing document pages written in bangla script only , and cmaterdb1 .2.1 , containing document pages written in bangla text mixed with english words . finally , we have also provided the useful ground truth images for the line segmentation purpose . to generate the ground truth images , we have first labeled each line in a document page automatically by applying one of our previously developed line extraction techniques khandelwal et al. , premi <digit> , pp. <digit> <digit> and then corrected any possible error by using our developed tool gt gen 1.1 . line extraction accuracies of 90.6 and 92.38 % are achieved on the two databases , respectively , using our algorithm . both the databases along with the ground truth annotations and the ground truth generating tool are available freely at http code.google.com p cmaterdb .
a time domain boundary element method for modeling the quasi static viscoelastic behavior of asphalt pavements . <eos> a time domain boundary element method ( bem ) is presented to model the quasi static linear viscoelastic behavior of asphalt pavements . in the viscoelastic analysis , the fundamental solution is derived in terms of elemental displacement discontinuities ( dds ) and a boundary integral equation is formulated in the time domain . the unknown dds are assumed to vary quadratically in the spatial domain and to vary linearly in the time domain . the equation is then solved incrementally through the whole time history using an explicit time marching approach . all the spatial and temporal integrations can be performed analytically , which guarantees the accuracy of the method and the stability of the numerical procedure . several viscoelastic models such as boltzmann , burgers , and power law models are considered to characterize the time dependent behavior of linear viscoelastic materials . the numerical method is applied to study the load induced stress redistribution and its effects on the cracking performance of asphalt pavements . some benchmark problems are solved to verify the accuracy and efficiency of the approach . numerical experiments are also carried out to demonstrate application of the method in pavement engineering .
a decision support system for animated film selection based on a multi criteria aggregation of referees ' ordinal preferences . <eos> this paper presents a decision support system devoted to the selection of films for the international animated film festival organized at annecy , france , every year . it deals with the representation and aggregation of referees ' preferences along predefined criteria in addition to their overall selection point of view . the practical requirements associated with this application ( often encountered in social or cultural areas as well ) are a common ordinal scale for the criteria scores , a procedure to deal with inconsistencies between criteria and overall scores , explanation tools of each referee 's preference model in order to facilitate the deliberation process and also to argument the selection decision . the processing of referees ' preferences is achieved thanks to a recent method which consists in finding a generalized mean aggregation operator representing the preferences of a referee , in a finite ordinal scale context . the method allows to deal with consistency conditions on referees ' behaviour in order to highlight the criteria or pair of criteria which are the most influential for each of the referees . all the functionalities have been implemented in an interactive decision software that facilitates a shared selection decision . results issued from the <digit> selection are presented and analysed from the preference representation and processing point of view . ( c ) <digit> elsevier ltd. all rights reserved .
semantic web search based on rough sets and fuzzy formal concept analysis . <eos> fuzzy formal concept analysis ( ffca ) is a generalization of formal concept analysis ( fca ) for modeling uncertainty information . ffca provides a mathematical framework which can support the construction of formal ontologies in the presence of uncertainty data for the development of the semantic web . in this paper , we show how rough set theory can be employed in combination with ffca to perform semantic web search and discovery of information in the web .
constrained consensus of asynchronous discrete time multi agent systems with time varying topology . <eos> the union graph is assumed to be strongly connected over each finite interval . an approach is proposed to transform the original network to a synchronous one . we show that the linear part converges and the projection error vanishes over time .
axiomatic theory of intuitionistic fuzzy sets . <eos> a bernays like axiomatic theory of intuitionistic fuzzy sets involving five primitives and seven axioms is presented . ( c ) <digit> elsevier science b.v. all rights reserved .
top down delayering to expose large inspection area on die side edge with platinum ( pt ) deposition technique . <eos> methodology to increase the flat inspection area useful in exposition for die side edge . platinum ( pt ) deposition technique to form a protection mask pt deposition to slow down the edging effect
short term robustness of production management systems a case study . <eos> whereas operations research concentrates on optimization , practitioners find the robustness of a proposed solution more important . therefore this paper presents a practical methodology that is a stagewise combination of four proven techniques ( <digit> ) simulation , ( <digit> ) optimization , ( <digit> ) risk or uncertainty analysis , and ( <digit> ) bootstrapping . this methodology is illustrated through a production control study . that illustration defines robustness as the capability to maintain short term service , in a variety of environments ( scenarios ) that is , the probability of the short term fill rate remains within a prespecified range . besides satisfying this probabilistic constraint , the system minimizes expected long term work in process . actually , the example compares four systemsnamely , kanban , conwip , hybrid , and genericfor the well known case of a production line with four stations and a single product . the conclusion is that in this particular example , hybrid is best when risk is not ignored otherwise generic is best that is , risk considerations do make a difference .
the variety generated by semi heyting chains . <eos> the purpose of this paper was to investigate the structure of semi heyting chains and the variety ( mathcal csh ) generated by them . we determine the number of non isomorphic n element semi heyting chains . as a contribution to the study of the lattice of subvarieties of ( mathcal csh , ) we investigate the inclusion relation between semi heyting chains . finally , we provide equational bases for ( mathcal csh ) and for the subvarieties of ( mathcal csh ) introduced in <digit> .
stochastic fault tree analysis with self loop basic events . <eos> this paper presents an analytical approach for performing fault tree analysis ( fta ) with stochastic self loop events . the proposed approach uses the flow graph concept , and moment generating function ( mgf ) to develop a new stochastic fta model for computing the probability , mean time to occurrence , and standard deviation time to occurrence of the top event . the application of the method is demonstrated by solving one example .
global view abstractions for user defined reductions and scans . <eos> since apl , reductions and scans have been recognized as powerful programming concepts . abstracting an accumulation loop ( reduction ) and an update loop ( scan ) , the concepts have efficient parallel implementations based on the parallel prefix algorithm . they are often included in high level languages with a built in set of operators such as sum , product , min , etc. mpi provides library routines for reductions that account for nearly nine percent of all mpi calls in the nas parallel benchmarks ( npb ) version 3.2 . some researchers have even advocated reductions and scans as the principal tool for parallel algorithm design.also since apl , the idea of applying the reduction control structure to a user defined operator has been proposed , and several implementations ( some parallel ) have been reported . this paper presents the first global view formulation of user defined scans and an improved global view formulation of user defined reductions , demonstrating them in the context of the chapel programming language . further , these formulations are extended to a message passing context ( mpi ) , thus transferring global view abstractions to local view languages and perhaps signaling a way to enhance local view languages incrementally . finally , examples are presented showing global view user defined reductions cleaning up and or speeding up portions of two nas benchmarks , is and mg . in consequence , these generalized reduction and scan abstractions make the full power of the parallel prefix technique available to both global and local view parallel programming .
partial derivative guidance for weak classifier mining in pedestrian detection . <eos> boosting over weak classifiers is widely used in pedestrian detection . as the number of weak classifiers is large , researchers always use a sampling method over weak classifiers before training . the sampling makes the boosting process harder to reach the fixed target . in this paper , we propose a partial derivative guidance for weak classifier mining method which can be used in conjunction with a boosting algorithm . using weak classifier mining method makes the sampling less degraded in the performance . it has the same effect as testing more weak classifiers while using acceptable time . experiments demonstrate that our algorithm can process quicker than <digit> algorithm in both training and testing , without any performance decrease . the proposed algorithms is easily extending to any other boosting algorithms using a window scanning style and hog like features .
the role of operators in apl . <eos> operators , which apply to functions to produce functions , are an important component of apl . despite their importance , their role is not well understood , and they are often lumped with functions in expositions of the language . this paper attempts to clarify the role of operators in apl by tracing their development , outlining possible future directions , and commenting briefly on their roles in other languages , both natural and programming .
theoretical modeling of micro scale biological phenomena in human coronary arteries . <eos> this paper presents a mathematical model of biological structures in relation to coronary arteries with atherosclerosis . a set of equations has been derived to compute blood flow through these transport vessels with variable axial and radial geometries . three dimensional reconstructions of diseased arteries from cadavers have shown that atherosclerotic lesions spiral through the artery . the theoretical framework is able to explain the phenomenon of lesion distribution in a helical pattern by examining the structural parameters that affect the flow resistance and wall shear stress . the study is useful for connecting the relationship between the arterial wall geometries and hemodynamics of blood . it provides a simple , elegant and non invasive method to predict flow properties for geometrically complex pathology at micro scale levels and with low computational cost .
unanticipated partial behavioral reflection adapting applications at runtime . <eos> dynamic , unanticipated adaptation of running systems is of interest in a variety of situations , ranging from functional upgrades to on the fly debugging or monitoring of critical applications . in this paper we study a particular form of computational reflection , called unanticipated partial behavioral reflection ( upbr ) , which is particularly well suited for unanticipated adaptation of real world systems . our proposal combines the dynamicity of unanticipated reflection , i.e. , reflection that does not require preparation of the code of any sort , and the selectivity and efficiency of partial behavioral reflection ( pbr ) . first , we propose unanticipated partial behavioral reflection which enables the developer to precisely select the required reifications , to flexibly engineer the metalevel and to introduce the metabehavior dynamically . second , we present a system supporting unanticipated partial behavioral reflection in squeak smalltalk , called geppetto , and illustrate its use with a concrete example of a web application . benchmarks validate the applicability of our proposal as an extension to the standard reflective abilities of smalltalk .
a weighted additive fuzzy programming approach for multi criteria supplier selection . <eos> in supply chain management , to build strategic and strong relationships , firms should select best suppliers by applying appropriate method and selection criteria . in this paper , to handle ambiguity and fuzziness in supplier selection problem effectively , a new weighted additive fuzzy programming approach is developed . firstly , linguistic values expressed as trapezoidal fuzzy numbers are used to assess the weights of the factors . by applying the distances of each factor between fuzzy positive ideal rating and fuzzy negative ideal rating , weights are obtained . then applying suppliers ' constraints , goals and weights of the factors , a fuzzy multi objective linear model is developed to overcome the selection problem and assign optimum order quantities to each supplier . the proposed model is explained by a numerical example . ( c ) <digit> elsevier ltd. all rights reserved .
firegrid an e infrastructure for next generation emergency response support . <eos> the firegrid project aims to harness the potential of advanced forms of computation to support the response to large scale emergencies ( with an initial focus on the response to fires in the built environment ) . computational models of physical phenomena are developed , and then deployed and computed on high performance computing resources to infer incident conditions by assimilating live sensor data from an emergency in real timeor , in the case of predictive models , faster than real time . the results of these models are then interpreted by a knowledge based reasoning scheme to provide decision support information in appropriate terms for the emergency responder . these models are accessed over a grid from an agent based system , of which the human responders form an integral part . this paper proposes a novel firegrid architecture , and describes the rationale behind this architecture and the research results of its application to a large scale fire experiment .
a branch and bound algorithm for minimizing total completion time on a single batch machine with incompatible job families and dynamic arrivals . <eos> in this paper , we consider a single batch machine scheduling problem with incompatible job families and dynamic job arrivals . the objective is to minimize the total completion time . this problem is known to be strongly np hard . we present several dominance properties and two types of lower bounds , which are incorporated to construct a basic branch and bound algorithm . furthermore , according to the characteristics of dynamic job arrivals , a decomposed branch and bound algorithm is proposed to improve the efficiency . the proposed algorithms are tested on a large set of randomly generated problem instances .
predicting saturates of sour vacuum gas oil using artificial neural networks and genetic algorithms . <eos> accurate predictions of chemical composition by physical properties of sour vaccum gas oil ( vgo ) fractions are important for the refinery . in this paper , a feed forward type network based on genetic algorithm ( ga ) , was developed and used for predicting saturates of sour vacuum gas oil . the number of neurons in the hidden layer , the momentum and the learning rates were determined by using the genetic algorithm . the five physical properties of sour vgo , namely , average boiling point , density at 20c , molecular weight , kinematic viscosity at 100c and refractive index at 70c were considered as input variables of the ann and the saturates of sour vgo was used as output variable . the study shows that genetic algorithm could find the optimal networks architecture and parameters of the back propagation algorithm . further , the artificial neural network models based on genetic algorithm are tested and the results indicate that the adopted model is very suitable for the forecasting of saturates of sour vgo . compared with other forecasting models , it can be found that this model can improve prediction accuracy .
implementing a hardware embedded reactive agents platform based on a service oriented architecture over heterogeneous wireless sensor networks . <eos> wireless sensor networks ( wsns ) represent a key technology for collecting important information from different sources in context aware environments . unfortunately , integrating devices from different architectures or wireless technologies into a single sensor network is not an easy task for designers and developers . in this sense , distributed architectures , such as service oriented architectures and multi agent systems , can facilitate the integration of heterogeneous sensor networks . in addition , the sensors capabilities can be expanded by means of intelligent agents that change their behavior dynamically . this paper presents the hardware embedded reactive agents ( hera ) platform . hera is based on services layers over light physical devices ( sylph ) , a distributed platform which integrates a service oriented approach into heterogeneous wsns . as sylph , hera can be executed over multiple devices independently of their wireless technology , their architecture or the programming language they use . however , hera goes one step ahead of sylph and adds reactive agents to the platform and also a reasoning mechanism that provides hera agents with case based planning features that allow solving problems considering past experiences . unlike other approaches , hera allows developing applications where reactive agents are directly embedded into heterogeneous wireless sensor nodes with reduced computational resources .
efficient algorithms for stream mining of constrained frequent patterns in a limited memory environment . <eos> as technology advances , streams of data can be rapidly generated in many real life applications . this calls for stream mining , which searches for implicit , previously unknown , and potentially useful information such as frequent patterns that might be embedded in continuous data streams . however , most of the existing algorithms do not allow users to express the patterns to be mined according to their intentions , via the use of constraints . as a result , these unconstrained mining algorithms can yield numerous patterns that are not interesting to the users . moreover , many existing tree based algorithms assume that all the trees constructed during the mining process can fit into memory . while this assumption holds for many situations , there are many other situations in which it does not hold . hence , in this paper , we develop efficient algorithms for stream mining of constrained frequent patterns in a limited memory environment . our algorithms allow users to impose a certain focus on the mining process , discover from data streams all those frequent patterns that satisfy the user constraints , and handle situations where the available memory space is limited .
synthesis of topology and sizing of analog electrical circuits by means of genetic programming . <eos> the design ( synthesis ) of an analog electrical circuit entails the creation of both the topology and sizing ( numerical values ) of all of the circuit 's components . there has previously been no general automated technique for automatically creating the design for an analog electrical circuit from a high level statement of the circuit 's desired behavior . this paper shows how genetic programming can be used to automate the design of eight prototypical analog circuits , including a lowpass filter , a highpass filter , a bandstop filter , a tri state frequency discriminator circuit , a frequency measuring circuit , a <digit> db amplifier , a computational circuit for the square root function , and a time optimal robot controller circuit .
a spectrum of compromise aggregation operators for multi attribute decision making . <eos> in many decision making problems , a number of independent attributes or criteria are often used to individually rate an alternative from an agent 's local perspective and then these individual ratings are combined to produce an overall assessment . now , in cases where these individual ratings are not in complete agreement , the overall rating should be somewhere in between the extremes that have been suggested . however , there are many possibilities for the aggregated value . given this , this paper systematically explores the space of possible compromise operators for such multi attribute decision making problems . specifically , we axiomatically identify the complete spectrum of such operators in terms of the properties they should satisfy , and show the main ones that are widely used namely averaging operators , uninorms and nullnorms represent only three of the nine types we identify . for each type , we then go onto analyse their properties and discuss how specific instances can actually be developed . finally , to illustrate the richness of our framework , we show how a wide range of operators are needed to model the various attitudes that a user may have for aggregation in a given scenario ( bidding in multi attribute auctions ) . ( c ) <digit> elsevier b.v. all rights reserved .
detecting categorical perception in continuous discrimination data . <eos> we present a method for assessing categorical perception from continuous discrimination data . until recently , categorical perception of speech has exclusively been measured by discrimination and identification experiments with a small number of different stimuli , each of which is presented multiple times . experiments by rogers and davis ( <digit> ) , however , suggest that using non repeating stimuli yields a more reliable measure of categorization . if this idea is applied to a single phonetic continuum , the continuum has to be densely sampled and the obtained discrimination data is nearly continuous . in the present study , we describe a maximum likelihood method that is appropriate for analysing such continuous discrimination data .
simplified similarity scoring using term ranks . <eos> we propose a method for document ranking that combines a simple document centric view of text , and fast evaluation strategies that have been developed in connection with the vector space model . the new method defines the importance of a term within a document qualitatively rather than quantitatively , and in doing so reduces the need for tuning parameters . in addition , the method supports very fast query processing , with most of the computation carried out on small integers , and dynamic pruning an effective option . experiments on a wide range of trec data show that the new method provides retrieval effectiveness as good as or better than the okapi bm25 formulation , and variants of language models .
midgar generation of heterogeneous objects interconnecting applications . a domain specific language proposal for internet of things scenarios . <eos> smart objects and internet of things are two ideas that describe the future . the interconnection of objects can make them intelligent or expand their intelligence . this is achieved by a network that connects all the objects in the world . a network where most of the data traffic comes from objects instead of people . cities , houses , cars or any other objects that come to life , respond , work and make their owners life easier . this is part of that future . but first , there are many basic problems that must be solved . in this paper we propose solutions for many of these problems the interconnection of ubiquitous , heterogeneous objects and the generation of applications allow inexperienced people to interconnect them . for that purpose , we present three possible solutions a domain specific language capable of abstracting the application generation problem a graphic editor that simplifies the creation of that dsl and an iot platform ( midgar ) able to interconnect different objects between them . through midgar , you can register objects and create interconnection between ubiquitous and heterogeneous objects through a graphic editor that generates a model defined by the dsl . from this model , midgar generates the interconnection defined by the user with the graphical editor .
roc curves in cost space . <eos> roc curves and cost curves are two popular ways of visualising classifier performance , finding appropriate thresholds according to the operating condition , and deriving useful aggregated measures such as the area under the roc curve ( auc ) or the area under the optimal cost curve . in this paper we present new findings and connections between roc space and cost space . in particular , we show that roc curves can be transferred to cost space by means of a very natural threshold choice method , which sets the decision threshold such that the proportion of positive predictions equals the operating condition . we call these new curves rate driven curves , and we demonstrate that the expected loss as measured by the area under these curves is linearly related to auc . we show that the rate driven curves are the genuine equivalent of roc curves in cost space , establishing a point point rather than a point line correspondence . furthermore , a decomposition of the rate driven curves is introduced which separates the loss due to the threshold choice method from the ranking loss ( kendall tau distance ) . we also derive the corresponding curve to the roc convex hull in cost space this curve is different from the lower envelope of the cost lines , as the latter assumes only optimal thresholds are chosen .
on the characteristics of growing cell structures ( gcs ) neural network . <eos> in this paper , a self developing neural network model , namely the growing cell structures ( gcs ) is characterized . in gcs each node ( or cell ) is associated with a local resource counter tau ( t ) . we show that gcs has the conservation property by which the summation of all resource counters always equals s ( <digit> alpha ) alpha , thereby s is the increment added to tau ( t ) of the wining node after each input presentation and alpha ( <digit> < alpha < 1.0 ) is the forgetting ( i.e. , decay ) factor applied to tau ( t ) of non wining nodes . the conservation property provides an insight into how gcs can maximize information entropy . the property is also employed to unveil the chain reaction effect and race condition which can greatly influence the performance of gcs . we show that gcs can perform better in terms of equi probable criterion if the resource counters are decayed by a smaller alpha .
limitations of multivariable controller tuning using genetic algorithms . <eos> in recent years evolutionary computation has come of age , with genetic algorithms ( ga ) being possibly the most popular technique . a study is presented revealing the performance of a ga in determining the pid tuning parameters for a multivariable process , including decoupling controllers . the process used for this investigation is a distillation column which is a mimo high order , nonlinear system . the results indicate some limitations of using gas for controller tuning when mimo systems are involved .
collaborative real time traffic information generation and sharing framework for the intelligent transportation system . <eos> real time traffic information collection and data fusion is one of the most important tasks in the advanced traffic management system ( atms ) , and sharing traffic information to users is an essential part of the advance traveler information system ( atis ) among the intelligent transportation systems ( its ) . traditionally , sensor based schemes or probing vehicle based schemes have been used for collecting traffic information , but the coverage , cost , and real time issues have remained unsolved . in this paper , a wiki like collaborative real time traffic information collection , fusion and sharing framework is proposed , which includes user centric traffic event reacting mechanism , and automatic agent centric traffic information aggregating scheme . smart traffic agents ( sta ) developed for various front end devices have the location aware two way real time traffic exchange capability , and built in event reporting mechanism to allow users to report the real time traffic events around their locations . in addition to collecting traffic information , the framework also integrates heterogeneous external real time traffic information data sources and internal historical traffic information database to predict real time traffic status by knowledge base system technique .
perspectives on wellness self monitoring tools for older adults . <eos> compared older adults and healthcare providers perceptions on self monitoring . explored advantages in older adults voluntary use of self monitoring . identified challenges in older adults voluntary use of self monitoring . suggested design implications for older adults self monitoring tools .
exploration of term relationship for bayesian network based sentence retrieval . <eos> sentence retrieval is to retrieve query relevant sentences in response to user query . however , limited information contained in sentence always incurs a lot of uncertainties , which heavily influence the retrieval performance . to solve this problem , bayesian network , which has been accepted as one of the most promising methodologies to deal with information uncertainty , is explored . correspondingly , three sentence retrieval models based on bayesian network are proposed , i.e. bnsr model , bnsr_tr model and bnsr_cr model . bnsr model assumes independency between terms and shows certain improvement in retrieval performance . bnsr_tr and bnsr_cr models relax the assumption of term independency but consider term relationships from two different points of view , namely term and term context . experiments verify the performance improvements produced by these two models , but bnsr_cr shows more advantages than bnsr_tr model , because of its more accurate identification of term dependency .
animation aerodynamics . <eos> methods based on aerodynamics are developed to simulate and control the motion of objects in fluid flows . to simplify the physics for animation , the problem is broken down into two parts a fluid flow regime and an object boundary regime . with this simplification one can approximate the realistic behaviour of objects moving in liquids or air . it also enables a simple way of designing and controlling animation sequences from a set of flow primitives , an animator can design the spatial arrangement of flows , create flows around obstacles and direct flow timing . the approach is fast , simple , and is easily fitted into simulators that model objects governed by classical mechanics . the methods are applied to an animation that involves hundreds of flexible leaves being blown by wind currents .
a fourier analytic approach to reed muller decoding . <eos> we present a fourier analytic approach to list decoding reed muller codes over arbitrary finite fields . we use this to show that quadratic forms over any field are locally list decodable up to their minimum distance . the analogous statement for linear polynomials was proved in the celebrated works of goldreich et al. previously , tight bounds for quadratic polynomials were known only for q <digit> and <digit> the best bound known for other fields was the johnson radius . departing from previous work on reed muller decoding which relies on some form of self corrector , our work applies ideas from fourier analysis of boolean functions to low degree polynomials over finite fields , in conjunction with results about the weight distribution . we believe that the techniques used here could find other applications , we present some applications to testing and learning .
monotonic solution concepts in coevolution . <eos> assume a coevolutionary algorithm capable of storing and utilizing all phenotypes discovered during its operation , for as long as it operates on a problem that is , assume an algorithm with a monotonically increasing knowledge of the search space . we ask if such an algorithm were to periodically report , over the course of its operation , the best solution found so far , would the quality of the solution reported by the algorithm improve monotonically over time to answer this question , we construct a simple preference relation to reason about the goodness of different individual and composite phenotypic behaviors . we then show that whether the solutions reported by the coevolutionary algorithm improve monotonically with respect to this preference relation depends upon the solution concept implemented by the algorithm . we show that the solution concept implemented by the conventional coevolutionary algorithm does not guarantee monotonic improvement in contrast , the game theoretic solution concept of nash equilibrium does guarantee monotonic improvement . thus , this paper considers <digit> ) whether global and objective metrics of goodness can be applied to coevolutionary problem domains ( possibly with open ended search spaces ) , and <digit> ) whether coevolutionary algorithms can , in principle , optimize with respect to such metrics and find solutions to games of strategy .
building roadmaps of minima and transitions in visual models . <eos> becoming trapped in suboptimal local minima is a perennial problem when optimizing visual models , particularly in applications like monocular human body tracking where complicated parametric models are repeatedly fitted to ambiguous image measurements . we show that trapping can be significantly reduced by building ' roadmaps ' of nearby minima linked by transition pathways paths leading over low ' mountain passes ' in the cost surface found by locating the transition state ( codimension <digit> saddle point ) at the top of the pass and then sliding downhill to the next minimum . we present two families of transition state finding algorithms based on local optimization . in eigenvector tracking , unconstrained newton minimization is modified to climb uphill towards a transition state , while in hypersurface sweeping , a moving hypersurface is swept through the space and moving local minima within it are tracked using a constrained newton method . these widely applicable numerical methods , which appear not to be known in vision and optimization , generalize methods from computational chemistry where finding transition states is critical for predicting reaction parameters . experiments on the challenging problem of estimating 3d human pose from monocular images show that our algorithms find nearby transition states and minima very efficiently , but also underline the disturbingly large numbers of minima that can exist in this and similar model based vision problems .
dynamic computational geometry on parallel computers . <eos> this paper surveys our parallel algorithms for determining geometric properties of systems of moving objects . the properties investigated include nearest ( farthest ) neighbor , closest ( farthest ) pair , collision , convex hull , diameter , and containment . the models of computation include the crew pram , mesh , and hypercube .
why does the single neuron activity change from trial to trial during sensory motor task . <eos> single neuron activities from cortical areas of a monkey were recorded while performing a sensory motor task ( a choice reaction time task ) . quantitative trial by trial analysis revealed that the timing of peak activity exhibited large variation from trial to trial , compared to the variation in the behavioral reaction time of the task . therefore , we developed a multi unit dynamic neural network model to investigate the effects of structure of neural connections on the variation of the timing of peak activity . computer simulation of the model showed that , even though the units are connected in a cascade fashion , a wide variation exists in the timing of peak activity of neurons because of parallel organization of neural network within each unit .
preventing design conflicts in distributed design systems composed of heterogeneous agents . <eos> we model the uncertainty in distributed design . we model the attitudes of design agents to develop novel collaboration indicators . monte carlo simulation is performed for heterogeneous design agents . design conflicts of heterogeneous design agents are prevented . design agent dominations are reduced to be coherent with agent characters .
survivability and performance optimization of mobile wireless communication networks in the event of base station failure . <eos> in this paper , we investigate the survivability of mobile wireless communication networks in the event of base station ( bs ) failure . a survivable network is modeled as a mathematical optimization problem in which the objective is to minimize the total amount of blocked traffic . we apply lagrangean relaxation as a solution approach and analyze the experiment results in terms of the blocking rate , service rate , and cpu time . the results show that the total call blocking rate ( cbr ) is much less sensitive to the call blocking probability ( cbp ) threshold of each bs when the load is light , rather than heavy therefore , the more traffic loaded , the less the service rate will vary . bs recovery is much more important when the network load is light . however , the bs recovery ratio ( bsrr ) , which is a key factor in reducing the blocking rate for a small number of bss , is more important when a system is heavily loaded . the proposed model provides network survivability subject to available resources . the model also fits capacity expansion requirements by locating mobile portable bss in the places they are most needed .
2d shallow water flow model for the hydraulic jump . <eos> a flow model is presented for predicting a hydraulic jump in a straight open channel . the model is based on the general 2d shallow water equations in strong conservation form , without artificial viscosity , which is usually incorporated into the flow equations to capture a hydraulic jump . the equations are discretised using the finite volume method . the results are compared with experimental data and available numerical results , and have shown that the present model can provide good results . the model is simple and easy to implement . to demonstrate the potential application of the model , several hydraulic jumps occurring in different situations are simulated , and the predictions are in good agreement with standard solution for open channel hydraulics . copyright ( c ) <digit> john wiley sons , ltd .
using multiple query representations in patent prior art search . <eos> before a patent application is made , it is important to search the appropriate databases for prior art ( i.e. , pre existing patents that may affect the validity of the application ) . previous work on prior art search has concentrated on single query representations of the patent application . in the following paper , we describe an approach which uses multiple query representations . we evaluate our technique using a well known test collection ( clef ip <digit> ) . our results suggest that multiple query representations significantly outperform single query representations .
a genetic algorithm with tabu search procedure for flexible job shop scheduling with transportation constraints and bounded processing times . <eos> in this paper , we propose a model for flexible job shop scheduling problem ( fjssp ) with transportation constraints and bounded processing times . this is a np hard problem . objectives are to minimize the makespan and the storage of solutions . a genetic algorithm with tabu search procedure is proposed to solve both assignment of resources and sequencing problems on each resource . in order to evaluate the proposed algorithm 's efficiency , five types of instances are tested . three of them consider sequencing problems with or without assignment of processing or and transport resources . the fourth and fifth ones introduce bounded processing times which mainly characterize surface treatment facilities ( stfs ) . computational results show that our model and method are efficient for solving both assignment and scheduling problems in various kinds of systems .
intrusion detection by integrating boosting genetic fuzzy classifier and data mining criteria for rule pre screening . <eos> the purpose of the work described in this paper is to provide an intelligent intrusion detection system ( iids ) that uses two of the most popular data mining tasks , namely classification and association rules mining together for predicting different behaviors in networked computers . to achieve this , we propose a method based on iterative rule learning using a fuzzy rule based genetic classifier . our approach is mainly composed of two phases . first , a large number of candidate rules are generated for each class using fuzzy association rules mining , and they are pre screened using two rule evaluation criteria in order to reduce the fuzzy rule search space . candidate rules obtained after pre screening are used in genetic fuzzy classifier to generate rules for the classes specified in iids namely normal , prb probe , dos denial of service , u2r user to root and r2l remote to local . during the next stage , boosting genetic algorithm is employed for each class to find its fuzzy rules required to classify data each time a fuzzy rule is extracted and included in the system . boosting mechanism evaluates the weight of each data item to help the rule extraction mechanism focus more on data having relatively more weight , i.e. , uncovered less by the rules extracted until the current iteration . each extracted fuzzy rule is assigned a weight . weighted fuzzy rules in each class are aggregated to find the vote of each class label for each data item .
an objective perceptual quality based adte for adapting mobile svc video content . <eos> in this paper , we propose an adaptation decision taking engine ( adte ) that targets the delivery of scalable video content in mobile usage environments . our adte design relies on an objective perceptual quality metric in order to achieve video adaptation according to human visual perception , thus allowing to maximize the quality of service ( qos ) . to describe the characteristics of a particular usage environment , as well as the properties of the scalable video content , mpeg <digit> digital item adaptation ( dia ) is used . our experimental results show that the proposed adte design provides video content with a higher subjective quality than an adte using the conventional maximum bit allocation method .
a memory efficient pipelined implementation of the aho corasick string matching algorithm . <eos> with rapid advancement in internet technology and usages , some emerging applications in data communications and network security require matching of huge volume of data against large signature sets with thousands of strings in real time . in this article , we present a memory efficient hardware implementation of the well known aho corasick ( ac ) string matching algorithm using a pipelining approach called p ac . an attractive feature of the ac algorithm is that it can solve the string matching problem in time linearly proportional to the length of the input stream , and the computation time is independent of the number of strings in the signature set . a major disadvantage of the ac algorithm is the high memory cost required to store the transition rules of the underlying deterministic finite automaton . by incorporating pipelined processing , the state graph is reduced to a character trie that only contains forward edges . together with an intelligent implementation of look up tables , the memory cost of p ac is only about <digit> bits per character for a signature set containing 6,166 strings extracted from snort . the control structure of p ac is simple and elegant . the cost of the control logic is very low . with the availability of dual port memories in fpga devices , we can double the system throughput by duplicating the control logic such that the system can process two data streams concurrently . since our method is memory based , incremental changes to the signature set can be accommodated by updating the look up tables without reconfiguring the fpga circuitry .
development of online suites of social science based resources for health researchers and practitioners . <eos> the burgeoning of the internet has enormous potential for bringing scientific research into the hands of both health practitioners and health researchers to enhance their job performance . in this article , the authors give two examples of how carefully developed and organized online resources can leverage the engaging multimedia formats , ubiquitous access , and low cost of the internet to address this goal . the article describes two new online suites of social and behavioral science based resources designed for those in the hiv aids and teen pregnancy prevention fields hiv research and practice resources and teen pregnancy research and practice resources . each online suite includes research data , survey instruments , prevention resources , and evaluation related publications and tools that can enhance prevention research and practice . the article ends by peering into the future at how the field of health related prevention and research might be further advanced using the internet .
deferring elimination of design alternatives in object oriented methods . <eos> while developing systems , software engineers generally have to deal with a large number of design alternatives . current object oriented methods aim to eliminate design alternatives whenever they are generated . alternatives , however , should be eliminated only when sufficient information to take such a decision is available . otherwise , alternatives have to be preserved to allow further refinements along the development process . too early elimination of alternatives results in loss of information and excessive restriction of the design space . this paper aims to enhance the current object oriented methods by modeling and controlling the design alternatives through the application of fuzzy logic based techniques . by using an example method , it is shown that the proposed approach increases the adaptability and reusability of design models . the method has been implemented and tested in our experimental case environment . copyright ( c ) <digit> john wiley sons , ltd .
fiber reinforced concrete properties a multiscale approach . <eos> this paper describes the development of a fiber reinforced concrete ( frc ) unit cell for analyzing concrete structures by executing a multiscale analysis procedure using the theory of homogenization . this was achieved through solving a periodic unit cell problem of the material in order to evaluate its macroscopic properties . our research describes the creation of an frc unit cell through the use of concrete paste generic information e.g. the percentage of aggregates , their distribution , and the percentage of fibers in the concrete . the algorithm presented manipulates the percentage and distribution of these aggregates along with fiber weight to create a finite element unit cell model of the frc which can be used in a multiscale analysis of concrete structures .
new spline spaces with generalized tension properties . <eos> the paper describes a new space of variable degree polynomials . this space is isomorphic to p ( <digit> ) , possesses a bernstein like basis and has generalized tension properties in the sense that , for limit values of the degrees , its functions approximate quadratic polynomials . the corresponding space of c ( <digit> ) , variable degree splines is also studied . this spline space can be profitably used in the construction of shape preserving curves or surfaces .
antheprot an integrated protein sequence analysis software with client server capabilities . <eos> programs devoted to the analysis of protein sequences exist either as stand alone programs or as web servers . however , stand alone programs can hardly accommodate for the analysis that involves comparisons on databanks , which require regular updates . moreover , web servers can not be as efficient as stand alone programs when dealing with real time graphic display . we describe here a stand alone software program called antheprot , which is intended to perform protein sequence analysis with a high integration level and clients server capabilities . it is an interactive program with a graphical user interface that allows handling of protein sequence and data in a very interactive and convenient manner . it provides many methods and tools , which are integrated into a graphical user interface . antheprot is available for windows based systems . it is able to connect to a web server in order to perform large scale sequence comparison on up to date databanks . antheprot is freely available to academic users and may be downloaded at http pbil.ibcp.fr antheprot .
cardioids based faster authentication and diagnosis of remote cardiovascular patients . <eos> in recent times , dealing with deaths associated with cardiovascular diseases ( cvd ) has been one of the most challenging issues . the usage of mobile phones and portable electrocardiogram ( ecg ) acquisition devices can mitigate the risks associated with cvd by providing faster patient diagnosis and patient care . the existing technologies entail delay in patient authentication and diagnosis . however , for the cardiologists minimizing the delay between a possible cvd symptom and patient care is crucial , as this has a proven impact in the longevity of the patient . therefore , every seconds counts in terms of patient authentication and diagnosis . in this paper , we introduce the concept of cardioid based patient authentication and diagnosis . according to our experimentations , the authentication time can be reduced from 30.64 s ( manual authentication in novice mobile user ) to 0.4398 s ( automated authentication ) . our ecg based patient authentication mechanism is up to <digit> times faster than conventional biometrics like , face recognition . the diagnosis time could be improved from several minutes to less than 0.5 s ( cardioid display on a single screen ) . therefore , with our presented mission critical alerting mechanism on wireless devices , minute 's worth of tasks can be reduced to second 's , without compromising the accuracy of authentication and quality of diagnosis . copyright ( c ) <digit> john wiley sons , ltd .
a multiprocessor system on chip for real time biomedical monitoring and analysis ecg prototype architectural design space exploration . <eos> in this article we focus on multiprocessor system on chip ( mpsoc ) architectures for human heart electrocardiogram ( ecg ) real time analysis as a hardware software ( hw sw ) platform offering an advance relative to state of the art solutions . this is a relevant biomedical application with good potential market , since heart diseases are responsible for the largest number of yearly deaths . hence , it is a good target for an application specific system on chip ( soc ) and hw sw codesign . we investigate a symmetric multiprocessor architecuture based on stmicroelecronics vliw dsps that process in real time <digit> lead ecg signals . this architecture improves upon state of the art soc designs for ecg analysis in its ability to analyze the full <digit> leads in real time , even with high sampling frequencies , and its ability to detect heart malfunction for the whole ecg signal interval . we explore the design space by considering a number of hardware and software architectural options . comparing our design with present day solutions from an soc and application point ofview shows that our platform can be used in real time and without failures .
accurate approximation of the earth mover 's distance in linear time . <eos> color descriptors are one of the important features used in content based image retrieval . the dominant color descriptor ( dcd ) represents a few perceptually dominant colors in an image through color quantization . for image retrieval based on dcd , the earth mover 's distance ( emd ) and the optimal color composition distance were proposed to measure the dissimilarity between two images . although providing good retrieval results , both methods are too time consuming to be used in a large image database . to solve the problem , we propose a new distance function that calculates an approximate earth mover 's distance in linear time . to calculate the dissimilarity in linear time , the proposed approach employs the space filling curve for multidimensional color space . to improve the accuracy , the proposed approach uses multiple curves and adjusts the color positions . as a result , our approach achieves order of magnitude time improvement but incurs small errors . we have performed extensive experiments to show the effectiveness and efficiency of the proposed approach . the results reveal that our approach achieves almost the same results with the emd in linear time .
variable selection in linear regression several approaches based on normalized maximum likelihood . <eos> the use of the normalized maximum likelihood ( nml ) for model selection in gaussian linear regression poses troubles because the normalization coefficient is not finite . the most elegant solution has been proposed by rissanen and consists in applying a particular constraint for the data space . in this paper , we demonstrate that the methodology can be generalized , and we discuss two particular cases , namely the rhomboidal and the ellipsoidal constraints . the new findings are used to derive four nml based criteria . for three of them which have been already introduced in the previous literature , we provide a rigorous analysis . we also compare them against five state of the art selection rules by conducting monte carlo simulations for families of models commonly used in signal processing . additionally , for the eight criteria which are tested , we report results on their predictive capabilities for real life data sets .
spacetime adaptive finite difference method for european multi asset options . <eos> the multi dimensional blackscholes equation is solved numerically for a european call basket option using a prioria posteriori error estimates . the equation is discretized by a finite difference method on a cartesian grid . the grid is adjusted dynamically in space and time to satisfy a bound on the global error . the discretization errors in each time step are estimated and weighted by the solution of the adjoint problem . bounds on the local errors and the adjoint solution are obtained by the maximum principle for parabolic equations . comparisons are made with monte carlo and quasi monte carlo methods in one dimension , and the performance of the method is illustrated by examples in one , two , and three dimensions .
alpha words and the radix order . <eos> let alpha ( a ( <digit> ) , a ( <digit> ) , ... ) be a sequence ( finite or infinite ) of integers with a ( <digit> ) > <digit> and a ( n ) > <digit> , for all n > <digit> . let a , b be an alphabet . for n > <digit> , and r r ( <digit> ) r ( <digit> ) ... r ( n ) is an element of n ( n ) with <digit> <digit> . many interesting combinatorial properties of alpha words have been studied by chuan . in this paper , we obtain some new methods of generating the distinct alpha words of the same order in lexicographic order . among other results , we consider another function r bar right arrow w r from the set of labels of alpha words to the set of alpha words . the string r is called a new label of the alpha word w r . using any new label of an nth order alpha word w , we can compute the number of the nth order alpha words that are less than w in the lexicographic order . with the radix orders < ( r ) on n ( n ) ( regarding n as an alphabet ) and a , b ( ) with a < ( r ) b , we prove that there exists a subset d of the set of all labels such that w r < ( r ) w s whenever r , s is an element of d and r < ( r ) s. ( c ) <digit> elsevier b.v. all rights reserved .
a buyerseller game model for selection and negotiation of purchasing bids extensions and new models . <eos> a number of efficiency based vendor selection and negotiation models have been developed to deal with multiple attributes including price , quality and delivery performance . the efficiency is defined as the ratio of weighted outputs to weighted inputs . by minimizing the efficiency , talluri eur . j. operat . res . <digit> ( <digit> ) ( <digit> ) <digit> proposes a buyerseller game model that evaluates the efficiency of alternative bids with respect to the ideal target set by the buyer . the current paper shows that this buyerseller game model is closely related to data envelopment analysis ( dea ) and can be simplified . the current paper also shows that setting the ( ideal ) target actually incorporates implicit tradeoff information on the multiple attributes into efficiency evaluation . we develop a new buyerseller game model where the efficiency is maximized with respect to multiple targets set by the buyer . the new model allows the buyer to evaluate and select the vendors in the context of best practice . by both minimizing and maximizing efficiency , the buyer can obtain an efficiency range within which the true efficiency lies given the implicit tradeoff information characterized by the targets . the current study establishes the linkage between buyerseller game models and dea . such a linkage can provide the buyer with correct evaluation methods based upon existing dea models regarding the nature of bidding .
a bayesian latent variable model with classification and regression tree approach for behavior and credit scoring . <eos> a bayesian latent variable model with classification and regression tree approach is built to overcome three challenges encountered by a bank in credit granting process . these three challenges include ( <digit> ) the bank wants to predict the future performance of an applicant accurately ( <digit> ) given current information about cardholders credit usage and repayment behavior , financial institutions would like to determine the optimal credit limit and apr for an applicant and ( <digit> ) the bank would like to improve its efficiency by automating the process of credit granting decisions . data from a leading bank in taiwan is used to illustrate the combined approach . the data set consists of each credit card holders credit usage and repayment data , demographic information , and credit report . empirical study shows that the demographic variables used in most credit scoring models have little explanatory ability with regard to a cardholders credit usage and repayment behavior . a cardholders credit history provides the most important information in credit scoring . the continuous latent customer quality from the bayesian latent variable model allows considerable latitude for producing finer rules for credit granting decisions . compared to the performance of discriminant analysis , logistic regression , neural network , multivariate adaptive regression splines ( mars ) and support vector machine ( svm ) , the proposed model has a 92.9 % accuracy rate in predicting customer types , is less impacted by prior probabilities , and has a significantly low type i errors in comparison with the other five approaches .
the representation of manufacturing requirements in model driven parts manufacturing . <eos> today there is a need to make process and production planning more cost effective while not compromising the quality of the product . manufacturing requirements are used to ensure producibility in early development phases and also as a source for continuous improvement of the manufacturing system . to make this possible it is essential to have correct , updated information available and to be able to trace the relations between requirements and their origin and subjects . to trace requirements ' origin in resources or processes is today very difficult owing to system integration problems . this article discusses the relations that need to be represented and proposes the use of model based methods to enable traceability of requirements . because requirements are a collaborative effort a standard for information exchange is needed . the iso10303 step application protocol ap233 system engineering is proposed for this purpose .
baldwinian learning utilizing genetic and heuristic algorithms for logic synthesis and minimization of incompletely specified data with generalized reedmuller ( andexor ) forms . <eos> this research applies a new heuristic combined with a genetic algorithm ( ga ) to the task of logic minimization for incompletely specified data , with both single and multi outputs , using the generalized reedmuller ( grm ) equation form . the grm equation type is a canonical expression of the exclusive or sum of products ( esops ) type , in which for every subset of input variables there exists not more than one term with arbitrary polarities of all variables . this andexor implementation has been shown to be economical , generally requiring fewer gates and connections than that of andor logic . grm logic is also highly testable , making it desirable for fpga designs . the minimization results of this new algorithm tested on a number of binary benchmarks are given . this minimization algorithm utilizes a ga with a two level fitness calculation , which combines human designed heuristics with the evolutionary process , employing baldwinian learning . in this algorithm , first a pure ga creates certain constraints for the selection of chromosomes , creating only genotypes ( polarity vectors ) . the phenotypes ( grms ) are then learned in the environment and contribute to the ga fitness ( which is the total number of terms of the best grm for each output ) , providing indirect feedback as to the quality of the genotypes ( polarity vectors ) but the genotype chromosomes ( polarity vectors ) remain unchanged . in this process , the improvement in genotype chromosomes ( polarity vectors ) is the product of the evolutionary processes from the ga only . the environmental learning is achieved using a human designed grm minimization heuristic . as much previous research has presented the merit of andexor logic for its high density and testability , this research is the first application of the grm ( a canonical andexor form ) to the minimization of incompletely specified data .
energy stable numerical methods for hyperbolic partial differential equations using overlapping domain decomposition . <eos> overlapping domain decomposition methods , otherwise known as overset grid or chimera methods , are useful for simplifying the discretization of partial differential equations in or around complex geometries . though in wide use , such methods are prone to numerical instability unless numerical diffusion or some other form of regularization is used , especially for higher order methods . to address this shortcoming , high order , provably energy stable , overlapping domain decomposition methods are derived for hyperbolic initial boundary value problems . the overlap is treated by splitting the domain into pieces and using generalized summation by parts derivative operators and polynomial interpolation . new implicit and explicit operators are derived that do not require regularization for stability in the linear limit . applications to linear and nonlinear problems in one and two dimensions are presented , where it is found the explicit operators are preferred to the implicit ones .
the impact of cluster representatives on the convergence of the k modes type clustering . <eos> as a leading partitional clustering technique , k modes is one of the most computationally efficient clustering methods for categorical data . in the k modes , a cluster is represented by a mode , which is composed of the attribute value that occurs most frequently in each attribute domain of the cluster , whereas , in real applications , using only one attribute value in each attribute to represent a cluster may not be adequate as it could in turn affect the accuracy of data analysis . to get rid of this deficiency , several modified clustering algorithms were developed by assigning appropriate weights to several attribute values in each attribute . although these modified algorithms are quite effective , their convergence proofs are lacking . in this paper , we analyze their convergence property and prove that they can not guarantee to converge under their optimization frameworks unless they degrade to the original k modes type algorithms . furthermore , we propose two different modified algorithms with weighted cluster prototypes to overcome the shortcomings of these existing algorithms . we rigorously derive updating formulas for the proposed algorithms and prove the convergence of the proposed algorithms . the experimental studies show that the proposed algorithms are effective and efficient for large categorical datasets .
service quality and erp implementation a conceptual and empirical study of semiconductor related industries in taiwan . <eos> this paper examines the effectiveness of the implementation of enterprise resource planning ( erp ) in improving service quality in the taiwanese semiconductor industry by assessing the expectations and the perceptions of service quality from the perspectives of both upstream manufacturers and downstream customers . the study first establishes a modified service quality gap model incorporating ( i ) the downstream customers ' expectations and perceptions , and ( ii ) the upstream manufacturers ' perceptions of the customers ' expectations and perceptions . an empirical study by questionnaire survey is then undertaken to investigate the gaps proposed in the research model . the results show that service quality gaps do exist in the taiwanese semiconductor industry between upstream manufacturers that are implementing erp and their downstream customers . the study shows that the proposed model provides valuable guidance to manufacturers with respect to the prevention , detection , and elimination of the demonstrated service quality gaps . the model thus helps manufacturers to evaluate the contribution of various erp modules to improved customer satisfaction with service quality and also provides guidance on improvement strategies to enhance service quality by eliminating quality gaps . ( c ) <digit> elsevier b.v. all rights reserved .
largest inscribed rectangles in convex polygons . <eos> we consider approximation algorithms for the problem of computing an inscribed rectangle having largest area in a convex polygon on n vertices . if the order of the vertices of the polygon is given , we present a randomized algorithm that computes an inscribed rectangle with area at least ( <digit> ) ( <digit> ) times the optimum with probability t in time o ( <digit> log n ) for any constant t < <digit> t < <digit> . we further give a deterministic approximation algorithm that computes an inscribed rectangle of area at least ( <digit> ) ( <digit> ) times the optimum in running time o ( <digit> <digit> log n ) and show how this running time can be slightly improved .
an effective node selection scheme for the energy efficiency of solar powered wsns in a stream environment . <eos> we propose an effective node selection scheme in the stream environment of solar powered wsns . we analyzed the stream environment including single stream and cross stream cases . the deployment conditions are appropriate to each stream case . based on the node selection scheme , the number of active nodes and transmitted packets is minimized . the proposed scheme prolongs the lifetime of the solar powered wsn in a stream environment .
the concept of a quasi particle and the non probabilistic interpretation of wave mechanics . <eos> in recent works of the author found phys <digit> ( <digit> ) <digit> <digit> , math comput simul <digit> ( <digit> ) <digit> <digit> , the argument has been made that hertz 's equations of electrodynamics reflect the material invariance ( indifference ) of the latter . then the principle of material invariance was postulated in heu of lorentz covariance . and the respective absolute medium wits named the metacontinuum here . we go further to assume that the metacontinuum is a very thin but very stuff 3d hypershell in the 4d space the equation for the deflection of the shell along the fourth dimension is the master nonlinear dispersive equation of wave mechanics whose linear part ( euler bernoulli equation ) is nothing else but the schrodinger wave equation written for the real or the imaginary part of the wave function . the wave function has a clear non probabilistic interpretation as the actual amplitude of the flexural deformation the master equation admits solitary wave solutions solutions that behave as quasi particles ( qps ) . we stipulate that particles are our perception of the qps ( schaumkommen in schrodinger 's own words ) . we show the passage from the continuous lagrangian of the field to the discrete lagrangian of the centers of qps and introduce the concept of ( pseudo ) mass . we interpret the membrane tension as all attractive ( gravitational ) force acting between the qps . thus . it self consistent unification of electrodynamics , wave mechanics , gravitation . and the wave particle duality is achieved ( c ) <digit> imacs . published by elsevier b.v all rights reserved .
an empirical study of the expressiveness of the functional basis . <eos> function models are frequently used in engineering design to describe the technical functions that a product performs . this paper investigates the use of the functional basis , a function vocabulary developed to aid in communication and archiving of product function information , in describing consumer products that have been decomposed , analyzed , modeled functionally , and stored in a web based design repository . the frequency of use of function terms and phrases in <digit> graphical and <digit> list based representations in the repository is examined and used to analyze the organization and expressiveness of the functional basis and function models . within the context of reverse engineering , we determined that the modeling resolution provided by the hierarchical levels , especially the tertiary level , is inadequate for function modeling the tertiary terms are inappropriate for capturing sufficient details desired by modelers for archiving and reuse , and there is a need for a more expressive flow terms and flow qualifiers in the vocabulary . a critical comparison is also presented of two representations in the design repository function structures and function lists . the conclusions are used to identify new research opportunities , including the extension of the vocabulary to incorporate flow qualifiers in addition to more expressive terms .
a 6.7 kbps vector sum excited linear prediction on tms320c54x digital signal processor . <eos> in this paper , a 6.7 kbps vector sum excited linear prediction ( vselp ) coder with less computational complexity is presented . a very efficient vselp codebook with nine basis vectors and a heuristic k selection method ( to reduce the search space and complexity ) is constructed to obtain the stochastic codebook vector . the nine basis vectors are obtained by optimizing a set of randomly generated basis vectors . during the optimization process , we have trained the basis vectors to give the system apriori knowledge of the characteristics of the input . the coder is implemented on a tms320c541 digital signal processor . the performance is evaluated by testing the 6.7 kbps vselp coder with different test speech data taken from different speakers . the quality of the coder is estimated by comparing the performance of the 6.7 kbps vselp coder with an <digit> kbps vselp speech coder based on the is <digit> standards . ( c ) <digit> elsevier science b.v. all rights reserved .
a design for digital testability circuit structure for sigma delta modulators . <eos> a design for digital testability ( dfdt ) switched capacitor circuit structure for testing sigma delta modulators with digital stimuli is presented to reduce the overall testing cost . in the test mode , the dfdt circuits are reconfigured as a one bit digital to charge converter to accept a repetitively applied sigma delta modulated bit stream as its stimulus . the single bit characteristic ensures that the generated stimulus is nonlinearity free . in addition , the proposed dfdt structure reuses most of the analog components in the test mode and keeps the same loads for the operational amplifiers as if they were in the normal mode . it thereby achieves many advantages including lower cost , higher fault coverage , higher measurement accuracy , and the capability of performing at speed tests . a second order sigma delta modulator was designed and fabricated to demonstrate the effectiveness of the dfdt structure . our experimental results show that the digital test is able to measure a harmonic distortion lower than <digit> dbfs . meanwhile , the dynamic range measured with the digital stimulus is as high as 84.4 db at an over sampling ratio of <digit> . the proposed dfdt scheme can be easily applied to other types of sigma delta modulators , making them also digitally testable .
local community detection using link similarity . <eos> exploring local community structure is an appealing problem that has drawn much recent attention in the area of social network analysis . as the complete information of network is often difficult to obtain , such as networks of web pages , research papers and facebook users , people can only detect community structure from a certain source vertex with limited knowledge of the entire graph . the existing approaches do well in measuring the community quality , but they are largely dependent on source vertex and putting too strict policy in agglomerating new vertices . moreover , they have predefined parameters which are difficult to obtain . this paper proposes a method to find local community structure by analyzing link similarity between the community and the vertex . inspired by the fact that elements in the same community are more likely to share common links , we explore community structure heuristically by giving priority to vertices which have a high link similarity with the community . a three phase process is also used for the sake of improving quality of community structure . experimental results prove that our method performs effectively not only in computer generated graphs but also in real world graphs .
leveraging information technology towards enhancing patient care and a culture of safety in the us . <eos> objectives to heighten awareness about the critical issues currently affecting patient care and to propose solutions based on leveraging information technologies to enhance patient care and influence a culture of patient safety . methods presentation and discussion of the issues affecting health care today , such as medical and medication related errors and analysis of their root causes proliferation of medical knowledge and medical technologies initiatives to improve patient safety steps necessary to develop a culture of safety introduction of relevant enabling technologies and evidence of results . results and conclusions medical errors affect not only mortality and morbidity , but they also create secondary costs leading to dissatisfaction by both provider and patient . health care has been slow to acknowledge the benefits of enabling technologies to affect the quality of care . evaluation of recent applications , such as the computerized patient record , physician order entry , and computerized alerting systems show tremendous potential to enhance patient care and influence the development of a culture focused on safety . they will also bring about changes in other areas , such as workflow and the creation of new partnerships among providers , patients , and payers .
mmapdng a new , fast code backed by a memory mapped database for simulating delayed ray emission with mcnpx package . <eos> the simulation of the emission of beta delayed gamma rays following nuclear fission and the calculation of time dependent energy spectra is a computational challenge . the widely used radiation transport code mcnpx includes a delayed gamma ray routine that is inefficient and not suitable for simulating complex problems . this paper describes the code mmapdng ( memory mapped delayed neutron and gamma ) , an optimized delayed gamma module written in c , discusses usage and merits of the code , and presents results . the approach is based on storing required fission product yield ( fpy ) data , decay data , and delayed particle data in a memory mapped file . when compared to the original delayed gamma ray code in mcnpx , memory utilization is reduced by two orders of magnitude and the ray sampling is sped up by three orders of magnitude . other delayed particles such as neutrons and electrons can be implemented in future versions of mmapdng code using its existing framework .
measurement of wireless pressure sensors fabricated in high temperature co fired ceramic mems technology . <eos> high temperature co fired ceramics ( htccs ) have wide applications with stable mechanical properties , but they have not yet been used to fabricate sensors . by introducing the wireless telemetric sensor system and ceramic structure embedding a pressure deformable cavity , the designed sensors made from htcc materials ( zirconia and <digit> % alumina ) are fabricated , and their capacities for the pressure measurement are tested using a wireless interrogation method . using the fabricated sensor , a study is conducted to measure the atmospheric pressure in a sealed vessel . the experimental sensitivity of the device is <digit> hz pa of zirconia and 1.08 hz pa of alumina below 0.5 mpa with a readout distance of 2.5 cm . the described sensor technology can be applied for monitoring of atmospheric pressure to evaluate important component parameters in harsh environments .
real time rectification using differentially encoded lookup table . <eos> in this paper , we propose a new real time rectification technique based on the compressed lookup table . to compress the lookup table we adopt a differential encoding . as a result , we successfully constructed the rectification with obtaining the compression ratio of <digit> % so as to fulfill real time requirement ( i.e. , <digit> fps at 74.25 mhz ) . furthermore , our result on performance is comparable to the result of <digit> that obtains <digit> fps at 90mhz for 640x512 images .
development of a maximum likelihood regression tree based model for predicting subway incident delay . <eos> we develop a maximum likelihood regression tree based model to predict subway incident delays . an aft model is assigned to each terminal node in the maximum likelihood regression tree . our tree based model outperforms the traditional aft models with fixed and random effects . our tree based model can account for the heterogeneity effect as well as avoid the over fitting problem .
solving inverse frequent itemset mining with infrequency constraints via large scale linear programs . <eos> inverse frequent set mining ( ifm ) is the problem of computing a transaction database d satisfying given support constraints for some itemsets , which are typically the frequent ones . this article proposes a new formulation of ifm , called ifmi ( ifm with infrequency constraints ) , where the itemsets that are not listed as frequent are constrained to be infrequent that is , they must have a support less than or equal to a specified unique threshold . an instance of ifmi can be seen as an instance of the original ifm by making explicit the infrequency constraints for the minimal infrequent itemsets , corresponding to the so called negative generator border defined in the literature . the complexity increase from pspace ( complexity of ifm ) to nexp ( complexity of ifmi ) is caused by the cardinality of the negative generator border , which can be exponential in the original input size . therefore , the article introduces a specific problem parameter . that computes an upper bound to this cardinality using a hypergraph interpretation for which minimal infrequent itemsets correspond to minimal transversals . by fixing a constant k , the article formulates a k bounded definition of the problem , called k ifmi , that collects all instances for which the value of the parameter . is less than or equal to k its complexity is in pspace as for ifm . the bounded problem is encoded as an integer linear program with a large number of variables ( actually exponential w.r.t. the number of constraints ) , which is thereafter approximated by relaxing integer constraints the decision problem of solving the linear program is proven to be in np . in order to solve the linear program , a column generation technique is used that is a variation of the simplex method designed to solve large scale linear programs , in particular with a huge number of variables . the method at each step requires the solution of an auxiliary integer linear program , which is proven to be np hard in this case and for which a greedy heuristic is presented . the resulting overall column generation solution algorithm enjoys very good scaling as evidenced by the intensive experimentation , thereby paving the way for its application in real life scenarios .
an efficient optimization procedure for tetrahedral meshes by chaos search algorithm . <eos> a simple and efficient local optimization based procedure for node repositioning smoothing of three dimensional tetrahedral meshes is presented . the initial tetrahedral mesh is optimized with respect , to a specified element shape measure by chaos search algorithm , which is very effective for the optimization problems with only a few design variables . examples show that the presented smoothing procedure can provide favorable conditions for local transformation approach and the quality of mesh can be significantly improved by the combination of these two procedures with respect to a specified element shape measure . meanwhile , several commonly used shape measures for tetrahedral element , which are considered to be equivalent in some weak sense over a long period of time , are briefly re examined in this paper . preliminary study indicates that using different measures to evaluate the change of element shape will probably lead to inconsistent result for both well shaped and poorly shaped elements . the proposed smoothing approach can be utilized as an appropriate and effective tool for evaluating element shape measures and their influence on mesh optimization process and optimal solution .
ethical issues in selecting embryos . <eos> people involved in assisted reproduction frequently make decisions about which of several embryos to implant or which of several embryos to reduce from a multiple pregnancy . yet , others have raised questions about the ethical acceptability of using sex or genetic characteristics as selection criteria . this paper reviews arguments for rejecting embryo selection and discusses the subject of choosing offspring in terms of the centrality of liberty and autonomous choice in ethics . it also presents a position on the acceptable scope of embryo selection and the professional responsibilities of those who practice reproductive medicine .
competitive equilibrium in e commerce pricing and outsourcing . <eos> the success of firms engaged in e commerce depends on their ability to understand and exploit the dynamics of the market . one component of this is the ability to extract maximum profit and minimize costs in the face of the harsh competition that the internet provides . we present a general framework for modeling the competitive equilibrium across two firms , or across a firm and the market as a whole . within this framework , we study pricing choices and analyze the decision to outsource it capability . our framework is novel in that it allows for any number of distributions on usage levels , priceqos tradeoffs , and price and cost structures .
numerical simulation of 3d fluid structure interaction flow using an immersed object method with overlapping grids . <eos> the newly developed immersed object method ( iom ) tai ch , zhao y , liew km . parallel computation of unsteady incompressible viscous flows around moving rigid bodies using an immersed object method with overlapping grids . j comput phys <digit> <digit> ( l ) <digit> <digit> is extended for 3d unsteady flow simulation with fluid structure interaction ( fsi ) , which is made possible by combining it with a parallel unstructured multigrid navier stokes solver using a matrix free implicit dual time stepping and finite volume method tai ch , zhao y , liew km . parallel computation of unsteady three dimensional incompressible viscous flow using an unstructured multigrid method . in the second m.i.t. conference on computational fluid and solid mechanics , june <digit> <digit> , mit , cambridge , ma <digit> , usa , <digit> tai ch , zhao y , liew km . parallel computation of unsteady three dimensional incompressible viscous flow using an unstructured multigrid method , special issue on preconditioning methods algorithms , applications and software environments . comput struct <digit> <digit> ( <digit> ) <digit> <digit> . this uniquely combined method is then employed to perform detailed study of 3d unsteady flows with complex fsi . in the iom , a body force term f is introduced into the momentum equations during the artificial compressibility ( ac ) sub iterations so that a desired velocity distribution v <digit> can be obtained on and within the object boundary , which needs not coincide with the grid , by adopting the direct forcing method . an object mesh is immersed into the flow domain to define the boundary of the object . the advantage of this is that bodies of almost arbitrary shapes can be added without grid restructuring , a procedure which is often time consuming and computationally expensive . it has enabled us to perform complex and detailed 3d unsteady blood flow and blood leaflets interaction in a mechanical heart valve ( mhv ) under physiological conditions . ( c ) <digit> elsevier ltd. all rights reserved .
joint pricing and inventory control with a markovian demand model . <eos> we consider the joint pricing and inventory control problem for a single product over a finite horizon and with periodic review . the demand distribution in each period is determined by an exogenous markov chain . pricing and ordering decisions are made at the beginning of each period and all shortages are backlogged . the surplus costs as well as fixed and variable costs are state dependent . we show the existence of an optimal ( s , s , p ) type feedback policy for the additive demand model . we extend the model to the case of emergency orders . we compute the optimal policy for a class of markovian demand and illustrate the benefits of dynamic pricing over fixed pricing through numerical examples . the results indicate that it is more beneficial to implement dynamic pricing in a markovian demand environment with a high fixed ordering cost or with high demand variability .
privacy preserving learning in negotiation . <eos> machine learning techniques are widely used in negotiation systems . to get more accurate and satisfactory learning results , negotiation parties have the desire to employ learning techniques on the union of their past negotiation records . however , negotiation records are usually confidential and private , and owners may not want to reveal the details of these records . in this paper , we introduce a privacy preserving negotiation learning scheme that incorporate secure multiparty computation techniques into negotiation learning algorithms to allow negotiation parties to securely complete the learning process on a union of distributed data sets . as an example , a detailed solution for secure negotiation q learning is presented based on two secure multiparty computations weighted mean and maximum . we also introduce a novel protocol for the secure maximum operation .
the relative utility of regression and artificial neural networks models for rapidly predicting the capacity of water supply reservoirs . <eos> rapid prediction tools for reservoir over year and within year capacities that dispense with the sequential analysis of time series runoff data are developed using multiple linear regression and multi layer perceptron , artificial neural networks ( mlp anns ) . linear regression was used to model the total ( i.e. within year over year ) capacity using the over year capacity as one of the inputs , while the anns were used to simultaneously model directly the over year and total capacities . the inputs used for the anns were basic runoff and systems variables such as the coefficient of variation ( cv ) of annual and monthly runoff , minimum monthly runoff , the demand ratio and reservoir reliability . the results showed that all the models performed well during their development and when they were tested with independent data sets . both models offer faster prediction tools for reservoir capacity at gauged sites when compared with behaviour simulation . additionally , when the predictor variables can be evaluated at un gauged sites using e.g. catchment characteristics , they make capacity estimation at such un gauged sites a feasible proposition .
an access delay model for ieee 802.11 e edca . <eos> we analyze the mac access delay of the ieee 802.11 e enhanced distributed channel access ( edca ) mechanism under saturation . we develop a detailed analytical model to evaluate the influence of all edca differentiation parameters , namely aifs , cwmin , cwmax , and txop limit , as well as the backoff multiplier beta . explicit expressions for the mean , standard deviation , and generating function of the access delay distribution are derived . by applying numerical inversion on the generating function , we are able to efficiently compute values of the distribution . comparison with simulation confirms the accuracy of our analytical model over a wide range of operating conditions . we derive simple asymptotics and approximations for the mean and standard deviation of the access delay , which reveal the salient model parameters for performance under different differentiation mechanisms . we also use the model to numerically study the differentiation performance and find that beta differentiation , though rejected during the standardization process , is an effective differentiation mechanism that has some advantages over the other mechanisms .
slide presentations as speech suppressors when and why learners miss oral information . <eos> the objective of this study was to test whether information presented on slides during presentations is retained at the expense of information presented only orally , and to investigate part of the conditions under which this effect occurs , and how it can be avoided . such an effect could be expected and explained either as a kind of redundancy effect due to excessive cognitive load caused by simultaneous presentation of oral and written information , or as a consequence of dysfunctional allocation of attention at the expense of oral information occurring in learners with a high subjective importance of slides . the hypothesized effect and these potential explanations were tested in an experimental study . in courses about literature search and access , <digit> university students received a presentation accompanied either by no slides or by regular or concise powerpoint slides . the retention of information presented orally and of information presented orally and on slides was measured separately in each condition and standardized for comparability . cognitive load and subjective importance of slides were also measured . the results indicate a speech suppression effect of regular slides at the expense of oral information ( within and across conditions ) , which can not be explained by cognitive overload but rather by dysfunctional allocation of attention , and can be avoided by concise slides . it is concluded that theoretical approaches should account for the allocation of attention below the threshold of cognitive overload and its role for learning , and that a culture of presentations with concise slides should be established . ( c ) <digit> elsevier ltd. all rights reserved .
development of a parallel poisson 's equation solver with adaptive mesh refinement and its application in field emission prediction . <eos> a parallel electrostatic poisson 's equation solver coupled with parallel adaptive mesh refinement ( pamr ) is developed in this paper . the three dimensional poisson 's equation is discretized using the galerkin finite element method using a tetrahedral mesh . the resulting matrix equation is then solved through the parallel conjugate gradient method using the non overlapping subdomain by subdomain scheme . a pamr module is coupled with this parallel poisson 's equation solver to adaptively refine the mesh where the variation of potentials is large . the parallel performance of the parallel poisson 's equation is studied by simulating the potential distribution of a cnt based triode type field emitter . results with <digit> <digit> nodes show that a parallel efficiency of 84.2 % is achieved in <digit> processors of a pc cluster system . the field emission properties of a single cnt triode and tetrode type field emitter in a periodic cell are computed to demonstrate their potential application in field emission prediction .
isogeometric analysis of the advective cahnhilliard equation spinodal decomposition under shear flow . <eos> we present a numerical study of the spinodal decomposition of a binary fluid undergoing shear flow using the advective cahnhilliard equation , a stiff , nonlinear , parabolic equation characterized by the presence of fourth order spatial derivatives . our numerical solution procedure is based on isogeometric analysis , an approximation technique for which basis functions of high order continuity are employed . these basis functions allow us to directly discretize the advective cahnhilliard equation without resorting to a mixed formulation . we present steady state solutions for rectangular domains in two dimensions and , for the first time , in three dimensions . we also present steady state solutions for the two dimensional taylorcouette cell . to enforce periodic boundary conditions in this curved domain , we derive and utilize a new periodic bzier extraction operator . we present an extensive numerical study showing the effects of shear rate , surface tension , and the geometry of the domain on the phase evolution of the binary fluid . theoretical and experimental results are compared with our simulations .
a vhdl compiler based on attribute grammar methodology . <eos> this paper presents aspects of a compiler for a new hardware description language ( vhdl ) written using attribute grammar techniques . vhdl is introduced , along with the new compiler challenges brought by a language that extends an ada subset for the purpose of describing hardware . attribute grammar programming solutions are presented for some of the language challenges . the organization of the compiler and of the target virtual machine represented by the simulation kernel are discussed , and performance and code size figures are presented . the paper concludes that attribute grammars can be used for large commercial compilers with excellent results in terms of rapid development time and enhanced maintainability , and without paying any substantial penalty in terms of either the complexity of the language that can be handled or the resulting compilation speed .
an agent based framework for virtual medical devices . <eos> in this paper we present the telemedical environment based on vmds implemented with java mobile agent technology , called aglets . the agent based vmd implementation provides ad hoc agent interaction , support for mobile agents and different user interface components in the telemedical system . we have developed a vmd agent framework with four types of agents data agents , processing agents , presentation agents , and monitoring agents . data agents abstract data source , creating uniform view on different types of data , independent of data acquisition device . processing agents produce derived data , such us fft power spectrum , from raw data provided by the data agents . presentation agents supply user interface components using a variety of user data views . user interface components are based on http , sms and wap protocols . monitoring agents collaborate with data and processing agents providing support for data mining operations , and search for relevant patterns . typical example is monitoring for possible epileptic attacks . we have applied vmds to facilitate distributed eeg analysis . we have found that the flexibility of distributed agent architecture is well suited for the telemedical application domain . this flexibility is particularly important in the case of an emergency , enabling swift system reconfiguration on the fly .
finite block method in elasticity . <eos> a new point collocation algorithm named finite block method ( fbm ) , which is based on the one dimensional differential matrix is developed for 2d and 3d elasticity problems in this paper . the main idea is to construct the first order one dimensional differential matrix for one block by using lagrange series with uniformly distributed nodes . the higher order derivative matrix for one dimensional problem is obtained . by introducing the mapping technique , a block of quadratic type is transformed from cartesian coordinate ( xyz ) ( x y z ) to normalised coordinate ( ) ( ) with <digit> seeds or <digit> seeds for two or three dimensions . the differential matrices in physical domain are determined from that in the normalised transformed system . several 2d and 3d examples are given and comparisons have been made with either analytical solutions or the boundary element method to demonstrate the accuracy and convergence of this method .
new adaptive compressors for natural language text . <eos> semistatic byte oriented word based compression codes have been shown to be an attractive alternative to compress natural language text databases , because of the combination of speed , effectiveness , and direct searchability they offer . in particular , our recently proposed family of dense compression codes has been shown to be superior to the more traditional byte oriented word based huffman codes in most aspects . in this paper , we focus on the problem of transmitting texts among peers that do not share the vocabulary . this is the typical scenario for adaptive compression methods . we design adaptive variants of our semistatic dense codes , showing that they are much simpler and faster than dynamic huffman codes and reach almost the same compression effectiveness . we show that our variants have a very compelling trade off between compression decompression speed , compression ratio , and search speed compared with most of the state of the art general compressors . copyright ( c ) <digit> john wiley sons , ltd .
organisational computer supported collaborative learning the affect of context . <eos> the purpose of this introduction is to provide a brief overview of the articles in this special issue and also a framework for understanding , designing and evaluating strategies for co operative learning in the workplace and in educational environments . the special edition is divided into two partsissue <digit> computer supported collaborative learning in formal education , and issue <digit> computer supported team and organisational learning in workplaces . in general , issue <digit> focuses on collaborative learning in primary and secondary schools and in the university setting . issue <digit> is meant to focus on learning in complex and often highly stressful work situations which mostly require intensive communication in groups or teams and in each case allow for learning in the wider organisation . this introduction outlines a set of themes that can be found in the following papers and traces briefly how each paper fits within each discussion .
estimation of the proportion ratio under a simple crossover trial . <eos> the proportion ratio ( pr ) of patient response is one of the most commonly used indices for measuring the relative treatment effect in a randomized clinical trial ( rct ) . assuming a random effect multiplicative risk model , we develop two point estimators and three interval estimators in closed forms for the pr under a simple crossover rct . on the basis of monte carlo simulation , we evaluate the performance of these estimators in a variety of situations . we note that the point estimator using a ratio of two arithmetic means of patient response probabilities over the two groups ( distinguished by the order of treatment received sequences ) is generally preferable to the corresponding one using a ratio of two geometric means of patient response probabilities . we note that the three interval estimators developed in this paper can actually perform well with respect to the coverage probability when the number of patients per group is moderate or large . we further note that the interval estimator based on the ratio of two arithmetic means of patient response probabilities with the logarithmic transformation is probably the best among the three interval estimators discussed here . we use a simple crossover trial studying the suitability of two new inhalation devices for patients who were using a standard inhaler device delivering salbutamol published elsewhere to illustrate the use of these estimators .
pattern identification in biogeography . <eos> identifying common patterns among area cladograms that arise in historical biogeography is an important tool for biogeographical inference . we develop the first rigorous formalization of these pattern identification problems . we develop metrics to compare area cladograms . we define the maximum agreement area cladogram ( maac ) and we develop efficient algorithms for finding the maac of two area cladograms , while showing that it is np hard to find the maac of several binary area cladograms . we also describe a linear time algorithm to identify if two area cladograms are identical .
ead and pebd two energy aware duplication scheduling algorithms for parallel tasks on homogeneous clusters . <eos> high performance clusters have been widely deployed to solve challenging and rigorous scientific and engineering tasks . on one hand , high performance is certainly an important consideration in designing clusters to run parallel applications . on the other hand , the ever increasing energy cost requires us to effectively conserve energy in clusters . to achieve the goal of optimizing both performance and energy efficiency in clusters , in this paper , we propose two energy efficient duplication based scheduling algorithms energy aware duplication ( ead ) scheduling and performance energy balanced duplication ( pebd ) scheduling . existing duplication based scheduling algorithms replicate all possible tasks to shorten schedule length without reducing energy consumption caused by duplication . our algorithms , in contrast , strive to balance schedule lengths and energy savings by judiciously replicating predecessors of a task if the duplication can aid in performance without degrading energy efficiency . to illustrate the effectiveness of ead and pebd , we compare them with a nonduplication algorithm , a traditional duplication based algorithm , and the dynamic voltage scaling ( dvs ) algorithm . extensive experimental results using both synthetic benchmarks and real world applications demonstrate that our algorithms can effectively save energy with marginal performance degradation .
design of k means clustering based polynomial radial basis function neural networks ( prbf nns ) realized with the aid of particle swarm optimization and differential evolution . <eos> in this paper , we introduce an advanced architecture of k means clustering based polynomial radial basis function neural networks ( p rbf nns ) designed with the aid of particle swarm optimization ( pso ) and differential evolution ( de ) and develop a comprehensive design methodology supporting their construction . the architecture of the p rbf nns comes as a result of a synergistic usage of the evolutionary optimization driven hybrid tools . the connections ( weights ) of the proposed p rbf nns being of a certain functional character and are realized by considering four types of polynomials . in order to design the optimized p rbf nns , a prototype ( center value ) of each receptive field is determined by running the k means clustering algorithm and then a prototype and a spread of the corresponding receptive field are further optimized through running particle swarm optimization ( pso ) and differential evolution ( de ) . the weighted least square estimation ( wlse ) is used to estimate the coefficients of the polynomials ( which serve as functional connections of the network ) . the performance of the proposed model and the comparative analysis involving models designed with the aid of pso and de are presented in case of a nonlinear function and two machine learning ( ml ) datasets
computers that recognise and respond to user emotion theoretical and practical implications . <eos> prototypes of interactive computer systems have been built that can begin to detect and label aspects of human emotional expression , and that respond to users experiencing frustration and other negative emotions with emotionally supportive interactions , demonstrating components of human skills such as active listening , empathy , and sympathy . these working systems support the prediction that a computer can begin to undo some of the negative feelings it causes by helping a user manage his or her emotional state . this paper clarifies the philosophy of this new approach to human computer interaction deliberately recognising and responding to an individual user 's emotions in ways , that help users meet their needs . we define user needs in a broader perspective than has been hitherto discussed in the hci community , to include emotional and social needs , and examine technology 's emerging capability to address and support such needs . we raise and discuss potential concerns and objections regarding this technology , and describe several opportunities for future work . ( c ) <digit> elsevier science b.v. all rights reserved .
on visualization techniques for solar data mining . <eos> large scale data mining is often aided with graphic visualizations to facilitate a better understanding of the data and results . this is especially true for visual data and highly detailed data too complex to be easily understood in raw forms . in this work , we present several of our recent interdisciplinary works in data mining solar image repositories and discuss the over arching need for effective visualizations of data , metadata , and results along the way . first , we explain the complex characteristics and overwhelming abundance of image data being produced by nasas solar dynamics observatory ( sdo ) . then we discuss the wide scope of solar data mining and highlight visual results from work in data labeling , classification , and clustering . lastly , we present an overview of the first ever content based image retrieval ( cbir ) system for solar images , and conclude with a brief look at the direction of our future research .
defeasible contextual reasoning with arguments in ambient intelligence . <eos> the imperfect nature of context in ambient intelligence environments and the special characteristics of the entities that possess and share the available context information render contextual reasoning a very challenging task . the accomplishment of this task requires formal models that handle the involved entities as autonomous logic based agents and provide methods for handling the imperfect and distributed nature of context . this paper proposes a solution based on the multi context systems paradigm in which local context knowledge of ambient agents is encoded in rule theories ( contexts ) , and information flow between agents is achieved through mapping rules that associate concepts used by different contexts . to handle imperfect context , we extend multi context systems with nonmonotonic features local defeasible theories , defeasible mapping rules , and a preference ordering on the system contexts . on top of this model , we have developed an argumentation framework that exploits context and preference information to resolve potential conflicts caused by the interaction of ambient agents through the mappings , and a distributed algorithm for query evaluation .
nonextensive information theoretic kernels on measures . <eos> positive definite kernels on probability measures have been recently applied to classification problems involving text , images , and other types of structured data . some of these kernels are related to classic information theoretic quantities , such as ( shannon 's ) mutual information and the jensen shannon ( js ) divergence . meanwhile , there have been recent advances in nonextensive generalizations of shannon 's information theory . this paper bridges these two trends by introducing nonextensive information theoretic kernels on probability measures , based on new js type divergences . these new divergences result from extending the the two building blocks of the classical js divergence convexity and shannon 's entropy . the notion of convexity is extended to the wider concept of q convexity , for which we prove a jensen q inequality . based on this inequality , we introduce jensen tsallis ( jt ) q differences , a nonextensive generalization of the js divergence , and define a k th order jt q difference between stochastic processes . we then define a new family of nonextensive mutual information kernels , which allow weights to be assigned to their arguments , and which includes the boolean , js , and linear kernels as particular cases . nonextensive string kernels are also defined that generalize the p spectrum kernel . we illustrate the performance of these kernels on text categorization tasks , in which documents are modeled both as bags of words and as sequences of characters .
point multiplication on ordinary elliptic curves over fields of characteristic three . <eos> in this paper we investigate the efficiency of cryptosystems based on ordinary elliptic curves over fields of characteristic three . we look at different representations for curves and consider some of the algorithms necessary to perform efficient point multiplication . we give example timings for our operations and compare them with timings for curves in characteristic two of a similar level of security . we show that using the hessian form in characteristic three produces a point multiplication algorithm under <digit> percent slower than the equivalent system in characteristic two . thus it is conceivable that curves in characteristic three , could offer greater performance than currently perceived by the community .
determination of the heat transfer coefficient during solidification of alloys . <eos> we consider a three phase inverse stefan problem . such a problem consists in a reconstruction of the function describing the coefficient of heat transfer , when the positions of the moving solid and liquid interfaces are well known . we introduce three partial problems for each phase ( liquid , solid and mushy ) separately . the solutions of these problems are used for the determination of the unknown heat transfer coefficient . the missing data for a mushy ( solid ) phase are computed from over determined data at the moving liquid ( solid ) interface taking into account the transmission condition . at the end we present numerical calculations in one dimension using piecewise linear continuous finite elements in order to demonstrate the efficiency of the designed numerical algorithm .
simulation of land use spatial pattern of towns and villages based on ca markov model . <eos> firstly , this paper analyzes the basic principles and processes of the spatial pattern changes of land use in towns and villages , and the result shows that the land resource demands of urban development and population growth lead to the spatial pattern changes . secondly , in order to grip land use changes better , the paper proposes a method for the simulation of spatial patterns . the simulating method can be divided into two parts one is a quantitative forecast by using the markov model , and the other is simulating the spatial pattern changes by using the ca model . the above two models construct the simulative model of the spatial pattern of land use in towns and villages . finally , selecting fangshan which is a district of beijing as the experimental area , both the quantity and spatial pattern changing characteristics are investigated through building a changing dataset of land use by using spatial analysis methods based on the land use data in <digit> , <digit> and <digit> ca markov is used to simulate the spatial pattern of land use in fangshan for <digit> . ( c ) <digit> elsevier ltd. all rights reserved .
observations of ipv6 traffic on a 6to4 relay . <eos> funet has been operating a public , globally used 6to4 ( rfc <digit> ) relay router since november <digit> . the traffic has been logged and is now analyzed to gather information of 6to4 and ipv6 deployment . among other figures , we note that the number of 6to4 capable nodes has increased by an order of magnitude in half a year in april <digit> , there are records of about <digit> million different 6to4 nodes using this particular relay . vast majority of this is just testing the availability of the relay , done by the microsoft windows systems , but the real traffic has also increased over time . while the observed 6to4 traffic has typically consisted of relatively simple system level applications , or applications by power users , the emergence of peer to peer applications such as bittorrent was also observed .
modelling and control of the motion of a riderless bicycle rolling on a moving plane . <eos> this work deals with the modelling and control of a riderless bicycle rolling on a moving plane . it is assumed here that the bicycle is controlled by a pedalling torque , a directional torque and by a rotor mounted on the crossbar that generates a tilting torque . in particular , a kinematic model of the bicycles motion is derived by using its dynamic model . then , using this kinematic model , the expressions for the applied torques are obtained .
simplified models of neocortical pyramidal cells preserving somatodendritic voltage attenuation . <eos> simplified models are needed for performing large scale network simulations involving thousands of cells . ideally , these models should be as simple as possible , but still capture important electrotonic properties , such as voltage attenuation . here , we propose a method to design simplified models with correct voltage attenuation , based on camera lucida reconstructions of neurons . the simplified model geometry is fit to the detailed model such that it preserves ( i ) total membrane area , ( ii ) input resistance , ( iii ) time constant and ( iv ) voltage attenuation for current injection in the soma . using the three dimensional reconstruction of a layer vi pyramidal cell , we show that this procedure leads to an efficient simplified model which preserves voltage attenuation for somatic current injection as well as for distributed synaptic inputs in dendrites . attenuation was also correctly captured in the presence of synaptic background activity . these simplified models should be useful for performing network simulations of neurons with electrotonic properties consistent with detailed morphologies .
hybrid force assisted <digit> d assembly of helical nanobelts . <eos> we present a new manipulation system for hybrid force assisted assembly . we propose a hybrid force assisted assembly process for <digit> d helical nanobelts . helical nanobelt tweezer and sensing probe have been assembled using the proposed method .
a stochastic optimization approach for roundness measurements . <eos> in this paper , we develop a vision based inspection system for roundness measurements . a stochastic optimization approach has been proposed to compute the reference circles of mic ( maximum inscribing circle ) , mcc ( minimum circumscribing circle ) and mzc ( minimum zone circle ) methods . the proposed algorithm is a hybrid optimization method based on simulated annealing and hookejeeves pattern search . from the experimental results , it is noted that the algorithm can solve the roundness assessment problems effectively and efficiently . the developed vision based inspection system can be an on line tool for the measurement of circular components .
asymptotic solvers for second order differential equation systems with multiple frequencies . <eos> in this paper , an asymptotic expansion is constructed to solve second order differential equation systems with highly oscillatory forcing terms involving multiple frequencies . an asymptotic expansion is derived in inverse of powers of the oscillatory parameter and its truncation results in a very effective method of dicretizing the differential equation system in question . numerical experiments illustrate the effectiveness of the asymptotic method in contrast to the standard rungekutta method .
power rate allocation in ds cdma systems based on discretized verhulst equilibrium . <eos> this paper proposes to extend the discrete verhulst power equilibrium approach , previously suggested in <digit> , to the power rate optimal allocation problem . multirate users associated to different types of traffic are aggregated to distinct user classes , with the assurance of minimum rate allocation per user and qos . herein , verhulst power allocation algorithm of <digit> was adapted to the ds cdma jointly power rate control problem . the analysis was carried out taking into account static and dynamic channels , as well as the convergence time ( number of iterations ) , quality of solution , in terms of the normalized mean squared error ( nse ) , when compared to the analytical solution based on interference matrix inversion , and the solution given by classical foschini algorithm <digit> as well , besides the computational complexity analysis .
a relational approach to probabilistic classification in a transductive setting . <eos> transduction is an inference mechanism adopted from several classification algorithms capable of exploiting both labeled and unlabeled data and making the prediction for the given set of unlabeled data only . several transductive learning methods have been proposed in the literature to learn transductive classifiers from examples represented as rows of a classical double entry table ( or relational table ) . in this work we consider the case of examples represented as a set of multiple tables of a relational database and we propose a new relational classification algorithm , named transc , that works in a transductive setting and employs a probabilistic approach to classification . knowledge on the data model , i.e. , foreign keys , is used to guide the search process . the transductive learning strategy iterates on a k nn based re classification of labeled and unlabeled examples , in order to identify borderline examples , and uses the relational probabilistic classifier mr sbc to bootstrap the transductive algorithm . experimental results confirm that transc outperforms its inductive counterpart ( mr sbc ) .
exploring the risk factors of preterm birth using data mining . <eos> preterm birth is the leading cause of perinatal morbidity and mortality , but a precise mechanism is still unknown . hence , the goal of this study is to explore the risk factors of preterm using data mining with neural network and decision tree c5 .0 . the original medical data were collected from a prospective pregnancy cohort by a professional research group in national taiwan university . using the nest case control study design , a total of <digit> motherchild dyads were recruited from 14,551 in the original data . thousands of variables are examined in this data including basic characteristics , medical history , environment , and occupation factors of parents , and variables related to infants . the results indicate that multiple birth , hemorrhage during pregnancy , age , disease , previous preterm history , body weight before pregnancy and height of pregnant women , and paternal life style risk factors related to drinking and smoking are the important risk factors of preterm birth . hence , the findings of our study will be useful for parents , medical staff , and public health workers in attempting to detect high risk pregnant women and provide intervention early to reduce and prevent preterm birth .
interval based analysis in embedded system design . <eos> complex multi processor systems on chip and distributed embedded systems exhibit a confusing variety of run time interdependencies . for reliable timing validation , not only application , but also architecture , scheduling and communication properties have to be considered . this is very different from functional validation , where architecture , scheduling and communication can be idealized . to avoid unknown corner case coverage in simulation based validation on one had , and the state space explosion or over simplification of unified formal performance models on the other , we take a compositional approach and combine different efficient models and methods for timing analysis of single processes , real time operating system ( rtos ) overhead , single processors and communication components , and finally multiple connected components . as a result , timing analysis of complex , heterogeneous embedded systems becomes feasible . ( c ) <digit> published by elsevier b.v. on behalf of imacs .
sequential verification of serializability . <eos> serializability is a commonly used correctness condition in concurrent programming . when a concurrent module is serializable , certain other properties of the module can be verified by considering only its sequential executions . in many cases , concurrent modules guarantee serializability by using standard locking protocols , such as tree locking or two phase locking . unfortunately , according to the existing literature , verifying that a concurrent module adheres to these protocols requires considering concurrent interleavings . in this paper , we show that adherence to a large class of locking protocols ( including tree locking and two phase locking ) can be verified by considering only sequential executions . the main consequence of our results is that in many cases , the ( manual or automatic ) verification of serializability can itself be done using sequential reasoning .
diversifying search results of controversial queries . <eos> diversifying search results of queries seeking for different view points about controversial topics is key to improving satisfaction of users . the challenge for finding different opinions is how to maximize the number of discussed arguments without being biased against specific sentiments . this paper addresses the issue by first introducing a new model that represents the patterns occurring in documents about controversial topics . second , proposing an opinion diversification model that uses ( <digit> ) relevance of documents , ( <digit> ) semantic diversification to capture different arguments and ( <digit> ) sentiment diversification to identify positive , negative and neutral sentiments about the query topic . we have conducted our experiments using queries on various controversial topics and applied our diversification model on the set of documents returned by google search engine . the results show that our model outperforms the native ranking of web pages about controversial topics by a significant margin .
memetic computation based on regulation between neural and immune systems the framework and a case study . <eos> lamarckian learning has been introduced into evolutionary computation to enhance the ability of local search . the relevant research topic , memetic computation , has received significant amount of interest . in this study , a novel memetic computational framework is proposed by simulating the integrated regulation between neural and immune systems . the lamarckian learning strategy of simulating the unidirectional regulation of neural system on immune system is designed . consequently , an immune memetic algorithm based on the lamarckian learning is proposed for numerical optimization . the proposed algorithm combines the advantages of immune algorithms and mathematical programming , and performs well in both global and local search . the simulation results based on ten low dimensional and ten high dimensional benchmark problems show that the immune memetic algorithm outperforms the basic genetic algorithm based memetic algorithm in solving most of the test problems .
optimal design of flywheels using an injection island genetic algorithm . <eos> this paper presents an approach to optimal design of elastic flywheels using an injection island genetic algorithm ( iiga ) , summarizing a sequence of results reported in earlier publications . an iiga in combination with a structural finite element code is used to search for shape variations and material placement to optimize the specific energy density ( sed , rotational energy per unit weight ) of elastic flywheels while controlling the failure angular velocity . iigas seek solutions simultaneously at different levels of refinement of the problem representation ( and correspondingly different definitions of the fitness function ) in separate subpopulations ( islands ) . solutions are sought first at low levels of refinement with an axi symmetric plane stress finite element code for high speed exploration of the coarse design space . next , individuals are injected into populations with a higher level of resolution that use an axi symmetric three dimensional finite element code to fine tune the structures . a greatly simplified design space ( containing two million possible solutions ) was enumerated for comparison with various approaches that include simple gas , threshold accepting ( ta ) , iigas and hybrid iigas . for all approaches compared for this simplified problem , all variations of the iiga were found to be the most efficient . this paper will summarize results obtained studying a constrained optimization problem with a huge design space approached with parallel gas that had various topological structures and several different types of iiga , to compare efficiency . for this problem , all variations of the iiga were found to be extremely efficient in terms of computational time required to final solution of similar fitness when compared to the parallel gas .
paving the way for crm success the mediating role of knowledge management and organizational commitment . <eos> data from an international sample of <digit> hotels ( from uk and spain ) is analyzed . the process through which crm technology translates into organizational performance is described . a crm technology , when properly implemented , shows a positive effect on performance . knowledge management and organizational commitment acted as relevant mediators . organizational commitment probed to be the main determinant of crm success .
susy_flavorv2 a computational tool for fcnc and cp violating processes in the mssm . <eos> we present susy_flavor version <digit> a fortran <digit> program that calculates low energy flavor observables in the general r r parity conserving mssm . for a set of mssm parameters as input , the code gives predictions for electric dipole moments of the leptons and the neutron . anomalous magnetic moments ( i.e.g <digit> g <digit> ) of the leptons . radiative lepton decays ( e e and , e , e ) . rare kaon decays ( k l <digit> <digit> and k ) . leptonic b b decays ( bs , d l l b s , d l l , b b and b d b d ) . radiative b b decays ( b x s ) . f <digit> f <digit> processes ( k <digit> k0 k <digit> , d d d , b d bd b d and b s bs b s mixing ) . program title susy_flavorv2 catalogue identifier aegv_v2_0 program summary url http cpc.cs.qub.ac.uk summaries aegv_v2_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions standard cpc licence , http cpc.cs.qub.ac.uk licence licence.html no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language fortran <digit> . computer any . operating system any , tested on linux . classification 11.6 . does the new version supersede the previous version yes catalogue identifier of previous version aegv_v1_0 journal reference of previous version comput . phys . comm . <digit> ( <digit> ) <digit> nature of problem predicting cp violating observables , meson mixing parameters and branching ratios for set of rare processes in the general r parity conserving mssm . solution method we use standard quantum theoretical methods to calculate wilson coefficients in mssm and at one loop including qcd corrections at higher orders when this is necessary and possible . the input parameters can be read from an external file in slha format . reasons for new version a major rewrite of the internal code structure to accommodate higher order corrections new observables added . summary of revisions susy_flavor v2 .0 is able to perform resummation of chirally enhanced corrections to all orders of perturbation expansion ( v1 .0 included <digit> loop terms only ) . routines calculating new observables are added g <digit> lepton magnetic moment anomaly , to e e and to e , e , decays , b b to d d decays , b b to e , e , e , e , decays . parameter initialization in the sfermion sector is simplified and follows , by default , the slha2 conventions . running time for a single parameter set approximately 1s in double precision on a powerbook mac g4 .
super resolution iris image restoration based on multiple mlps and cls filter . <eos> iris recognition is a biometric technology which shows a very high level of recognition accuracy , but low resolution ( lr ) iris images cause the degradation of the recognition performance . therefore , a zoom lens with a long focal length is used in an iris camera . however , a bulky and costly zoom lens whose focal length is longer than <digit> mm is required for capturing the iris image at a distance , which can increase the size and cost of the system . in order to overcome this problem , we propose a new super resolution method which restores a single lr iris image into a high resolution ( hr ) iris image . our research is novel in the following three ways compared to previous works . first , in order to prevent the loss of the middle and high frequency components of the iris patterns in the original image , the lr iris image is up sampled using multiple multi layered perceptrons ( mlps ) . second , a point spread function ( psf ) and a constrained least square ( cls ) filter are used to remove sensor blurring in the up sampled image . third , the optimal parameters of the cls filter and psf in terms of the recognition accuracy are determined according to the zoom factor of the lr image . the experimental results show that the accuracy of iris recognition with the hr images restored by the proposed method is much enhanced compared to the three previous methods .
a language independent and formal approach to pattern based modelling with support for composition and analysis . <eos> patterns are used in different disciplines as a way to record expert knowledge for problem solving in specific areas . their systematic use in software engineering promotes quality , standardization , reusability and maintainability of software artefacts . the full realisation of their power is however hindered by the lack of a standard formalization of the notion of pattern . our goal is to provide a language independent formalization of the notion of pattern , so that it allows its application to different modelling languages and tools , as well as generic methods to enable pattern discovery , instantiation , composition , and conflict analysis . for this purpose , we present a new visual and formal , language independent approach to the specification of patterns . the approach is formulated in a general way , based on graphs and category theory , and allows the specification of patterns in terms of ( nested ) variable submodels , constraints on their allowed variance , and inter pattern synchronization across several diagrams ( e.g. class and sequence diagrams for uml design patterns ) . we provide a formal notion of pattern satisfaction by models and propose mechanisms to suggest model transformations so that models become consistent with the patterns . we define methods for pattern composition , and conflict analysis . we illustrate our proposal on uml design patterns , and discuss its generality and applicability on different types of patterns , e.g. workflow patterns , enterprise integration patterns and interaction patterns . the approach has proven to be powerful enough to formalize patterns from different domains , providing methods to analyse conflicts and dependencies that usually are expressed only in textual form . its language independence makes it suitable for integration in meta modelling tools and for use in model driven engineering .
early mover advantage in e commerce platforms with low entry barriers the role of customer relationship management capabilities . <eos> this research investigates whether early mover advantage ( ema ) exists among entrepreneurial e tailers operating on third party e commerce platforms . contrary to traditional wisdom , the current research hypothesizes that e tailers may enjoy early mover advantages because of the consumer demand inertia amplified by the nature of the internet and the system design characteristics of e commerce platforms . we also argue that customer relationship management capabilities help enhance early mover advantages in an online setting . we employ panel data on <digit> e tailers to perform analyses and find empirical evidence that strongly supports the abovementioned hypotheses .
contextualized monitoring and root cause discovery in iptv systems using data visualization . <eos> this article describes the architecture and design of an iptv network monitoring system and some of the use cases it enables . the system is based on distributed agents within iptv terminal equipment ( set top box ) , which collect and send the data to a server where it is analyzed and visualized . in the article we explore how large amounts of collected data can be utilized for monitoring the quality of service and user experience in real time , as well as for discovering trends and anomalies over longer periods of time . furthermore , the data can be enriched using external data sources , providing a deeper understanding of the system by discovering correlations with events outside of the monitored domain . four supported use cases are described , among them using weather information for explaining away the iptv quality degradation . the system has been successfully deployed and is in operation at the slovenian iptv provider telekom slovenije .
mean field and propagation of chaos in multi class heterogeneous loss models . <eos> we consider a system consisting of n n parallel servers , where jobs with different resource requirements arrive and are assigned to the servers for processing . each server has a finite resource capacity and therefore can serve only a finite number of jobs at a time . we assume that different servers have different resource capacities . a job is accepted for processing only if the resource requested by the job is available at the server to which it is assigned . otherwise , the job is discarded or blocked . we consider randomized schemes to assign jobs to servers with the aim of reducing the average blocking probability of jobs in the system . in particular , we consider a scheme that assigns an incoming job to the server having maximum available vacancy or unused resource among d d randomly sampled servers . we consider the system in the limit where both the number of servers and the arrival rates of jobs are scaled by a large factor . this gives rise to a mean field analysis . we show that in the limiting system the servers behave independentlya property termed as propagation of chaos . stationary tail probabilities of server occupancies are obtained from the stationary solution of the mean field which is shown to be unique and globally attractive . we further characterize the rate of decay of the stationary tail probabilities . numerical results suggest that the proposed scheme significantly reduces the average blocking probability of jobs as compared to static schemes that probabilistically route jobs to servers independently of their states .
conceptual indexing and active retrieval of video for interactive learning environments . <eos> selecting an instructive story from a video case base is an information retrieval problem , but standard indexing and retrieval techniques <digit> were not developed with such applications in mind . the classical model assumes a passive retrieval system queried by interested and well informed users . in educational situations , students can not be expected to form appropriate queries or to identify their own ignorance . systems that teach must , therefore , be active retrievers that formulate their own retrieval cues and reason about the appropriateness of intervention . the story producer for interactive learning ( spiel ) is an active retrieval system for recalling stories to tell to students who are learning social skills in a simulated environment 2and3 . spiel is a component of the guided social simulation ( guss ) architecture <digit> used to build yello , a program that teaches account executives the fine points of selling yellow pages advertising . spiel uses structured , conceptual indices derived from research in case based reasoning 5and6 . spiel 's manually created indices are detailed representations of what stories are about , and they are needed to make precise assessments of stories ' relevance . spiel 's opportunistic retrieval architecture operates in two phases . during the storage phase , the system uses its educational knowledge encapsulated in a library of storytelling strategies to determine , for each story , what an opportunity to tell that story would look like . during the retrieval phase , the system tries to recognize those opportunities while the student interacts with the simulation . this design is similar to opportunistic memory architectures proposed for opportunistic planning 7and8 .
when agile meets the enterprise . <eos> we present challenges of using agile practices in traditional enterprise environments . we organize the challenges under two factors . for both factors , we identify successful mitigation strategies .
presenting a new multiclass classifier based on learning automata . <eos> among the various traditional approaches of pattern recognition , the statistical approach has been most intensively studied and used in practice . this paper presents a new classifier called mlac for multiclass classification based on the learning automata . the proposed classifier using a soft decision method could find the optimal hyperplanes in solution space and separate available classes from each other well . we have tested the mlac classifier on some multiclass datasets including iris , wine and glass .1 the results show a significant improvement in comparison with the previous learning automata based classifiers as it has more accuracy and lower running time . also , in order to evaluate performance of the proposed mlac classifier , it has been compared with conventional classifiers such as k nearest neighbor , multilayer perceptron , genetic classifier and particle swarm classifier on these datasets in terms of accuracy . the obtained results show that the proposed mlac classifier not only improves the classification 's accuracy , but also reduces time complexity .
secured communication protocol for internetworking zigbee cluster networks . <eos> zigbee uses the network security and application profile layers of the ieee 802.15.4 and zigbee alliance standards for reliable , low powered , wireless data communications . however , the zigbee has problems of being less secure , and has a difficulty in distributing shared symmetric keys between each pair of nodes . in addition , the zigbee protocol is inadequate for large sensor networks , which may consist of several very large scale clusters . in this paper , we first construct a secure zigbee scheme for realistic scenarios consisting of a large network with several clusters containing coordinators and numerous devices . we present a new key management protocol for zigeee networks , which can be used among participants of different clusters and analyze its performance . ( c ) <digit> elsevier b.v. all rights reserved .
hp finite element simulations for stokes flow stable and stabilized . <eos> the stable galerkin formulation and a stabilized galerkin least squares formulation for the stokes problem are analyzed in the context of the hp version of the finite element method . theoretical results for both formulations establish exponential rates of convergence under realistic assumptions on the input data . we confirm these results by a series of numerical experiments on an l shaped domain where the solution exhibits corner singularities .
joint optimization of complexity and overhead for the routing in hierarchical networks . <eos> the hierarchical network structure was proposed in the early 80s and becomes popular nowadays . the routing complexity and the routing table size are the two primary performance measures in a dynamic route guidance system . although various algorithms exist for finding the best routing policy in a hierarchical network , hardly exists any work in studying and evaluating the aforementioned measures for a hierarchical network . in this paper , a new mathematical framework to carry out the averages of the routing complexity and the routing table size is proposed to express the routing complexity and the routing table size as the functions of the hierarchical network parameters such as the number of the hierarchical levels and the subscriber density ( cluster population ) for each hierarchical level .
the solitary wave solution of coupled kleingordonzakharov equations via two different numerical methods . <eos> in this research , we propose two different methods to solve the coupled kleingordonzakharov ( kgz ) equations the differential quadrature ( dq ) and globally radial basis functions ( grbfs ) methods . in the dq method , the derivative value of a function with respect to a point is directly approximated by a linear combination of all functional values in the global domain . the principal work in this method is the determination of weight coefficients . we use two ways for obtaining these coefficients cosine expansion ( cdq ) and radial basis functions ( rbfs dq ) , the former is a mesh based method and the latter categorizes in the set of meshless methods . unlike the dq method , the grbf method directly substitutes the expression of the function approximation by rbfs into the partial differential equation . the main problem in the grbfs method is ill conditioning of the interpolation matrix . avoiding this problem , we study the bases introduced in pazouki and schaback ( <digit> ) <digit> . some examples are presented to compare the accuracy and easy implementation of the proposed methods . in numerical examples , we concentrate on inverse multiquadric ( imq ) and second order thin plate spline ( tps ) radial basis functions . the variable shape parameter ( exponentially and random ) strategies are applied in the imq function and the results are compared with the constant shape parameter .
on tests for global maximum of the log likelihood function . <eos> given the location of a relative maximum of the log likelihood function , how to assess whether it is the global maximum this paper investigates an existing statistical tool , which , based on asymptotic analysis , answers this question by posing it as a hypothesis testing problem . a general framework for constructing tests for global maximum is given . the characteristics of the tests are investigated for two cases correctly specified model and model mismatch . a finite sample approximation to the power is given , which gives a tool for performance prediction and a measure for comparison between tests . the sensitivity of the tests to model mismatch is analyzed in terms of the renyi divergence and the kullback leibler divergence between the true underlying distribution and the assumed parametric class and tests that are insensitive to small deviations from the model are derived thereby overcoming a fundamental weakness of existing tests . the tests are illustrated for three applications passive localization or direction finding using an array of sensors , estimating the parameters of a gaussian mixture model , and estimation of superimposed exponentials in noise problems that are known to suffer from local maxima .
using state equation to prove non reachability in timed petrinets . <eos> non reachability proofs in timed petrinets were usually done by proving the non reachability within the underlying timeless net . however , in many cases this approach fails . in this paper , we present an approach to prove non reach ability within the actual timed petrinet . for this purpose , we introduce a state equation for timed petrinets in analogy to timeless nets . using this state equation , we can express reachability as a system of equations and inequations , which is solvable in polynomial time .
microglial nadph oxidase mediates leucine enkephalin dopaminergic neuroprotection . <eos> abstract here , we report that leucine enkephalin ( le ) is neuroprotective to dopaminergic ( da ) neurons at femtomolar concentrations through anti inflammatory properties . mesencephalic neuron glia cultures pretreated with femtomolar concentrations of le ( <digit> <digit> <digit> <digit> m ) protected da neurons from lipopolysaccharide ( lps ) induced da neurotoxicity , as determined by da uptake assay and tyrosine hydroxylase ( th ) immunocytochemistry ( icc ) . however , des tyrosine leucine enkephalin ( dtle ) , an le analogue that is missing the tyrosine residue required for binding to the kappa opioid receptor , was also neuroprotective ( <digit> <digit> <digit> <digit> m ) , as determined by da uptake assay and th icc . both le and dtle ( <digit> <digit> <digit> <digit> m ) reduced lps induced superoxide production from microglia enriched cultures . further , both le and dtle ( <digit> <digit> , <digit> <digit> m ) reduced the lps induced tumor necrosis factor alpha ( tnf ) mrna and tnf protein from phox microglia , as determined by quantitative real time rt pcr and elisa analysis in mesencephalic neuron glia cultures , respectively . however , both peptides failed to inhibit tnf expression in phox cultures , which are unable to produce extracellular superoxide in response to lps . additionally , le and dtle ( <digit> <digit> , <digit> <digit> m ) failed to show any neuroprotection against lps in phox cultures . together , these data indicate that le and dtle are neuroprotective at femtomolar concentrations through the inhibition of oxidative insult associated with microglial nadph oxidase and the attenuation of the ros mediated amplification of tnf gene expression in microglia .
query rewriting for swift ( first ) answers . <eos> traditionally , the answer to a database query is construed as the set of all tuples that meet the criteria stated . strict adherence to this notion in query evaluation is , however , increasingly unsatisfactory because decision makers are more prone to adopting an exploratory strategy for information search which we call getting some answers quickly , and perhaps more later . from a decision maker 's perspective , such a strategy is optimal for coping with information overload and makes economic sense ( when used in conjunction with a micropayment mechanism ) . these new requirements present new opportunities for database query optimization . in this paper , we propose a progressive query processing strategy that exploits this behavior to conserve system resources and to minimize query response time and user waiting time . this is accomplished by the heuristic decomposition of user queries into subqueries that can be evaluated on demand . to illustrate the practicality of the proposed methods , we describe the architecture of a prototype system that provides a nonintrusive implementation of our approach . finally , we present experimental results obtained from an empirical study conducted using an oracle server that demonstrate the benefits of the progressive query processing strategy .
ramsey games with giants . <eos> the classical result in the theory of random graphs , proved by erd . os and renyi in <digit> , concerns the threshold for the appearance of the giant component in the random graph process . we consider a variant of this problem , with a ramsey flavor . now , each random edge that arrives in a sequence of rounds must be colored with one of r colors . the goal can be either to create a giant component in every color class , or alternatively , to avoid it in every color . one can analyze the offline or online setting for this problem . in this paper , we consider all these variants and provide nontrivial upper and lower bounds in certain cases ( like online avoidance ) the obtained bounds are asymptotically tight . ( c ) <digit> wiley periodicals , inc. random struct . alg. , <digit> , <digit> <digit> , <digit>
an adaptive speed sensorless induction motor drive with artificial neural network for stability enhancement . <eos> an artificial neural network ( ann ) based adaptive estimator is presented in this paper for the estimation of rotor speed in a sensorless vector controlled induction motor ( im ) drive . the model reference adaptive system ( mras ) is formed with instantaneous and steady state reactive power . selection of reactive power as the functional candidate in mras automatically makes the system immune to the variation of stator resistance . such adaptive system performs satisfactorily at very low speed . however , it is observed that an unstable region exists in the speed torque domain during regeneration . in this work , ann is applied to overcome such stability related problem . the proposed method is validated through computer simulation using matlab simulink . sample results from a laboratory prototype ( using dspace <digit> ) have confirmed the usefulness of the proposed estimator .
adaptive anisotropic meshing for steady convection dominated problems . <eos> obtaining accurate solutions for convectiondiffusion equations is challenging due to the presence of layers when convection dominates the diffusion . to solve this problem , we design an adaptive meshing algorithm which optimizes the alignment of anisotropic meshes with the numerical solution . three main ingredients are used . first , the streamline upwind petrovgalerkin method is used to produce a stabilized solution . second , an adapted metric tensor is computed from the approximate solution . third , optimized anisotropic meshes are generated from the computed metric tensor by an anisotropic centroidal voronoi tessellation algorithm . our algorithm is tested on a variety of two dimensional examples and the results shows that the algorithm is robust in detecting layers and efficient in avoiding non physical oscillations in the numerical approximation .
cell communication in tissue p systems universality results . <eos> we introduce an evolution communication model for tissue p systems where communication rules are inspired by the general mechanism of cell communication based on signals and receptors a multiset can enter a cell only in the presence of another multiset . some basic variants of this model are also considered where communication is restricted either to be unidirectional or to use special multisets of objects called receptors . the universality for all these variants of tissue p systems is then proved by using two cells ( three cells in the case of unidirectional communication ) and rules of a minimal size .
activity rhythm detection and modeling . <eos> we present an algorithm for detecting and modeling rhythmic temporal patterns in the record of an individual 's computer activity , or online presence . the model is both predictive and descriptive of temporal features and is constructed with minimal a priori knowledge .
energy efficient paths in radio networks . <eos> we consider a radio network consisting of n stations represented as the complete graph on a set of n points in the euclidean plane with edge weights omega ( p , q ) pq ( delta ) c ( p ) , for some constant delta > <digit> and nonnegative offset costs c ( p ) . our goal is to find paths of minimal energy cost between any pair of points that do not use more than some given number k of hops . we present an exact algorithm for the important case when delta <digit> , which requires o ( kn log n ) time per query pair ( p , q ) . for the case of an unrestricted number of hops we describe a family of algorithms with query time o ( n ( <digit> alpha ) ) , where alpha > <digit> can be chosen arbitrarily . if we relax the exactness requirement , we can find an approximate ( <digit> epsilon ) solution in constant time by querying a data structure which has linear size and which can be build in o ( n log n ) time . the dependence on epsilon is polynomial in <digit> epsilon . one tool we employ might be of independent interest for any pair of points ( p , q ) epsilon ( p x p ) we can report in constant time the cluster pair ( a , b ) representing ( p , q ) in a well separated pair decomposition of p.
novelty detection for the inspection of light emitting diodes . <eos> we propose novel feature extraction and classification methods for the automatic visual inspection of manufactured leds . the defects are located at the area of the p electrodes and lead to a malfunction of the led . besides the complexity of the defects , low contrast and strong image noise make this problem very challenging . for the extraction of image characteristic we compute radially encoded features that measure discontinuities along the p electrode . therefore , we propose two different methods the first method divides the object into several radial segments for which mean and standard deviation are computed and the second method computes mean and standard deviation along different orientations . for both methods we combine the features over several segments or orientations by computing simple measures such as the ratio between maximum and mean or standard deviation . since defect free leds are frequent and defective leds are rare , we apply and evaluate different novelty detection methods for classification . therefore , we use a kernel density estimator , kernel principal component analysis , and a one class support vector machine . we further compare our results to pearson 's correlation coefficient , which is evaluated using an artificial reference image . the combination of one class support vector machine and radially encoded segment features yields the best overall performance by far , with a false alarm rate of only 0.13 % at a <digit> % defect detection rate , which means that every defect is detected and only very few defect free p electrodes are rejected . our inspection system does not only show superior performance , but is also computationally efficient and can therefore be applied to further real time applications , for example solder joint inspection . moreover , we believe that novelty detection as used here can be applied to various expert system applications . ( c ) <digit> elsevier ltd. all rights reserved .
finding a tree structure in a resolution proof is np complete . <eos> the resolution tree problem consists of deciding whether a given sequence like resolution refutation admits a tree structure . this paper shows the np completeness of both the resolution tree problem and a natural generalization of the resolution tree problem that does not involve resolution . ( c ) <digit> elsevier b.v. all rights reserved .
a real time simulator for interventional radiology . <eos> interventional radiologists manipulate guidewires and catheters and steer stents through the patient 's vascular system under x ray imaging for treatment of vascular diseases . the complexity of these procedures makes training mandatory in order to master hand eye coordination , instrument manipulation and procedure protocols for each radiologist . in this paper we present a simulator for interventional radiology , which deploys a model of guidewire catheter based on the cosserat theory applied to one dimensional structures . this model starts from the energetic formulation of the flament considering the hook laws of continuum mechanics . the lagrange formulations are used to describe the model deformation . this model takes ( self ) collisions into account and it is revealed to be very efficient for interactive applications . the simulation environment allows to carry out the most common procedures guidewire and catheter navigation , contrast dye injection to visualize the vessels , balloon angioplasty and stent placement . moreover , heartbeat as well as breathing are also simulated visually .
a semantic based probabilistic approach for real time video event recognition . <eos> this paper presents an approach for real time video event recognition that combines the accuracy and descriptive capabilities of , respectively , probabilistic and semantic approaches . based on a state of art knowledge representation , we define a methodology for building recognition strategies from event descriptions that consider the uncertainty of the low level analysis . then , we efficiently organize such strategies for performing the recognition according to the temporal characteristics of events . in particular , we use bayesian networks and probabilistically extended petri nets for recognizing , respectively , simple and complex events . for demonstrating the proposed approach , a framework has been implemented for recognizing humanobject interactions in the video monitoring domain . the experimental results show that our approach improves the event recognition performance as compared to the widely used deterministic approach .
reproductive weak solutions of magneto micropolar fluid equations in exterior domains . <eos> we establish the existence of a reproductive weak solution , a so called periodic weak solution , for the equations of motion of magneto micropolar fluids in exterior domains in r <digit> . ( c ) <digit> elsevier science ltd. all rights reserved .
beam bounding box a novel approach for beam concept modeling and optimization handling . <eos> simplified models of the vehicle structure are often used during the concept phase of vehicle development to improve the noise , vibration and harshness ( nvh ) performance . together with the structural joints and panels , beams are one of the constituent parts of these models . there are different approaches for their modeling and optimization handling , which however are either not able to maintain the similarity with the detailed finite element ( fe ) model ( reference and or optimized ) or suffering some flexibility and performance issues . the objective of the current work is to develop and validate a new method which is an improved alternative of the existing approaches . it keeps the reference cross sectional shapes of all 1d beams , but when each of them is rescaled during optimization , the beam is represented by means of generic cross sectional properties . thus a lighter and simpler representation of the concept beams is created and at the same time the connection with the detailed fe model is not broken . the feasibility of the approach is successfully verified for a set of representative beam cross sections and then for an industrial case study . its benefit in terms of computational time is also demonstrated . the proposed method can be easily implemented and then applied to make concept modeling for the vehicle structure faster and more flexible .
exploring the use of large displays in american megachurches . <eos> within the hci community , there is a growing interest in how technology is used and appropriated outside the workplace . in this paper , we present preliminary findings of how large displays , projection systems , and presentation software are used in american megachurches to support religious practice . these findings are based on ten visits to church services by the study.s authors . we describe how large display technology augments and replaces certain church traditions , and finish by discussing issues related to the design for church environments that are highlighted by this use of technology .
at home 2.0 an educational framework for home based healthcare . <eos> this paper intends to describe some of the primary social and cultural dynamics in south african home based healthcare , using ethnographic case study material and design methodology . this constitutes a detailed narrative of the care experience in a poor community , emphasising the needs of and barriers to educational information , particularly concerning caregivers . in reaction to this context , a collaborative training model at home 2.0 emerges through design intervention . this is foreseen as a basic framework whereby caregivers ( are encouraged to ) develop educational content via information and communication technologies ( e. g. mobile phones and social media ) . such content can and may include experiences , suggestions , and guidelines that are relevant to the practice of caregiving . at home 2.0 will conceptually and in some cases practically demonstrate how educational content may be generated , published , and disseminated in the sphere of home based healthcare .
overcoming limitations of prefetching in multiprocessors by compiler initiated coherence action . <eos> in this paper we first identify limitations of compiler controlled prefetching in a cc numa multiprocessor with a write invalidate cache coherence protocol . compiler controlled prefetch techniques for cc numas often are focused only , on stride accesses , and this introduces a major limitation . we consider combining prefetch with two other compiler controlled techniques to partly remedy the situation ( <digit> ) load exclusive to reduce write latency and ( <digit> ) store update to reduce read latency . the purpose of each of these techniques in a machine with prefetch is to let them reduce latency for accesses which the prefetch technique could not handle . we evaluate two different scenarios , firstly with a hybrid compiler hardware prefetch technique and secondly with an optimal stride prefetcher . we find that the combined gains under the hybrid prefetch technique are significant for six applications we have studied in average , <digit> % of the original write stall time remains after using the hybrid prefetcher , and of these ownership requests , <digit> % would be eliminated using load exclusive in average , <digit> % of the read stall time remains after using the hybrid prefetcher and of these read misses , <digit> % were serviced by remote caches and would be converted by store update into misses serviced by a clean copy in memory which reduces the read latency . with an optimal stride prefetcher our results show that it beneficient to complement prefetch , with the two techniques here as well .
modeling and optimization of non blocking checkpointing for optimistic simulation on myrinet clusters . <eos> checkpointing and communication library ( ccl ) is a recently developed software implementing cpu offloaded checkpointing functionalities in support of optimistic parallel simulation on myrinet clusters . specifically , ccl implements a non blocking execution mode of memory to memory data copy associated with checkpoint operations , based on data transfer capabilities provided by a programmable dma engine on board of myrinet network cards . re synchronization between cpu and dma activities must sometimes be employed for several reasons , such as maintenance of data consistency , thus adding some overhead to ( otherwise cpu cost free ) non blocking checkpoint operations . in this paper we present a cost model for non blocking checkpointing and derive a performance effective re synchronization semantic which we call minimum cost re synchronization mc . with this semantic , an occurrence of re synchronization either commits an on going dma based checkpoint operation ( causing suspension of cpu activities ) or aborts the operation ( with possible increase in the expected rollback cost due to a reduced amount of committed checkpoints ) on the basis of a minimum overhead expectation evaluated through the cost model . we have implemented mc within ccl , and we also report experimental results demonstrating the performance benefits from this optimized re synchronization semantic , in terms of increase in the execution speed , for a personal communication system ( pcs ) simulation application .
efficiency of ip packets pre marking for h264 video quality guarantees in streaming applications . <eos> for the last few years , many classifications and marking strategies have been proposed with the consideration of video streaming applications . according to ietf recommendation , two groups of solutions have been proposed . the firts one assumes that applications or if end points pre mark their packets . the second solution applies the router which is topologically closest to video source . it should perform multifield classification and mark all incomming packets . this paper investigates the most popular marking strategies belonging to both mentioned above groups of solutions . the pre marking strategies based on h264 coder extensions are simulated based on ns <digit> network simulator and evalvid ra framework . the results are compared with the ietf recommendations for video traffic shaping in the ip networks and marking algorithms proposed by other researchers .
on sets of vectors of a finite vector space in which every subset of basis size is a basis ii . <eos> this article contains a proof of the mds conjecture for k a parts per thousand currency sign 2p <digit> . that is , that if s is a set of vectors of in which every subset of s of size k is a basis , where q p ( h ) , p is prime and q is not and k a parts per thousand currency sign 2p <digit> , then s a parts per thousand currency sign q <digit> . it also contains a short proof of the same fact for k a parts per thousand currency sign p , for all q.
group topic model organizing topics into groups . <eos> latent dirichlet allocation defines hidden topics to capture latent semantics in text documents . however , it assumes that all the documents are represented by the same topics , resulting in the forced topic problem . to solve this problem , we developed a group latent dirichlet allocation ( glda ) . glda uses two kinds of topics local topics and global topics . the highly related local topics are organized into groups to describe the local semantics , whereas the global topics are shared by all the documents to describe the background semantics . glda uses variational inference algorithms for both offline and online data . we evaluated the proposed model for topic modeling and document clustering . our experimental results indicated that glda can achieve a competitive performance when compared with state of the art approaches .
co authorship networks in the digital library research community . <eos> the field of digital libraries ( dls ) coalesced in <digit> the first digital library conferences were held that year , awareness of the world wide web was accelerating , and the national science foundation awarded <digit> million ( us ) for the digital library initiative ( dli ) . in this paper we examine the state of the dl domain after a decade of activity by applying social network analysis to the co authorship network of the past acm , ieee , and joint acm ieee digital library conferences . we base our analysis on a common binary undirectional network model to represent the co authorship network , and from it we extract several established network measures . we also introduce a weighted directional network model to represent the co authorship network , for which we define authorrank as an indicator of the impact of an individual author in the network . the results are validated against conference program committee members in the same period . the results show clear advantages of pagerank and authorrank over degree , closeness and betweenness centrality metrics . we also investigate the amount and nature of international participation in joint conference on digital libraries ( jcdl ) .
bipartite matching in the semi streaming model . <eos> we present the first deterministic <digit> epsilon approximation algorithm for finding a large matching in a bipartite graph in the semi streaming model which requires only o ( ( <digit> epsilon ) ( <digit> ) ) passes over the input stream . in this model , the input graph g ( v , e ) is given as a stream of its edges in some arbitrary order , and storage of the algorithm is bounded by o ( npolylog n ) bits , where n v . the only previously known arbitrarily good approximation for general graphs is achieved by the randomized algorithm of mcgregor ( proceedings of the internationalworkshop on approximation algorithms for combinatorial optimization problems and randomization and computation , berkeley , ca , usa , pp. <digit> <digit> , <digit> ) , which uses omega ( ( <digit> epsilon ) <digit> epsilon ) passes . we show that even for bipartite graphs , mcgregor 's algorithm needs omega ( <digit> epsilon ) ( omega ( <digit> epsilon ) ) passes , thus it is necessarily exponential in the approximation parameter . the design as well as the analysis of our algorithm require the introduction of some new techniques . a novelty of our algorithm is a new deterministic assignment of matching edges to augmenting paths which is responsible for the complexity reduction , and gets rid of randomization . we repeatedly grow an initial matching using augmenting paths up to a length of 2k <digit> for k <digit> epsilon . we terminate when the number of augmenting paths found in one iteration falls below a certain threshold also depending on k , that guarantees a <digit> epsilon approximation . the main challenge is to find those augmenting paths without requiring an excessive number of passes . in each iteration , using multiple passes , we grow a set of alternating paths in parallel , considering each edge as a possible extension as it comes along in the stream . backtracking is used on paths that fail to grow any further . crucial are the so called position limits when a matching edge is the ith matching edge in a path and it is then removed by backtracking , it will only be inserted into a path again at a position strictly lesser than i. this rule strikes a balance between terminating quickly on the one hand and giving the procedure enough freedom on the other hand .
correlation analysis of principal components from two populations . <eos> we investigate a correlation coefficient of principal components from two sets of variables . using perturbation expansion , we get a limiting distribution of the correlation . in addition , we obtain a limiting distribution of the fisher 's z transformation of the above correlation . additionally , we verify the accuracy of the limiting distributions using monte carlo simulations . finally in this study , we present two examples and a bootstrap estimation .
modeling by singular value decomposition and the elimination of statistically insignificant coefficients . <eos> numerical advantages of singular value decomposition over other least squares techniques . elimination of statistically insignificant coefficients . benefit of a statistical rejection procedure .
linguistic majorities with difference in support . <eos> a new linguistic aggregation rule that extends numerical majorities based on difference in support is introduced . linguistic majorities with difference in support are formalised for fuzzy set and <digit> tuples . both representations are proved to be mathematically isomorphic . a set of normative properties have been demonstrated to hold for the new linguistic majorities .
an underfrequency load shedding scheme for islanded microgrids . <eos> in this paper an ufls scheme for implementing in mg was proposed . this load shedding method estimates the power deficit based on the frequency first derivative . it considers power generation variations during the load shedding process . the proposed load shedding scheme is independent from mg parameters . a microgrid with several der is adopted to demonstrate the effectiveness of the proposed method .
some new fuzzy entropy formulas . <eos> the purpose of this paper is twofold . firstly , a general conclusion about fuzzy entropy induced by distance measure is presented based on the axiom definitions of fuzzy entropy and distance measure . secondly , some fuzzy entropy formulas which relate to the fuzzy entropy formula defined by de luca and termini ( inform . control <digit> ( <digit> ) <digit> ) are given . ( c ) <digit> elsevier science b.v. all rights reserved .
adhesion properties of polymethylsilsesquioxane based low dielectric constant materials by the modified edge lift off test . <eos> delamination occurring during the chemical and mechanical planarization process or wire bonding steps in packaging is a fundamental issue in integrating of low dielectric constant ( low k ) materials into the multilayer structures of semiconductor chips . since it is known that low adhesion strength is mainly attributed to the failure phenomenon , the measurement of interfacial fracture toughness is critical to provide a quantitative basis in the choice of the materials . in this study , a modified edge lift off test was adopted to measure the fracture toughness of polymethylsilsesquioxane based low k materials with various chemical and physical structures . interfacial fracture toughness was improved by adding multi functional monomers to methylsilsesquioxane monomers or by increasing the percentage of functional end groups inside the prepolymers . in addition , the change in curing conditions and thickness influenced the adhesion performance presumably by changing the morphology of low k materials .
when consensus meets self stabilization . <eos> this paper presents a shared memory self stabilizing failure detector , asynchronous consensus and replicated state machine algorithm suite , the components of which can be started in an arbitrary state and converge to act as a virtual state machine . self stabilizing algorithms can cope with transient faults . transient faults can alter the system state to an arbitrary state and hence , cause a temporary violation of the safety property of the consensus . started in an arbitrary state , the long lived , memory bounded and self stabilizing failure detector , asynchronous consensus , and replicated state machine suite , presented in the paper , recovers to satisfy eventual safety and eventual liveness requirements . several new techniques and paradigms are introduced . the bounded memory failure detector abstracts away synchronization assumptions using bounded heartbeat counters combined with a balance unbalance mechanism . the practically infinite paradigm is introduced in the scope of self stabilization , where an execution of , say , <digit> ( <digit> ) sequential steps is regarded as ( practically ) infinite . finally , we present the first self stabilizing wait free reset mechanism that ensures eventual safety and can be used to implement efficient self stabilizing timestamps that are of independent interest . ( c ) <digit> elsevier inc. all rights reserved .
hybrid perturbation polynomial chaos approaches to the random algebraic eigenvalue problem . <eos> the analysis of structures is affected by uncertainty in the structures material properties , geometric parameters , boundary conditions and applied loads . these uncertainties can be modelled by random variables and random fields . amongst the various problems affected by uncertainty , the random eigenvalue problem is specially important when analyzing the dynamic behavior or the buckling of a structure . the methods that stand out in dealing with the random eigenvalue problem are the perturbation method and methods based on monte carlo simulation . in the past few years , methods based on polynomial chaos ( pc ) have been developed for this problem , where each eigenvalue and eigenvector are represented by a pc expansion . in this paper four variants of a method hybridizing perturbation and pc expansion approaches are proposed and compared . the methods use rayleigh quotient , the power method , the inverse power method and the eigenvalue equation . pc expansions of eigenvalues and eigenvectors are obtained with the proposed methods . the new methods are applied to the problem of an euler bernoulli beam and a thin plate with stochastic properties .
a conservative box scheme for the euler equations . <eos> the work presented in this paper shows that the mixed type scheme of murman and cole , originally developed for a scalar equation , can be extended to systems of conservation laws . a characteristic scheme for the equations of gas dynamics is introduced that has a close connection to a four operator scheme for the burgers hopf equation . the results indicate that the scheme performs well on the classical test cases . the scheme has no tuning parameters and can be interpreted as the projection of an l infinity stable scheme . at steady state second order accuracy is obtained as a by product of the box scheme feature . copyright ( c ) <digit> john wiley sons , ltd .
using a new protocol to enhance path reliability and realize load balancing in mobile ad hoc networks . <eos> in this paper we introduce a novel end to end approach for achieving the dual goal of enhanced reliability under path failures , and multi path load balancing in mobile ad hoc networks ( manets ) . these goals are achieved by fully exploiting the presence of multiple paths in mobile ad hoc networks in order to jointly attack the problems of frequent route failures and load balancing . more specifically , we built a disjoint path identification mechanism for maintaining multiple routes between two endpoints on top of the stream control transmission protocol ( sctp ) , and the dynamic source routing ( dsr ) protocol . a number of additional modifications are incorporated to the sctp protocol in order to allow its smooth operation . the proposed approach differs from previously related work since it consists of an entirely end to end scheme built on top of a transport layer protocol . we provide both analytical and simulation results that prove the efficiency of our approach over a wide range of mobility scenarios .
situative managementuntersttzungssysteme . <eos> sogenannte digital natives sind mittlerweile auch auf den obersten fhrungsebenen von organisationen zu finden . diese neue managergeneration betrachtet managementuntersttzungssysteme ( mus ) mittlerweile als eine selbstverstndlichkeit , hat aber auch zunehmend hohe erwartungen , dass diese ihre individuellen nutzungsprferenzen erfllen . dementsprechend hinterfragen sie mus , die keine anpassungsmechanismen fr ihren jeweiligen arbeitsstil , die verschiedenen relevanten mus nutzungsflle und die unterschiedlichen mus zugangsmglichkeiten vorsehen . dieser beitrag zeigt verschiedene nutzungssituationen von fhrungskrften auf , definiert als klassen hnlicher nutzergruppenprferenzen und schlgt stellhebel vor , um die mus gestaltung konzeptionell daran anzupassen . basierend auf den ergebnissen einer literaturrecherche werden zunchst nutzergruppenprferenzen in form von <digit> nutzungssituationen klassifiziert . hierauf aufbauend machen wir vorschlge zur endgerteauswahl . wir vervollstndigen das konfigurationsmodell , indem wir auch die gestaltung der mus benutzerschnittstelle einbeziehen . schlielich zeigen wir die ntzlichkeit unseres vorschlags mithilfe einer pilotumsetzung auf und evaluieren diese .
regular polytopes of nearly full rank . <eos> an abstract regular polytope p of rank n can only be realized faithfully in euclidean space e ( d ) of dimension d if d > n when p is finite , or d > n <digit> when p is infinite ( that is , p is an apeirotope ) . in case of equality , the realization p of p is said to be of full rank . if there is a faithful realization p of p of dimension d n <digit> or d n ( as p is finite or not ) , then p is said to be of nearly full rank . in previous papers , all the at most four dimensional regular polytopes and apeirotopes of nearly full rank have been classified . this paper classifies the regular polytopes and apeirotopes of nearly full rank in all higher dimensions .
acceptable consistency analysis of interval reciprocal comparison matrices . <eos> when a decision maker expresses his her opinions by means of an interval reciprocal comparison matrix , the study of consistency becomes a very important aspect in decision making in order to avoid a misleading solution . in the present paper . an acceptably consistent interval reciprocal comparison matrix is defined , which can be reduced to an acceptably consistent crisp reciprocal comparison matrix when the intervals become exact numbers . an interval reciprocal comparison matrix with unacceptable consistency can be easily adjusted such that the revised matrix possesses acceptable consistency . utilizing a convex combination method , a family of crisp reciprocal comparison matrices with acceptable consistency can be obtained , whose weights are further found to exhibit a style of convex combination , and aggregated to obtain interval weights from an acceptably consistent interval reciprocal comparison matrix . a novel , simple yet effective formula of possibility degree is presented to rank interval weights . numerical results are calculated to show the quality and quantity of the proposed approaches and compare with other existing procedures . ( c ) <digit> elsevier b.v. all rights reserved .
wireless internet access 3g vs. wifi . <eos> this article compares and contrasts two technologies for delivering broadband wireless internet access services 3g vs. wifi . the former , 3g , refers to the collection of third generation mobile technologies that are designed to allow mobile operators to offer integrated data and voice services over mobile networks . the latter , wifi , refers to the 802.11 b wireless ethernet standard that was designed to support wireless lans . although the two technologies reflect fundamentally different service , industry , and architectural design goals , origins , and philosophies , each has recently attracted a lot of attention as candidates for the dominant platform for providing broadband wireless access to the internet . it remains an open question as to the extent to which these two technologies are in competition or , perhaps , may be complementary . if they are viewed as in competition , then the triumph of one at the expense of the other would be likely to have profound implications for the evolution of the wireless internet and structure of the service provider industry .
decentralized fuzzy control of multiple nonholonomic vehicles . <eos> this work considers the problem of controlling multiple nonholonomic vehicles so that they converge to a scent source without colliding with each other . since the control is to be implemented on a simple <digit> bit microcontroller , fuzzy control rules are used to simplify a linear quadratic regulator control design . the inputs to the fuzzy controllers for each vehicle are the noisy direction to the source , the distance to the closest neighbor vehicle , and the direction to the closest vehicle . these directions are discretized into four values forward , behind , left , and right and the distance into three values near , far , and gone . the values of the control at these discrete values are obtained based on the collision avoidance repulsive forces and an attractive force towards the goal . a fuzzy inference system is used to obtain control values from a small number of discrete input values . simulation results are provided which demonstrate that the fuzzy control law performs well compared to the exact controller . in fact , the fuzzy controller demonstrates improved robustness to noise .
on optimal p cycle based protection in wdm optical networks with sparse partial wavelength conversion . <eos> we study the optimal configuration of p cycles in survivable wavelength division multiplexing ( wdm ) optical mesh networks with sparse partial wavelength conversion while <digit> % restorability is guaranteed against any single failures . we formulate the problem as two integer linear programs ( optimization models i , and ii ) which have the same constraints , but different objective functions . p cycles and wavelength converters are optimally determined subject to the constraint that only a given number of nodes have wavelength conversion capability , and the maximum number of wavelength converters that can be placed at such nodes is limited . optimization model i has a composite sequential objective function that first ( g1 ) minimizes the cost of link capacity used by all p cycles in order to accommodate a set of traffic demands and then ( g2 ) minimizes the total number of wavelength converters used in the entire network . in optimization model ii , the cost of one wavelength . converter is measured as the cost of a deployed wavelength link with a length of a units and the objective is to minimize the total cost of link capacity wavelength converters required by p cycle configuration . during p cycle configuration , our schemes fully takes into account wavelength converter sharing , which reduces the number of converters required while attaining a satisfactory level of performance . our simulation results indicate that the proposed schemes significantly outperform existing approaches in terms of protection cost , number of wavelength conversion sites , and number of wavelength converters needed .
biomechanical simulation of the fetal descent without imposed theoretical trajectory . <eos> the medical training concerning childbirth for young obstetricians involves performing real deliveries , under supervision . this medical procedure becomes more complicated when instrumented deliveries requiring the use of forceps or suction cups become necessary . for this reason , the use of a versatile , configurable childbirth simulator , taking into account different anatomical and pathological cases , would provide an important benefit in the training of obstetricians , and improve medical procedures . the production of this type of simulator should be generally based on a computerized birth simulation , enabling the computation of the reproductive organs deformation of the parturient woman and fetal interactions as well as the calculation of efforts produced during the second stage of labor . in this paper , we present a geometrical and biomechanical modeling of the main parturient 's organs involved in the birth process , interacting with the fetus . instead of searching for absolute precision , we search to find a good compromise between accuracy and model complexity . at this stage , to verify the correctness of our hypothesis , we use finite element analysis because of its reliability , precision and stability . moreover , our study improves the previous work carried out on childbirth simulators because ( a ) our childbirth model takes into account all the major organs involved in birth process , thus potentially enabling different childbirth scenarios ( b ) fetal head is not treated as a rigid body and its motion is computed by taking into account realistic boundary conditions , i.e. we do not impose a pre computed fetal trajectory ( c ) we take into account the cyclic uterine contractions as well as voluntary efforts produced by the muscles of the abdomen ( d ) a slight pressure is added inside the abdomen , representing the residual muscle tone . the next stage of our work will concern the optimization of our numerical resolution approach to obtain interactive time simulation , enabling it to be coupled to our haptic device .
complex patterns in networks of hyperexcitable neurons . <eos> complex patterns in neuronal networks emerge from the cooperative activity of the participating neurons , synaptic connectivity and network topology . several neuron types exhibit complex intrinsic dynamics due to the presence of nonlinearities and multiple time scales . in this paper we extend previous work on hyperexcitability of neuronal networks , a hallmark of epileptic brain seizure generation , which results from the net imbalance between excitation and inhibition and the ability of certain neuron types to exhibit abrupt transitions between low and high firing frequency regimes as the levels of recurrent ampa excitation change . we examine the effect of different topologies and connection delays on the hyperexcitability phenomenon in networks having recurrent synaptic ampa ( fast ) excitation ( in the absence of synaptic inhibition ) and demonstrate the emergence of additional time scales .
decentralized management of building indoors through embedded software agents . <eos> in order to support personalized people comfort and building energy efficiency as well as safety , emergency , and context aware information exchange scenarios , next generation buildings will be smart . in this paper we propose an agent oriented decentralized and embedded architecture based on wireless sensor and actuator networks ( wsans ) for enabling efficient and effective management of buildings . the main objective of the proposed architecture is to fully support distributed and coordinated sensing and actuation operations . the building management architecture is implemented at the wsan side through maps ( mobile agent platform for sun spots ) , an agent based framework for programming wsn applications based on the sun spot sensor platform , and at the base station side through an osgi based application . the proposed agent oriented architecture is demonstrated in a simple yet effective operating scenario related to monitoring workstation usage in computer laboratories offices . the high modularity of the proposed architecture allows for easy adaptation of higher level application specific agents that can therefore exploit the architecture to implement intelligent building management policies .
electronic governance for sustainable development conceptual framework and state of research . <eos> electronic governance ( egov ) research studies the use of information and communication technologies to improve governance processes . sustainable development ( sd ) research studies possible development routes that satisfy the needs of the present generation without compromising the ability of the future generations to meet their own needs . despite substantial progress in advancing both domains independently , little research exists at their intersection how to utilize egov in support of sd . we call this intersection electronic governance for sustainable development ( egov4sd ) . this paper <digit> ) proposes a conceptual framework for egov4sd , <digit> ) proposes egov4sd research assessment framework and <digit> ) applies both frameworks to determine the state of egov4sd research . the main contribution of the paper is establishing a foundation for egov4sd research .
selgen system to dynamically evaluate and compare the agronomic behavior of genotypes that participate in networks of comparative yield trials . <eos> one of the decisions of major impact on crop profitability is the selection of the genotype to sow . selgen is a software that makes it possible to evaluate and dynamically compare genotypes this allows a faster selection of the suitable genotype . the software processes , according to the options selected by the user , the information contained in a database regarding the genotype yield in different environments . the results are shown in graphs and tables . the database included in the program is updated twice a year . users can analyze their own databases .
control of mineral wool thickness using predictive functional control . <eos> the production process of mineral wool is affected by several constantly changing factors . the ingredients for the mineral wool are melted in a furnace . the molten mineral charge exits the bottom of the furnace in a water cooled trough and falls into a fiberization device ( the centrifuge ) . the centrifuge forms the fibers . at this stage binders are injected to bind the fibers together . to ensure the quality of the end product ( the consistent thickness ) the flow of the bounded fibers must be as constant as possible . one way to ensure that is to control the speed of the conveyor belt that transports the bounded fibers from the centrifuge to the curing process . predictive functional controller and pid controller are considered to replace an existing algorithm . both can easily replace an existing one as they do not require any new sensor installation . all three algorithms are presented and tested on a developed plant model . the study showed that the predictive control gives better results than the existing and pid controller .
bioinformatics resource manager v2 .3 an integrated software environment for systems biology with microrna and cross species analysis tools . <eos> micrornas ( mirnas ) are noncoding rnas that direct post transcriptional regulation of protein coding genes . recent studies have shown mirnas are important for controlling many biological processes , including nervous system development , and are highly conserved across species . given their importance , computational tools are necessary for analysis , interpretation and integration of high throughput ( htp ) mirna data in an increasing number of model species . the bioinformatics resource manager ( brm ) v2 .3 is a software environment for data management , mining , integration and functional annotation of htp biological data . in this study , we report recent updates to brm for mirna data analysis and cross species comparisons across datasets .
integrated process planning using tool process capabilities and heuristic search . <eos> cad cam integration has involved either design with standard manufacturing features ( feature based design ) , or interpretation of a solid model based on a set of predetermined feature patterns ( automatic feature recognition ) . thus existing approaches are limited in application to predefined features , and also disregard the dynamic nature of the process and tool availability in the manufacturing shop floor . to overcome this problem , we develop a process oriented approach to design interpretation , and model the shape producing capabilities of the tools into tool classes . we then interpret the part by matching regions of it with the tool classes directly . in addition , there could be more than one way in which a part can be interpreted , and to obtain an optimal plan , it is necessary for an integrated computer aided process planning system to examine these alternatives . we develop a systematic search algorithm to generate the different interpretations , and a heuristic approach to sequence operations ( set ups tools ) for the features of the interpretations generated . the heuristic operation sequencing algorithm considers features and their manufacturing constraints ( precedences ) simultaneously , to optimally allocate set ups and tools for the various features . the modules within the design interpretation and process planner are linked through an abstracted qualitative model of feature interactions . such an abstract representation is convenient for geometric reasoning tasks associated with planning and design interpretation .
reliable approaches of variational iteration method for nonlinear operators . <eos> in this paper , new approaches of the variational iteration method are developed to handle nonlinear problems . the proposed approaches are capable of reducing the size of calculations and easily overcome the difficulty arising in calculating complicated integrals . numerical examples are examined to show the efficiency of the techniques . the modified approaches show improvements over the existing numerical schemes . ( c ) <digit> elsevier ltd. all rights reserved .
program termination and well partial orderings . <eos> the following known observation is useful in establishing program termination if a transitive relation r is covered by finitely many well founded relations u ( <digit> ) , ... , u ( n ) then r is well founded . a question arises how to bound the ordinal height vertical bar r vertical bar of the relation r in terms of the ordinals alpha ( i ) vertical bar u ( i ) vertical bar . we introduce the notion of the stature parallel to p parallel to of a well partial ordering p and show that vertical bar r vertical bar < parallel to alpha ( <digit> ) x ... x alpha ( n ) parallel to and that this bound is tight . the notion of stature is of considerable independent interest . we define parallel to p parallel to as the ordinal height of the forest of nonempty bad sequences of p , but it has many other natural and equivalent definitions . in particular , parallel to p parallel to is the supremum , and in fact the maximum , of the lengths of linearizations of p. and parallel to alpha ( <digit> ) x ... x alpha ( n ) parallel to is equal to the natural product alpha ( <digit> ) circle times ... circle times alpha ( n ) .
linear predictive coding and cepstrum coefficients for mining time variant information from software repositories . <eos> this paper presents an approach to recover time variant information from software repositories . it is widely accepted that software evolves due to factors such as defect removal , market opportunity or adding new features . software evolution details are stored in software repositories which often contain the changes history . on the other hand there is a lack of approaches , technologies and methods to efficiently extract and represent time dependent information . disciplines such as signal and image processing or speech recognition adopt frequency domain representations to mitigate differences of signals evolving in time . inspired by time frequency duality , this paper proposes the use of linear predictive coding ( lpc ) and cepstrum coefficients to model time varying software artifact histories . lpc or cepstrum allow obtaining very compact representations with linear complexity . these representations can be used to highlight components and artifacts evolved in the same way or with very similar evolution patterns . to assess the proposed approach we applied lpc and cepstral analysis to <digit> linux kernel releases ( i.e. , from 1.0 to 1.3.100 ) , to identify files with very similar size histories . the approach , the preliminary results and the lesson learned are presented in this paper .
outlier detection for patient monitoring and alerting . <eos> we develop and evaluate a data driven approach for detecting unusual ( anomalous ) patient management decisions using past patient cases stored in electronic health records ( ehrs ) . our hypothesis is that a patient management decision that is unusual with respect to past patient care may be due to an error and that it is worthwhile to generate an alert if such a decision is encountered . we evaluate this hypothesis using data obtained from ehrs of <digit> post cardiac surgical patients and a subset of <digit> alerts generated from the data . we base the evaluation on the opinions of a panel of experts . the results of the study support our hypothesis that the outlier based alerting can lead to promising true alert rates . we observed true alert rates that ranged from <digit> % to <digit> % for a variety of patient management actions , with <digit> % corresponding to the strongest outliers .
maplets for correspondence based object recognition . <eos> we present a correspondence based system for visual object recognition with invariance to position , orientation , scale and deformation . the system is intermediate between high and low dimensional representations of correspondences . the essence of the approach is based on higher order links , called here maplets , which are specific to narrow ranges of mapping parameters ( position , scale and orientation ) , which interact cooperatively with each other , and which are assumed to be formed by learning . while being based on dynamic links , the system overcomes previous problems with that formulation in terms of speed of convergence and range of allowed variation . we perform face recognition experiments , comparing ours to other published systems . we see our work as a step towards a reformulation of neural dynamics that includes rapid network self organization as essential aspect of brain state organization .
self interruptions in discretionary multitasking . <eos> human multitasking is often the result of self initiated interruptions in the performance of an ongoing task . these self interruptions occur in the absence of external triggers such as electronic alerts or email notifications . compared to externally induced interruptions , self interruptions have not received enough research attention . to address this gap , this paper develops a typology of self interruptions based on the integration of flow theory and self regulation theory . in this new typology , the two major categories stem from positive and negative feelings of task progress and prospects of goal attainment . the proposed classification is validated in an experimental multitasking environment with pre defined tasks . empirical findings indicate that negative feelings trigger more self interruptions than positive feelings . in general , more self interruptions result in lower accuracy in all tasks . the results suggest that negative internal triggers of self interruptions unleash a downward spiral that may degrade performance .
pid controlled particle swarm optimization . <eos> premature convergence is a major challenge for particle swarm optimization algorithm ( pso ) when dealing with multi modal problems . the reason is partly due to the insufficient exploration capability because of the fast convergent speed especially in the final stage . in this paper , the pso is regarded as a two inputs one output feedback system , and two pid controllers are incorporated into the methodology of pso to improve the population diversity . different from the integral controller , pid controller has three independent parameters and adjusts them dynamically . theoretical results with support set theory and stability analysis both demonstrate that pid controller provides more chances to escaping from a local optimum . to validate the efficiency of this new variant , four other famous variants are used to compare including the comprehensive leaning pso , modified time varying accelerator coefficients pso , integral controlled pso and the standard version , the test suit consists five unconstrained numerical benchmarks with dimensionality <digit> and <digit> , respectively . simulation results show pid controlled pso is suitable for high dimensional multi modal problems due to the large exploration capability in the final stage .
small k pyramids and the complexity of determining k . <eos> motivated by the computational complexity of determining whether a graph is hamiltonian , we study under algorithmic aspects a class of polyhedra called k pyramids , introduced in <digit> , and discuss related applications . we prove that determining whether a given graph is the <digit> skeleton of a k pyramid , and if so whether it is belted or not , can be done in polynomial time for k <digit> k <digit> . the impact on hamiltonicity follows from the traceability of all <digit> pyramids and non belted <digit> pyramids , and from the hamiltonicity of all non belted <digit> pyramids . the algorithm can also be used to determine the outcome for larger values of k , but the complexity increases exponentially with k. lastly , we present applications of the algorithm , and improve the known bounds for the minimal cardinality of systems of bases called foundations in graph families with interesting properties concerning traceability and hamiltonicity .
performance of mrc combining multi antenna cooperative relay network . <eos> performance of cooperative relaying employing infrastructure based fixed relays having multiple antennas has been investigated . employing mgf based approach , closed form expression for outage probability and bit error rate performance of bpsk signal have been derived , when relay and destination are assumed to perform mrc combining of the signals . the effect of relay placement on the system performance has also been studied under different path loss conditions .
novel tight closed form bounds for the symbol error rate of egc and mrc diversity receivers employing linear modulations over ( alpha mu ) fading . <eos> in this paper , we propose novel lower and upper bounds on the average symbol error rate ( ser ) of the dual branch maximal ratio combining and equal gain combining diversity receivers assuming independent branches . ( m ) ary pulse amplitude modulation and ( m ) ary phase shift keying schemes are employed and operation over the ( alpha mu ) fading channel is assumed . the proposed bounds are given in closed form and are very simple to calculate as they are composed of a double finite summation of basic functions that are readily available in the commercial software packages . furthermore , the proposed bounds are valid for any combination of the parameters ( alpha ) and ( mu ) as well as ( m ) . numerical results presented show that the proposed bounds are very tight when compared to the exact ser obtained via performing the exact integrations numerically making them an attractive much simpler alternative for ser evaluation studies .
future directions in evaluation research people , organizational , and social issues . <eos> objective to review evaluation literature concerning people , organizational , and social issues and provide recommendations for future research . method analyze this research and make recommendations . results and conclusions evaluation research is key in identifying how people , organizational , and social issues all crucial to system design , development , implementation , and use interplay with informatics projects . building on a long history of contributions and using a variety of methods , researchers continue developing evaluation theories and methods while producing significant interesting studies . we recommend that future research i ) address concerns of the many individuals involved in or affected by informatics applications . <digit> ) conduct studies in different type and size sites , and with different scopes of systems and different groups of users . do multi site or multi system comparative studies . <digit> ) incorporate evaluation into all phases of a project . <digit> ) study failures , partial successes , and changes in project definition or outcome . <digit> ) employ evaluation approaches that take account of the shifting nature of health care and project environments , and do formative evaluations . <digit> ) incorporate people , social , organizational , cultural , and concomitant ethical issues into the mainstream of medical informatics . <digit> ) diversify research approaches and continue to develop new approaches . <digit> ) conduct investigations at different levels of analysis . <digit> ) integrate findings from different applications and contextual settings , different areas of health care , studies in other disciplines , and also work that is not published in traditional research outlets . <digit> ) develop and test theory to inform both further evaluation research and informatics practice .
an intervention method for occupational safety in farming evaluation of the effect and process . <eos> in order to increase safety in swedish farming an intervention methodology to influence attitudes and behaviour was tested . eightyeight farmers and farm workers in nine groups gathered on seven occasions during <digit> year . the basic concept was to create socially supportive networks and encourage discussions and reflection , focusing on risk manageability . six of the groups made structured incident accident analyses . three of the latter groups also received information on risks and accident consequences . effects were evaluated in a pre post questionnaire using six graded scales . a significant increase in safety activity and significant reduction in stress and risk acceptance was observed in the total sample . risk perception and perceived risk manageability did not change . analysing incidents accidents , but not receiving information , showed a more positive outcome . qualitative data indicated good feasibility and that the long duration of the intervention was perceived as necessary . the socially supportive network was reported as beneficial for the change process .
a performance model of the parallel ocean program . <eos> in this paper we describe a performance model of the parallel ocean program ( pop ) . in particular , the latest version of pop ( v2 .0 ) is considered , which has similarities and differences to the earlier version ( v1 .4.3 ) as commonly used in climate simulations . the performance model encapsulates an understanding of pop 's data decomposition , processing flow , and scaling characteristics . the model is parametrized in many of the main input parameters to pop as well as characteristics of a processing system such as network latency and bandwidth . the performance model has been validated to date on a medium sized ( <digit> processor ) alphaserver es40 system with the qsnet <digit> interconnection network , and also on a larger scale ( <digit> processor ) blue gene light system . the accuracy of the performance model is high when using two standard benchmark configurations , one of which represents a realistic configuration similar to that used in community climate system model coupled climate simulations . the performance model is also used to explore the performance of pop after possible optimizations to the code , and different task to processor assignment strategies , whose performance can not be currently measured .
biosignals modulated by tumor associated carbohydrate antigens . <eos> based on the remodeling of glycosphingolipids on the human tumor cell lines with manipulation of glycosyltransferase genes , roles of sugar moieties in tumor associated carbohydrate antigens have been analyzed . two main topics , that is , the roles of ganglioside gd3 in human malignant melanomas and those of gd2 in small cell lung cancer ( sclc ) were reported . gd3 enhances tyrosine phosphorylation of two adaptor molecules , p130cas and paxillin , resulting in the increased cell growth and invasion in melanoma cells . gd2 also enhances the proliferation and invasion of sclc cells . gd2 also mediates apoptosis with anti gd2 monoclonal antibodies ( mabs ) via dephosphorylation of the focal adhesion kinase . these approaches have promoted further understanding of mechanisms by which gangliosides modulate malignant properties of human cancer , and the results obtained here propose novel targets for cancer therapy
a novel hybrid intelligent system for multi objective machine parameter optimization . <eos> this multidisciplinary research presents a novel hybrid intelligent system to perform a multi objective industrial parameter optimization process . the intelligent system is based on the application of evolutionary and neural computation in conjunction with identification systems , which makes it possible to optimize the implementation conditions in the manufacturing process of high precision parts , including finishing precision , while saving time , financial costs and or energy . empirical verification of the proposed hybrid intelligent system is performed in a real industrial domain , where a case study is defined and analyzed . the experiments are carried out based on real dental milling processes using a high precision machining centre with five axes , requiring high finishing precision of measures in micrometers with a large number of process factors to analyze . the results of the experiments which validate the performance of the proposed approach are presented in this study .
a general approach for modeling the motion of rigid and deformable ellipsoids in ductile flows . <eos> a general approach for modeling the motion of rigid or deformable objects in viscous flows is presented . it is shown that the rotation of a 3d object in a viscous fluid , regardless of the mechanical property and shape of the object , is defined by a common and simple differential equation , d q d t q , where q is a matrix defined by the orientation of the object and is the angular velocity tensor of the object . the difference between individual cases lies only in the formulation for the angular velocity . thus the above equation , together with jeffery 's theory for the angular velocity of rigid ellipsoids , describes the motion of rigid ellipsoids in viscous flows . the same equation , together with eshelby 's theory for the angular velocity of deformable ellipsoids , describes the motion of deformable ellipsoids in viscous flows . both problems are solved here numerically by a general approach that is much simpler conceptually and more economic computationally , compared to previous approaches that consider the problems separately and require numerical solutions to coupled differential equations about euler angles or spherical ( polar coordinate ) angles . a rungekutta approximation is constructed for solving the above general differential equation . singular cases of eshelby 's equations when the object is spheroidal or spherical are handled in this paper in a much simpler way than in previous work . the computational procedure can be readily implemented in any modern mathematics application that handles matrix operations . four mathcad worksheets are provided for modeling the motion of a single rigid or deformable ellipsoid immersed in viscous fluids , as well as the evolution of a system of noninteracting rigid or deformable ellipsoids embedded in viscous flows .
an mdd based generalized arc consistency algorithm for positive and negative table constraints and some global constraints . <eos> a table constraint is explicitly represented as its set of solutions or non solutions . this ad hoc ( or extensional ) representation may require space exponential to the arity of the constraint , making enforcing gac expensive . in this paper , we address the space and time inefficiencies simultaneously by presenting the mddc constraint . mddc is a global constraint that represents its ( non ) solutions with a multi valued decision diagram ( mdd ) . the mdd based representation has the advantage that it can be exponentially smaller than a table . the associated gac algorithm ( called mddc ) has time complexity linear to the size of the mdd , and achieves full incrementality in constant time . in addition , we show how to convert a positive or negative table constraint into an mddc constraint in time linear to the size of the table . our experiments on structured problems , car sequencing and still life , show that mddc is also a fast gac algorithm for some global constraints such as sequence and regular . we also show that mddc is faster than the state of the art generic gac algorithms in gent et al. ( <digit> ) , lecoutre and szymanek ( <digit> ) , lhomme and r , gin ( <digit> ) for table constraint .
opinion dynamics driven by leaders , media , viruses and worms . <eos> a model on the effects of leader , media , viruses , worms , and other agents on the opinion of individuals is developed and utilized to simulate the formation of consensus in society and price in market via excess between supply and demand . the effects of some time varying drives ( harmonic and hyperbolic ) are also investigated .
on the learning potential of the approximated quantron . <eos> the quantron is a hybrid neuron model related to perceptrons and spiking neurons . the activation of the quantron is determined by the maximum of a sum of input signals , which is difficult to use in classical learning algorithms . thus , training the quantron to solve classification problems requires heuristic methods such as direct search . in this paper , we present an approximation of the quantron trainable by gradient search . we show this approximation improves the classification performance of direct search solutions . we also compare the quantron and the perceptron 's performance in solving the iris classification problem .
trust management services in relational databases . <eos> trust management represents today a promising approach for supporting access control in open environments . while several approaches have been proposed for trust management and significant steps have been made in this direction , a major obstacle that still exists in the realization of the benefits of this paradigm is represented by the lack of adequate support in the dbms.in this paper , we present a design that can be used to implement trust management within current relational dbmss . we propose a trust model with a sql syntax and illustrate the main issues arising in the implementation of the model in a relational dbms . specific attention is paid to the efficient verification of a delegation path for certificates . this effort permits a relatively inexpensive realization of the services of an advanced trust management model within current relational dbmss .
note on algebraic solutions of differential equations with known finite galois group . <eos> given a linear differential equation with known finite differential galois group , we discuss methods to construct the minimal polynomial of a solution . we first outline a well known general method involving a basis transformation of the basis of formal solutions at a singular point . in the second part we construct directly the minimal polynomial of an eigenvector of the monodromy matrix at a singular point . the method is very efficient for irreducible second and third order linear differential equations where a one dimensional eigenspace of some monodromy matrix always exists .
security of kuwakado tanaka transitive signature scheme for directed trees . <eos> recently , kuwakado and tanaka proposed a transitive signature scheme for directed trees . in this letter , we show that kuwakado tanaka scheme is insecure against a forgery attack , in which an attacker is able to forge edge signatures by composing edge signatures provided by a signer .
an improved virtualization layer to support distribution of multimedia contents in pervasive social applications . <eos> pervasive social computing is a new paradigm of computer science that aims to facilitate the realization of activities in whichever context , with the aid of information devices and considering social relations between users . this vision requires means to support the shared experiences by harnessing the communication and computing capabilities of the connected devices , relying on direct or hop by hop communications among people who happen to be close to each other . in this paper , we present an approach to turn mobile ad hoc networks ( manets ) into stable communication environments for pervasive social applications . the proposal is based on an evolution of the vnlayer , a virtualization layer that defined procedures for mobile devices to collaboratively emulate an infrastructure of stationary virtual nodes . we refine the vnlayer procedures and introduce new ones to increase the reliability and the responsiveness of the virtual nodes , which serves to boost the performance of routing with a virtualized version of the well known aodv algorithm . we prove the advantages of the resulting routing scheme by means of simulation experiments and measurements on a real deployment of an application for immersive and collective learning about history in museums and their surroundings .
a repartitioning hypergraph model for dynamic load balancing . <eos> in parallel adaptive applications , the computational structure of the applications changes over time , leading to load imbalances even though the initial load distributions were balanced . to restore balance and to keep communication volume low in further iterations of the applications , dynamic load balancing ( repartitioning ) of the changed computational structure is required . repartitioning differs from static load balancing ( partitioning ) due to the additional requirement of minimizing migration cost to move data from an existing partition to a new partition . in this paper , we present a novel repartitioning hypergraph model for dynamic load balancing that accounts for both communication volume in the application and migration cost to move data , in order to minimize the overall cost . the use of a hypergraph based model allows us to accurately model communication costs rather than approximate them with graph based models . we show that the new model can be realized using hypergraph partitioning with fixed vertices and describe our parallel multilevel implementation within the zoltan load balancing toolkit . to the best of our knowledge , this is the first implementation for dynamic load balancing based on hypergraph partitioning . to demonstrate the effectiveness of our approach , we conducted experiments on a linux cluster with <digit> processors . the results show that , in terms of reducing total cost , our new model compares favorably to the graph based dynamic load balancing approaches , and multilevel approaches improve the repartitioning quality significantly .
assigning location information to display individuals on a map for web people search results . <eos> distinguishing people with identical names is becoming more and more important in web search . this research aims to display person icons on a map to help users select person clusters that are separated into different people from the result of person searches on the web . we propose a method to assign person clusters with one piece of location information . our method is comprised of two processes ( a ) extracting location candidates from web pages and ( b ) assigning location information using a local search engine . our main idea exploits search engine rankings and character distance to obtain good location information among location candidates . experimental results revealed the usefulness of our proposed method . we also show a developed prototype system .
a declarative approach to procedural modeling of virtual worlds . <eos> with the ever increasing costs of manual content creation for virtual worlds , the potential of creating it automatically becomes too attractive to ignore . however , for most designers , traditional procedural content generation methods are complex and unintuitive to use , hard to control , and generated results are not easily integrated into a complete and consistent virtual world . we introduce a novel declarative modeling approach that enables designers to concentrate on stating what they want to create instead of on describing how they should model it . it aims at reducing the complexity of virtual world modeling by combining the strengths of semantics based modeling with manual and procedural approaches . this article describes two of its main contributions to procedural modeling of virtual worlds interactive procedural sketching and virtual world consistency maintenance . we discuss how these techniques , integrated in our modeling framework sketchaworld , build up to enable designers to create a complete 3d virtual world in minutes . procedural sketching provides a fast and more intuitive way to model virtual worlds , by letting designers interactively sketch their virtual world using high level terrain features , which are then procedurally expanded using a variety of integrated procedural methods . consistency maintenance guarantees that the semantics of all terrain features is preserved throughout the modeling process . in particular , it automatically solves conflicts possibly emerging from interactions between terrain features . we believe that these contributions together represent a significant step towards providing more user control and flexibility in procedural modeling of virtual worlds . it can therefore be expected that by further reducing its complexity , virtual world modeling will become accessible to an increasingly broad group of users .
lagrangian finite element modelling of dam fluid interaction accurate absorbing boundary conditions . <eos> the dynamic dam fluid interaction is considered via a lagrangian approach , based on a fluid finite element ' fe , model under the assumption of small displacement and inviscid fluid . the fluid domain is discretized by enhanced displacement based finite elements , which can be considered an evolution of those derived from the pioneering works of bathe and hahn bathe kj , hahn wf . on transient analysis of fluid structure system . comp struct <digit> <digit> <digit> <digit> and of wilson and khalvati wilson el , khalvati m. finite element for the dynamic analysis of fluid solid system . int . j numer methods eng <digit> <digit> <digit> <digit> . the irrotational condition for inviscid fluids is imposed by the penalty method and consequentially leads to a type of micropolar media . the model is implemented using a fe code , and the numerical results of a rectangular bidimensional basin ( subjected to horizontal sinusoidal acceleration ) are compared with the analytical solution . it is demonstrated that the lagrangian model is able to perform pressure and gravity wave propagation analysis , even if the gravity ( or surface ) waves are dispersive . the dispersion nature of surface waves indicates that the wave propagation velocity is dependent on the wave frequency . for the practical analysis of the coupled dam fluid problem the analysed region of the basin must be reduced and the use of suitable asymptotic boundary conditions must be investigated . the classical sommerfeld condition is implemented by means of a boundary layer of dampers and the analysis results are shown for the cases of sinusoidal forcing . the classical sommerfeld condition is highly efficient for pressure based fe modelling , but may not be considered fully adequate for the displacement based fe approach . in the present paper a high order boundary condition proposed by higdom higdom rl . radiation boundary condition for dispersive waves . siam j numer anal <digit> <digit> <digit> <digit> is considered . its implementation requires the resolution of a multifreedom constraint problem , defined in terms of incremental displacements , in the ambit of dynamic time integration problems . the first and second order higdon conditions are developed and implemented . the results are compared with the sommerfeld condition results , and with the analytical unbounded problem results . finally , a number of finite element results are presented and their related features are discussed and critically compared . ( c ) <digit> elsevier ltd. all rights reserved .
optimal independent spanning trees on odd graphs . <eos> the use of multiple independent spanning trees ( ists ) for data broadcasting in networks provides a number of advantages , including the increase of fault tolerance and bandwidth . the designs of multiple ists on several classes of networks have been widely investigated . in this paper we show a construction algorithm of ists on odd graphs , and we analyze that all the lengths of the paths in the ists are less than or equal to the length of the shortest path <digit> , which is optimal . we also prove that the heights of the ists we constructed are d <digit> , which again is optimal , since the fault diameter of an odd graph is d <digit> .
reliability evaluation for single event transients on digital circuits . <eos> the effect of single event transient ( set ) on reliability has become a significant concern for digital circuits . this paper proposed an algorithm for evaluating the reliability for set on digital circuits , based on signal probability , universal generating function technique , and generalized reliability block diagrams . the algorithm provides an expression for the reliability of set under consideration for the effects of logic masking , error attenuation of gates , and crosstalk effects among interconnect wires . we perform simulations of iscas85 circuits . the results indicate that the proposed algorithm can effectively evaluate the reliability for set on circuits . the error attenuation of gates can increase the reliability by more than 41.6 % , and the masking and crosstalk effects will improve the reliability by more than <digit> % .
identifying shapes using self assembly . <eos> in this paper , we introduce the following problem in the theory of algorithmic self assembly given an input shape as the seed of a tile based self assembly system , design a finite tile set that can , in some sense , uniquely identify whether or not the given input shape drawn from a very general class of shapes matches a particular target shape . we first study the complexity of correctly identifying squares . then we investigate the complexity associated with the identification of a considerably more general class of non square , hole free shapes .
advanced multimodal visualisation of clinical gait and fluoroscopy analyses in the assessment of total knee replacement . <eos> traditional gait and fluoroscopy analysis of human movement are largely utilised but are still limited in registration , integration , synchronisation and visualisation capabilities . the present work exploits the features of a recently developed software tool based on multimodal display ( data manager developed within the eu funded project multimod ) in an exemplary clinical case . standard lower limb gait analysis , comprising segment position , ground reaction force and emg data collection , and three dimensional fluoroscopy analysis at the replaced joint were performed in a total knee replacement patient while ascending stairs . clinical information such as x rays and standard scores were also available . data manager was able to import all this variety of data and to structure these in an original hierarchical tree . bone and prosthesis component models were registered to corresponding marker position data for effective three dimensional animations . these were also synchronised with corresponding standard video sequences . animations , video , time histories of collected and also processed data were shown in various combinations , according to specific interests of the bioengineering and medical professionals expected to observe and to interpret this large amount of data . this software tool demonstrated to be a valuable means to enhance representation and interpretation of measurements coming from human motion analysis . in a single software , a thorough and effective clinical and biomechanical analysis of human motion was performed .
a high performance angular speed measurement method based on adaptive hysteresis switching techniques . <eos> a new adaptive angular speed ( as ) estimate method minimizes as errors . hysteresis switching technology is applied to avoid speed fluctuations . errors caused by the mechanical errors of optical encoder are removed . merits of high accuracy , fast response , and less fluctuation are achieved . two layer hysteresis switches system is realized in dsc .
a multidimensional segmentation evaluation for medical image data . <eos> evaluation of segmentation methods is a crucial aspect in image processing , especially in the medical imaging field , where small differences between segmented regions in the anatomy can be of paramount importance . usually , segmentation evaluation is based on a measure that depends on the number of segmented voxels inside and outside of some reference regions that are called gold standards . although some other measures have been also used , in this work we propose a set of new similarity measures , based on different features , such as the location and intensity values of the misclassified voxels , and the connectivity and the boundaries of the segmented data . using the multidimensional information provided by these measures , we propose a new evaluation method whose results are visualized applying a principal component analysis of the data , obtaining a simplified graphical method to compare different segmentation results . we have carried out an intensive study using several classic segmentation methods applied to a set of mri simulated data of the brain with several noise and rf inhomogeneity levels , and also to real data , showing that the new measures proposed here and the results that we have obtained from the multidimensional evaluation , improve the robustness of the evaluation and provides better understanding about the difference between segmentation methods .
probabilistic querying over uncertain data streams . <eos> inherent imprecision of data in many applications motivates us to support uncertainty as a first class concept . data stream and probabilistic data have been recently considered noticeably in isolation . however , there are many applications including sensor data management systems and object monitoring systems which need both issues in tandem . our main contribution is designing a probabilistic data stream management system , called sarcheshmeh , ( a ) for continuous querying over probabilistic data streams . sarcheshmeh supports uncertainty from input data to final query results . in this paper , after reviewing requirements and applications of probabilistic data streams , we present our new data model for probabilistic data streams and define our main logical operators formally . then , we present query language and physical operators . in addition , we introduce the architecture of sarcheshmeh and also describe some major challenges like memory management and our floating precision mechanism toward designing a more robust system . finally , we report evaluation of our system and the effect of floating precision on the tradeoff between accuracy and efficiency .
numerical study of the 3d failure envelope of a single pile in sand . <eos> the paper presents a comprehensive study of the failure envelope ( or capacity diagram ) of a single elastic pile in sand . the behavior of a pile subjected to different load combinations is simulated using a large number of finite element numerical calculations . the sand is modeled using a constitutive law based on hypoplasticity . in order to find the failure envelope in the three dimensional space ( i.e. horizontal force h , bending moment m and vertical force v ) , the radial displacement method and swipe tests are numerically performed . it is found that with increasing vertical load the horizontal bearing capacity of the pile decreases . furthermore , the presence of bending moment on the pile head significantly influences the horizontal bearing capacity and the capacity diagram in the hm plane manifests an inclined elliptical shape . an analytical equation providing good agreement with the 3d numerical results is finally proposed . the formula is useful for design purposes and the development of simplified modeling numerical strategies such as macro element .
semantic linkages in research information systems as a new data source for scientometric studies . <eos> a growing number of research information systems use a semantic linkage technique to represent in explicit mode information about relationships between elements of its content . this practice is coming nowadays to a maturity when already existed data on semantically linked research objects and expressed by this scientific relationships can be recognized as a new data source for scientometric studies . recent activities to provide scientists with tools for expressing in a form of semantic linkages their knowledge , hypotheses and opinions about relationships between available information objects also support this trend . the study presents one of such activities performed within the socionet research information system with a special focus on ( a ) taxonomy of scientific relationships , which can exist between research objects , especially between research outputs and ( b ) a semantic segment of a research e infrastructure that includes a semantic interoperability support , a monitoring of changes in linkages and linked objects , notifications and a new model of scientific communication , and at lastscientometric indicators built by processing of semantic linkages data . based on knowledge what is a semantic linkage data and how it is stored in a research information system we propose an abstract computing model of a new data source . this model helps with better understanding what new indicators can be designed for scientometric studies . using current semantic linkages data collected in socionet we present some statistical experiments , including examples of indicators based on two data sets ( a ) what objects are linked and ( b ) what scientific relationships ( semantics ) are expressed by the linkages .
flash memory efficient ltl model checking . <eos> as the capacity and speed of flash memories in form of solid state disks grow , they are becoming a practical alternative for standard magnetic drives . currently , most solid state disks are based on nand technology and much faster than magnetic disks in random reads , while in random writes they are generally not . so far , large scale ltl model checking algorithms have been designed to employ external memory optimized for magnetic disks . we propose algorithms optimized for flash memory access . in contrast to approaches relying on the delayed detection of duplicate states , in this work , we design and exploit appropriate hash functions to re invent immediate duplicate detection . for flash memory efficient on the fly ltl model checking , which aims at finding any counter example to the specified ltl property , we study hash functions adapted to the two level hierarchy of ram and flash memory . for flash memory efficient off line ltl model checking , which aims at generating a minimal counterexample and scans the entire state space at least once , we analyze the effect of outsourcing a memory based perfect hash function from ram to flash memory . since the characteristics of flash memories are different to magnetic hard disks , the existing i o complexity model is no longer sufficient . therefore , we provide an extended model for the computation of the i o complexity adapted to flash memories that has a better fit to the observed behavior of our algorithms . ( c ) <digit> elsevier b.v. all rights reserved .
data fusion trees for detection does architecture matter . <eos> we consider the problem of decentralized detection in a network consisting of a large number of nodes arranged as a tree of bounded height , under the assumption of conditionally independent and identically distributed ( i.i.d. ) observations . we characterize the optimal error exponent under a neyman pearson formulation . we show that the type ii error probability decays exponentially fast with the number of nodes , and the optimal error exponent is often the same as that corresponding to a parallel configuration . we provide sufficient , as well as necessary , conditions for this to happen . for those networks satisfying the sufficient conditions , we propose a simple strategy that nearly achieves the optimal error exponent , and in which all non leaf nodes need only send <digit> bit messages .
conversion of generalization hierarchies and union types from extended entity relationship model to an xml logical model . <eos> this short paper proposes alternative rules for converting generalization specialization hierarchies and union types , defined in the extended entity relationship model , to an xml logical model . our approach considers all the possible constraints and constructs for generalization and union types , generating abstract schemas for the logical design of xml documents .
a qualitative process system for modeling nf b and ap <digit> gene regulation in immune cell biology research . <eos> an experiment oriented integrated model of the regulation of the biologically ubiquitous nf b and ap <digit> gene transcription promoters was built by extending a previously developed qualitative process system for simulating cell behavior in the immune system . the core knowledge base ( kb ) implemented a deep biological ontology including molecular , ultrastructural , cytological , histological , and organismic definitions . kb states , relationships , predicates , and heuristics also represented process interactions between reactive oxygen species , growth factors , and a variety of kinases phosphorylating intermediate molecules in the nf b and ap <digit> regulatory signaling pathways . the system successfully simulated the molecular process steps underlying outcomes of eight different molecular genetics laboratory experiments , including those dealing with nf b and ap <digit> regulation in immunodeficiency virus infection and tumor necrosis factor responses .
universality in two dimensional enhancement percolation . <eos> we consider a type of dependent percolation introduced in <digit> , where it is shown that certain enhancements of independent ( bernoulli ) percolation , called essential , make the percolation critical probability strictly smaller . in this study we first prove that , for two dimensional enhancements with a natural monotonicity property , being essential is also a necessary condition to shift the critical point . we then show that ( some ) critical exponents and the scaling limit of crossing probabilities of a two dimensional percolation process are unchanged if the process is subjected to a monotonic enhancement that is not essential . this proves a form of universality for all dependent percolation models obtained via a monotonic enhancement ( of bernoulli percolation ) that does not shift the critical point . for the case of site percolation on the triangular lattice , we also prove a stronger form of universality by showing that the full scaling limit <digit> , <digit> is not affected by any monotonic enhancement that does not shift the critical point . ( c ) <digit> wiley periodicals , inc .
proactive information caching for efficient resource discovery in a self structured grid . <eos> the cornerstone of successful deployment of large scale grid systems depends on efficient resource discovery mechanisms . in this respect , this paper presents a grid information system supported by a self structured overlay topology and proactive information caching . the proposed approach features an ant inspired self organized overlay construction that maintains a bounded diameter overlay , and a selective flooding based discovery algorithm that exploit local caches to reduce the number of visited nodes . the caches are periodically exchanged between neighboring nodes using an epidemic replication mechanism that is based on a gossiping algorithm , thus allowing nodes to have a more general view of the network and its resources . we conducted extensive experimentation that provides evidence that the average number of hops required to efficiently locate resources is limited and that our framework performs well with respect to hit rate and network overhead .
robust id based threshold signcryption scheme from pairings . <eos> recently bilinear pairings on elliptic curves have raised great interest in cryptographic community . based on their good properties , many excellent id based cryptographic schemes have been proposed . however , in these proposed schemes , the private key generator should be assumed trusted , while in real environment , this assumption does not always hold . to overcome this weakness , in this paper , we will use the threshold technology to devise a secure id based signcryption scheme . since the threshold technology is adopted not only in the master key management but also in the group signature , our scheme can achieve high security and resist some malicious attacks under a certain threshold .
non convex fuzzy data and fuzzy statistics a first descriptive approach to data analysis . <eos> lr fuzzy numbers are widely used in fuzzy set theory applications based on the standard definition of convex fuzzy sets . however , in some empirical contexts such as , for example , human decision making and ratings , convex representations might not be capable to capture more complex structures in the data . moreover , non convexity seems to arise as a natural property in many applications based on fuzzy systems ( e.g. , fuzzy scales of measurement ) . in these contexts , the usage of standard fuzzy statistical techniques could be questionable . a possible way out consists in adopting ad hoc data manipulation procedures to transform non convex data into standard convex representations . however , these procedures can artificially mask relevant information carried out by the non convexity property . to overcome this problem , in this article we introduce a novel computational definition of non convex fuzzy number which extends the traditional definition of lr fuzzy number . moreover , we also present a new fuzzy regression model for crisp input non convex fuzzy output data based on the fuzzy least squares approach . in order to better highlight some important characteristics of the model , we applied the fuzzy regression model to some datasets characterized by convex as well as non convex features . finally , some critical points are outlined in the final section of the article together with suggestions about future extensions of this work .
training a neural network to select dispatching rules in real time . <eos> dispatching rules are often suggested to schedule manufacturing systems in real time . numerous dispatching rules exist . unfortunately no dispatching rule ( dr ) is known to be globally better than any other . their efficiency depends on the characteristics of the system , operating condition parameters and the production objectives , several authors have demonstrated the benefits of changing dynamically these rules , so as to take into account the changes that can occur in the system state , a new approach based on neural networks ( nn ) is proposed here to select in real time , each time a resource becomes available , the most suited dr. the selection is made in accordance with the current system state and the workshop operating condition parameters . contrarily to the few learning approaches presented in the literature to select scheduling heuristics , no training set is needed . the nn parameters are determined through simulation optimization . the benefits of the proposed approach are illustrated through the example of a simplified flow shop already published . it is shown that the nn can automatically select efficient drs dynamically the knowledge is only generated from simulation experiments , which are driven by the optimization method . once trained offline , the resulting nn can be used online , in connection with the monitoring system of a flexible manufacturing system . ( c ) <digit> elsevier ltd. all rights reserved .
the optimisation of block layout and aisle structure by a genetic algorithm . <eos> the design of a manufacturing layout is incomplete without consideration of aisle structure for material handling . this paper presents a method to solve the layout and aisle structure problems simultaneously by a slicing floorplan . in this representation , the slicing lines are utilised as the aisles for a material handling system . the method decomposes the problem into two stages . the first stage minimises the material handling cost with aisle distance , and the second stage optimises the aisles in the aisle structure . a representation of slicing floorplan is introduced for the optimisation by genetic algorithms ( gas ) . the corresponding operators of the ga are also developed . computational tests demonstrate the goodness of the method . a comparison study of the ga and the random search ( rs ) for the problem was performed . it showed that the ga has a much higher efficiency than a rs , though further study is still needed to improve the efficiency of the ga. ( c ) <digit> elsevier science ltd. all rights reserved .
constrained high accuracy stereo reconstruction method for surgical instruments positioning . <eos> in this paper , a high accuracy stereo reconstruction method for surgery instruments positioning is proposed . usually , the problem of surgical instruments reconstruction is considered as a basic task in computer vision to estimate the <digit> d position of each marker on a surgery instrument from three pairs of image points . however , the existing methods considered the <digit> d reconstruction of the points separately thus ignore the structure information . meanwhile , the errors from light variation , imaging noise and quantization still affect the reconstruction accuracy . this paper proposes a method which takes the structure information of surgical instruments as constraints , and reconstructs the whole markers on one surgical instrument together . firstly , we calibrate the instruments before navigation to get the structure parameters . the structure parameters consist of markers ' number , distances between each markers and a linearity sign of each instrument . then , the structure constraints are added to stereo reconstruction . finally , weighted filter is used to reduce the jitter . experiments conducted on surgery navigation system showed that our method not only improve accuracy effectively but also reduce the jitter of surgical instrument greatly .
plastic collapse of circular conical shells under uniform external pressure . <eos> the article reports on two theoretical investigations and an experimental investigation into the collapse of six circular conical shells under uniform external pressure . four of the vessels collapsed through plastic non symmetric bifurcation buckling and one vessel collapsed through plastic axisymmetric buckling . a sixth vessel failed in a mixed mode of plastic non symmetric bifurcation buckling , combined with plastic axisymmetric buckling . the theoretical and experimental investigations appeared to indicate that there was a link between plastic non symmetric bifurcation buckling and plastic axisymmetric buckling . the theoretical investigations were via the finite element method and were used to provide a design chart for these vessels .
meta heuristics for reconstructing cross cut shredded text documents . <eos> in this work , we present two new approaches based on variable neighborhood search ( vns ) and ant colony optimization ( aco ) for the reconstruction of cross cut shredded text documents . for quickly obtaining initial solutions , we consider four different construction heuristics . while one of them is based on the well known algorithm of prim , another one tries to match shreds according to the similarity of their borders . two further construction heuristics rely on the fact that in most cases the left and right edges of paper documents are blank , i.e. no text is written on them . randomized variants of these construction heuristics are applied within the aco . experimental tests reveal that regarding the solution quality the proposed aco variants perform better than the vns approaches in most cases , while the running times needed are shorter for vns . the high potential of these approaches for reconstructing cross cut shredded text documents is underlined by the obtained results .
the cybernetics of architecture a tribute to the contribution of gordon pask . <eos> discusses the relation between cybernetics and architecture and pays tribute to gordon pask 's role and influence . indicates pask 's contribution to an increasingly environmentally responsive architectural theory that may lead to a more humane and ecologically conscious environment .
forecasting volatility based on wavelet support vector machine . <eos> one of the challenging problems in forecasting the conditional volatility of stock market returns is that general kernel functions in support vector machine ( svm ) can not capture the cluster feature of volatility accurately . while wavelet function yields features that describe of the volatility time series both at various locations and at varying time granularities , so this paper construct a multidimensional wavelet kernel function and prove it meeting the mercer condition to address this problem . the applicability and validity of wavelet support vector machine ( wsvm ) for volatility forecasting are confirmed through computer simulations and experiments on real world stock data .
the role of user capability and incentives in group and individual decision support systems an economics perspective . <eos> we model the decision making processes in decision support systems and programs as sequential information acquisition processes and compare their usefulness . a bayesian decision maker is shown to be indifferent between the two approaches . in contrast , a decision maker with bounded rationality prefers the decision support systems approach . the model is extended to group decision support systems where the interaction between the decision support systems approach . the model is extended to group decision support systems where the interaction between the decision makers and the group facilitator is modelled as a non cooperative economics game . we show that in some instances the group facilitator would prefer precommitment to an interaction plan rather than allow evolutionary planning of the interaction . this planning is similar to that in a program and may take the form of an organization chart .
using chi scores to reward honest feedback from repeated interactions . <eos> online communities increasingly rely on reputation information to foster cooperation and deter cheating . as rational agents can often benefit from misreporting their observations , explicit incentives must be created to reward honest feedback . reputation side payments ( e.g. , agents get paid for submitting feedback ) can be designed to make truth telling optimal . in this paper , we present a new side payment scheme adapted for settings where agents repeatedly submit feedback . we rate the feedback set of an agent , rather than individual reports . the chi score of the feedback set is computed based on a chi square independence test that assesses the correlation between the agent 's feedback and the feedback submitted by the rest of the community . the mechanism has intuitive appeal and generates significantly lower costs than existing incentive compatible reporting mechanisms .
embedding 0,1 <digit> cuts in a branch and cut framework a computational study . <eos> embedding cuts into a branch and cut framework is a delicate task , especially when a large set of cuts is available . in this paper we describe a separation heuristic for <digit> , <digit> <digit> cuts , a special case of chvatal gomory cuts , that tends to produce many violated inequalities within relatively short time . we report computational results on a large testbed of integer linear programming ( ilp ) instances of combinatorial problems including satisfiability , max satisfiability , and linear ordering problems , showing that a careful cut selection strategy produces a considerable speedup with respect to the cases in which either the separation heuristic is not used at all , or all of the cuts it produces are added to the lp relaxation .
<digit> antbal an ant colony optimisation algorithm for balancing two sided assembly lines . <eos> two sided assembly lines are a special type of assembly lines in which workers perform assembly tasks in both sides of the line . this type of lines is of crucial importance , especially in the assembly of large sized products , like automobiles , buses or trucks , in which some tasks must be performed at it specific side of the product . this paper presents ail approach to address the two sided mixed model assembly line balancing problem . first , a mathematical programming model is presented to formally describe the problem . then , an ant colony optimisation algorithm is proposed to solve the problem . in the proposed procedure two ants ' work ' simultaneously , one at each side of the line , to build a balancing solution which verifies the precedence , zoning , capacity , side and synchronism constraints of the assembly process . the main goal is to minimise the number of workstations of the line , but additional goals are also envisaged . the proposed procedure is illustrated with a numerical example and results of a computational experience that exhibit its superior performance are presented . ( c ) <digit> elsevier ltd. all rights reserved .
evaluation of icl670 , a once daily oral iron chelator in a phase iii clinical trial of thalassemia patients with transfusional iron overload . <eos> abstract osteoporosis and osteopenia are frequent complications of thalassemia major ( tm ) and intermedia ( ti ) . osteoporosis was found in <digit> <digit> patients with ti and in <digit> <digit> patients with tm . in tm , no association was found with specific polymorphisms in candidate genes ( vitamin d receptor , estrogen receptor , calcitonin receptor , and collagen type <digit> alpha <digit> ) . osteoporosis in tm female was strongly associated with primary amenorrhea ( p < .0001 ) , while in male patients with tm hypogonadism was not significantly related to bmd ( p .0001 ) . low bmd was also associated with cardiomiopathy ( p .01 ) , diabetes mellitus ( p .0001 ) , chronic hepatitis ( p .0029 ) , and increased alt ( p .01 ) .
maximum local energy an effective approach for multisensor image fusion in beyond wavelet transform domain . <eos> the benefits of multisensor fusion have motivated research in this area in recent years . redundant fusion methods are used to enhance fusion system capability and reliability . the benefits of beyond wavelets have also prompted scholars to conduct research in this field . in this paper , we propose the maximum local energy method to calculate the low frequency coefficients of images and compare the results with those of different beyond wavelets . an image fusion step was performed as follows first , we obtained the coefficients of two different types of images through beyond wavelet transform . second , we selected the low frequency coefficients by maximum local energy and obtaining the high frequency coefficients using the sum modified laplacian method . finally , the fused image was obtained by performing an inverse beyond wavelet transform . in addition to human vision analysis , the images were also compared through quantitative analysis . three types of images ( multifocus , multimodal medical , and remote sensing images ) were used in the experiments to compare the results among the beyond wavelets . the numerical experiments reveal that maximum local energy is a new strategy for attaining image fusion with satisfactory performance .
hla federate migration . <eos> the high level architecture ( hla ) is a standardized framework for distributed simulation that promotes reuse and interoperability of simulation components ( federates ) . federates are processes which communicate with each other in the simulation via the run time infrastructure ( rti ) . when running a large scale simulation over many nodes workstations , some may get more workload than others . to run the simulation as efficiently as possible , the workload should be uniformly distributed over the nodes . current rti implementations are very static , and do not allow any load balancing . load balancing of a hla federation can be achieved by scheduling new federates on the node with least load and migrating executing federates from a highly loaded node to a lightly loaded node . process migration has been a topic of research for many years , but not within the context of hla . this paper focuses on process migration within the hla framework .
online activity , motivation , and reasoning among adult learners . <eos> college students motivational beliefs influence their online behavior and ability to think critically . in the present study , doctoral health science students reports of motivation , as measured by the california measure of mental motivation , reasoning skill , as measured by the health science reasoning test , and web ct records of online activity during a web ct based statistics course were explored . critical thinking skill and disposition each contributed unique variance to student grades , with age , organization disposition , and analysis skill as the strongest predictors . the youngest students , those so called millennial age , and born after <digit> , were those with the lowest critical thinking skill and dispositions , and the lowest grades in the class . future research must take into consideration discrepancies between skill and disposition and interactions with age or cohort . at present , and contrary to popular wisdom , older students may make better online learners than younger .
automatic simulation and verification of pipelined microcontrollers . <eos> this paper presents a methodology for automatic simulation and verification of pipelined microcontrollers . using this methodology , we can generate the simulation for the instruction set architecture ( isa ) , abstract finite slate machine ( fsm ) and pipelined register transfer level design and compare the simulation results across different levels quickly . we have implemented our method in the simulation and verification of a synthesized microcontroller ht_4 using our behavioral synthesis tool .
constraint satisfaction for planning and scheduling problems . <eos> the areas of planning and scheduling ( from the artificial intelligence point of view ) have seen important advances thanks to application of constraint satisfaction techniques . currently , many important real world problems require efficient constraint handling for planning , scheduling and resource allocation to competing goal activities over time in the presence of complex state dependent constraints . solutions to these problems require integration of resource allocation and plan synthesis capabilities . hence to manage such complex problems planning , scheduling and constraint satisfaction must be interrelated . this special issue on constraint satisfaction for planning and scheduling problems compiles a selection of papers dealing with various aspects of applying constraint satisfaction techniques in planning and scheduling . the core of submitted papers was formed by the extended versions of papers presented at coplas ' <digit> icaps <digit> workshop on constraint satisfaction techniques for planning and scheduling problems . this issue presents novel advances on planning , scheduling , constraint programming constraint satisfaction problems ( csps ) and many other common areas that exist among them . on the whole , this issue mainly focus on managing complex problems where planning , scheduling , constraint satisfaction and search must be combined and or interrelated , which entails an enormous potential for practical applications and future research .
induction of m convex functions by linking systems . <eos> induction ( or transformation ) by bipartite graphs is one of the most important operations on matroids , and it is well known that the induction of a matroid by a bipartite graph is again a matroid . as an abstract form of this fact , the induction of a matroid by a linking system is known to be a matroid . m convex functions are quantitative extensions of matroidal structures , and they are known as discrete convex functions . as with matroids , it is known that the induction of an m convex function by networks generates an m convex function . as an abstract form of this fact , this paper shows that the induction of an m convex function by linking systems generates an m convex function . furthermore , we show that this result also holds for m convex functions on constant parity jump systems . previously known operations such as aggregation , splitting , and induction by networks can be understood as special cases of this construction .
some scheduling problems with sum of processing times based and job position based learning effects . <eos> in this paper we introduce a new scheduling model with learning effects in which the actual processing time of a job is a function of the total normal processing times of the jobs already processed and of the jobs scheduled position . we show that the single machine problems to minimize makespan and total completion time are polynomially solvable . in addition , we show that the problems to minimize total weighted completion time and maximum lateness are polynomially solvable under certain agreeable conditions . finally , we present polynomial time optimal solutions for some special cases of the m machine flowshop problems to minimize makespan and total completion time .
on the expressiveness of single pass instruction sequences . <eos> we perceive programs as single pass instruction sequences . a single pass instruction sequence under execution is considered to produce a behaviour to be controlled by some execution environment . threads as considered in basic thread algebra model such behaviours . we show that all regular threads , i.e. threads that can only be in a finite number of states , can be produced by single pass instruction sequences without jump instructions if use can be made of boolean registers . we also show that , in the case where goto instructions are used instead of jump instructions , a bound to the number of labels restricts the expressiveness .
adaptive two stage qos provisioning schemes for cdma networks . <eos> in this paper , adaptive qos provisioning schemes are proposed for cdma wireless networks . the proposed schemes consist of two stages . in the first stage , a call admission control ( cac ) and a bandwidth allocation schemes are proposed to determine the bandwidth assignment for new connections according to available bandwidth and adaptable range of active connections . the proposed methods utilize the markov chain model to estimate the required bandwidth for all connections . this markov chain model considers the traffic characteristics of both the incoming call request and the existing connections , and estimates the resulting quality of service ( qos ) . in the second stage , net cbk and net share schemes are presented to regulate the active connections according to call blocking rates and unused bandwidth . from the simulation results , the proposed schemes are able to carry more connections and improve system utilization .
a fine motor skill training system using multi fingered haptic interface robot . <eos> we proposed optimal switching method for transferring multiple fingertip forces . based on the proposed method , a haptic training system was developed . the proposed method enhanced the training effect compared with our earlier method .
visualization of bubble fluid interaction by a moving object flow image analyzer system . <eos> this paper deals with interaction between a bubble and fluid around it , visualized by a moving object flow image analyzer ( mofia ) consisting of a three dimensional ( 3d ) moving object image analyzer ( moia ) and two dimensional particle image velocimetry ( piv ) . the experiments were carried out for rising bubbles of various sizes and shapes in stagnant water in a vertical pipe . in the mofia employed , 3d moia was used to measure bubble motion and piv to measure fluid flow . the 3d position and shape of a bubble and the velocity field were measured simultaneously . the experimental results showed that the interaction was characterized by the shape , size and density of a bubble . concretely , they showed the characteristics of bubble motion , wake shedding , and flow field .
fibonacci ( p p , r r ) cubes which are median graphs . <eos> the fibonacci ( p p , r r ) cube is an interconnection topology , which unifies a wide range of connection topologies , such as hypercube , fibonacci cube , postal network , etc. it is known that the fibonacci cubes are median graphs s. klavar , on median nature and enumerative properties of fibonacci like cubes , discrete math . <digit> ( <digit> ) <digit> . the question for determining which fibonacci ( p p , r r ) cubes are median graphs is solved completely in this paper . we show that fibonacci ( p p , r r ) cubes are median graphs if and only if either r p r p and r <digit> r <digit> , or p <digit> p <digit> and r n r n .
fast isogeometric boundary element method based on independent field approximation . <eos> we present an alternative isogeometric bem for elasticity on nurbs patches . boundary data and geometry are approximated independently which avoids redundancies . hierarchical matrices provide almost linear computational complexity . comparison of different ansatz functions on nurbs patches . the results show optimal convergence for all tested orders .
expectations of random sets and their boundaries using oriented distance functions . <eos> shape estimation and object reconstruction are common problems in image analysis . mathematically , viewing objects in the image plane as random sets reduces the problem of shape estimation to inference about sets . currently existing definitions of the expected set rely on different criteria to construct the expectation . this paper introduces new definitions of the expected set and the expected boundary , based on oriented distance functions . the proposed expectations have a number of attractive properties , including inclusion relations , convexity preservation and equivariance with respect to rigid motions . the paper introduces a special class of decomposable oriented distance functions for parametric sets and gives the definition and properties of decomposable random closed sets . further , the definitions of the empirical mean set and the empirical mean boundary are proposed and empirical evidence of the consistency of the boundary estimator is presented . in addition , the paper discusses loss functions for set inference in frequentist framework and shows how some of the existing expectations arise naturally as optimal estimators . the proposed definitions are illustrated on theoretical examples and real data .
a numerical comparison of several unconditional exact tests in problems of equivalence based on the difference of proportions . <eos> in order to compare two independent proportions ( p ( <digit> ) and p ( <digit> ) ) there are several useful tests for the parameter d p ( <digit> ) p ( <digit> ) h ( sg ) d delta , h ( sg2 ) d delta vs. k ( sg2 ) d not equal delta ( where <digit> delta ( where delta > <digit> ) and h ( pe ) vertical bar d vertical bar > delta vs. k ( pe ) vertical bar d vertical bar <digit> ) . the exact unconditional test requires an ordering statistic , which is usually the z pooled statistic , to be defined . the paper gives the definition of <digit> new ordering statistics with a similar computational time , and compares the number of points which each introduces into the critical region obtained to error alpha <digit> % . the article reaches the conclusion that the most generally powerful statistics are the z pooled one with a small continuity correction ( c <digit> n if n ( <digit> ) not equal n ( <digit> ) or c <digit> n if n ( <digit> ) n ( <digit> ) , where n n ( <digit> ) <digit> n ( <digit> ) <digit> and n ( i ) are the sample sizes ) and those z pooled with yates ' continuity correction ( c n ( <digit> ) n ( <digit> ) 2n ( <digit> ) n ( <digit> ) ) . in this paper the author also showed that barnard 's two classic convexity conditions are redundant , because when one of them is verified the other is also verified . the programs for these tests may be obtained free of charge from the site http www.ugr.es local bioest software.htm .
minimum energy control of positive continuous time linear systems with bounded inputs . <eos> the minimum energy control problem for positive continuous time linear systems with bounded inputs is formulated and solved . sufficient conditions for the existence of a solution to the problem are established . a procedure for solving the problem is proposed and illustrated with a numerical example .
relevance zone oriented proof search for connect6 . <eos> wu and huang ( advances in computer games , pp. <digit> <digit> , <digit> ) presented a new family of k in a row games , among which connect6 ( a kind of six in a row ) attracted much attention . for connect6 as well as the family of k in a row games , this paper proposes a new threat based proof search method , named relevance zone oriented proof ( rzop ) search , developed from the lambda search proposed by thomsen ( int . comput . games assoc. j. , vol . <digit> , no. <digit> , pp. <digit> <digit> , <digit> ) . the proposed rzop search is a novel , general , and elegant method of constructing and promoting relevance zones . using this method together with a proof number search , this paper solved effectively and successfully many new connect6 game positions , including several connect6 openings , especially the mickey mouse opening , which used to be one of the popular openings before we solved it .
interaction of tcp flows as billiards . <eos> the aim of this paper is to analyze the performance of a large number of long lived tcp controlled flows sharing many routers ( or links ) , from the knowledge of the network parameters ( capacity , buffer size , topology ) and of the characteristics of each tcp flow ( rtt , route etc. ) when taking synchronization into account . it is shown that in the small buffer case , the dynamics of such a network can be described in terms of iterate of random piecewise affine maps , or geometrically as a billiards in the euclidean space with as many dimensions as the number of flow classes and as many reflection facets as there are routers . this class of billiards exhibits both periodic and nonperiodic asymptotic oscillations , the characteristics of which are extremely sensitive to the parameters of the network . it is also shown that for large populations and in the presence of synchronization , aggregated throughputs exhibit fluctuations that are due to the network as a whole , that follow some complex fractal patterns , and that come on top of other and more classical flow or packet level fluctuations . the consequences on tcp 's fairness are exemplified on a few typical cases of small dimension .
feature selection , mutual information , and the classification of high dimensional patterns . <eos> we propose a novel feature selection filter for supervised learning , which relies on the efficient estimation of the mutual information between a high dimensional set of features and the classes . we bypass the estimation of the probability density function with the aid of the entropic graphs approximation of rnyi entropy , and the subsequent approximation of the shannon entropy . thus , the complexity does not depend on the number of dimensions but on the number of patterns samples , and the curse of dimensionality is circumvented . we show that it is then possible to outperform algorithms which individually rank features , as well as a greedy algorithm based on the maximal relevance and minimal redundancy criterion . we successfully test our method both in the contexts of image classification and microarray data classification . for most of the tested data sets , we obtain better classification results than those reported in the literature .
analyzing the techniques that improve fault tolerance of aggregation trees in sensor networks . <eos> sensor networks are finding significant applications in large scale distributed systems . one of the basic operations in sensor networks is in network aggregation . among the various approaches to in network aggregation , such as gossip and tree , including the hash based techniques , the tree based approaches have better performance and energy saving characteristics . however , sensor networks are highly prone to failures . numerous techniques suggested in the literature to counteract the effect of failures have not been carefully analyzed . in this paper , we focus on the performance of these tree based aggregation techniques in the presence of failures . first , we identify a fault model that captures the important failure traits of the system . then , we analyze the correctness of simple tree aggregation with our fault model . we then use the same fault model to analyze the techniques that utilize redundant trees to improve the variance . the impact of techniques for maintaining the correctness under faults , such as rebuilding or locally fixing the tree , is then studied under the same fault model . we also do the cost benefit analysis of using the hash based schemes which are based on fm sketches , we conclude that these fault tolerance techniques for tree aggregation do not necessarily result in substantial improvement in fault tolerance . ( c ) <digit> published by elsevier inc .
lean government and platform based governancedoing more with less . <eos> governments from all over the world are looking for ways to reduce costs while at the same time to stimulate innovation . while pursuing both objectives , governments face a major challengeto operate in a connected environment , engage stakeholders and solve societal problems by utilizing new methods , tools , practices and governance models . as result , fundamental changes are taking place on how government operates . such changes are under the larger umbrella of lean government ( l government ) . lean government is a new wave which is appearing as a response to traditional approacheslike electronic government ( e government ) and transformational government ( t government ) , and aims at reducing the complexity of the public sector by simplifying and streamlining organizational structures and processes , at the same time at stimulating innovation by mobilizing stakeholders . in l government , public organizations introduce platforms facilitating innovation and interactions with other public organizations , business and citizens , and focus on their orchestration role . experimentation , assessment and gradual improvement based on user requirements are key factors for realizing l government .
resolving ip aliases in building traceroute based internet maps . <eos> alias resolution , the task of identifying ip addresses belonging to the same router , is an important step in building traceroute based internet topology maps . inaccuracies in alias resolution affect the representativeness of constructed topology maps . this in turn affects the conclusions derived from studies that use these maps . this paper presents two complementary studies on alias resolution . first , we present an experimental study to demonstrate the impact of alias resolution on topology measurement studies . then , we introduce an alias resolution approach called analytic and probe based alias resolver ( apar ) . apar consists of an analytical component and a probe based component . given a set of path traces , the analytical component utilizes the common ip address assignment scheme to infer ip aliases . the probe based component introduces a minimal probing overhead to improve the accuracy of apar . compared to the existing state of the art tool ally , apar uses an orthogonal approach to resolve a large number of ip aliases that ally fails to identify . our extensive verification study on sample data sets shows that our approach is effective in resolving many aliases with good accuracy . our evaluations also indicate that the two approaches ( ally and apar ) should be used together to maximize the success of the alias resolution process .
beta p a novel approach to filter out malicious rating profiles from recommender systems . <eos> recommender systems are widely deployed to provide user purchasing suggestion on ecommerce websites . the technology that has been adopted by most recommender systems is collaborative filtering . however , with the open nature of collaborative filtering recommender systems , they suffer significant vulnerabilities from being attacked by malicious raters , who inject profiles consisting of biased ratings . in recent years , several attack detection algorithms have been proposed to handle the issue . unfortunately , their applications are restricted by various constraints . pca based methods while having good performance on paper , still suffer from missing values that plague most user item matrixes . classification based methods require balanced numbers of attacks and normal profiles to train the classifiers . the detector based on spc ( statistical process control ) assumes that the rating probability distribution for each item is known in advance . in this research , beta protection ( beta p ) is proposed to alleviate the problem without the abovementioned constraints . beta p grounds its theoretical foundation on beta distribution for easy computation and has stable performance when experimenting with data derived from the public websites of movielens . ( c ) <digit> elsevier b.v. all rights reserved .
on the lifetime of node to node communication in wireless ad hoc networks . <eos> lifetime of node to node communication in a wireless ad hoc network is defined as the duration that two nodes can communicate with each other . failure of the two nodes or failure of the last available route between them ends their communication . in this paper , we analyze the maximum lifetime of node to node communication in static ad hoc networks when alternative routes that keep the two nodes connected to each other are node disjoint . we target ad hoc networks with random topology modeled as a random geometric graph . the analysis is provided for ( <digit> ) networks that support automatic repeat request ( arq ) at the medium access control level and ( <digit> ) networks that do not support arq . on the basis of this analysis , we propose numerical algorithms to predict at each moment of network operation , the maximum duration that two nodes can still communicate with each other . then , we derive a closed form expression for the expected value of maximum node to node communication lifetime in the network . as a byproduct of our analysis , we also derive upper and lower bounds on the lifetime of node disjoint routes in static ad hoc networks . we verify the accuracy of our analysis using extensive simulation studies . ( c ) <digit> elsevier b.v. all rights reserved .
human factors in the adoption and performance of advanced manufacturing technology in unionized firms . <eos> some researchers have found that unionized firms are less likely to pursue automation because high wage demands deprive them of the necessary capital required to invest in advanced manufacturing technology ( amt ) . it has also been suggested that stringent work rules and technology agreements can make the substitution of new technology for union labor too expensive . others have found , however , that the pursuit of high wage policies and the resultant requirement for improved worker and machine productivity can create a positive environment for technological change . this exploratory study examines the relationships between firm level union status and the adoption and performance of amt in the discrete parts durable goads manufacturing industry . analyses of our sample , which included chi square tests , t tests , correlation analyses and multiple linear regression analyses . revealed a union effect an the adoption of just in time technology and a moderately positive union effect on performance . results of analyses of the impact of union status , firm sire and several human factor variables on firm performance are also presented and discussed .
dividing protein interaction networks for modular network comparative analysis . <eos> the increasing growth of data on protein protein interaction ( ppi ) networks has boosted research on their comparative analysis . in particular , recent studies proposed models and algorithms for performing network alignment , that is , the comparison of networks across species for discovering conserved functional complexes . in this paper , we present an algorithm for dividing ppi networks , prior to their alignment , into small sub graphs that are likely to cover conserved complexes . this allows one to perform network alignment in a modular fashion , by acting on pairs of resulting small sub graphs from different species . the proposed dividing algorithm combines a graph theoretical property ( articulation ) with a biological one ( orthology ) . extensive experiments on various ppi networks are conducted in order to assess how well the sub graphs generated by this dividing algorithm cover protein functional complexes and whether the proposed pre processing step can be used for enhancing the performance of network alignment algorithms . source code of the dividing algorithm is available upon request for academic use . ( c ) <digit> elsevier b.v. all rights reserved .
clustering with the multivariate normal inverse gaussian distribution . <eos> many model based clustering methods are based on a finite gaussian mixture model . the gaussian mixture model implies that the data scatter within each group is elliptically shaped . hence non elliptical groups are often modeled by more than one component , resulting in model over fitting . an alternative is to use a meanvariance mixture of multivariate normal distributions with an inverse gaussian mixing distribution ( mnig ) in place of the gaussian distribution , to yield a more flexible family of distributions . under this model the component distributions may be skewed and have fatter tails than the gaussian distribution . the mnig based approach is extended to include a broad range of eigendecomposed covariance structures . furthermore , mnig models where the other distributional parameters are constrained is considered . the bayesian information criterion is used to identify the optimal model and number of mixture components . the method is demonstrated on three sample data sets and a novel variation on the univariate kolmogorovsmirnov test is used to assess goodness of fit .
linear multiuser receivers effective interference , effective bandwidth and user capacity . <eos> multiuser receivers improve the performance of spread spectrum and antenna array systems by exploiting the structure of the multiaccess interference when demodulating the signal of a user . much of the previous work on the performance analysis of multiuser receivers has focused on their ability to reject worst case interference . their performance in a power controlled network and the resulting user capacity are less well understood . in this paper , me show that in a large system with each user using random spreading sequences , the limiting interference effects under several linear multiuser receivers can be decoupled , such that each interferer can be ascribed a level of effective interference that it provides to the user to be demodulated , applying these results to the uplink of a single power controlled cell , we derive an effective bandwidth characterization of the user capacity the signal to interference requirements of all the users can be met if and only if the sum of the effective bandwidths of the users is less than the total number of degrees of freedom in the system . the effective bandwidth of a user depends only on its own sir requirement , and simple expressions are derived for three linear receivers the conventional matched filter , the decorrelator , and the mmse receiver . the effective bandwidths under the three receivers serve as a basis for performance comparison .
the multiple originator broadcasting problem in graphs . <eos> given a graph g g and a vertex subset s s of v ( g ) v ( g ) , the broadcasting time with respect to s s , denoted by b ( g , s ) b ( g , s ) , is the minimum broadcasting time when using s s as the broadcasting set . and the k k broadcasting number , denoted by bk ( g ) b k ( g ) , is defined by bk ( g ) min b ( g , s ) s v ( g ) , s k b k ( g ) min b ( g , s ) s v ( g ) , s k . given a graph g g and two vertex subsets s s , s s of v ( g ) v ( g ) , define d ( v , s ) min u s d ( v , u ) , d ( s , s ) min d ( u , v ) u s d ( s , s ) min d ( u , v ) u s , v s v s , and d ( g , s ) max v v ( g ) d ( v , s ) for all v v ( g ) v v ( g ) . for all k k , <digit> k v ( g ) <digit> k v ( g ) , the k k radius of g g , denoted by rk ( g ) r k ( g ) , is defined as rk ( g ) min d ( g , s ) s v ( g ) r k ( g ) min d ( g , s ) s v ( g ) , s k s k . in this paper , we study the relation between the k k radius and the k k broadcasting numbers of graphs . we also give the <digit> <digit> radius and the <digit> <digit> broadcasting numbers of the grid graphs , and the k k broadcasting numbers of the complete n n partite graphs and the hypercubes .
a concurrent rule scheduling algorithm for active rules . <eos> the use of rules in a distributed environment creates new challenges for the development of active rule execution models . in particular , since a single event can trigger multiple rules that execute over distributed sources of data , it is important to make use of concurrent rule execution whenever possible . this paper presents the details of the integration rule scheduling ( irs ) algorithm . integration rules are active database rules that are used for component integration in a distributed environment . the irs algorithm identifies rule conflicts for multiple rules triggered by the same event through static , compile time analysis of the read and write sets of each rule . a unique aspect of the algorithm is that the conflict analysis includes the effects of nested rule execution that occurs as a result of using an execution model with an immediate coupling mode . the algorithm therefore identifies conflicts that may occur as a result of the concurrent execution of different rule triggering sequences . the rules are then formed into a priority graph before execution , defining the order in which rules triggered by the same event should be processed . rules with the same priority can be executed concurrently . the irs algorithm guarantees confluence in the final state of the rule execution . the irs algorithm is applicable for rule scheduling in both distributed and centralized rule execution environments .
identification of human implicit visual search intention based on eye movement and pupillary analysis . <eos> we propose a novel approach for the identification of human implicit visual search intention based on eye movement patterns and pupillary analysis , in general , as well as pupil size , gradient of pupil size variation , fixation length and fixation count corresponding to areas of interest , and fixation count corresponding to non areas of interest , in particular . the proposed model identifies human implicit visual search intention as task free visual browsing or task oriented visual search . task oriented visual search is further identified as task oriented visual search intent generation , task oriented visual search intent maintenance , or task oriented visual search intent disappearance . during a visual search , measurement of the pupillary response is greatly influenced by external factors such the intensity and size of the visual stimulus . to alleviate the effects of external factors , we propose a robust baseline model that can accurately measure the pupillary response . graphical representation of the measured parameter values shows significant differences among the different intent conditions , which can then be used as features for identification . by using the eye movement patterns and pupillary analysis , we can detect the transitions between different implicit intentionstask free visual browsing intent to task oriented visual search intent and task oriented visual search intent maintenance to task oriented visual search intent disappearanceusing a hierarchical support vector machine . in the proposed model , the hierarchical support vector machine is able to identify the transitions between different intent conditions with greater than <digit> % accuracy .
a methodology and tools for applying context specific usability guidelines to interface design . <eos> this paper presents a methodology and an associated technology to create context specific usability guidelines . the objective is to transform usability guidelines into a proactive resource that software developers can employ early and often in the development process . the methodology ensures conformance with established guidelines , but has the flexibility to use design experiences to adapt the guidelines to meet the emergent and diverse requirements of modern user interface design . case based and organizational learning technology is used to support the methodology and provide valuable resources for software developers . ( c ) <digit> elsevier science b.v. all rights reserved .
towards an engineering tool for implementing reusable distributed control systems . <eos> the iec model for distributed control systems ( dcss ) was adopted for the implementation of a new generation engineering tool . however , it was found that this approach does not exploit all the benefits of the object and component technologies . in this paper , we present the enhanced <digit> layer architecture that proved to be very helpful in the identification of the key abstractions required for the design of the new generation of function block based engineering tools . despite being iec compliant , the proposed approach introduces a number of extensions and modifications to the iec model to improve the development process . the unified modelling language is exploited during the requirements phase of dcss , but the use of the fb construct is confected during the design phase .
power allocation in cooperative communication system based on stackelberg game . <eos> cooperative communication has great potential to improve the wireless channel capacity by exploiting the antennas on wireless devices for spatial diversity . the performance in cooperative communication depends on careful resource allocation such as relay selection and power control . in this paper , the network is expanded and more than one source is used . what is proposed is a distributed buyer seller game theoretic framework over multiuser cooperative communication networks in order to stimulate cooperation and improve the system performance . a two level stackelberg game is employed to jointly consider the benefits of the source node and the relay nodes in which the source node is modeled as a buyer and the relay nodes are modeled as sellers , respectively . in this work we proposed coded method in which relays amplify and code source data and send it to destination at the same time and then signal detection occur in destination , but in the codeless network relays send source data separately to destination . so , here coded and codeless networks are compared and contrasted . the stimulation results revealed that the proposed coded method performed better than the codeless ones furthermore , the research shows that relays near the sources can play a significant role in increasing source nodes utility , so every source would like to buy more power from these preferred relays . also , the proposed algorithm enforces truthful power demands .
inhibitory conductance dynamics in cortical neurons during activated states . <eos> during activated states in vivo , neocortical neurons are subject to intense synaptic activity and high amplitude membrane potential ( vm ) ( v m ) fluctuations . these high conductance states may strongly affect the integrative properties of cortical neurons . we investigated the responsiveness of cortical neurons during different states using a combination of computational models and in vitro experiments ( dynamic clamp ) in the visual cortex of adult guinea pigs . spike responses were monitored following stochastic conductance injection in both experiments and models . we found that cortical neurons can operate in a continuum between two different modes during states with equal excitatory and inhibitory conductances , the firing is mostly correlated with an increase in excitatory conductance , which is a rather classic scenario . in contrast , during states dominated by inhibition , the firing is mostly related to a decrease in inhibitory conductances ( dis inhibition ) . this model prediction was tested experimentally using dynamic clamp , and the same modes of firing were identified . we also found that the signature of spikes evoked by dis inhibition is a transient drop of the total membrane conductance prior to the spike , which is typical of states with dominant inhibitory conductances . such a drop should be identifiable from intracellular recordings in vivo , which would provide an important test for the presence of inhibition dominated states . in conclusion , we show that in artificial activated states , not only inhibition can determine the conductance state of the membrane , but inhibitory inputs may also have a determinant influence on spiking . future analyses and models should focus on verifying if such a determinant influence of inhibitory conductance dynamics is also present in vivo .
centralized fleet management system for cybernetic transportation . <eos> in this article , we present a centralized fleet management system ( cfms ) for cybernetic vehicles called cybercars . cybercars are automatically guided vehicles for passenger transport on dedicated networks like amusement parks , shopping centres etc. the users make reservations for the vehicles through phone , internet , kiosk etc and the cfms schedules the cybercars to pick the users at their respective stations at desired time intervals . the cfms has centralized control of the routing network and performs pooling of customer requests , scheduling and routing of cybercars to customers , empty cybercars to new services or parking stations and those running below their threshold battery levels to recharging stations . the challenges before cfms are to assure conflict free routing , accommodate immediate requests from customers , dynamic updation of vehicle paths and minimize congestion on the whole network . we present the approaches used by cfms to ensure these functionalities and demonstrate a numerical illustration on a test network .
signal processing interpolation educational workbench . <eos> this article presents an educational tool to be used in signal processing interpolation related subjects . the aim is to contribute to the better consolidation of acquired theoretical knowledge , allowing students to test signal reconstruction algorithms and visualize the results obtained by the usage of such algorithms , and how several parameters affect their convergence and performance . <digit> wiley periodicals , inc. comput appl eng educ 20 356363 , <digit>
unsupervised view and rate invariant clustering of video sequences . <eos> videos play an ever increasing role in our everyday lives with applications ranging from news , entertainment , scientific research , security and surveillance . coupled with the fact that cameras and storage media are becoming less expensive , it has resulted in people producing more video content than ever before . this necessitates the development of efficient indexing and retrieval algorithms for video data . most state of the art techniques index videos according to the global content in the scene such as color , texture , brightness , etc. in this paper , we discuss the problem of activity based indexing of videos . to address the problem , first we describe activities as a cascade of dynamical systems which significantly enhances the expressive power of the model while retaining many of the computational advantages of using dynamical models . second , we also derive methods to incorporate view and rate invariance into these models so that similar actions are clustered together irrespective of the viewpoint or the rate of execution of the activity . we also derive algorithms to learn the model parameters from a video stream and demonstrate how a single video sequence may be clustered into different clusters where each cluster represents an activity . experimental results for five different databases show that the clusters found by the algorithm correspond to semantically meaningful activities .
closed inter sequence pattern mining . <eos> inter sequence pattern mining can find associations across several sequences in a sequence database , which can discover both a sequential pattern within a transaction and sequential patterns across several different transactions . however , inter sequence pattern mining algorithms usually generate a large number of recurrent frequent patterns . we have observed mining closed inter sequence patterns instead of frequent ones can lead to a more compact yet complete result set . therefore , in this paper , we propose a model of closed inter sequence pattern mining and an efficient algorithm called cisp miner for mining such patterns , which enumerates closed inter sequence patterns recursively along a search tree in a depth first search manner . in addition , several effective pruning strategies and closure checking schemes are designed to reduce the search space and thus accelerate the algorithm . our experiment results demonstrate that the proposed cisp miner algorithm is very efficient and outperforms a compared eisp miner algorithm in most cases .
symmetry detection by generalized complex ( gc ) moments a close form solution . <eos> this paper presents a unified method for detecting both reflection symmetry and rotation symmetry of 2d images based on an identical set of features , i.e. , the first three nonzero generalized complex ( gc ) moments . this method is theoretically guaranteed to detect all the axes of symmetries of every 2d image , if more nonzero gc moments are used in the feature set . furthermore , we establish the relationship between reflectional symmetry and rotational symmetry in an image , which can be used to check the correctness of symmetry detection . this method has been demonstrated experimentally using more than <digit> images .
on drunken sinusoids and their fourier series . <eos> the fourier series of trigonometric exponential functions f ( alpha ) ( x ) sin ( x alpha ) exp ( cos ( x ) ) are studied . dual recursive formulae for the corresponding fourier coefficients are derived . the above coefficients are transcendental . ( c ) <digit> elsevier science b.v. all rights reserved .
automated abstractions for contract validation . <eos> pre postcondition based specifications are commonplace in a variety of software engineering activities that range from requirements through to design and implementation . the fragmented nature of these specifications can hinder validation as it is difficult to understand if the specifications for the various operations fit together well . in this paper , we propose a novel technique for automatically constructing abstractions in the form of behavior models from pre postcondition based specifications . abstraction techniques have been used successfully for addressing the complexity of formal artifacts in software engineering however , the focus has been , up to now , on abstractions for verification . our aim is abstraction for validation and hence , different and novel trade offs between precision and tractability are required . more specifically , in this paper , we define and study enabledness preserving abstractions , that is , models in which concrete states are grouped according to the set of operations that they enable . the abstraction results in a finite model that is intuitive to validate and which facilitates tracing back to the specification for debugging . the paper also reports on the application of the approach to two industrial strength protocol specifications in which concerns were identified .
communication delay distribution dependent networked control for a class of t s fuzzy systems . <eos> this paper investigates a robust networked control for a class of takagi sugeno ( t s ) fuzzy systems . the controller design specifically takes probabilistic interval distribution of the communication delay into account . a general framework of networked control is first proposed . the two main features are <digit> ) the zero order hold can choose the latest control input signal when the packets received are out of order , and <digit> ) as the result of <digit> ) , the models of the all kinds of uncertainties in networked signal transfer including network induced delay and data packet dropout are under a unified framework . next , if the probability distribution of communication delay is known or specified in a design process , sufficient stability conditions for networked t s fuzzy systems are derived , which are based on the lyapunov theory . following this , a stabilizing controller design method is developed , which shows that the solvability of the design depends not only on the upper and lower bounds of the delay but on its probability distribution as well . finally , a numerical example is used to show the application of the theoretical results obtained in this paper .
branched polymers and hyperplane arrangements . <eos> we generalize the construction of connected branched polymers and the notion of the volume of the space of connected branched polymers studied by brydges and imbrie ( ann math , 158 1019 1039 , <digit> ) , and kenyon and winkler ( am math mon , <digit> ( <digit> ) <digit> <digit> , <digit> ) to any central hyperplane arrangement . the volume of the resulting configuration space of connected branched polymers associated to the hyperplane arrangement is expressed through the value of the characteristic polynomial of at <digit> . we give a more general definition of the space of branched polymers , where we do not require connectivity , and introduce the notion of q volume for it , which is expressed through the value of the characteristic polynomial of at . finally , we relate the volume of the space of branched polymers to broken circuits and show that the cohomology ring of the space of branched polymers is isomorphic to the orlik solomon algebra .
mobile dynamic content distribution networks . <eos> mobile networks are becoming increasingly popular as means for distributing inform tion to large number of users . in comparison to wired networks , mobile networks distinguished by potentially much higher variability in demand due to user mobility . most previous content techniques ssume static client demand distribution and , thus , may not perform well in mobile networks.this paper proposes and analyzes mobile dynamic content distribution network model , which takes demand variations into account to decide whether to replicate content and whether to remove previously created replicas in order to minimize total network traffic . we develop two solutions to our model an offline optimal , which provides an ideal lower bound on the total traffic , and practical heuristic online algorithm , which uses demand forecasting to make replication decisions . we provide thorough evaluation of our solutions , comparing them against acdn , the only previous dynamic content placement algorithm targeting bandwidth minimization that we are aware of . our results show that our online algorithm significantly outperforms the acdn one , reducing total network traffic by up to <digit> % in a number of experiments covering large system design space .
an empirical investigation of the effects of data warehousing on decision performance . <eos> organizations implement data warehouses to overcome the limitations of dss by adding this database component and thereby improve decision performance . however , no empirical evidence is available to show the effects of a data warehouse ( dw ) on decision quality and performance . to examine this , a laboratory experiment was conducted . the data warehouse variables considered were the time horizon of the data and its level of aggregation . it was found that using a full data warehouse resulted in significantly better performance and that using it resulted in better performance than using a partial data warehouse ( long time history with no aggregated data ) . however , using a partial data warehouse was not significantly better than not using a data warehouse at all .
measuring and modelling the performance of a parallel odmg compliant object database server . <eos> object database management systems ( odbmss ) are now established as the database management technology of choice for a range of challenging data intensive applications . furthermore , the applications associated with object databases typically have stringent performance requirements , and some are associated with very large data sets . an important feature for the performance of object databases is the speed at which relationships can be explored . in queries , this depends on the effectiveness of different join algorithms into which queries that follow relationships can be compiled . this paper presents a performance evaluation of the polar parallel object database system , focusing in particular on the performance of parallel join algorithms . polar is a parallel , shared nothing implementation of the object database management group ( odmg ) standard for object databases . the paper presents an empirical evaluation of queries expressed in the odmg query language ( oql ) , as well as a cost model for the parallel algebra that is used to evaluate oql queries . the cost model is validated against the empirical results for a collection of queries using four different join algorithms , one that is value based and three that are pointer based . copyright ( c ) <digit> john wiley sons , ltd .
performance comparison of machine learning methods for prognosis of hormone receptor status in breast cancer tissue samples . <eos> we examined the classification and prognostic scoring performances of several computer methods on different feature sets to obtain objective and reproducible analysis of estrogen receptor status in breast cancer tissue samples . radial basis function network , k nearest neighborhood search , support vector machines , naive bayes , functional trees , and k means clustering algorithm were applied to the test datasets . several features were employed and the classification accuracies of each method for these features were examined . the assessment results of the methods on test images were also experimentally compared with those of two experts . according to the results of our experimental work , a combination of functional trees and the naive bayes classifier gave the best prognostic scores indicating very good kappa agreement values ( 0.899 and 0.949 , p < 0.001 ) with the experts . this combination also gave the best dichotomization rate ( 96.3 % ) for assessment of estrogen receptor status . wavelet color features provided better classification accuracy than laws texture energy and co occurrence matrix features .
corpus based hit mw database for offline recognition of general purpose chinese handwritten text . <eos> a chinese handwriting database named hit mw is presented to facilitate the offline chinese handwritten text recognition . both the writers and the texts for handcopying are carefully sampled with a systematic scheme . to collect naturally written handwriting , forms are distributed by postal mail or middleman instead of face to face . the current version of hit mw includes <digit> forms and 186,444 characters that are produced under an unconstrained condition without preprinted character boxes . the statistics show that the database has an excellent representation of the real handwriting . many new applications concerning real handwriting recognition can be supported by the database .
fault diagnosis based on walsh transform and rough sets . <eos> an accurate and fast method for fault diagnosis is an important issue most techniques have sought to . for this reason , a fast fault diagnosis method based on walsh transform and rough sets is proposed in this paper . firstly , fault signals are fast transformed by walsh matrix , and the walsh spectrums are obtained , whose statistical characteristics constitute feature vectors . secondly , the feature vectors are discretized and reduced by the rough sets theory , as a result , key features are retained and diagnosis rules are provided . finally , utilized these diagnosis rules , fault diagnosis is carried out experimentally in the spectrum domain and its performance is compared with that of other methods , the higher accuracy is achieved and much time is saved , which fully validates the effectiveness of our approach .
weighted fuzzy interpolative reasoning for sparse fuzzy rule based systems . <eos> in this paper , we present a weighted fuzzy interpolative reasoning method for sparse fuzzy rule based systems , where the antecedent variables appearing in the fuzzy rules have different weights . we also present a weights learning algorithm to automatically learn the optimal weights of the antecedent variables of the fuzzy rules for the proposed weighted fuzzy interpolative reasoning method . we also apply the proposed weighted fuzzy interpolative reasoning method and the proposed weights learning algorithm to handle the truck backer upper control problem . the experimental results show that the proposed fuzzy interpolative reasoning method using the optimally learned weights by the proposed weights learning algorithm gets better truck backer upper control results than the ones by the traditional fuzzy inference system and the existing fuzzy interpolative reasoning methods . the proposed method provides us with a useful way for fuzzy rules interpolation in sparse fuzzy rule based systems .
a plug in approach to neyman pearson classification . <eos> the neyman pearson ( np ) paradigm in binary classification treats type i and type ii errors with different priorities . it seeks classifiers that minimize type ii error , subject to a type i error constraint under a user specified level a. in this paper , plug in classifiers are developed under the np paradigm . based on the fundamental neyman pearson lemma , we propose two related plug in classifiers which amount to thresholding respectively the class conditional density ratio and the regression function . these two classifiers handle different sampling schemes . this work focuses on theoretical properties of the proposed classifiers in particular , we derive oracle inequalities that can be viewed as finite sample versions of risk bounds . np classification can be used to address anomaly detection problems , where asymmetry in errors is an intrinsic property . as opposed to a common practice in anomaly detection that consists of thresholding normal class density , our approach does not assume a specific form for anomaly distributions . such consideration is particularly necessary when the anomaly class density is far from uniformly distributed .
analytical transformation of the volume integral in the boundary integral equation for 3d anisotropic elastostatics involving body force . <eos> in the boundary element method ( bem ) , it is well known that the presence of body force shall give rise to an additional volume integral that conventionally requires domain discretization for numerical computations . to restore the bems distinctive notion of boundary discretization , the present work analytically transforms the volume integral to surface ones for the body force effect in the 3d anisotropic elasticity . on applying greens theorem , new fundamental solutions with explicit forms of fourier series are introduced to facilitate the volume to surface transformation . the coefficients of the fourier series representations are determined by solving a banded matrix formulated from integrations of the constrained equation . of no doubt , such an approach has fully restored the boundary element method as a truly boundary solution technique for analyzing 3d anisotropic elasticity involving body force . at the end , numerical verifications of the volume to surface integral transformation are presented . also , such an approach has been implemented in an existing bem code . for demonstrating the implementation , numerical examples are presented with comparisons with ansys analysis . to the authors knowledge , this is the first work in the open literature that reports the successful transformation for 3d anisotropic elasticity .
an integrated framework for adapting ws bpel scenario execution using qos and collaborative filtering techniques . <eos> we present an algorithm combining qos and collaborative filtering for bpel adaptation . the combination introduces collaborating filtering functionality maintaining high qos . we exploit the sparsity of the rating matrix to tackle known issues of cf. we evaluate the approach both in terms of performance and adaptation qos .
basic skeletons in 11c . <eos> 11c is a high level parallel language that provides support for some of the most widely used algorithmic skeletons . the language has a syntax based on openmp like directives and the compiler uses direct translation to mpi to produce parallel code . to evaluate the performance of our prototype compiler we present computational results for some of the skeletons available in the language on different architectures . particularly in the presence of coarse grain parallelism , the results reflect similar performances for the 11c compiled version and ad hoc mpi or openmp implementations . in all cases , the performance loss with respect to a direct mpi implementation is clearly compensated by a significantly smaller development effort . ( c ) <digit> elsevier b.v. all rights reserved .
a multisolution method for cell formation exploring practical alternatives in group technology manufacturing . <eos> an approach based on the decision maker 's judgment is proposed by furnishing multiple solutions of part family and machine cell formation of a cellular manufacturing system . the reason for relying on the judgment of the decision maker is due to the complexity and the many constraints encountered in practice . some examples of these practical constraints are workload balancing , ill defined systems , the existence of exceptional elements , and the presence of the various uncertain factors in the system . the basic approach is based on the concept of nearest neighborhood between machines and parts . the procedure , which can be used to identify multiple grouping pat terns of machines and parts , consists of two algorithms grouping and branching , association , and combining . numerical examples are provided , especially for ill structured problems , to illustrate the approach . ( c ) <digit> elsevier science ltd. all rights reserved .
a complex communication network for distribution automation using a fiber optic network and wlans . <eos> in order to provide electricity economically and safely to users , a distribution automation system ( das ) monitors and operates the components of distribution systems remotely through communication networks . fiber optic communication networks have primarily been used for dass in korea because of their huge bandwidth and dielectric noise immunity . however , the fiber optic communication network has some shortcomings , particularly that its installation cost and communication fees are expensive . this paper proposes a complex communication network , where wlans are linked into a fiber optic network to expand dass in distribution lines inexpensively . a das wireless bridge ( dwb ) is designed for the proposed communication network using ieee 802.11 a wlan technology . feasibility of the proposed network is checked experimentally .
multiscale morphological segmentation of gray scale images . <eos> in this paper , the authors have proposed a method of segmenting gray level images using multiscale morphology . the approach resembles watershed algorithm in the sense that the dark ( respectively bright ) features which are basically canyons ( respectively mountains ) on the surface topography of the gray level image are gradually filled ( respectively clipped ) using multiscale morphological closing ( respectively opening ) by reconstruction with isotropic structuring element . the algorithm detects valid segments at each scale using three criteria namely growing , merging and saturation . segments extracted at various scales are integrated in the final result . the algorithm is composed of two passes preceded by a preprocessing step for simplifying small scale details of the image that might cause over segmentation . in the first pass feature images at various scales are extracted and kept in respective level of morphological towers . in the second pass , potential features contributing to the formation of segments at various scales are detected . finally the algorithm traces the contours of all such contributing features at various scales . the scheme after its implementation is executed on a set of test images ( synthetic as well as real ) and the results are compared with those of few other standard methods . a quantitative measure of performance is also formulated for comparing the methods .
data mining in web services discovery and monitoring . <eos> the business needs , the availability of huge volumes of data and the continuous evolution in web services functions derive the need of application of data mining in the web service domain . this article recommends several data mining applications that can leverage problems concerned with the discovery and monitoring of web services . this article then presents a case study on applying the clustering data mining technique to the web service usage data to improve the web service discovery process . this article also discusses the challenges that arise when applying data mining to web services usage data and abstract information .
reasoning theories . <eos> our long term goal is the development of a general framework for specifying , structuring , and interoperating provers . our main focus is on the formalization of the architectural and implementational choices that underlie the construction of such systems . this paper has two main goals . the first is to introduce the main intuitions underlying the proposed framework . we concentrate on its use in the integration of provers . the second is the development of the notion of reasoning theory , meant as the formalization of the notion of implementation of the logic of a prover . as an example we sketch an analysis , at the reasoning theory level , of the integration of linear arithmetic into the nqthm simplification process .
automatic generation of human animation based on motion programming . <eos> in motion simulations , video games and animation films , lots of interactions between characters and virtual environments are needed . even though realistic motion data can be derived from mocap system , motion editing and synthesis , animators must adapt these motion data to specific virtual environment manually , which is a boring and time consuming job . here we propose a framework to program the movements of characters and generate navigation animations in virtual environment . given a virtual environment , a visual user interface is provided for animators to interactively generate motion scripts , describing the characters ' movements in this scene and finally used to retrieve motion clips from mocap database and generate navigation animations automatically . this framework also provides flexible mechanism for animators to get varied resulting animations by configurable table of motion bias coefficients and interactive visual user interface . copyright ( c ) <digit> john wiley sons , ltd .
bipartite graph based mismatch removal for wide baseline image matching . <eos> the conventional wide baseline image matching aims to establish point to point correspondence pairs across the two images under matching . this is normally accomplished by identifying those feature points with most similar local features represented by feature descriptors and measuring the feature vector distance based on the nearest neighbor matching criterion . however , a large number of mismatches would be incurred especially when the two images under comparison have a large viewpoint variation with respect to each other or involve very different backgrounds . in this paper , a new mismatch removal method is proposed by utilizing the bipartite graph to first establish one to one coherent region pairs ( crps ) , which are then used to verify whether each point to point matching pair is a correct match or not . the generation of crps is achieved by applying the hungarian method to the bipartite graph , together with the use of the proposed region to region similarity measurement metric . extensive experimental results have demonstrated that our proposed mismatch removal method is able to effectively remove a significant number of mismatched point to point correspondences .
performance evaluation of evolutionary algorithms for road detection . <eos> in this paper we present the first comparative study of evolutionary classifiers for the problem of road detection . we use seven evolutionary algorithms ( gassist adi , xcs , ucs , cant , evrbf , fuzzy ab and fuzzyslave ) for this purpose and to develop better understanding we also compare their performance with two well known non evolutionary classifiers ( knn , c4 .5 ) . further we identify vision based features that enable a single classifier to learn to successfully classify a variety of regions in various roads as opposed to training a new classifier for each type of road . for this we collect a real world dataset of road images of various roads taken at different times of the day . then , using information gain ( i.g ) and cfssubsetmerit values we evaluate the efficacy of our features in facilitating the detection . our results indicate that intelligent features coupled with right evolutionary technique provides a promising solution for the domain of road detection .
modeling dynamic scenarios for local sensor based motion planning . <eos> this paper addresses the modeling of the static and dynamic parts of the scenario and how to use this information with a sensor based motion planning system . the contribution in the modeling aspect is a formulation of the detection and tracking of mobile objects and the mapping of the static structure in such a way that the nature ( static dynamic ) of the observations is included in the estimation process . the algorithm provides a set of filters tracking the moving objects and a local map of the static structure constructed on line . in addition , this paper discusses how this modeling module is integrated in a real sensor based motion planning system taking advantage selectively of the dynamic and static information . the experimental results confirm that the complete navigation system is able to move a vehicle in unknown and dynamic scenarios . furthermore , the system overcomes many of the limitations of previous systems associated to the ability to distinguish the nature of the parts of the scenario .
large margin feature selection for monotonic classification . <eos> monotonic classification plays an important role in the field of decision analysis , where decision values are ordered and the samples with better feature values should not be classified into a worse class . the monotonic classification tasks seem conceptually simple , but difficult to utilize and explain the order structure in practice . in this work , we discuss the issue of feature selection under the monotonicity constraint based on the principle of large margin . by introducing the monotonicity constraint into existing margin based feature selection algorithms , we design two new evaluation algorithms for monotonic classification . the proposed algorithms are tested with some artificial and real data sets , and the experimental results show its effectiveness .
using formal metamodels to check consistency of functional views in information systems specification . <eos> uml notations require adaptation for applications such as information systems ( is ) . thus we have defined is uml . the purpose of this article is twofold . first , we propose an extension to this language to deal with functional aspects of is . we use two views to specify is transactions the first one is defined as a combination of behavioural uml diagrams ( collaboration and state diagrams ) , and the second one is based on the definition of specific classes of an extended class diagram . the final objective of the article is to consider consistency issues between the various diagrams of an is uml specification . in common with other uml languages , we use a metamodel to define is uml . we use class diagrams to summarize the metamodel structure and a formal language , b , for the full metamodel . this allows us to formally express consistency checks and mapping rules between specific metamodel concepts .
denoising and enhancing digital mammographic images for visual screening . <eos> dense regions in digital mammographic images are usually noisy and have low contrast , and their visual screening is difficult . this paper describes a new method for mammographic image noise suppression and enhancement , which can be effective particularly for screening image dense regions . initially , the image is preprocessed to improve its local contrast and the discrimination of subtle details . next , image noise suppression and edge enhancement are performed based on the wavelet transform . at each resolution , coefficients associated with noise are modelled by gaussian random variables coefficients associated with edges are modelled by generalized laplacian random variables , and a shrinkage function is assembled based on posterior probabilities . the shrinkage functions at consecutive scales are combined , and then applied to the wavelets coefficients . given a resolution of analysis , the image denoising process is adaptive ( i.e. does not require further parameter adjustments ) , and the selection of a gain factor provides the desired detail enhancement . the enhancement function was designed to avoid introducing artifacts in the enhancement process , which is essential in mammographic image analysis . our preliminary results indicate that our method allows to enhance local contrast , and detect microcalcifications and other suspicious structures in situations where their detection would be difficult otherwise . compared to other approaches , our method requires less parameter adjustments by the user .
principles for modeling language design . <eos> modeling languages , like programming languages , need to be designed if they are to be practical , usable , accepted , and of lasting value . we present principles for the design of modeling languages . to arrive at these principles , we consider the intended use of modeling languages . we conject that the principles are applicable to the development of new modeling languages , and for improving the design of existing modeling languages that have evolved , perhaps through a process of unification . the principles are illustrated and explained by several examples , drawing on object oriented and mathematical modeling languages .
systems , systems of systems , and the education of engineers . <eos> the thesis presented here is that the result of engineering is the design , construction , or operation of systems or their subsystems and components and that the teaching of systems must be central to engineering education . it is maintained that current undergraduate engineering curricula do not give the student adequate appreciation of this major intellectual element of their profession . five proposals for approaches to correct this deficiency are offered opportunities for clinical practice throughout all the undergraduate years the use of distributed interactive simulation technology in semester long projects courses or course material on the phenomenology and behavior of systems use of project management tools in engineering clinics and encouraging engineering faculty to spend some part of their sabbaticals engaged in system design or operation . issues of implementation are addressed , including the scaling of these ideas to universities that must meet the needs of large numbers of students .
optimal erasure protection for scalably compressed video streams with limited retransmission on channels with iid and bursty loss characteristics . <eos> in this paper we combine priority encoding transmission ( pet ) with a limited retransmission ( lr ) capacity . we propose the resulting lr pet scheme as a framework for efficient rd optimized delivery of streaming media . previous work on scalable media protection with pet has largely ignored the possibility of retransmission . in the proposed lr pet framework , an optimization algorithm determines the level of protection for each element in each transmission slot , subject to transmission bandwidth constraints . to balance the protection assigned to elements being transmitted for the first time with those being retransmitted , the proposed algorithm formulates a collection of hypotheses concerning its own behavior in future transmission slots . we show that this formulation of hypotheses is central to the success of the proposed lr pet algorithm . indeed , without this element , a greedy version of lr pet performs only slightly better than pet without retransmission . experimental results are reported using both iid and ge channel models , with a motion jpeg2000 video source , demonstrating substantial performance benefits from the proposed framework .
verification of various pipeline models . <eos> the paper deals with the verification of three pipeline models the non linear distributed parameters model , the linear distributed parameter model and the linear lumped parameters model . all the models were comparatively verified on the basis of the measurements on a real pipeline . ( c ) <digit> imacs . published by elsevier science b.v. all rights reserved .
on some test statistics for testing homogeneity of variances a comparative study . <eos> in this paper , we have reviewed <digit> test procedures that are widely reported in the literature for testing the hypothesis of homogeneity of variances under various experimental conditions . since a theoretical comparison was not possible , a simulation study has been conducted to compare the performance of the test statistics in terms of robustness and empirical power . monte carlo simulation was performed for various symmetric and skewed distributions , number of groups , sample size per group , degree of group size inequalities , and degree of variance heterogeneity . using simulation results and based on the robustness and power of the tests , some promising test statistics are recommended for practitioners .
synchronization control of stochastic memristor based neural networks with mixed delays . <eos> in this paper , the synchronization control of stochastic memristor based neural networks with mixed delays is studied . based on the drive response concept , the stochastic differential inclusions theory and lyapunov functional method some new criteria are established to guarantee the exponential synchronization in the pth moment of stochastic memristor based neural networks . the obtained sufficient conditions can be checked easily and improve the results in earlier publications . finally , a numerical example is given to illustrate the effectiveness of the new scheme .
methods for parallel computation of complex flow problems . <eos> this paper is an overview of some of the methods developed by the team for advanced flow simulation and modeling ( t afsm ) http www , mems , rice . edu tafsm to support flow simulation and modeling in a number of targeted challenges . the targeted challenges include unsteady flows with interfaces , fluid object and fluid structure interactions , airdrop systems , and air circulation and contaminant dispersion . the methods developed include special numerical stabilization methods for compressible and incompressible flows , methods for moving boundaries and interfaces , advanced mesh management methods , and multi domain computational methods . we include in this paper a number of numerical examples from the simulation of complex flow problems . ( c ) <digit> elsevier science b.v. all rights reserved .
environmental sensor bridge system for communication robots . <eos> this paper proposes an environmental sensor bridge system named cospi for autonomous communication robots . in the near future , various sensors will get installed in day to day environments and robots will use information from these sensors to recognize their surroundings . to this end , different sensors can be installed in different environments . cospi enables the robots to work with different sensor configuration environments without requiring any robot software reconfiguration . the basic idea of cospi is the abstraction of recognition types . we investigate <digit> recognition routines in behavior modules of a communication robot that works in practical situations and classify them based on communication cues of the communication robot . cospi facilitates robot behavior development as robot behavior and sensor processing procedures can be independently developed . furthermore , cospi encourages the reuse of environmental sensor modules for other robot tasks or applications within the same environment . we have conducted an experiment to confirm that a robot can work in a visual sensor environment , in an infrared ray and pyroelectric sensor environment , and in an optical motion capture system environment . the results showed that robots can work using sensor information from environmental sensors in all these environments .
a note on machine scheduling with sum of logarithm processing time based and position based learning effects . <eos> recently , biskup <digit> classifies the learning effect models in scheduling environments into two types position based and sum of processing time based . in this paper , we study scheduling problem with sum of logarithm processing time based and position based learning effects . we show that the single machine scheduling problems to minimize the makespan and the total completion time can both be solved by the smallest ( normal ) processing time first ( spt ) rule . we also show that the problems to minimize the maximum lateness , the total weighted completion times and the total tardiness have polynomial time solutions under agreeable wspt rule and agreeable edd rule . in addition , we show that m machine permutation flowshop problems are still polynomially solvable under the proposed learning model .
towards a global control strategy for induction motor speed regulation , flux optimization and power factor correction . <eos> a great deal of interest has been paid to induction machine control over the last years . however , most previous works have focused on the speed flux torque regulation supposing the machine magnetic circuit to be linear and ignoring the machine power conversion equipments . the point is that speed regulation can not be ensured in optimal efficiency conditions , for a wide range of speed set point and load torque , unless the magnetic circuit nonlinearity is explicitly accounted for in the motor model . on the other hand , the negligence of the power conversion equipments makes it impossible to deal properly with the harmonic pollution issue due to motor power supply grid interaction . this paper presents a theoretical framework for a global control strategy of the induction machine and related power equipments . the proposed strategy involves a multi loop nonlinear adaptive controller designed to meet the three main control objectives , i.e. tight speed regulation for a wide range speed reference variation , flux optimization for energy consumption and power factor correction ( pfc ) . tools from the averaging theory are resorted to formally describe the control performances .
solving fuzzy fredholm linear integral equations using sinc method and double exponential transformation . <eos> in this paper , numerical solution of fuzzy fredholm linear integral equations is considered by applying sinc method based on double exponential transformation with dual fuzzy linear systems . for this purpose , we convert the given fuzzy integral equation to a fuzzy linear system of equation . in this case , the sinc collocation method with double exponential transformation is used . numerical examples are provided to verify the validity of the proposed algorithm .
fuzzy approximation based adaptive control of strict feedback nonlinear systems with time delays . <eos> this paper focuses on the problem of adaptive control for a class of nonlinear time delay systems with unknown nonlinearities and strict feedback structure . based on the lyapunov krasovskii functional approach , a state feedback adaptive controller is constructed by backstepping . the proposed adaptive controller guarantees that the system output converges into a small neighborhood of the reference signal , and all the signals of the closed loop system remain bounded . compared with the results that exist , the main advantage of the proposed method is that the controller design is independent of the choice of the fuzzy membership functions therefore , a priori knowledge of fuzzy approximators is not necessary for control design , and the proposed approach requires only one adaptive law for an nth order system . two numerical examples are used to illustrate the effectiveness of the proposed approach .
some results on greedy embeddings in metric spaces . <eos> geographic routing is a family of routing algorithms that uses geographic point locations as addresses for the purposes of routing . such routing algorithms have proven to be both simple to implement and heuristically effective when applied to wireless sensor networks . greedy routing is a natural abstraction of this model in which nodes are assigned virtual coordinates in a metric space , and these coordinates are used to perform point to point routing . here we resolve a conjecture of papadimitriou and ratajczak that every <digit> connected planar graph admits a greedy embedding into the euclidean plane . this immediately implies that all <digit> connected graphs that exclude k ( 3,3 ) as a minor admit a greedy embedding into the euclidean plane . we also prove a combinatorial condition that guarantees nonembeddability . we use this result to construct graphs that can be greedily embedded into the euclidean plane , but for which no spanning tree admits such an embedding .
investigating the effects of multiple factors towards more accurate <digit> d object retrieval . <eos> this paper proposes a novel framework for <digit> d object retrieval , taking into account most of the factors that may affect the retrieval performance . initially , a novel <digit> d model alignment method is introduced , which achieves accurate rotation estimation through the combination of two intuitive criteria , plane reflection symmetry and rectilinearity . after the pose normalization stage , a low level descriptor extraction procedure follows , using three different types of descriptors , which have been proven to be effective . then , a novel combination procedure of the above descriptors takes place , which achieves higher retrieval performance than each descriptor does separately . the paper provides also an in depth study of the factors that can further improve the <digit> d object retrieval accuracy . these include selection of the appropriate dissimilarity metric , feature selection dimensionality reduction on the initial low level descriptors , as well as manifold learning for re ranking of the search results . experiments performed on two <digit> d model benchmark datasets confirm our assumption that future research in <digit> d object retrieval should focus more on the efficient combination of low level descriptors as well as on the selection of the best features and matching metrics , than on the investigation of the optimal <digit> d object descriptor .
nonlinear hydrodynamic models of traffic flow modelling and mathematical problems . <eos> this paper deals with nonlinear hydrodynamic modelling of traffic flow on roads and with the solution of related nonlinear initial and boundary value problems . the paper is in two parts . the first one provides the general framework of hydrodynamic modelling of traffic flow . some new models are proposed and related to the ones which are known in the literature . the second one is on mathematical methods related to the solution of initial boundary value problems . a critical analysis and an overview on research perspectives conclude the paper . ( c ) <digit> elsevier science ltd. all rights reserved .
skeleton enhanced line drawings for 3d models . <eos> we present a novel line drawing approach for 3d models by introducing their skeleton information into the rendering process . based on the silhouettes of the input 3d models , we first extract feature lines in geometric regions by utilizing their curvature , torsion and view dependent information . then , the skeletons of the models are extracted by our newly developed skeleton extraction algorithm . after that , we draw the skeleton guided lines from non geometric regions through the skeleton information . these lines are combined with the feature lines to render the final line drawing result using the line optimization . experimental results show that our algorithm can render line drawings more effectively with enhanced skeletons . the resulting artistic effects can capture the local geometries as well as the global skeletons of the input 3d models .
low complexity video coding based on two dimensional singular value decomposition . <eos> in this paper , we propose a low complexity video coding scheme based upon <digit> d singular value decomposition ( <digit> d svd ) , which exploits basic temporal correlation in visual signals without resorting to motion estimation ( me ) . by exploring the energy compaction property of <digit> d svd coefficient matrices , high coding efficiency is achieved . the proposed scheme is for the better compromise of computational complexity and temporal redundancy reduction , i.e. , compared with the existing video coding methods . in addition , the problems caused by frame decoding dependence in hybrid video coding , such as unavailability of random access , are avoided . the comparison of the proposed <digit> d svd coding scheme with the existing relevant non me based low complexity codecs shows its advantages and potential in applications .
use of oswald for analyzing longitudinal data with informative dropout . <eos> oswald ( object oriented software for the analysis of longitudinal data ) is flexible and powerful software written for s plus for the analysis of longitudinal data with dropout for which there is little other software available in the public domain . the implementation of oswald is described through analysis of a psychiatric clinical trial that compares antidepressant effects in an elderly depressed sample and a simulation study . in the simulation study , three different dropout mechanisms completely random dropout ( crd ) , random dropout ( rd ) and informative dropout ( id ) , are considered and the results from using oswald are compared across mechanisms . the parameter estimates for id simulated data show less bias with oswald under the id missing data assumption than under the crd or rd assumptions . under an id mechanism , oswald does not provide standard error estimates . we supplement oswald with a bootstrap procedure to derive the standard errors . this report illustrates the usage of oswald for analyzing longitudinal data with dropouts and how to draw appropriate conclusions based on the analytic results under different assumptions regarding the dropout mechanism . ( c ) <digit> elsevier ireland ltd. all rights reserved .
a survey of nat behavior discovery in voip applications . <eos> because of the foreseeing depletion of internet protocol ( ip ) addresses , network address translation ( nat ) is ubiquitously deployed to allow hosts to connect to the internet through a single shared public ip address , which is a popular approach in deploying wireless local area network ( wlan ) . although nat proves to work well with traditional client server applications , its existence and non standard behaviors are the major problem which cripples voice over ip ( voip ) applications . in addition to some efforts which attempt to devise complicated protocols to tackle all nat varieties , there are also efforts in internet communities trying to standardize the behaviors of nat . therefore , it becomes crucial for a network device to discover the existence of nat in its subnet and to determine the nat behaviors , so that it can choose the optimal nat traversal mechanisms to apply . in this paper , we surveyed the divergent nat behaviors and then proposed a simplified nat behavior discovery approach which is more suitable for voip applications . the proposed approach can reduce the call establishment time of voip applications , which is useful in scenarios where voip devices are administrated within a specific domain , e.g. , 3g cellular networks .
an automorphic approach to verification pattern generation for soc design verification using port order fault model . <eos> embedded cores are being increasingly used in the design of large system on a chip ( soc ) . because of the high complexity of soc , the design verification is a challenge for system integrators . to reduce the verification complexity , the port order fault ( pof ) model was proposed . it has been used for verifying core based designs and the corresponding verification pattern generation has been developed . here , the authors present an automorphic technique to improve the efficiency of the automatic verification pattern generation ( avpg ) for soc design verification based on the pof model . on average , the size of pattern sets obtained on the iscas <digit> and mcnc benchmarks are <digit> % smaller and the run time decreases <digit> % as compared with the previous results of avpg .
data page layouts for relational databases on deep memory hierarchies . <eos> relational database systems have traditionally optimized for i o performance and organized records sequentially on disk pages using the n ary storage model ( nsm ) ( a.k.a. , slotted pages ) . recent research , however , indicates that cache utilization and performance is becoming increasingly important on modern platforms . in this paper , we first demonstrate that in page data placement is the key to high cache performance and that nsm exhibits low cache utilization on modern platforms . next , we propose a new data organization model called pax ( partition attributes across ) , that significantly improves cache performance by grouping together all values of each attribute within each page . because pax only affects layout inside the pages , it incurs no storage penalty and does not affect i o behavior . according to our experimental results ( which were obtained without using any indices on the participating relations ) , when compared to nsm ( a ) pax exhibits superior cache and memory bandwidth utilization , saving at least <digit> % of nsm 's stall time due to data cache accesses ( b ) range selection queries and updates on memory resident relations execute <digit> <digit> % faster and ( c ) tpc h queries involving i o execute <digit> <digit> % faster . finally , we show that pax performs well across different memory system designs .
a soft computing approach to projecting locational marginal price . <eos> the increased deregulation of electricity markets in most nations of the world in recent years has made it imperative that electricity utilities design accurate and efficient mechanisms for determining locational marginal price ( lmp ) in power systems . this paper presents a comparison of two soft computing based schemes artificial neural networks and support vector machines for the projection of lmp . our system has useful power system parameters as inputs and the lmp as output . experimental results obtained suggest that although both methods give highly accurate results , support vector machines slightly outperform artificial neural networks and do so with manageable computational time costs .
composite right left handed artificial transmission line structures in cmos for controlled insertion phase at <digit> ghz . <eos> two cmos integrated circuits are presented that utilize metamaterial composite right left handed ( crlh ) transmission lines ( tls ) for zero insertion phase at <digit> ghz . specifically , <digit> and <digit> unit cell structures are presented with controlled insertion phase that is achieved by cascading lumped element capacitors and spiral inductors in an lc network configuration defining the tl unit cells . furthermore , the fixed tl structures suggest the possibility of zero , advanced or delayed insertion phases by element variation , or by the use of simple active components . simulation and measured results are in good agreement with crlh tl theory . and display a linear insertion phase and flat group delay values that are dependent on the number of unit cells with an insertion loss of similar to 0.8 db per cell . these findings suggest that such high speed crlh tls structures can he implemented for linear array feeding networks and compact antenna designs in cmos at millimeter wave frequencies . ( c ) <digit> wiley periodicals . inc. int j rf and microwave cae 19 163 169 2009 .
on the hardness of offline multi objective optimization . <eos> it has been empirically established that multiobjective evolutionary algorithms do not scale well with the number of conflicting objectives . this paper shows that the convergence rate of all comparison based multi objective algorithms , for the hausdorff distance , is not much better than the convergence rate of the random search under certain conditions . the number of objectives must be very moderate and the framework should hold the following assumptions the objectives are conflicting and the computational cost is lower bounded by the number of comparisons is a good model . our conclusions are ( i ) the number of conflicting objectives is relevant ( ii ) the criteria based on comparisons with random search for multi objective optimization is also relevant ( iii ) having more than <digit> objectives optimization is very hard . furthermore , we provide some insight into cross over operators .
the spatial market of business advice and consultancy to smes . <eos> this paper demonstrates the existence of spatial markets for business advice services . a large sample of <digit> clientadvisor links is investigated using gis software . seventy per cent of links are less than <digit> km in extent , <digit> % are to the nearest local business centre , and only a few are with hinterlands or areas peripheral to main centres . the maximum reach of market areas varies by advisor type , averaging only <digit> km for chambers of commerce and public sector advice services such as business link . the maximum reach is <digit> km for accountants and banks , and increases to <digit> km for customers and suppliers and <digit> km for consultants . a threshold for regional level services in major centres can be identified , which ranges from 12,000 to 24,000 businesses in size , depending on advisor type . service sector firms are generally more localised than manufacturing , and local sourcing of advisors generally declines with firm size and size of business centre . regional differences are relatively small , but scotland , yorkshire and humberside are the most self contained for advice , whilst london and the south east are the least self contained . this is a contrast to earlier findings by ofarrell and others . the paper demonstrates a hierarchical and spatial market structure for business advice services that is similar to that in retailing , with firm size and advisor type being the primary influence on differences in demand , and with regional centres most distinct from local centres of supply . intense localised sourcing of advice from customers and suppliers does not appear to be frequent .
a calculus for stochastic qos analysis . <eos> the issue of quality of service ( qos ) performance analysis in packet switched networks has drawn a lot of attention in the networking community . there is a lot of work including an elegant theory under the name of network calculus , which focuses on analysis of deterministic worst case qos performance bounds . in the meantime , researchers have studied stochastic qos performance for specific schedulers . however , most previous works on deterministic qos analysis or stochastic qos analysis have only considered a server that provides deterministic service , i.e.deterministically bounded rate service . few have considered the behavior of a stochastic server that provides input flows with variable rate service , for example wireless links . in this paper , we propose a stochastic network calculus to analyze the end to end stochastic qos performance of a system with stochastically bounded input traffic over a series of deterministic and stochastic servers . we also prove that a server serving an aggregate of flows can be regarded as a stochastic server for individual flows within the aggregate . based on this , the proposed framework is further applied to analyze per flow stochastic qos performance under aggregate scheduling .
dynamic consistency for stochastic optimal control problems . <eos> for a sequence of dynamic optimization problems , we aim at discussing a notion of consistency over time . this notion can be informally introduced as follows . at the very first time stept <digit> , the decision maker formulates an optimization problem that yields optimal decision rules for all the forthcoming time stepst <digit> , t <digit> , , t at the next time stept <digit> , he is able to formulate a new optimization problem starting at timet <digit> that yields a new sequence of optimal decision rules . this process can be continued until the final timet is reached . afamily of optimization problems formulated in this way is said to be dynamically consistent if the optimal strategies obtained when solving the original problem remain optimal for all subsequent problems . the notion of dynamic consistency , well known in the field of economics , has been recently introduced in the context of risk measures , notably by artzner et al. ( ann . oper . res . <digit> ( <digit> ) <digit> , <digit> ) and studied in the stochastic programming framework by shapiro ( oper . res . lett . <digit> ( <digit> ) <digit> , <digit> ) and for markov decision processes ( mdp ) by ruszczynski ( math . program . <digit> ( <digit> ) <digit> , <digit> ) . we here link this notion with the concept of state variable in mdp , and show that a significant class of dynamic optimization problems are dynamically consistent , provided that an adequate state variable is chosen .
multiobjective optimization of expensive to evaluate deterministic computer simulator models . <eos> many engineering design optimization problems contain multiple objective functions all of which are desired to be minimized , say . this paper proposes a method for identifying the pareto front and the pareto set of the objective functions when these functions are evaluated by expensive to evaluate deterministic computer simulators . the method replaces the expensive function evaluations by a rapidly computable approximator based on a gaussian process ( gp ) interpolator . it sequentially selects new input sites guided by values of an improvement function given the current data . the method introduced in this paper provides two advances in the interpolator improvement framework . first , it proposes an improvement function based on the modified maximin fitness function which is known to identify well spaced non dominated outputs when used in multiobjective evolutionary algorithms . second , it uses a family of gp models that allows for dependence among output function values but which permits zero covariance should the data be consistent with this model . a closed form expression is derived for the improvement function when there are two objective functions simulation is used to evaluate it when there are three or more objectives . examples from the multiobjective optimization literature are presented to show that the proposed procedure can improve substantially previously proposed statistical improvement criteria for the computationally intensive multiobjective optimization setting .
early activation and induction of apoptosis in t cells is independent of c fos . <eos> we used c fos deficient activated t cells from the spleen and c fos deficient thymocytes to address the capacity of these cells to undergo apoptosis in response to various stimuli . to determine the role of c fos in apoptosis regulation in thymocytes , we challenged thymocytes from wild type and c fos deficient mice with either tpa or the glucocorticoid dexamethasone . after various time points cells were stained according to the nicoletti method and analyzed by facs . thymocytes from both genotypes exhibited similar efficiency of apoptosis in response to treatment with tpa or dexamethasone . our data provide clear evidence that c fos is not required for apoptosis regulation in activated t cells as well as in thymocytes .
a shear deformable beam element for the analysis of laminated composites . <eos> a <digit> degree of freedom element , based on the fsdt , is derived to study the response of unsymmetrically laminated composite structures subject to both static and dynamic problems . in the fsdt model used here we have employed an accurate model to obtain the transverse shear correction factor . the dynamic version of the principle of virtual work for laminated composites is expressed in its nondimensional form and the element tangent stiffness and mass matrices are obtained using analytical integration . the element consists of four equally spaced nodes and a node at the middle . the results for the one dimensional case are within <digit> % when compared to equivalent one and two dimensional problems of static loading , free vibrations and buckling loads .
identifying potential adverse effects using the web a new approach to medical hypothesis generation . <eos> medical message boards are online resources where users with a particular condition exchange information , some of which they might not otherwise share with medical providers . many of these boards contain a large number of posts and contain patient opinions and experiences that would be potentially useful to clinicians and researchers . we present an approach that is able to collect a corpus of medical message board posts , de identify the corpus , and extract information on potential adverse drug effects discussed by users . using a corpus of posts to breast cancer message boards , we identified drug event pairs using co occurrence statistics . we then compared the identified drug event pairs with adverse effects listed on the package labels of tamoxifen , anastrozole , exemestane , and letrozole . of the pairs identified by our system , <digit> % were documented on the drug labels . some of the undocumented pairs may represent previously unidentified adverse drug effects .
analytic hierarchy based policy design method ( ahpo ) for solving societal problems that require a multifaceted approach . <eos> this paper proposes an ahp based statistical method for the design of a comprehensive policy alternative , ahpo , for solving societal problems that require a multifaceted approach . in the proposed method , criteria relevant to the goal or focus are structured in the same way as in the conventional ahp . however , these two methods are quite different in regard to the method of quantification . the new method predicts or analyses the impact of the policy alternatives on the overall goal . in other words , it predicts or rationalizes the way people appreciate the situation in which an alternative is adopted and implemented . it will serve as a tool for supporting ( especially political ) decision making .
growing a language environment with editor libraries . <eos> large software projects consist of code written in a multitude of different ( possibly domain specific ) languages , which are often deeply interspersed even in single files . while many proposals exist on how to integrate languages semantically and syntactically , the question of how to support this scenario in integrated development environments ( ides ) remains open how can standard ide services , such as syntax highlighting , outlining , or reference resolving , be provided in an extensible and compositional way , such that an open mix of languages is supported in a single file based on our library based syntactic extension language for java , sugarj , we propose to make ides extensible by organizing editor services in editor libraries . editor libraries are libraries written in the object language , sugarj , and hence activated and composed through regular import statements on a file by file basis . we have implemented an ide for editor libraries on top of sugarj and the eclipse based spoofax language workbench . we have validated editor libraries by evolving this ide into a fully fledged and schema aware xml editor as well as an extensible latex editor , which we used for writing this paper .
assessment of the classification capability of prediction and approximation methods for hrv analysis . <eos> the goal of this paper is to examine the classification capabilities of various prediction and approximation methods and suggest which are most likely to be suitable for the clinical setting . various prediction and approximation methods are applied in order to detect and extract those which provide the better differentiation between control and patient data , as well as members of different age groups . the prediction methods are local linear prediction , local exponential prediction , the delay times method , autoregressive prediction and neural networks . approximation is computed with local linear approximation , least squares approximation , neural networks and the wavelet transform . these methods are chosen since each has a different physical basis and thus extracts and uses time series information in a different way .
hyperspeech . <eos> hyperspeech is a speech only hypermedia application that explores issues of speech user interfaces , navigation , and system architecture in a purely audio environment without a visual display . the system uses speech recognition input and synthetic speech feedback to aid in navigating through a database of digitally recorded speech segments .
outline of a centralised multihop ad hoc wireless network . <eos> a concept of a multihop ad hoc network and associated algorithms for adaptive clustering in wireless ad hoc networks are presented in this paper . the algorithms take into account the connectivity of the stations as well as the quality of service requirements . the concept of a centralised ad hoc network is adopted , in which a cluster is defined by a central controller granting access to the radio interface to all terminals in its cluster . by these means the cc contributes to provide quality of service guarantees to the users . this concept is also used in the hiperlan <digit> ( hl <digit> ) home environment extension ( hee ) , an ad hoc wireless lan standardised by the european telecommunications standardisation institute ( etsi ) . the hee is restricted to one single cluster . it is shown in this article how the network can be extended over several clusters by the introduction of so called forwarding stations . these forwarders interconnect the clusters and enable multihop connections of users roaming in different clusters . a solution is presented to ensure , as far as possible , an interconnection of clusters by means of the clustering algorithm .
an architecture for security oriented perfective maintenance of legacy software . <eos> this work presents an implementation strategy which exploits the separation of concerns and reuse in a multi tier architecture to improve the security ( availability , integrity , and confidentiality ) level of an existing application . functional properties are guaranteed via wrapping of the existing software modules . security mechanisms are handled by the business logic of the middle tier availability and integrity are achieved via replication of the functional modules and the confidentiality is obtained via cryptography . the technique is presented with regard to a case study application . we believe that our experience can be used as a guideline for software practitioners to solve similar problems . we thus describe the conceptual model behind the architecture , discuss implementation issues , and present technical solutions .
current and future models for nursing e journals making the most of the webs potential . <eos> we are presently witnessing an increasing number of nursing , medical and health related electronic journals ( e journals ) being made available on the world wide web , a minority of which are specifically devoted to informatics . we would expect , given the potential of interacting multimedia and computer mediated communications ( i.e. telematics ) , to also see an increasing diversity of models , but this is not currently the case . following a brief discussion of some of the issues relevant to electronic publications , the authors present a taxonomy of current nursing e journal models , including discussion of some examples from around the world that fall into categories within this taxonomy . we describe the model and levels of usage of one particular e journal , nursing standard online . some of the issues presented may account for the current relative paucity of high quality content and innovative models in the development of web based e journals for nurses and other health professionals . we believe it likely that nursing e journals using current models will need to be specialist rather than generalist if they are to attract a larger audience . in concluding our paper , we advocate the development of innovative and increasingly interactive nursing e journals as the way forward , discussing one particular model which holds promise .
trust management in vehicular ad hoc network a systematic review . <eos> the basis of vehicular ad hoc networks ( vanets ) is the exchange of data between entities , and making a decision on received data event is usually based on information provided by other entities . many researchers utilize the concept of trust to assess the trustworthiness of the received data . nevertheless , the lack of a review to sum up the best available research on specific questions on trust management in vehicular ad hoc networks is sensible . this paper presents a systematic literature review to provide comprehensive and unbiased information about various current trust conceptions , proposals , problems , and solutions in vanets to increase quality of data in transportation . for the purpose of the writing of this paper , a total of <digit> articles related to the trust model in vanets published between <digit> and <digit> were extracted from the most relevant scientific sources ( ieee computer society , acm digital library , springer link , science direct , and wiley online library ) . finally , ten articles were eventually analyzed due to several reasons such as relevancy and comprehensiveness of discussion presented in the articles . using the systematic method of review , this paper succeeds to reveal the main challenges and requirements for trust in vanets and future research within this scope .
qspr study of the henry 's law constant for hydrocarbons . <eos> we establish a qspr inodel between the henry 's law constant in the air water system and the molecular structure of <digit> aliphatic hydrocarbons . the simultaneous linear regression analyzes on <digit> numerical descriptors reflecting topological , geometrical , and electronic aspects lead to a seven parameter equation that , when compared to previously reported models , exhibits good calibration and cross validated parameters r 0.996 , r ( <digit> <digit> % o ) 0.997 . as a realistic application , we employ this relationship to estimate the partition coefficient for <digit> non yet measured chemicals . crown copyright ( c ) <digit> published by elsevier b.v. all rights reserved .
sleep transistor distribution in row based mtcmos designs . <eos> the multi threshold cmos ( mtcmos ) technology has become a popular technique for standby power reduction . this technology utilizes high vth sleep transistors to reduce sub threshold leakage currents during the standby mode of cmos vlsi circuits . the performance of mtcmos circuits strongly depends on the size of the sleep transistors and the parasitics on the virtual ground network . given a placed net list of a row based mtcmos design and the number of sleep transistor cells on each standard cell row , this paper introduces an optimal algorithm for linearly placing the allocated sleep transistors on each standard cell row so as to minimize the performance degradation of the mtcmos circuit , which is in part due to unwanted voltage drops on its virtual ground network . experimental results show that , compared to existing methods of placing the sleep transistors on cell rows , the proposed technique results in up to <digit> % reduction in the critical path delay of the circuit .
learning the mean a neural network approach . <eos> one of the key problems in machine learning theory and practice is setting the correct value of the regularization parameter this is particularly crucial in kernel machines such as support vector machines , regularized least square or neural networks with weight decay terms . well known methods such as leave one out ( or gcv ) and evidence maximization offer a way of predicting the regularization parameter . this work points out the failure of these methods for predicting the regularization parameter when coping with the , apparently trivial and here introduced , regularized mean problem this is the simplest form of tikhonov regularization , that , in turn , is the primal form of the learning algorithm regularized least squares . this controlled environment gives the possibility to define oracular notions of regularization and to experiment new methodologies for predicting the regularization parameter that can be extended to the more general regression case . the analysis stems from jamesstein theory , shows the equivalence of shrinking and regularization and is carried using multiple kernels learning for regression and svd analysis a mean value estimator is built , first via a rational function and secondly via a balanced neural network architecture suitable for estimating statistical quantities and gaining symmetric expectations . the obtained results show that a non linear analysis of the sample and a non linear estimation of the mean obtained by neural networks can be profitably used to improve the accuracy of mean value estimations , especially when a small number of realizations is provided .
clustering by pattern similarity in large data sets . <eos> clustering is the process of grouping a set of objects into classes of similar objects . although definitions of similarity vary from one clustering model to another , in most of these models the concept of similarity is based on distances , e.g. , euclidean distance or cosine distance . in other words , similar objects are required to have close values on at least a set of dimensions . in this paper , we explore a more general type of similarity . under the pcluster model we proposed , two objects are similar if they exhibit a coherent pattern on a subset of dimensions . for instance , in dna microarray analysis , the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli . although the magnitude of their expression levels may not be close , the patterns they exhibit can be very much alike . discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks . e commerce applications , such as collaborative filtering , can also benefit from the new model , which captures not only the closeness of values of certain leading indicators but also the closeness of ( purchasing , browsing , etc. ) patterns exhibited by the customers . our paper introduces an effective algorithm to detect such clusters , and we perform tests on several real and synthetic data sets to show its effectiveness .
measuring the performance of underplatform dampers for turbine blades by rotating laser doppler vibrometer . <eos> underplatform friction dampers are commonly used to control the vibration level of turbine blades in order to prevent high cycle fatigue failures . experimental validation of highly non linear response predictions obtained from fem bladed disk models incorporating underplatform dampers models has proved to be very difficult so as the assessment of the performance of a chosen design . in this paper , the effect of wedge shaped underplatform dampers on the dynamics of a simple bladed disk under rotating conditions is measured and the effect of the excitation level on the upds performances is investigated at different number of the engine order excitation nearby resonance frequencies of the 1st blade bending modes of the system . the measurements are performed with an improved configuration of a rotating test rig , designed with a non contact magnetic excitation and a non contact rotating sldv measurement system .
failure analysis of r c columns using a triaxial concrete model . <eos> inelastic failure analysis of concrete structures has been one of the central issues in concrete mechanics . especially , the effect of confinement has been of great importance to capture the transition from brittle to ductile fracture of concrete under triaxial loading scenarios . moreover , it has been a challenge to implement numerically material descriptions , which are susceptible to loss of stability and localization . in this article , a novel triaxial concrete model is presented , which captures the full spectrum of triaxial stress and strain histories in reinforced concrete structures . thereby , inelastic dilatation is controlled by a non associated flow rule to attain realistic predictions of inelastic volume change at various confinement levels . different features of distributed and localized failure of the concrete model are examined under confined compression , uniaxial tension , pure shear , and simple shear . the performance at the structural level is illustrated with the example of a reinforced concrete column subjected to combined axial and transverse loading .
toward sophisticated detection with distributed triggers . <eos> recent research has proposed efficient protocols for distributed triggers , which can be used in monitoring infrastructures to maintain system wide invariants and detect abnormal events with minimal communication overhead . to date , however , this work has been limited to simple thresholds on distributed aggregate functions like sums and counts . in this paper , we present our initial results that show how to use these simple threshold triggers to enable sophisticated anomaly detection in near real time , with modest communication overheads . we design a distributed protocol to detect unusual traffic patterns buried in an origin destination network flow matrix that a ) uses a principal components analysis decomposition technique to detect anomalies via a threshold function on residual signals <digit> and b ) efficiently tracks this threshold function in near real time using a simple distributed protocol . in addition , we speculate that such simple thresholding can be a powerful tool for a variety of monitoring tasks beyond the one presented here , and we propose an agenda to explore additional sophisticated applications .
fully abstract trace semantics for protected module architectures . <eos> formalises a i a i an assembly language extended with protected module architectures an isolation mechanism found in emerging processors . presents two trace semantics for a i a i programs and proves that both are fully abstract w.r.t. the operational semantics . details which problems arise when considering readout and writeout labels in the trace semantics of a i a i programs .
a new inversion method for ( t2 , d ) 2d nmr logging and fluid typing . <eos> one dimensional nuclear magnetic resonance ( 1d nmr ) logging technology has some significant limitations in fluid typing . however , not only can two dimensional nuclear magnetic resonance ( 2d nmr ) provide some accurate porosity parameters , but it can also identify fluids more accurately than 1d nmr . in this paper , based on the relaxation mechanism of ( t2 , d ) 2d nmr in a gradient magnetic field , a hybrid inversion method that combines least squares based qr decomposition ( lsqr ) and truncated singular value decomposition ( tsvd ) is examined in the 2d nmr inversion of various fluid models . the forward modeling and inversion tests are performed in detail with different acquisition parameters , such as magnetic field gradients ( g ) and echo spacing ( te ) groups . the simulated results are discussed and described in detail , the influence of the above mentioned observation parameters on the inversion accuracy is investigated and analyzed , and the observation parameters in multi te activation are optimized . furthermore , the hybrid inversion can be applied to quantitatively determine the fluid saturation . to study the effects of noise level on the hybrid method and inversion results , the numerical simulation experiments are performed using different signal to noise ratios ( snrs ) , and the effect of different snrs on fluid typing using three fluid models are discussed and analyzed in detail .
testing collaborative strategies by computational simulation cognitive and task effects . <eos> a theory of communication between autonomous agents should make testable predictions about which communicative behaviors are collaborative , and provide a framework for determining the features of a communicative situation that affect whether a behavior is collaborative . the results presented here are derived from a two phase empirical method . first , we analyze a corpus of naturally occurring problem solving dialogues in order to identify potentially collaborative communicative strategies . second , we experimentally test hypotheses that arise from the corpus analysis in design world , an experimental environment for simulating dialogues . the results indicate that collaborative behaviors must be defined relative to the cognitive limitations of the agents and the cognitive demands of the task . the method of computational simulation provides an additional empirical basis for theories of human computer collaboration .
pedestrian navigation aids information requirements and design implications . <eos> recent years have seen an increased interest in navigational services for pedestrians . to ensure that these services are successful , it is necessary to understand the information requirements of pedestrians when navigating , and in particular , what information they need and how it is used . a requirements study was undertaken to identify these information requirements within an urban navigation context . results show that landmarks were by far the most predominant navigation cue , that distance information and street names were infrequently used , and that information is used to enable navigation decisions , but also to enhance the pedestrians confidence and trust . the implications for the design of pedestrian navigation aids are highlighted .
cognitive representation of a complex motor action executed by different motor systems . <eos> the present study evaluates the cognitive representation of a kicking movement performed by a human and a humanoid robot , and how they are represented in experts and novices of soccer and robotics , respectively . to learn about the expertise dependent development of memory structures , we compared the representation structures of soccer experts and robot experts concerning a human and humanoid robot kicking movement . we found different cognitive representation structures for both expertise groups under two different motor performance conditions ( human vs. humanoid robot ) . in general , the expertise relies on the perceptual motor knowledge of the human motor system . thus , the soccer experts cognitive representation of the humanoid robot movement is dominated by their representation of the corresponding human movement . additionally , our results suggest that robot experts , in contrast to soccer experts , access functional features of the technical system of the humanoid robot in addition to their perceptual motor knowledge about the human motor system . thus , their perceptual motor and neuro functional machine representation are integrated into a cognitive representation of the humanoid robot movement .
postmarketing surveillance based on electronic patient records the ipci project . <eos> researchers claim that data in electronic patient records can be used for a variety of purposes including individual patient care , management , and resource planning for scientific research . our objective in the project integrated primary care information ( ipci ) was to assess whether the electronic patient records of dutch general practitioners contain sufficient data to perform studies in the area of postmarketing surveillance studies , we determined the data requirements for postmarketing surveillance studies , implemented additional software in the electronic patient records of the general practitioner , developed an organization to monitor the use of data , and performed validation studies to test the quality of the data . analysis of the data requirements showed that additional software had to be installed to collect data that is not recorded in routine practice . to avoid having to obtain informed consent from each enrolled patient , we developed ipci as a semianonymous system both patients and participating general practitioners are anonymous for the researchers . under specific circumstances , the researcher can contact indirectly ( through a trusted third party ) the physician that made the data available . only the treating general practitioner is able to decode the identity of his patients . a board of supervisors predominantly consisting of participating general practitioners monitors the use of data . validation studies show the data can be used for postmarketing surveillance . with additional software to collect data not normally recorded in routine practice , data from electronic patient record of general practitioners can be used for postmarketing surveillance .
efficient algorithms for multichromosomal genome rearrangements . <eos> hannenhalli and pevzner ( 36th annual symposium on foundations of computer science , milwaukee , wi , ieee computer soc . press , los alamitos , ca , <digit> , p. <digit> ) gave a polynomial time algorithm for computing the minimum number of reversals , translocations , fissions , and fusions , that would transform one multichromosomal genome to another when both have the same set of genes without repeats . we fixed some problems with the construction ( <digit> ) they claim it can exhibit such a sequence of steps , but there was a gap in the construction . ( <digit> ) their construction had an asymmetry in the number of chromosomes in the two genomes , whereby forward scenarios could have fissions but not fusions . we also improved the speed by combining the algorithm with the algorithm of bader et al. ( j. comput . biol . <digit> ( <digit> ) ( <digit> ) <digit> ) that computes reversal distances for permutations in linear time .
eye gaze interfaces using electro oculography ( eog ) . <eos> using electro oculography ( eog ) , two types of eye gaze interfaces have been developed eog pointer and eog switch . the former enables a user to move a computer cursor or to control a machine using only eye gaze , regardless of drifting signal and blinking artifacts . in contrast , the latter output an on off signal only . although it has the least simple function , it enables every user easily to turn on off a nurse call device or to send one bit signal to a personal computer with high stability and reliability . since the eog switch was commercialized in <digit> , it has been widely used among amyotrophic lateral sclerosis ( als ) patients in japan .
creational object oriented design pattern applied to the development of software tools for electric power systems dynamic simulations . <eos> the development of software for dynamic simulation of electrical power systems requires a comprehensive range of complex studies , which encompasses many areas of electrical engineering as well as software engineering . this study aims at to develop an efficient strategy applied to the development of software tools for dynamic power systems simulations studies . the proposed strategy is based on the object oriented creational pattern . this approach has the advantage of makes easy the application development process , by performing a mapping between block diagram model representation and the corresponding specialized classes . firstly , a conceptual mapping between block diagram and the object oriented paradigm , based on the factory method , is carried out . after that , some flexible strategies are presented in order to obtain an improved efficiency for the numerical routines , based on the builder standard . this allows for the parameterization of the selected numerical integration techniques . the proposed strategy was evaluated by using a <digit> generator multi machine power system . the simulation results shown that the proposed strategy was able to provides a good power system dynamic performance .
algorithm for faster computation of non zero graph based invariants . <eos> this paper presents a detailed study of the graph based algorithm used to generate geometric moment invariant functions . the graph based algorithm has been found to suffer from high computational complexity . one major cause of this problem is that the algorithm generates too many graphs that produce zero moment invariant functions . hence , we propose an algorithm to determine and eliminate the zero moment invariant generating graphs and thereby generate non zero moment invariant functions with reduced computational complexity . the correctness of the algorithm has been verified and discussed with suitable induction proofs and sample graphs . asymptotic analysis has been presented to clearly illustrate the reduction in computational complexity achieved by the proposed algorithm . it has been found and illustrated with examples that the computational time for identifying non zero invariants could be largely reduced with the help of our proposed algorithm .
removing photography artifacts using gradient projection and flash exposure sampling . <eos> flash images are known to suffer from several problems saturation of nearby objects , poor illumination of distant objects , reflections of objects strongly lit by the flash and strong highlights due to the reflection of flash itself by glossy surfaces . we propose to use a flash and no flash ( ambient ) image pair to produce better flash images . we present a novel gradient projection scheme based on a gradient coherence model that allows removal of reflections and highlights from flash images . we also present a brightness ratio based algorithm that allows us to compensate for the falloff in the flash image brightness due to depth . in several practical scenarios , the quality of flash no flash images may be limited in terms of dynamic range . in such cases , we advocate using several images taken under different flash intensities and exposures . we analyze the flash intensity exposure space and propose a method for adaptively sampling this space so as to minimize the number of captured images for any given scene . we present several experimental results that demonstrate the ability of our algorithms to produce improved flash images .
a stable 3d energetic galerkin bem approach for wave propagation interior problems . <eos> we consider 3d interior wave propagation problems with vanishing initial and mixed boundary conditions , reformulated as a system of two boundary integral equations with retarded potentials . these latter are then set in a weak form , based on a natural energy identity satisfied by the solution of the differential problem , and discretized by the energetic galerkin boundary element method . numerical results are presented and discussed in order to show the stability and accuracy of the proposed technique .
a differential notion of place for local search . <eos> for extracting the characteristics a specific geographic entity , and notably a place , we propose to use dynamic extreme tagging systems in combination with the classic approach of static kr models like ontologies , thesauri and gazetteers . indeed , we argue that in local search , the what that is queried is implicitly about places . however existing knowledge representation ( kr ) models , such as ontologies based on logical theories , conceptual spaces , affordance or other , can not capture in isolation all aspects of the meaning of a place . therefore we propose to use a combination of them based on the underlying notion of differences , linked elements of meaning without commitment to any kr model . mapping to elements of different kr models can be made later to follow the requirements of a given task , supported by a kr representation of the elements that support this task . we show the usefulness of the approach for local search by applying it to the notion of place defined as a location that supports a homogeneous affordance field , i.e. the spatial area which allows me the do a particular thing , while allowing the homogeneity of movement , meaning that the previous field is not interrupted by any boundaries .
innovation outsourcing risks and quality issues . <eos> innovation is the creation of new idea , practice , object , or even product by an individual or company . a competitive organization needs to continuously offer new line of products and services to the market for their customers . in order to cut down their r d costs , companies seek external or even global vendors to pursue their research and development ( r d ) tasks . this paper discusses the issues related to innovation outsourcing , including uncertainty , risks , productivity and quality issues .
firm orientation , community of practice , and internet enabled interfirm communication evidence from chinese firms . <eos> what motivates firms to develop internet enabled interfirm communication we draw upon the work of alavi et al. ( <digit> <digit> ) and propose that the use of the internet in interfirm communication is influenced by a firm 's firm orientation and its internal communities of practice . based on data collected from <digit> international trade firms in the beijing area , we find that internet enabled interfirm communication is directly driven by internal community of practices and customer orientation , and indirectly by competitor orientation and learning orientation . the internal community of practice is affected by learning orientation and competitor orientation , but not by customer orientation . the present study contributes to the literature by providing empirical investigation on firm 's strategic communications from the perspective of firm orientations , delineating how different firm orientations vary in impacting firm 's strategic communications , and exploring the bridging effect of communities of practices on the influences of firm orientations on knowledge management initiatives . ( c ) <digit> elsevier by . all rights reserved .
index of a point of <digit> d digital binary image and algorithm for computing its euler characteristic . <eos> in this work a concept of index of a point of a <digit> d ( <digit> , <digit> ) digital image is defined . basing on this concept a new characterization of the so called simple points <digit> as well as an algorithm for computing euler characteristics of <digit> d ( <digit> , <digit> ) digital pictures is proposed . ( c ) <digit> pattern recognition society . published by elsevier science ltd. all rights reserved .
fluxing botnet command and control channels with url shortening services . <eos> url shortening services ( usses ) , which provide short aliases to registered long urls , have become popular owing to twitter . despite their popularity , researchers do not carefully consider their security problems . in this paper , we explore botnet models based on usses to prepare for new security threats before they evolve . specifically , we consider using usses for alias flux to hide botnet command and control ( c c ) channels . in alias flux , a botmaster obfuscates the ip addresses of his c c servers , encodes them as urls , and then registers them to usses with custom aliases generated by an alias generation algorithm . later , each bot obtains the encoded ip addresses by contacting usses using the same algorithm . for usses that do not support custom aliases , the botmaster can use shared alias lists instead of the shared algorithm . dns based botnet detection schemes can not detect an alias flux botnet , and network level detection and blacklisting of the fluxed aliases are difficult . we also discuss possible countermeasures to cope with these new threats and investigate operating usses . ( c ) <digit> elsevier b.v. all rights reserved .
a generation model to unify topic relevance and lexicon based sentiment for opinion retrieval . <eos> opinion retrieval is a task of growing interest in social life and academic research , which is to find relevant and opinionate documents according to a user 's query . one of the key issues is how to combine a document 's opinionate score ( the ranking score of to what extent it is subjective or objective ) and topic relevance score . current solutions to document ranking in opinion retrieval are generally ad hoc linear combination , which is short of theoretical foundation and careful analysis . in this paper , we focus on lexicon based opinion retrieval . a novel generation model that unifies topic relevance and opinion generation by a quadratic combination is proposed in this paper . with this model , the relevance based ranking serves as the weighting factor of the lexicon based sentiment ranking function , which is essentially different from the popular heuristic linear combination approaches . the effect of different sentiment dictionaries is also discussed . experimental results on trec blog datasets show the significant effectiveness of the proposed unified model . improvements of 28.1 % and 40.3 % have been obtained in terms of map and p <digit> respectively . the conclusion is not limited to blog environment . besides the unified generation model , another contribution is that our work demonstrates that in the opinion retrieval task , a bayesian approach to combining multiple ranking functions is superior to using a linear combination . it is also applicable to other result re ranking applications in similar scenario .
anomalous behavior of the pulse transfer characteristic of a selectively doped alxga1 xas gaas heterostructure containing deep traps . <eos> the pulse transfer characteristic of a normal selectively doped alxga1 xas gaas heterostructure containing deep traps in the alxga1 xas layer is considered . it is shown that these deep traps are responsible for an undershoot in the drain source current at the end of a positive voltage pulse applied to the gate ( the pulse voltage is measured from the initial gate bias ) and the trap depth can be determined from this undershoot .
wireless image sensor networks event acquisition in attack prone and uncertain environments . <eos> wireless image sensor networks ( wisns ) consisting of untethered camera nodes and sensors may be deployed in a variety of unattended and possibly hostile environments to obtain surveillance data . in such settings , the wisn nodes must perform reliable event acquisition to limit the energy , computation and delay drains associated with forwarding large volumes of image data wirelessly to a sink node . in this work we investigate the event acquisition properties of wisns that employ various techniques at the camera nodes to distinguish between event and non event frames in uncertain environments that may include attacks . these techniques include lightweight image processing , decisions from n sensors with without cluster head fault and attack detection , and a combination approach relying on both lightweight image processing and sensor decisions . we analyze the relative merits and limitations of each approach in terms of the resulting probability of event detection and false alarm in the face of occasional errors , attacks and stealthy attacks .
constructing comprehensive summaries of large event sequences . <eos> event sequences capture system and user activity over time . prior research on sequence mining has mostly focused on discovering local patterns appearing in a sequence . while interesting , these patterns do not give a comprehensive summary of the entire event sequence . moreover , the number of patterns discovered can be large . in this article , we take an alternative approach and build short summaries that describe an entire sequence , and discover local dependencies between event types . we formally define the summarization problem as an optimization problem that balances shortness of the summary with accuracy of the data description . we show that this problem can be solved optimally in polynomial time by using a combination of two dynamic programming algorithms . we also explore more efficient greedy alternatives and demonstrate that they work well on large datasets . experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high quality results , and reveal interesting local structures in the data .
combining data remapping and voltage frequency scaling of second level memory for energy reduction in embedded systems . <eos> in this paper we show that the energy reductions obtained from using two techniques , data remapping ( dr ) and voltage frequency scaling of off chip bus and memory , combine to provide interesting trade offs between energy , execution time and power . both methods aim to reduce the energy consumed by the memory subsystem . dr is a fully automatic compile time technique applicable to pointer intensive dynamic applications . voltage frequency scaling of off chip memory is a technique applied at the hardware level . when combined together , energy reductions can be as high as 49.45 % . the improvements are verified in the context of three olden pointer centric benchmarks , namely perimeter , health and tsp .
benefits and barriers of user evaluation in software engineering research . <eos> in this paper , we identify trends about , benefits from , and barriers to performing user evaluations in software engineering research . from a corpus of over 3,000 papers spanning ten years , we report on various subtypes of user evaluations ( e.g. , coding tasks vs. questionnaires ) and relate user evaluations to paper topics ( e.g. , debugging vs. technology transfer ) . we identify the external measures of impact , such as best paper awards and citation counts , that are correlated with the presence of user evaluations . we complement this with a survey of over <digit> researchers from over <digit> different universities and labs in which we identify a set of perceived barriers to performing user evaluations .
quasi one dimensional approximation in the hmo model of polymethine dyes . <eos> in the framework of the hueckel molecular orbital ( hmo ) model , an analytical method has been elaborated which enables calculation of energy levels and wave functions for polymethine dye molecules with arbitrary end groups characterized by two effective additive parameters . the method represents a generalization of the known long chain approximation ( lca ) manipulating only frontier pi mos and yields analytical relations for molecular characteristics based on all occupied dye pi mos .
ontology combined structural and operational semantics for resource oriented service composition . <eos> resource oriented services recently become an enabling technology to integrate and configure information from different heterogeneous systems so as to meet ever changing environment which not only need the concepts for entities but also require the semantics for operations . by the aim of combining structural and operational semantics agilely , a semantic resource service model ( srsm ) is proposed . firstly , srsm describes entity oriented and transition oriented resource by semantic meta model which contains data structures and operation semantics . secondly , by describing structural semantics for entity oriented resource , heterogonous inputs outputs of a service can be automatically matched . thirdly , by describing operational semantics for transition oriented resource , the service composition sequence can be inferred after ontology reasoning . then , both entity oriented and transition oriented resources are encapsulated into composited restful service . at last , a case study and several comparisons are applied in a prototype system . the result shows that the proposed approach provides a flexible way for resource oriented service composition .
analysis of sidewall quality in through wafer deep reactive ion etching . <eos> the quality of channel sidewalls resulting from through wafer deep reactive ion etching is analysed using scanning electron microscopy , atomic force microscopy and interferometry . sidewall quality and profile are highly dependent on the width of the etched channel . channels narrower than <digit> m show generally good sidewall smoothness , though with a bowed profile . this profile leads to ion induced damage towards the bottom of the channel sidewall . wider channels , in contrast , exhibit overpassivation of the sidewalls with a region of thick polymer build up followed by vertical striations and a very rough surface , but with an overall vertical profile . redeposition of the passivation from the trench bottom to the sidewalls as suggested by other researchers is supported by our observations .
design , modelling and analysis of a six component force balance for hypervelocity wind tunnel testing . <eos> a combination of modelling and analysis techniques was used to design a six component force balance . the balance was designed specifically for the measurement of impulsive aerodynamic forces and moments characteristic of hypervelocity shock tunnel testing using the stress wave force measurement technique . aerodynamic modelling was used to estimate the magnitude and distribution of forces and finite element modelling to determine the mechanical response of proposed balance designs . simulation of balance performance was based on aerodynamic loads and mechanical responses using convolution techniques . deconvolution was then used to assess balance performance and to guide further design modifications leading to the final balance design .
towards a pivotal based approach for business process alignment . <eos> this article focuses on business process engineering , especially on alignment between business analysis and implementation . through a business process management approach , different transformations interfere with process models in order to make them executable . to keep the consistency of process model from business model to it model , we propose a pivotal metamodel centric methodology . it aims at keeping or giving all requisite structural and semantic data needed to perform such transformations without loss of information . through this we can ensure the alignment between business and it . this article describes the concept of pivotal metamodel and proposes a methodology using such an approach . in addition , we present an example and the resulting benefits .
ensembles of relational classifiers . <eos> relational classification aims at including relations among entities into the classification process , for example taking relations among documents such as common authors or citations into account . however , considering more than one relation can further improve classification accuracy . here we introduce a new approach to make use of several relations as well as both , relations and local attributes for classification using ensemble methods . to accomplish this , we present a generic relational ensemble model that can use different relational and local classifiers as components . furthermore , we discuss solutions for several problems concerning relational data such as heterogeneity , sparsity , and multiple relations . especially the sparsity problem will be discussed in more detail . we introduce a new method called prnmultihop that tries to handle this problem . furthermore we categorize relational methods in a systematic way . finally , we provide empirical evidence , that our relational ensemble methods outperform existing relational classification methods , even rather complex models such as relational probability trees ( rpts ) , relational dependency networks ( rdns ) and relational bayesian classifiers ( rbcs ) .
3d feature surface properties and their application in geovisualization . <eos> new acquisition methods have increased the availability of surface property data that capture location dependent data on feature surfaces . however , these data are not supported as fully in the geovisualization of the digital city as established data categories such as feature attributes , 2d rasters , or geometry . consequently , 3d surface properties are largely excluded from the information extraction and knowledge creation process of geovisualization despite their potential for being an effective tool in many such tasks . to overcome this situation , this paper examines the benefits of a better integration into geovisualization systems in terms of two examples and discusses technological foundations for surface property support . the main contribution is the identification of computer graphics techniques as a suitable basis for such support . this way , the processing of surface property data fits well into existing visualization systems . this finding is demonstrated through an interactive prototypic visualization system that extends an existing system with surface property support . while this prototype concentrates on technology and neglects user related and task related aspects , the paper includes a discussion on challenges for making surface properties accessible to a wider audience .
an optimization of the icosahedral grid modified by spring dynamics . <eos> we have investigated an optimum form of the modified icosahedral grid that is generated by applying the spring dynamics to the standard icosahedral grid system . the spring dynamics can generate a more homogeneous grid system than the standard icosahedral grid system by tuning the natural spring lenght as the natural spring length becomes longer , the ratio of maximum grid interval to minimum one becomes closer to unit . when the natural spring length is larger than a critical value , however , the spring dynamic system does not have a stable equilibrium . by setting the natural spring length to be the marginally critical value , we can obtain the most homogeneous grid system , which is most efficient in terms of the cfl condition . we have analyzed eigenmodes involved in the initial error of the geostrophic balance problem test case <digit> of d. l. williamson et al. ( <digit> , j. comput . phys . <digit> , <digit> ) . since the balance state in the discrete system differs slightly from the exact solution of the analytic system , the initial error field includes both the gravity wave mode and the rossby wave mode . as the results of the analysis are based on hough harmonics decompositions , we detected rossby and gravity wave modes with zonal wavenumber <digit> , which are asymmetric against the equator . these errors are associated with icosahedral grid structure . the symmetric gravity wave mode with zonal wavenumber <digit> also appears in the error field . to clarify the evolution of rossby waves , we introduce divergence damping to reduce the gravity wave mode . from the simulated results of the geostrophic problem with various grid systems , we found that the spuriously generated rossby wave mode is eliminated most effectively when the most homogeneously distributed grid system is used . it is therefore , concluded that the most homogeneous grid system is the best choice from the viewpoint of numerical accuracy as well as computational efficiency . ( c ) <digit> elsevier science ( usa ) .
design and implementation of the glue nail database system . <eos> we describe the design and implementation of the glue nail database system . the nail language is a purely declarative query language glue is a procedural language used for non query activities . the two languages combined are sufficient to write a complete application . nail and glue code both compile into the target language iglue . the nail compiler uses variants of the magic sets algorithm , and supports well founded models . static optimization is performed by the glue compiler using techniques that include peephole methods and data flow analysis . the iglue code is executed by the iglue interpreter , which features a run time adaptive optimizer . the three optimizers each deal with separate optimization domains , and experiments indicate that an effective synergism is achieved . the glue nail system is largely complete and has been tested using a suite of representative applications .
analog integrated circuit for detection of an approaching object with simple shape recognition based on lower animal vision . <eos> a network for the detection of an approaching object with simple shape recognition is proposed based on lower animal vision . the locust can detect an approaching object through a simple process in the descending contralateral movement detector ( dcmd ) in the locust brain , by which the approach velocity and direction of the object is determined . the frog can recognize simple shapes through a simple process in the tectum and thalamus in the frog brain . the proposed network is constructed of simple analog complementary metal oxide semiconductor ( cmos ) circuits . the integrated circuit of the proposed network is fabricated with the 1.2 mu m cmos process . measured results for the proposed circuit indicate that the approach velocity and direction of an object can be detected by the output current of the analog circuit based on the dcmd response . the shape of moving objects having simple shapes , such as circles , squares , triangles and rectangles , was recognized using the proposed frog visual system based circuit .
the safety of electronic prescribing manifestations , mechanisms , and rates of system related errors associated with two commercial systems in hospitals . <eos> objectives to compare the manifestations , mechanisms , and rates of system related errors associated with two electronic prescribing systems ( e ps ) . to determine if the rate of system related prescribing errors is greater than the rate of errors prevented . methods audit of <digit> inpatient admissions at two hospitals in sydney , australia using the csc medchart and cerner millennium e ps . system related errors were classified by manifestation ( eg , wrong dose ) , mechanism , and severity . a mechanism typology comprised errors made selecting items from drop down menus constructing orders editing orders or failing to complete new e ps tasks . proportions and rates of errors by manifestation , mechanism , and e ps were calculated . results 42.4 % ( n <digit> ) of <digit> prescribing errors were system related ( <digit> <digit> admissions ) . this result did not differ by e ps ( medchart 42.6 % ( <digit> % ci 39.1 to 46.1 ) cerner 41.9 % ( 37.1 to 46.8 ) ) . for 13.4 % ( n <digit> ) of system related errors there was evidence that the error was detected prior to study audit . 27.4 % ( n <digit> ) of system related errors manifested as timing errors and 22.5 % ( n <digit> ) wrong drug strength errors . selection errors accounted for 43.4 % ( 34.2 <digit> admissions ) , editing errors 21.1 % ( 16.5 <digit> admissions ) , and failure to complete new e ps tasks 32.0 % ( 32.0 <digit> admissions ) . medchart generated more selection errors ( or 4.17 p 0.00002 ) but fewer new task failures ( or 0.37 p 0.003 ) relative to the cerner e ps . the two systems prevented significantly more errors than they generated ( <digit> <digit> admissions ( <digit> % ci <digit> to <digit> ) vs <digit> ( <digit> % ci <digit> to <digit> ) ) . conclusions system related errors are frequent , yet few are detected . e ps require new tasks of prescribers , creating additional cognitive load and error opportunities . dual classification , by manifestation and mechanism , allowed identification of design features which increase risk and potential solutions . e ps designs with fewer drop down menu selections may reduce error risk .
sliding windows and persistence an application of topological methods to signal analysis . <eos> we develop in this paper a theoretical framework for the topological study of time series data . broadly speaking , we describe geometrical and topological properties of sliding window embeddings , as seen through the lens of persistent homology . in particular , we show that maximum persistence at the point cloud level can be used to quantify periodicity at the signal level , prove structural and convergence theorems for the resulting persistence diagrams , and derive estimates for their dependency on window size and embedding dimension . we apply this methodology to quantifying periodicity in synthetic data sets and compare the results with those obtained using state of the art methods in gene expression analysis . we call this new method sw1pers , which stands for sliding windows and <digit> dimensional persistence scoring .
stochastic learning solution for distributed discrete power control game in wireless data networks . <eos> distributed power control is an important issue in wireless networks . recently , noncooperative game theory has been applied to investigate interesting solutions to this problem . the majority of these studies assumes that the transmitter power level can take values in a continuous domain . however , recent trends such as the gsm standard and qualcomm 's proposal to the is <digit> standard use a finite number of discretized power levels . this motivates the need to investigate solutions for distributed discrete power control which is the primary objective of this paper . we first note that , by simply discretizing , the previously proposed continuous power adaptation techniques will not suffice . this is because a simple discretization does not guarantee convergence and uniqueness . we propose two probabilistic power adaptation algorithms and analyze their theoretical properties along with the numerical behavior . the distributed discrete power control problem is formulated as an n person , nonzero sum game . in this game , each user evaluates a power strategy by computing a utility value . this evaluation is performed using a stochastic iterative procedures . we approximate the discrete power control iterations by an equivalent ordinary differential equation to prove that the proposed stochastic learning power control algorithm converges to a stable nash equilibrium . conditions when more than one stable nash equilibrium or even only mixed equilibrium may exist are also studied . experimental results are presented for several cases and compared with the continuous power level adaptation solutions .
a novel faster than at speed transition delay test method considering ir drop effects . <eos> interconnect defects such as weak resistive opens , shorts , and bridges increase the path delay affected by a pattern during manufacturing test but are not significant enough to cause a failure at functional frequency . in this paper , a new faster than at speed method is presented for delay test pattern application to screen small delay defects . given a test pattern set , the technique groups the patterns into multiple subsets with close path delay distribution and determines an optimal test frequency considering both positive slack and performance degradation due to ir drop effects . since , the technique does not increase the test frequency to an extent that any paths exercised at the rated functional frequency may fail , it avoids any scan flip flop masking . as most semiconductor companies currently deploy compression technologies to reduce test costs , scan cell masking is highly undesirable for pattern modification as it would imply pattern count increase and might result in pattern regeneration . therefore , our solution is more practical as the test engineer can run the same pattern set without any changes to the test flow other than the at speed test frequency .
a fuzzy mcdm approach for evaluating banking performance based on balanced scorecard . <eos> the paper proposed a fuzzy multiple criteria decision making ( fmcdm ) approach for banking performance evaluation . drawing on the four perspectives of a balanced scorecard ( bsc ) , this research first summarized the evaluation indexes synthesized from the literature relating to banking performance . then , for screening these indexes , <digit> indexes fit for banking performance evaluation were selected through expert questionnaires . furthermore , the relative weights of the chosen evaluation indexes were calculated by fuzzy analytic hierarchy process ( fahp ) . and the three mcdm analytical tools of saw , topsis , and vikor were respectively adopted to rank the banking performance and improve the gaps with three banks as an empirical example . the analysis results highlight the critical aspects of evaluation criteria as well as the gaps to improve banking performance for achieving aspired desired level . it shows that the proposed fmcdm evaluation model of banking performance using the bsc framework can be a useful and effective assessment tool .
geographical information systems and location science . <eos> since the 1970s the field of geographical information systems ( gis ) has evolved into a mature research and application area involving a number of academic fields including geography , civil engineering , computer science , land use planning , and environmental science . gis can support a wide range of spatial queries that can be used to support location studies . gis will play a significant role in future location model development and application . we review existing work that forms the interface between gis and location science and discuss some of the potential research areas involving both gis and location science . during the past <digit> years there have been many developments in spatial data analysis , spatial data storage and retrieval , and mapping . many of these developments have occurred in the field of geographical information science . geographical information systems software now supports many elementary and advanced spatial analytic approaches including the production of high quality maps . gis will have a major impact on the field of location science in terms of model application and model development . the purpose of this paper is to explore the interface between the field of location science and gis .
introduction of r ( m ( rank ) ) ( <digit> ) metric incorporating rank order predictions as an additional tool for validation of qsar qspr models . <eos> in silica techniques involving the development of quantitative regression models have been extensively used for prediction of activity , property and toxicity of new chemicals . the acceptability and subsequent applicability of the models for predictions is determined based on several internal and external validation statistics . among different validation metrics , q ( <digit> ) and r pred ( <digit> ) represent the classical metrics for internal validation and external validation respectively . additionally , the r ( m ) ( <digit> ) metrics introduced by roy and coworkers have been widely used by several groups of authors to ensure the close agreement of the predicted response data with the observed ones . however , none of the currently available and commonly used validation metrics provides any information regarding the rank order predictions for the test set . thus , to incorporate the concept of ranking order predictions while calculating the common validation metrics originally using the pearson 's correlation coefficient based algorithm , the new r ( m ( rank ) ) ( <digit> ) metric has been introduced in this work as a new variant of the r ( m ) ( <digit> ) series of metrics . the ability of this new metric to perform the rank order prediction is determined based on its application in judging the quality of predictions of regression based quantitative structure activity property relationship ( qsar qspr ) models for four different data sets . the different validation metrics calculated in each case were compared for their ability to reflect the rank order predictions based on their correlation with the conventional spearman 's rank correlation coefficient . based on the results of the sum of ranking differences analysis performed using the spearman 's rank correlation coefficient as the reference , it was observed that the ( <digit> ) ( m ( rank ) ) metric exhibited the least difference in ranking from that of the reference metric . thus , the close correlation of the ( <digit> ) ( m ( rank ) ) metric with the spearman 's rank correlation coefficient inferred that the new metric could aptly perform the rank order prediction for the test data set and can be utilized as an additional validation tool , besides the conventional metrics , for assessing the acceptability and predictive ability of a qsar qspr model . ( c ) <digit> elsevier b.v. all rights reserved .
designing ubiquitous information systems for a community of homeless young people precaution and a way forward . <eos> drawing upon and distinguishing themselves from domestic , public , work , and natural settings , homeless communities offer new cultural frontiers into which ubiquitous computing could diffuse . we report on one such frontier , a community of homeless young people , located in seattle , wa , seeking both to foresee the consequences of pervasive access to digital media and communications and to prepare for its seemingly inevitable uptake . the community consists of hundreds of young people living without stable housing , often in the public , and an alliance of nine service agencies that seek to stabilize youth and equip them to escape homelessness . we examine the opportunities for ubiquitous computing in this community by , in part , developing a precautionary stance on intervention . this stance is then used to critically examine a scenario in which information about the service agencies is made public . from this scenario , and a description of the social and material constraints of this community , we argue that precaution offers productive counsel on decisions on whether and how to intervene with ubiquitous computing . a precautionary point of view is especially important as ubiquitous computing diffuses into communities that , by their social and material conditions , are vulnerable . in such communities , the active avoidance of harms and plans for their mitigation is particularly important .
on the equivalence of two optimization methods for fuzzy linear programming problems . <eos> the paper analyses the linear programming problem with fuzzy coefficients in the objective function . the set of nondominated ( nd ) solutions with respect to an assumed fuzzy preference relation , according to orlovsky 's concept , is supposed to be the solution of the problem . special attention is paid to unfuzzy nondominated ( und ) solutions ( the solutions which are nondominated to the degree one ) . the main results of the paper are sufficient conditions on a fuzzy preference relation allowing to reduce the problem of determining und solutions to that of determining the optimal solutions of a classical linear programming problem . these solutions can thus be determined by means of classical linear programming methods .
identifying failure causes in java programs an application of change impact analysis . <eos> during program maintenance , a programmer may make changes that enhance program functionality or fix bugs in code . then , the programmer usually will run unit regression tests to prevent invalidation of previously tested functionality . if a test fails unexpectedly , the programmer needs to explore the edit to find the failure inducing changes for that test . crisp uses results from chianti , a tool that performs semantic change impact analysis <digit> , to allow the programmer to examine those parts of the edit that affect the failing test . crisp then builds a compilable intermediate version of the program by adding a programmer selected partial edit to the original code , augmenting the selection as necessary to ensure compilation . the programmer can reexecute the test on the intermediate version in order to locate the exact reasons for the failure by concentrating on the specific changes that were applied . in nine initial case studies on pairs of versions from two real java programs , daikon <digit> and eclipse jdt compiler <digit> , we were able to use crisp to identify the failure inducing changes for all but <digit> of <digit> failing tests . on average , <digit> changes were found to affect each failing test ( of the <digit> ) , but only <digit> <digit> of these changes were found to be actually failure inducing .
dexterous workspace optimization of an asymmetric six degree of freedom stewart gough platform type manipulator . <eos> in this paper , an asymmetric generalized stewart gough platform ( gsp ) type parallel manipulator is designed by considering the type synthesis approach . the asymmetric six degree of freedom ( doe ) manipulator optimized in this paper is selected among the gsps classified under the name of 6d . the dexterous workspace optimization of asymmetric parallel manipulator with ten different linear actuator lengths ( amedial ) subject to kinematics and geometric constraints is performed by using the particle swarm optimization ( pso ) . the condition number and minimum singular value ( msv ) of homogenized jacobian matrix are employed to obtain the dexterous workspace of amedlal . finally , the six dof amedlal is also compared with the optimized traditional stewart gough platform manipulator ( tspm ) considering the volume of the dexterous workspace in order to demonstrate its kinematic performance . comparisons show that the manipulator proposed in this study illustrates better kinematic performance than tspm . ( c ) <digit> elsevier b.v. all rights reserved .
solving systems of two sided ( max , min ) linear equations . <eos> a finite iteration method for solving systems of ( max , min ) linear equations is presented . the systems have variables on both sides of the equations . the algorithm has polynomial complexity and may be extended to wider classes of equations with a similar structure .
fuzzy association degree with delayed time in temporal data model . <eos> this paper presents an expression of the semantic proximity . based on the temporal data model , a method of the temporal approximation is given . using these concepts , this paper provides an evaluated method of fuzzy and dynamic association degree with delayed time and a superposition method of association degrees . particularly , by means of the fuzzy and dynamic association degree , the connection between the weather data of two regions can be discovered .
on line prediction of micro turning multi response variables by machine vision system using adaptive neuro fuzzy inference system ( anfis ) . <eos> in this paper , a new attempt has been made in the area of tool based micromachining for automated , non contact , and flexible prediction of quality responses such as average surface roughness ( r ( a ) ) , tool wear ratio ( twr ) and metal removal rate ( mrr ) of micro turned miniaturized parts through a machine vision system ( mvs ) which is integrated with an adaptive neuro fuzzy inference system ( anfis ) . the images of machined surface grabbed by the mvs could be extracted using the algorithm developed in this work , to get the features of image texture average gray level ( g ( a ) ) . this work presents an area based surface characterization technique which applies the basic light scattering principles used in other optimal optical measurement systems . these principles are applied in a novel fashion which is especially suitable for in process prediction and control . the main objective of this study is to design an anfis for estimation of r ( a ) , twr , and mrr in micro turning process . cutting speed ( s ) , feed rate ( f ) , depth of cut ( d ) , g ( a ) were taken as input parameters and r ( a ) , twr , mrr as the output parameters . the results obtained from the anfis model were compared with experimental values . it is found that the predicted values of the responses are in good agreement with the experimental values .
gallium arsenide passivation method for the employment of high electron mobility transistors in liquid environment . <eos> we report on effective prevention of gaas corrosion in a cell culture liquid environment by means of polymerized ( <digit> mercaptopropyl ) trimethoxysilane thin film coatings . aging in physiological solution kept at 37c revealed no significant oxidation after 2weeks , which is the typical period of incubation of a neuron cells culture . the method was also applied to high electron mobility transistors ( hemt ) arrays with unmetallized gate regions , in view of their application as neural signal transducers . significant reduction of the degradation of the hemt behavior was obtained , as compared to uncoated hemts , with good channel modulation efficiency still after 30days aging .
using shape distributions to compare solid models . <eos> our recent work has described how to use feature and topology in formation to compare <digit> d solid models . in this work we describe a new method to compare solid models based on shape distributions . shape distribution functions are common in the computer graphics and computer vision communities . the typical use of shape dis tributions is to compare <digit> d objects , such as those obtained from imaging devices ( cameras and other computer vision equipment ) . recent work has applied shape distribution metrics for compari son of approximate models found in the graphics community , such as polygonal meshes , faceted representation , and virtual reality modeling language ( vrml ) models . this paper examines how to adapt these techniques to comparison of <digit> d solid models , such as those produced by commercial cad systems . we provide a brief review of shape matching with distribution functions and present an approach to matching solid models . first , we show how to ex tend basic distribution based techniques to handle cad data that has been exported to vrml format . these extensions address specific geometries that occur in mechanical cad data . second , we describe how to use shape distributions to directly interrogate solid models . lastly , we show how these techniques can be put together to provide a query by example interface to a large , het erogeneous , cad database the national design repository . one significant contribution of our work is the systematic technique for performing consistent , engineering content based comparisons of cad models produced by different cad systems .
navigation based self optimization handover mechanism for mobile relay stations in wimax networks . <eos> organic computing has similar characteristics of organism which can be self adjustment for a variety of conditions . moreover , during the wireless communication technological evolution progress , wimax ( worldwide interoperability for microwave access ) offers ability of high capacity and far distance transmission . wimax provides high speed access and a coverage range across several kilometers , but the actual coverage range was merely a few kilometers due to the shelter of buildings or terrain . ieee 802.16 working group designed 802.16 j based rs ( relay station ) to overcome above problem . in this paper , we present a mechanism called self optimization handover mechanism . this mechanism is using gps ( global positioning system ) navigation system to gather the related information for the position and combine the mobility characteristics of mobile relay station . especially , the concept of self optimization of organic computing has been integrated into this mechanism . there are some advantages for this new mechanism , including ( <digit> ) the base station can provide advance plan and select the path . ( <digit> ) the mechanism can reduce the number of possible handover and hop . ( <digit> ) the mechanism can reduce the time of channel scan .
evolutionary computation approaches for real offshore wind farm layout a case study in northern europe . <eos> this paper presents new evolutionary computation algorithms for a problem of wind farm design . the algorithms tackle two different problems of offshore wind farm layout . experiments in a real offshore wind farm layout case are shown and discussed .
scalability comparison of peer to peer similarity search structures . <eos> due to the increasing complexity of current digital data , similarity search has become a fundamental computational task in many applications . unfortunately , its costs are still high and grow linearly on single server structures , which prevents them from efficient application on large data volumes . in this paper , we shortly describe four recent scalable distributed techniques for similarity search and study their performance in executing queries on three different datasets . though all the methods employ parallelism to speed up query execution , different advantages for different objectives have been identified by experiments . the reported results would be helpful for choosing the best implementations for specific applications . they can also be used for designing new and better indexing structures in the future .
estimation of power quality indices in distributed generation systems during power islanding conditions . <eos> this paper presents a new , fast modified recursive gaussnewton ( mrgn ) method for the estimation of power quality indices in distributed generating systems during both islanding and non islanding conditions . a forgetting factor weighted error cost function is minimized by the well known gaussnewton algorithm and the resulting hessian matrix is approximated by ignoring the off diagonal terms . this simplification produces a decoupled algorithm , for the fundamental and harmonic components and results in a large reduction of computational effort , when the power signal contains a large number of harmonics . numerical experiments have shown that the proposed approach results in higher speed of convergence , accurate tracking of power signal parameters in the presence noise , waveform distortion , etc. , which are suitable for the estimation of power quality indices . in the case of a distribution network , power islands occur when power supply from the main utility is interrupted due to faults or otherwise and the distributed generation system ( dg ) keeps supplying power into the network . further , due to unbalanced load conditions the dg is subject to unbalanced voltages at its terminals and suffers from increased total harmonic distortion ( thd ) . thus , the power quality indices estimation , along with the power system frequency estimation will play a vital role in detecting power islands in distributed generating systems . extensive studies , both on simulated and real , benchmark hybrid distribution networks , involving distributed generation systems reveal the effectiveness of the proposed approach to calculate the power quality indices accurately .
the bud scar based screening system for hunting human genes extending life span . <eos> we developed a high throughput screening system that allows identification of genes prolonging life span in the budding yeast saccharomyces cerevisiae . the method is based on isolating yeast mother cells with an extended number of cell divisions as indicated by the increased number of bud scars on their surface . fluorescently labeled wheat germ agglutinin ( wga ) was used for specific staining of bud scars . screening of a human hepg2 cdna expression library in yeast resulted in the isolation of several yeast transformants with a potentially prolonged life span . the budding yeast s. cerevisiae , one of the favorite models used to study aging , has been studied extensively for the better understanding of the mechanisms of human aging . because human disease genes often have yeast counterparts , they can be studied efficiently in this organism . one interesting example is the wrn gene , the human dna helicase , which participates in the dna repair pathway . the mutation of the wrn gene causes werner syndrome showing premature aging phenotype . budding yeast contains wrn homologue , sgs1 , and its mutation results in shortening yeast life span . the knowledge gained from the studies of budding yeast will benefit studies in humans for better understanding of aging and aging related disease .
matching office firms types and location characteristics an exploratory analysis using bayesian classifier networks . <eos> while most models of location decisions of firms are based on the principle of utility maximizing behavior , the present study assumes that location decisions are just part of business cycle models , in which location is considered along other business decisions . the business model results in a series of location requirements and these are matched against location characteristics . given this theoretical perspective , the modeling challenge then becomes how to find the match between firm types and the set of location characteristics using observations of the spatial distribution of firms . in this paper , several bayesian classifier networks are compared in terms of their performance , using a large data set collected for the netherlands . results demonstrate that by taking relationships between predictor variables into account the bayesian classifiers can improve prediction accuracy compared to commonly used decision tree . from a substantive point of view , our results indicate that different sets of urban characteristics and accessibility requirements are relevant to different office types as reflected in the spatial distribution of these office firms .
mental models of recursive computations vs. recursive analysis in the problem domain . <eos> the work outlined here was inspired by <digit> , <digit> , where the authors analyze the mental models of recursion by looking at how students trace simple recursive computations . besides trying to understand if their results generalize to a different context , i was interested to see the correlations between the mental models of the computation process and the ability to establish recursive relationships in the problem domain . my investigation essentially lends further support to the findings of <digit> . however , a consistent mental model of recursive computations , although implied by the ability to use recursion in problem solving , does not seem to be sufficient for the achievement of this higher level skill .
scaling up multi touch selection and querying interfaces and applications for combining mobile multi touch input with large scale visualization displays . <eos> we present a mobile multi touch interface for selecting , querying , and visually exploring data visualized on large , high resolution displays . although emerging large ( e.g. , 10m wide ) , high resolution displays provide great potential for visualizing dense , complex datasets , their utility is often limited by a fundamental interaction problem the need to interact with data from multiple positions around a large room . our solution is a selection and querying interface that combines a hand held multi touch device with <digit> degree of freedom tracking in the physical space that surrounds the large display . the interface leverages context from both the user 's physical position in the room and the current data being visualized in order to interpret multi touch gestures . it also utilizes progressive refinement , favoring several quick approximate gestures as opposed to a single complex input in order to most effectively map the small mobile multi touch input space to the large display wall . the approach is evaluated through two interdisciplinary visualization applications a multi variate data visualization for social scientists , and a visual database querying tool for biochemistry . the interface was effective in both scenarios , leading to new domain specific insights and suggesting valuable guidance for future developers .
knowledge based linguistic equations for defect detection through functional testing of printed circuit boards . <eos> increasing globalization of the economy is imposing tough challenges to manufacturing companies . the ability to produce highly customized products , in order to satisfy market niches , requires the introduction of new features in automation systems . flexible manufacturing processes must be able to handle unforeseen events , but their complexity makes the supervision and maintenance task difficult to perform by human operators . this paper describes how linguistic equations ( le ) , an intelligent method derived from fuzzy algorithms , has been used in a decision helping tool for electronic manufacturing . in our case the company involved in the project is mainly producing control cards for the automotive industry . in their business , nearly <digit> % of the cost of a product is material cost . detecting defects and repairing the printed circuit boards is therefore a necessity . with an ever increasing complexity of the products , defects are very likely to occur , no matter how much attention is put into their prevention . therefore , the system described in this paper comes into use only during the final testing of the product and is purely oriented towards the detection and localization of defects . final control is based on functional testing . using linguistic equations and expert knowledge , the system is able to analyze that data and successfully detect and trace a defect in a small area of the printed circuit board . if sufficient amount of data is provided , self tuning and self learning methods can be used . diagnosis effectiveness can therefore be improved from detection of a functional area towards component level analysis .
dynamic neural network approach for tool cutting force modelling of end milling operations . <eos> this paper uses the artificial neural networks ( anns ) approach to evolve an efficient model for estimation of cutting forces , based on a set of input cutting conditions . neural network ( nn ) algorithms are developed for use as a direct modelling method , to predict forces for ball end milling operations . prediction of cutting forces in ball end milling is often needed in order to establish automation or optimization of the machining processes . supervised nns are used to successfully estimate the cutting forces developed during end milling processes . the training of the networks is preformed with experimental machining data . the predictive capability of using analytical and nn approaches is compared . nn predictions for three cutting force components were predicted with <digit> % error by comparing with the experimental measurements . exhaustive experimentation is conduced to develop the model and to validate it . by means of the developed method , it is possible to forecast the development of events that will take place during the milling process without executing the tests . the force model can be used for simulation purposes and for defining threshold values in cutting tool condition monitoring system . it can be used also in the combination for monitoring and optimizing of the machining process cutting parameters .
contexts built and found a pilot study on the process of archival meaning making . <eos> over the last <digit> years , humanities and archival scholars have theorized the ways in which archives imbue records with meaning . however , archival scholars have not sufficiently examined how users understand the meaning of the records they find . building on the premise that how users come to make meaning from records is greatly in need of examination , this paper reports on a pilot study of four book history students and their processes of archival meaning making . we focus in particular on behaviors of an interpretive rather than forensic nature . this article includes a discussion of the theoretical concepts and scholarly literature that shaped our goals for this paper . it then discusses the methodology and our interpretations of the research findings , before turning to a discussion of the findings implications and directions for future work .
localizing web videos using social images . <eos> while inferring the geo locations of web images has been widely studied , there is limited work engaging in geo location inference of web videos due to inadequate labeled samples available for training . however , such a geographical localization functionality is of great importance to help existing video sharing websites provide location aware services , such as location based video browsing , video geo tag recommendation , and location sensitive video search on mobile devices . in this paper , we address the problem of localizing web videos through transferring large scale web images with geographic tags to web videos , where near duplicate detection between images and video frames is conducted to link the visually relevant web images and videos . to perform our approach , we choose the trustworthy web images by evaluating the consistency between the visual features and associated metadata of the collected images , therefore eliminating the noisy images . in doing so , a novel transfer learning algorithm is proposed to align the landmark prototypes across both domains of images and video frames , leading to a reliable prediction of the geo locations of web videos . a group of experiments are carried out on two datasets which collect flickr images and youtube videos crawled from the web . the experimental results demonstrate the effectiveness of our video geo location inference approach which outperforms several competing approaches using the traditional frame level video geo location inference .
bixid a bidirectional transformation language for xml . <eos> often , independent organizations de . ne and advocate different xml formats for a similar purpose and , as a result , application programs need to mutually convert between such formats . existing xml transformation languages , such as xslt and xduce , are unsatisfactory for this purpose since we would have to write , e. g. , two programs for the forward and the backward transformations in case of two formats , incur high developing and maintenance costs . this paper proposes the bidirectional xml transformation language bixid , allowing us to write only one program for both directions of conversion . our language adopts a common paradigm programming by relation , where a program defines a relation over documents and transforms a document to another in a way satisfying this relation . our contributions here are specific language features for facilitating realistic conversions whose target formats are loosely in parallel but have many discrepancies in details . concretely , we ( <digit> ) adopt xduce style regular expression patterns for describing and analyzing xml structures , ( <digit> ) fully permit ambiguity for treating formats that do not have equivalent expressivenesses , and ( <digit> ) allow non linear pattern variables for expressing non trivial transformations that can not be written only with linear patterns , such as conversion between unordered and ordered data . we further develop an efficient evaluation algorithm for bixid , consisting of the parsing phase that transforms the input document to an intermediate parse tree structure and the unparsing phase that transforms it to an output document . both phases use a variant of finite tree automata for performing a one pass scan on the input or the parse tree by using a standard technique that maintains the set of all transitable states . however , the construction of the unparsing phase is challenging since ambiguity causes different ways of consuming the parse tree and thus results in multiple possible outputs that may have different structures . we have implemented a prototype system of bixid and confirmed that it has enough expressiveness and a linear time performance from experiments with several realistic bidirectional transformations including one between vcard xml and contactxml .
real time self maintenable data warehouse . <eos> data warehousing is an approach to data integration wherein integrated information is stored in a data warehouse for direct querying and analysis . to provide fast access , a data warehouse stores materialized views of the sources of its data . as a result , a data warehouse needs to be maintained to keep its contents consistent with the contents of its data sources . incremental maintenance is generally regarded as a more efficient way to maintain materialized views in a data warehouse . in this paper a strategy for the maintenance of data warehouse is presented . it has the following characteristics it is self maintainable ( weak ) , incremental , non blocking ( the analysts transactions and the maintenance transaction are executed concurrently ) and is performed in real time . the proposed algorithm is implemented for view definition spj ( select project join ) queries and it calculates the aggregate functions sum , avg , count , min and max . aggregate functions are calculated like algebraic functions ( the new result of the function can be computed using some small , constant size storage that accompanies the existing value of the aggregate ) . we have named this improved algorithm vnltr ( unlimited v ( versions ) , nl ( non blocking ) , tr ( in real time ) ) .
content based tag generation to enable a tag based collaborative tv recommendation system . . <eos> with the application of the web 2.0 philosophy to more and more online services and platforms , tagging has become a well established collaboration method . it is often used to simplify organization , navigation and discovery of information and resources in huge archives . in parallel , due to recent developments in digital television , audiences are confronted with a rising amount of available content and demand for better ways to discover programs of interest . in this paper , we propose a tagging based solution to this problem . using a content based filtering approach , we present an individualized and flexible tag generation process . user specific as well as collaborative tag generation is enabled . based on generated and user added tags , program recommendations are derived in a collaborative filtering step .
the m m <digit> queue with inventory , lost sale , and general lead times . <eos> we consider an m m <digit> queueing system with inventory under the ( ( r , q ) ) policy and with lost sales , in which demands occur according to a poisson process and service times are exponentially distributed . all arriving customers during stockout are lost . we derive the stationary distributions of the joint queue length ( number of customers in the system ) and on hand inventory when lead times are random variables and can take various distributions . the derived stationary distributions are used to formulate long run average performance measures and cost functions in some numerical examples .
dynamic data rectification using particle filters . <eos> the basis of dynamic data rectification is a dynamic process model . the successful application of the model requires the fulfilling of a number of objectives that are as wide ranging as the estimation of the process states , process signal denoising and outlier detection and removal . current approaches to dynamic data rectification include the conjunction of the extended kalman filter ( ekf ) and the expectation maximization algorithm . however , this approach is limited due to the ekf being less applicable where the state and measurement functions are highly non linear or where the posterior distribution of the states is non gaussian . this paper proposes an alternative approach whereby particle filters , based on the sequential monte carlo method , are utilized for dynamic data rectification . by formulating the rectification problem within a probabilistic framework , the particle filters generate monte carlo samples from the posterior distribution of the system states , and thus provide the basis for rectifying the process measurements . furthermore , the proposed technique is capable of detecting changes in process operation and thus complements the task of process fault diagnosis . the appropriateness of particle filters for dynamic data rectification is demonstrated through their application to an illustrative non linear dynamic system , and a benchmark ph neutralization process . ( c ) <digit> elsevier ltd. all rights reserved .
neighbor sensor networks increasing lifetime and eliminating partitioning through cooperation . <eos> in this paper we consider neighbor sensor networks which are defined as multiple wireless sensor networks under the administration of different authorities but located physically on the same area or close to each other . we construct a linear programming framework to characterize the cooperation of neighbor sensor networks in comparison to non cooperating networks . we show that if neighbor sensor networks cooperate with each other for relaying data packets then this cooperation brings two advantages as compared to no cooperation case . first , lifetime of both networks is prolonged the results of our analysis show that cooperation between neighbor sensor networks can significantly extend the overall network lifetime . second , cooperation reduces the probability of disjoint partitions arising due to the limited transmission ranges of sensor nodes . when neighbor sensor networks cooperate , eliminating disjoint partitions is possible with sensors having shorter transmission ranges as demonstrated and quantified by our analysis .
image error concealment based on qim data hiding in dual tree complex wavelets . <eos> transmission of block coded images through error prone radio mobile channel often results in lost blocks . error concealment ( ec ) techniques exploit inherent redundancy and reduce visual artifacts through post processing at the decoder side . in this paper , we propose an efficient quantization index modulation ( qim ) based data hiding scheme using dual tree complex wavelet transform ( dtcwt ) for the application of image error concealment . the goal is achieved by embedding important information ( image digest ) as watermark signal that is extracted from the original image itself and is used to introduce sufficient redundancy in the transmitted image . at the decoder side , the extracted image digest is used to correct the damaged regions . dtcwt offers three fold advantages viz. ( <digit> ) high embedding capacity due to inherent redundancy that leads to the better reconstruction of high volume missing data , ( <digit> ) better imperceptibly after data embedding since it most closely captures human visual system ( hvs ) characteristics than conventional dwt and ( <digit> ) better watermark decoding reliability . simulation results duly support the claims and relative performance improvement with respect to the existing results .
visualizing typical and exotic internet traffic data . <eos> the threat of cyber attacks motivates the need to monitor internet traffic data for potentially abnormal behavior . due to the enormous volumes of such data , statistical process monitoring tools , such as those traditionally used on data in the product manufacturing arena , are inadequate . exotic data may indicate a potential attack detecting such data requires a characterization of typical data . we devise some new graphical displays , including a skyline plot , that permit ready visual identification of unusual internet traffic patterns in streaming data , and use appropriate statistical measures to help identify potential cyberattacks . these methods are illustrated on a moderate sized data set ( 135,605 records ) collected at george mason university . ( c ) <digit> elsevier b.v. all rights reserved .
scheduling and student performance . <eos> we present data showing strong correlation between students ' time management and a successful outcome on programming assignments . students who spread their work over more time will produce a better result without additional expenditure of total effort . we examined performance of students who sometimes did well and sometimes did poorly , and found that their good performance occurred on the projects where they displayed better time management . while these results will not surprise most instructors , hard data is more compelling than intuition when trying to train students to use good time management .
access control and key management for mobile agents . <eos> security is a fundamental precondition for the acceptance of mobile agent systems . in this paper we present a mobile agent structure which supports authentication , security management and access control for mobile agents .
empirical distribution of k word matches in biological sequences . <eos> this study focuses on an alignment free sequence comparison method the number of words of length k shared between two sequences , also known as the d2 d <digit> statistic . the advantages of the use of this statistic over alignment based methods are firstly that it does not assume that homologous segments are contiguous , and secondly that the algorithm is computationally extremely fast , the runtime being proportional to the size of the sequence under scrutiny . existing applications of the d2 d <digit> statistic include the clustering of related sequences in large est databases such as the stack database . such applications have typically relied on heuristics without any statistical basis . rigorous statistical characterisations of the distribution of d2 d <digit> have subsequently been undertaken , but have focussed on the distribution 's asymptotic behaviour , leaving the distribution of d2 d <digit> uncharacterised for most practical cases . the work presented here bridges these two worlds to give usable approximations of the distribution of d2 d <digit> for ranges of parameters most frequently encountered in the study of biological sequences .
replications types in experimental disciplines . <eos> experiment replication is a key component of the scientific paradigm . the purpose of replication is to verify previously observed findings . although some software engineering ( se ) experiments have been replicated , yet , there is still disagreement about how replications should be run in our field . with the aim of gaining a better understanding of how replications are carried out , this paper examines different replication types in other scientific disciplines . we believe that by analysing the replication types proposed in other disciplines it is possible to clarify some of the question marks still hanging over experimental se replication .
optimization models to characterize the broadcast capacity of vehicular ad hoc networks . <eos> broadcast capacity of the entire network is one of the fundamental properties of vehicular ad hoc networks ( vanets ) . it measures how efficiently the information can be transmitted in the network and usually it is limited by the interference between the concurrent transmissions in the physical layer of the network . this study defines the broadcast capacity of vehicular ad hoc network as the maximum successful concurrent transmissions . in other words , we measure the maximum number of packets which can be transmitted in a vanet simultaneously , which characterizes how fast a new message such as a traffic incident can be transmitted in a vanet . integer programming ( ip ) models are first developed to explore the maximum number of successful receiving nodes as well as the maximum number of transmitting nodes in a vanet . the models embed an traffic flow model in the optimization problem . since ip model can not be efficiently solved as the network size increases , this study develops a statistical model to predict the network capacity based on the significant parameters in the transportation and communication networks . mitsimlab is used to generate the necessary traffic flow data . response surface method and linear regression technologies are applied to build the statistical models . thus , this paper brings together an array of tools to solve the broadcast capacity problem in vanets . the proposed methodology provides an efficient approach to estimate the performance of a vanet in real time , which will impact the efficacy of travel decision making .
improving hierarchical cluster analysis a new method with outlier detection and automatic clustering . <eos> techniques based on agglomerative hierarchical clustering constitute one of the most frequent approaches in unsupervised clustering . some are based on the single linkage methodology , which has been shown to produce good results with sets of clusters of various sizes and shapes . however , the application of this type of algorithms in a wide variety of fields has posed a number of problems , such as the sensitivity to outliers and fluctuations in the density of data points . additionally , these algorithms do not usually allow for automatic clustering . in this work we propose a method to improve single linkage hierarchical cluster analysis ( hca ) , so as to circumvent most of these problems and attain the performance of most sophisticated new approaches . this completely automated method is based on a self consistent outlier reduction approach , followed by the building up of a descriptive function . this , in turn , allows to define natural clusters . finally , the discarded objects may be optionally assigned to these clusters . the validation of the method is carried out by employing widely used data sets available from literature and others for specific purposes created by the authors . our method is shown to be very efficient in a large variety of situations . ( c ) <digit> elsevier b.v. all rights reserved .
the sensitivity of the inventory model with partial backorders . <eos> this paper uses the rigorous methods of mathematics to explore the analysis of the sensitivity of park int . j. syst . sci . <digit> ( <digit> ) <digit> . however , park discusses the analysis of the sensitivity by numerical examples . the results obtained by this paper show that the sensitivity of park is not always true sometimes . therefore , the researchers may be very careful to use the conclusions of the analysis of the sensitivity made by numerical examples in general .
performance evaluation of an advanced dwdm rofso system for transmitting multiple rf signals . <eos> with the increase of communication demand and the emergence of new services . various innovative wireless technologies have been deployed recently . free space optics ( fso ) links combined with radio over fiber ( rof ) technology can realize a cost effective heterogeneous wireless access system for both urban and rural areas . in this paper , we introduce a newly developed advanced dwdm radio on fso ( rofso ) system capable of simultaneously transmitting multiple radio frequency ( rf ) signals carrying various wireless services including w cdma , wlan ieee802 .1 lg and isdb t signals over fso link . we present an experimental performance evaluation of transmitting rf signals using the rofso system over a i kin link under different deployment environment conditions , this , work represents a pioneering attempt , based on a realistic operational scenario , aiming at demonstrating the rofso system can be conveniently used as a reliable alternative broadband wireless technology for complementing optical fiber networks in areas where the deployment of optical fiber is not feasible .
sonar based mobile robot localization by using fuzzy triangulation . <eos> the objective of this paper is to identify the robot 's location in a global map from solely sonar based information . this is achieved by using fuzzy sets to model sonar data and by using fuzzy triangulation to identify robot 's position and orientation . as a result we obtain a fuzzy position region where each point in the region has a degree of certainty of being the actual position of the robot . ( c ) <digit> elsevier science b.v. all rights reserved .
fuzzy representation for classification of basic bruise colours . <eos> an application of the fuzzy inference system ( fis ) for bruise colour recognition is suggested in the paper . input information to the system will be taken from the images , which includes a bruise and surrounding healthy skin . there are formulated six basic colour groups for the bruise images red , blue , yellow , brown , green and purple . the input variables of the fis are connected with the information from the pixels of the images in some colour models ( rgb , hsv or lab ) . the output variables are the classes the basic colour groups . matlab environment was used for representation of the membership functions .
silicon template fabrication for imprint process with good demolding characteristics . <eos> demolding force for thermal imprint process to polymethylmethacrylate ( pmma ) film is examined by use of si templates with various side wall profiles . patterns with tapered side wall profile can be fabricated by control of etching conditions . side wall profile can be smoothened by anisotropic etching by use of mixed solution of potassium hydroxide ( koh ) solution and isopropyl alcohol . it is confirmed that demolding force can be reduced when mold with tapered side wall pattern is used . demolding force can be greatly reduced by koh treatment . especially , when the template with taper and smooth side wall patterns is used , demolding force is below our measurement system limit of 0.1 kgf . it is confirmed that the koh treatment is very effective in order to reduce demolding force .
by all these lovely tokens ... merging conflicting tokenizations . <eos> given the contemporary trend to modular nlp architectures and multiple annotation frameworks , the existence of concurrent tokenizations of the same text represents a pervasive problem in everydays nlp practice and poses a non trivial theoretical problem to the integration of linguistic annotations and their interpretability in general . this paper describes a solution for integrating different tokenizations using a standoff xml format , and discusses the consequences from a corpus linguistic perspective .
an application of quantum finite automata to interactive proof systems . <eos> quantum finite automata have been studied intensively since their introduction in late 1990s as a natural model of a quantum computer working with finite dimensional quantum memory space . this paper seeks their direct application to interactive proof systems in which a mighty quantum prover communicates with a quantum automaton verifier through a common communication cell . our quantum interactive proof systems are juxtaposed to dwork stockmeyer 's classical interactive proof systems whose verifiers are two way probabilistic finite automata . we demonstrate strengths and weaknesses of our systems by studying how various restrictions on the behaviors of quantum automaton verifiers affect the power of quantum interactive proof systems . ( c ) <digit> published by elsevier inc .
an accurate radio channel model for wireless sensor networks simulation . <eos> simulations are currently an essential tool to develop and test wireless sensor networks ( wsns ) protocols and to analyze future wsns applications performance . researchers often simulate their proposals rather than deploying high cost test beds or develop complex mathematical analysis . however , simulation results rely on physical layer assumptions , which are not usually accurate enough to capture the real behavior of a wsn . such an issue can lead to mistaken or questionable results . besides , most of the envisioned applications for wsns consider the nodes to be at the ground level . however , there is a lack of radio propagation characterization and validation by measurements with nodes at ground level for actual sensor hardware . in this paper , we propose to use a low computational cost , two slope , log normal path loss near ground outdoor channel model at <digit> mhz in wsn simulations . the model is validated by extensive real hardware measurements obtained in different scenarios . in addition , accurate model parameters are provided . this model is compared with the well known one slope path loss model . we demonstrate that the two slope log normal model provides more accurate wsn simulations at almost the same computational cost as the single slope one . it is also shown that the radio propagation characterization heavily depends on the adjusted model parameters for a target deployment scenario the model parameters have a considerable impact on the average number of neighbors and on the network connectivity .
personalized text snippet extraction using statistical language models . <eos> in knowledge discovery in a text database , extracting and returning a subset of information highly relevant to a user 's query is a critical task . in a broader sense , this is essentially identification of certain personalized patterns that drives such applications as web search engine construction , customized text summarization and automated question answering . a related problem of text snippet extraction has been previously studied in information retrieval . in these studies , common strategies for extracting and presenting text snippets to meet user needs either process document fragments that have been delimitated a priori or use a sliding window of a fixed size to highlight the results . in this work , we argue that text snippet extraction can be generalized if the user 's intention is better utilized . it overcomes the rigidness of existing approaches by dynamically returning more flexible startend positions of text snippets , which are also semantically more coherent . this is achieved by constructing and using statistical language models which effectively capture the commonalities between a document and the user intention . experiments indicate that our proposed solutions provide effective personalized information extraction services .
upper and lower bounds on the makespan of schedules for tree dags on linear arrays . <eos> we find , in polynomial time , a schedule for a complete binary tree directed acyclic graph ( dag ) with n unit execution time tasks on a linear array whose makespan is optimal within a factor of <digit> o ( <digit> ) . further , given a binary tree dag t with n tasks and height h , we find , in polynomial time , a schedule for t on a linear array whose makespan is optimal within a factor of <digit> o ( <digit> ) . on the other hand , we prove that explicit lower and upper bounds on the makespan of optimal schedules of binary tree dags on linear arrays differ at least by a factor of <digit> root <digit> <digit> . we also find , in polynomial time , schedules for bounded tree dags with n unit execution time tasks , degree d , and height h is an element of o ( n ( <digit> <digit> ) ) boolean or omega ( n ( <digit> <digit> ) ) on a linear array which are optimal within a factor of <digit> o ( <digit> ) , this time under the assumption of links with unlimited bandwidth . finally , we compute an improved upper bound on the makespan of an optimal schedule for a tree dag on the architecture independent model of papadimitriou and yannakakis <digit> , provided that its height is not too large .
domain adaptation for face recognition targetize source domain bridged by common subspace . <eos> in many applications , a face recognition model learned on a source domain but applied to a novel target domain degenerates even significantly due to the mismatch between the two domains . aiming at learning a better face recognition model for the target domain , this paper proposes a simple but effective domain adaptation approach that transfers the supervision knowledge from a labeled source domain to the unlabeled target domain . our basic idea is to convert the source domain images to target domain ( termed as targetize the source domain hereinafter ) , and at the same time keep its supervision information . for this purpose , each source domain image is simply represented as a linear combination of sparse target domain neighbors in the image space , with the combination coefficients however learnt in a common subspace . the principle behind this strategy is that , the common knowledge is only favorable for accurate cross domain reconstruction , but for the classification in the target domain , the specific knowledge of the target domain is also essential and thus should be mostly preserved ( through targetization in the image space in this work ) . to discover the common knowledge , specifically , a common subspace is learnt , in which the structures of both domains are preserved and meanwhile the disparity of source and target domains is reduced . the proposed method is extensively evaluated under three face recognition scenarios , i.e. , domain adaptation across view angle , domain adaptation across ethnicity and domain adaptation across imaging condition . the experimental results illustrate the superiority of our method over those competitive ones .
threat analysis of online health information system . <eos> electronic health records are increasingly used to enhance availability , recovery , and transfer of health records . newly developed online health systems such as google health create new security and privacy risks . in this paper , we elucidate a clear threat model for online health information systems . we distinguish between privacy and security threats . in response to these risks , we propose a traitor tracing solution , which embeds proof to trace an attacker who leaks data from a repository . we argue that the application of traitor tracing techniques to online health systems can align incentives and decrease risks .
models and mechanisms for artificial morphogenesis . <eos> embryological development provides an inspiring example of the creation of complex hierarchical structures by self organization . likewise , biological metamorphosis shows how these complex systems can radically restructure themselves . our research investigates these principles and their application to artificial systems in order to create intricately structured systems that are ordered from the nanoscale up to the macroscale . however these processes depend on mutually interdependent unfoldings of an information process and of the body in which it is occurring . such embodied computation provides challenges as well as opportunities , and in order to fulfill its promise , we need both formal and informal models for conceptualizing , designing , and reasoning about embodied computation . this paper presents a preliminary design for one such model especially oriented toward artificial morphogenesis .
icp registration using invariant features . <eos> this paper investigates the use of euclidean invariant features in a generalization of iterative closest point registration of range images . pointwise correspondences are chosen as the closest point with respect to a weighted linear combination of positional and feature distances . it is shown that under ideal noise free conditions , correspondences formed using this distance function are correct more often than correspondences formed using the positional distance alone . in addition , monotonic convergence to at least a local minimum is shown to hold for this method . when noise is present , a method that automatically sets the optimal relative contribution of features and positions is described . this method trades off error in feature values due to noise against error in positions due to misalignment , experimental results suggest that using invariant features decreases the probability of being trapped in a local minimum and may be an effective solution for difficult range image registration problems where the scene is very small compared to the model .
where were you development of a time geographic approach for activity destination re construction . <eos> with the use of individual level travel survey datasets describing the detailed activities of households , it is possible to analyze human movements with a high degree of precision . however , travel survey data are not without quality issues . potential exists for origins and destinations of reported trips not to be geo referenced , perhaps due to misreported information or inconsistencies in spatial address databases , which can limit the usefulness of the survey data . from an analytical standpoint , this is a serious problem because a single unreferenced stop in a trip record in effect renders that individuals data useless , especially in cases where analyzing chains of activity locations is of interest . this paper presents a framework and basic computational approach for exploring unlocatable activity locations inherent to travel surveys . derived from recent work in developing a network based , probabilistic time geography , the proposed methods are able to estimate the likely locations of missing trip origins and destinations . the methods generate probabilistic potential path trees which are used to visualize and quantify potential locations for the unreferenced destinations . the methods are demonstrated with simulated survey data from a smaller metropolitan area .
new primal dual algorithms for steiner tree problems . <eos> we present new primal dual algorithms for several network design problems . the problems considered are the generalized steiner tree problem ( gst ) , the directed steiner tree problem ( dst ) , and the set cover problem ( sc ) which is a subcase of dst . all our problems are np hard so we are interested in their approximation algorithms . first , we give an algorithm for dst which is based on the traditional approach of designing primal dual approximation algorithms . we show that the approximation factor of the algorithm is k , where k is the number of terminals . in the case when the problem is restricted to quasi bipartite graphs . we also give pathologically bad examples for the algorithm performance . to overcome the problems exposed by the bad examples , we design a new framework for primal dual algorithms which can be applied to all of our problems . the main feature of the new approach is that , unlike the traditional primal dual algorithms , it keeps the dual solution in the interior of the dual feasible region . the new approach allows us to avoid including too many arcs in the solution , and thus achieves a smaller cost solution . our computational results show that the interior point version of the primal dual most of the time performs better than the original primal dual method . ( c ) <digit> published by elsevier ltd .
an expert trajectory design for control of nuclear research reactors . <eos> in this study , an expert trajectory was proposed for control of nuclear research reactors . the trajectory being followed by the reactor power is composed of three parts . in order to calculate periods at the midpoint of each part of the trajectory , a period generator was designed based on artificial neural networks . the contribution of the expert trajectory to the reactor control system was investigated . furthermore , the behavior of the controller with the expert trajectory was tested for various initial and desired power levels , as well as under disturbance . it was seen that the controller could control the system successfully under all conditions within the acceptable error tolerance .
facebook or renren a comparative study of social networking site use and social capital among chinese international students in the united states . <eos> facebook and renren use are positively associated with bridging social capital . facebooks relationship with bridging social capital is stronger than renren . renren use is positively associated with maintaining home country social capital .
detection of phishing attacks in iranian e banking using a fuzzyrough hybrid system . <eos> identifying outstanding phishing features that best fit the iranian bank websites . extracting a reduct of influential indicators in phishing detection for iranian e banking system using rough sets theory . determining critical phishing detection rules and forming a flexible rule base for phishing detection . building a fuzzyrough hybrid system as a core processing unit of phishing detection applications or web browser add ons and extensions concentrated on iranian e banking . applying the proposed system on iranian phishing sites and achieving an efficiency of <digit> % .
play it by eye collect movies and improvise perspectives with tangible video objects . <eos> we present an alternative video making framework for children with tools that integrate video capture with movie production . we propose different forms of interaction with physical artifacts to capture storytelling . play interactions as input to video editing systems assuage the interface complexities of film construction in commercial software . we aim to motivate young users in telling their stories , extracting meaning from their experiences by capturing supporting video to accompany their stories , and driving reflection on the outcomes of their movies . we report on our design process over the course of four research projects that span from a graphical user interface to a physical instantiation of video . we interface the digital and physical realms using tangible metaphors for digital data , providing a spontaneous and collaborative approach to video composition . we evaluate our systems during observations with <digit> to <digit> year old users and analyze their different approaches to capturing , collecting , editing , and performing visual and sound clips .
exploring association between perceived importance of travel traffic information and travel behaviour in natural disasters a case study of the <digit> brisbane floods . <eos> using the <digit> brisbane flood as a case study . respondents perceptions of the importance of travel traffic information were modelled . the hysteresis phenomenon in respondents perceived information importance . socio demographic features have a significant impact on such perceptions . no evidence of the influence of travel traffic information on respondents travel mode .
a shape reconstructability measure of object part importance with applications to object detection and localization . <eos> we propose a computational model which computes the importance of <digit> d object shape parts , and we apply it to detect and localize objects with and without occlusions . the importance of a shape part ( a localized contour fragment ) is considered from the perspective of its contribution to the perception and recognition of the global shape of the object . accordingly , the part importance measure is defined based on the ability to estimate recall the global shapes of objects from the local part , namely the parts shape reconstructability . more precisely , the shape reconstructability of a part is determined by two factorspart variation and part uniqueness . ( i ) part variation measures the precision of the global shape reconstruction , i.e. the consistency of the reconstructed global shape with the true object shape and ( ii ) part uniqueness quantifies the ambiguity of matching the part to the object , i.e. taking into account that the part could be matched to the object at several different locations . taking both these factors into consideration , an information theoretic formulation is proposed to measure part importance by the conditional entropy of the reconstruction of the object shape from the part . experimental results demonstrate the benefit with the proposed part importance in object detection , including the improvement of detection rate , localization accuracy , and detection efficiency . by comparing with other state of the art object detectors in a challenging but common scenario , object detection with occlusions , we show a considerable improvement using the proposed importance measure , with the detection rate increased over ( <digit> % ) . on a subset of the challenging pascal dataset , the interpolated average precision ( as used in the pascal voc challenge ) is improved by <digit> % . moreover , we perform a psychological experiment which provides evidence suggesting that humans use a similar measure for part importance when perceiving and recognizing shapes .
in situ fabrication of a poly acrylamide membrane in a microfluidic channel . <eos> we present a novel technological approach for the in situ realization of a micron thin poly acrylamide membrane in the center of a microfluidic channel . the membrane is formed by interfacial polymerization of an inner stream of monomer solution in between two streams of initiator catalyst solution in a hydrodynamic focusing chip . <digit> m thick su <digit> structures are used to replicate the chip in poly dimethylsiloxane ( pdms ) . the chip is fitted within an adaptor allowing easy fluidic connections , temperature control and optical monitoring under a microscope . with this system , we can easily tune the internal stream width from <digit> to <digit> m , by varying the internal external flow ratio .
enhanced colliding bodies optimization for design problems with continuous and discrete variables . <eos> a new multi agent algorithm inspired by a collision between two objects in one dimension is presented . an enhanced colliding bodies optimization which uses memory to save some best solutions is developed . a mechanism is utilized to escape from local optima . performance of the proposed algorithm is compared to those of standard cbo and some optimization techniques .
detecting online commercial intention ( oci ) . <eos> understanding goals and preferences behind a user 's online activities can greatly help information providers , such as search engine and e commerce web sites , to personalize contents and thus improve user satisfaction . understanding a user 's intention could also provide other business advantages to information providers . for example , information providers can decide whether to display commercial content based on user 's intent to purchase . previous work on web search defines three major types of user search goals for search queries navigational , informational and transactional or resource <digit> <digit> . in this paper , we focus our attention on capturing commercial intention from search queries and web pages , i.e. , when a user submits the query or browse a web page , whether he she is about to commit or in the middle of a commercial activity , such as purchase , auction , selling , paid service , etc. we call the commercial intentions behind a user 's online activities as oci ( online commercial intention ) . we also propose the notion of commercial activity phase ( cap ) , which identifies in which phase a user is in his her commercial activities research or commit . we present the framework of building machine learning models to learn oci based on any web page content . based on that framework , we build models to detect oci from search queries and web pages . we train machine learning models from two types of data sources for a given search query content of algorithmic search result page ( s ) and contents of top sites returned by a search engine . our experiments show that the model based on the first data source achieved better performance . we also discover that frequent queries are more likely to have commercial intention . finally we propose our future work in learning richer commercial intention behind users ' online activities .
easy doesnt do it skill and expression in tangible aesthetics . <eos> in this paper , we articulate the role of movement within a perceptual motor view of tangible interaction . we argue that the history of humanproduct interaction design has exhibited an increasing neglect of the intrinsic importance of movement . on one hand , humanproduct interaction design has shown little appreciation in practice of the centrality of our bodily engagement in the world . this has resulted in technologies that continue to place demands on our cognitive abilities , and deny us the opportunity of building bodily skill . on the other hand , the potential for movement in products to be a meaningful component of our interaction with them has also been ignored . both of these directions ( design for bodily engagement and the expressiveness of product movements ) are sketched out , paying particular respect for their potential to impact both interaction aesthetics and usability . we illustrate a number of these ideas with examples .
rlc coupling aware simulation and on chip bus encoding for delay reduction . <eos> this paper shows . that the worst case switching pattern that incurs the longest bus delay while considering the rlc effect is quite different from that while considering the rc effect alone . it implies that the existing encoding schemes based on the rc model may not improve or possibly worsen the delay when the inductance effects become dominant . a bus invert method is also proposed to reduce the on chip bus delay based on the rlc model . simulation results show that the proposed encoding scheme significantly reduces the worst case coupling delay of the inductance dominated buses .
studying knowledge management in information systems research discourses and theoretical assumptions . <eos> in information systems , most research on knowledge management assumes that knowledge has positive implications for organizations . however , knowledge is a double edged sword while too little might result in expensive mistakes , too much might result in unwanted accountability . the purpose of this paper is to highlight the lack of attention paid to the unintended consequences of managing organizational knowledge and thereby to broaden the scope of is based knowledge management research . to this end , this paper analyzes the is literature on knowledge management . using a framework developed by deetz ( <digit> ) , research articles published between <digit> and <digit> in six is journals are classified into one of four scientific discourses . these discourses are the normative , the interpretive , the critical , and the dialogic . for each of these discourses , we identify the research focus , the metaphors of knowledge , the theoretical foundations , and the implications apparent in the articles representing it . the metaphors of knowledge that emerge from this analysis are knowledge as object , asset , mind , commodity , and discipline . furthermore , we present a paper that is exemplary of each discourse . our objective with this analysis is to raise is researchers ' awareness of the potential and the implications of the different discourses in the study of knowledge and knowledge management .
partial information network queries . <eos> introducing the partial information network query ( pinq ) problem . developing a parameterized algorithm for pinq . for topology free network queries improving upon previous running times . for two types of alignment network queries improving upon previous running times .
rule based and case based reasoning approach for internal audit of bank . <eos> banks currently have a great interest in internal audits to reduce risks , to prevent themselves from insolvency , and to take quick action for financial incidents . this study presents an integrated audit approach of rule based and case based reasoning , which includes two stages of reasoning , i.e. , screening stage based on rule based reasoning and auditing stage based on case based reasoning . rule based reasoning uses induction rules to determine whether a new problem should be inspected further or not . case based reasoning performs similarity based matching to find the most similar case in case base to the new problem . the method presented is applied to data of internal audits of a bank .
under approximating loops in c programs for fast counterexample detection . <eos> many software model checkers only detect counterexamples with deep loops after exploring numerous spurious and increasingly longer counterexamples . we propose a technique that aims at eliminating this weakness by constructing auxiliary paths that represent the effect of a range of loop iterations . unlike acceleration , which captures the exact effect of arbitrarily many loop iterations , these auxiliary paths may under approximate the behaviour of the loops . in return , the approximation is sound with respect to the bit vector semantics of programs . our approach supports arbitrary conditions and assignments to arrays in the loop body , but may as a result introduce quantified conditionals . to reduce the resulting performance penalty , we present two quantifier elimination techniques specially geared towards our application . loop under approximation can be combined with a broad range of verification techniques . we paired our techniques with lazy abstraction and bounded model checking , and evaluated the resulting tool on a number of buffer overflow benchmarks , demonstrating its ability to efficiently detect deep counterexamples in c programs that manipulate arrays .
robust mimo radar target localization via nonconvex optimization . <eos> this communication addresses the problem of robust target localization in distributed multiple input multiple output ( mimo ) radar using possibly outlier range measurements . to achieve robustness against outliers , we construct an objective function for mimo target localization via the maximum correntropy criterion . to deal with such a nonconvex and nonlinear function , we apply a half quadratic optimization technique to determine the target position and auxiliary variables alternately . especially , we derive a semidefinite relaxation formulation for the aforementioned position determination step . the robust performance of the developed approach is demonstrated by comparing with several conventional localization methods via computer simulation .
plant invasions across the northern hemisphere a deep time perspective . <eos> few invasion biologists consider the long term evolutionary context of an invading organism and its invaded ecosystem . here , i consider patterns of plant invasions across eastern north america , europe , and east far east asia , and explore whether biases in exchanges of plants from each region reflect major selection pressures present within each region since the late miocene , during which temperate northern hemisphere floras diverged taxonomically and ecologically . although there are many exceptions , the european flora appears enriched in species well adapted to frequent , intense disturbances such as cultivation and grazing the north american composite ( asteraceae ) flora appears particularly well adapted to nutrient rich meadows and forest openings and the east asian flora is enriched in shade tolerant trees , shrubs , and vines of high forest invasive potential . i argue that such directionality in invasions across different habitat types supports the notion that some species are preadapted to become invasive as a result of differences in historical selection pressures between regions .
on classification with bags , groups and sets . <eos> many classification tasks are not entirely suitable for supervised learning . instead of individual feature vectors , bags of feature vectors can be considered . many learning scenarios with bags in training and or test phase have been proposed . we provide an overview and taxonomy of these learning scenarios .
technological support for the enactment of collaborative scripted learning activities across multiple spatial locations . <eos> support for computer supported collaborative blended learning scripts is proposed . requirements are replicability , adaptability , flexibility , scalability . the system integrates several technologies and derives a more general architecture . an experiment , based on a previous script , was conducted to validate the proposal . the findings show that the script reduces the management workload .
unboundedly parallel simulations via recurrence relations . <eos> new methods are presented for parallel simulation of discrete event systems that , when applicable , can usefully employ a number of processors much larger than the number of objects in the system being simulated . abandoning the distributed event list approach , the simulation problem is posed using recurrence relations . we bring three algorithmic ideas to bear on parallel simulation parallel prefix computation , parallel merging , and iterative folding . efficient parallel simulations are given for ( in turn ) the g g <digit> queue , a variety of queueing networks having a global first come first served structure ( e.g. , a series of queues with finite buffers ) , acyclic networks of queues , and networks of queues with feedbacks and cycles . in particular , the problem of simulating the arrival and departure times for the first n jobs to a single g g <digit> queue is solved in time proportional to n p log p using p processors .
assembly and disassembly an overview and framework for cooperation requirement planning with conflict resolution . <eos> assembly , one of the oldest forms of industrial production , and its twin area , disassembly , have enjoyed tremendous modernization in the era of the information revolution . new enabling technologies , including prominent examples such as virtual cad , design for assembly and disassembly ( dfad ) , robotic and intelligent assembly , and flexible assembly ( fa ) , are now becoming commonplace . this article reviews some of the newer solutions , and an extended framework for cooperation requirement planning ( ecrp ) in robotic assembly disassembly is developed . recent research under the prism program at purdue university to enable ecrp and other relevant projects is presented . the challenges to researchers in this field , in adapting these solutions to the emerging environment of global and local supply networks are also discussed .
robust optimisation for self scheduling and bidding strategies of hybrid cspfossil power plants . <eos> a robust milp approach is proposed for self scheduling of hybrid cspfossil fuel plant . uncertainty is introduced in the model by asymmetric prediction intervals . the robustness cost is controlled by the budget of uncertainty . plant self scheduling and bidding strategies in a day ahead market are simultaneously considered .
self managing , disconnected processes and mechanisms for mobile e business . <eos> with the tremendous advances in hand held computing and communication capabilities , rapid proliferation of mobile devices , and decreasing device costs , we are seeing a growth in mobile e business in various consumer and business markets . in this paper , we present a novel architecture and framework for end to end mobile e business applications such as purchasing , retail point of sales , and order management . the design takes into consideration disconnection , application context and failure modes to provide mobile users with seamless and transparent access to commerce and content activities . in our architecture , we consider a novel business process design based on state machines and event management to handle disconnection and resource limitations . we designed , implemented and deployed a system for mobile e business on clients integrated with private exchanges and sell side servers . the e business framework on mobile clients is implemented based on j2me and open xml standards . a performance study of simple e business transactions was done on the client using the above mechanisms and programming environment . we show that the performance of a purchasing process using the framework does reasonably well .
parafac parallel factor analysis . <eos> we review the method of parallel factor analysis , which simultaneously fits multiple two way arrays or ' slices ' of a three way array in terms of a common set of factors with differing relative weights in each ' slice ' . mathematically , it is a straightforward generalization of the bilinear model of factor ( or component ) analysis ( x ( ij ) sigma ( r <digit> ) r a ( ir ) b ( jr ) ) to a trilinear model ( x ( ijk ) sigma ( r <digit> ) r a ( ir ) b ( jr ) c ( kr ) ) . despite this simplicity , it has an important property not possessed by the two way model if the latent factors show adequately distinct patterns of three way variation , the model is fully identified the orientation of factors is uniquely determined by minimizing residual error , eliminating the need for a separate ' rotation ' phase of analysis . the model can be used several ways . it can be directly fit to a three way array of observations with ( possibly incomplete ) factorial structure , or it can be indirectly fit to the original observations by fitting a set of covariance matrices computed from the observations , with each matrix corresponding to a two way subset of the data . even more generally , one can simultaneously analyze covariance matrices computed from different samples , perhaps corresponding to different treatment groups , different kinds of cases , data from different studies , etc. to demonstrate the method we analyze data from an experiment on right vs. left cerebral hemispheric control of the hands during various tasks . the factors found appear to correspond to the causal influences manipulated in the experiment , revealing their patterns of influence in all three ways of the data . several generalizations of the parallel factor analysis model are currently under development , including ones that combine parallel factors with tucker like factor ' interactions ' . of key importance is the need to increase the method 's robustness against nonstationary factor structures and qualitative ( nonproportional ) factor change .
helper threads via virtual multithreading on an experimental ltanium ( r ) <digit> processor based platform . <eos> helper threading is a technology to accelerate a program by exploiting a processor 's multithreading capability to run assist threads . previous experiments on hyper threaded processors have demonstrated significant speedups by using helper threads to prefetch hard to predict delinquent data accesses . in order to apply this technique to processors that do not have built in hardware support for multithreading , we introduce virtual multithreading ( vmt ) , a novel form of switch on event user level multithreading , capable of fly weight multiplexing of event driven thread executions on a single processor without additional operating system support . the compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user level threads . the vmt approach makes it possible to launch dynamic helper thread instances in response to long latency cache miss events , and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled . the concept of vmt is prototyped on an itanium ( r ) <digit> processor using features provided by the processor abstraction layer ( pal ) firmware mechanism already present in currently shipping processors . on a <digit> way mp physical system equipped with vmt enabled itanium <digit> processors , helper threading via the vmt mechanism can achieve significant performance gains for a diverse set of real world workloads , ranging from single threaded workstation benchmarks to heavily multithreaded large scale decision support systems ( dss ) using the ibm db2 universal database . we measure a wall clock speedup of 5.8 % to 38.5 % for the workstation benchmarks , and 5.0 % to 12.7 % on various queries in the dss workload .
a systematic review of simulation studies investigating emergency department overcrowding . <eos> the problem of emergency department ( ed ) overcrowding has reached crisis proportions in the last decade . in <digit> , the national academy of engineering and the institute of medicine reported on the important role of simulation as a systems analysis tool that can have an impact on care processes at the care team , organizational , and environmental levels . simulation has been widely used to understand causes of ed overcrowding and to test interventions to alleviate its effects . in this paper , we present a systematic review of ed simulation literature from <digit> to <digit> from healthcare , systems engineering , operations research and computer science publication venues . the goals of this review are to highlight the contributions of these simulation studies to our understanding of ed overcrowding and to discuss how simulation can be better used as a tool to address this problem . we found that simulation studies provide important insights into ed overcrowding but they also had major limitations that must be addressed .
an effective lower bound on l max in a worker constrained job shop . <eos> a common industrial operation is a dual resource constrained job shop where ( a ) the objective is to minimize l max , the maximum job lateness ( b ) machines are organized into groups and ( c ) each worker is assigned to a specific machine group . because this problem is np hard , finding optimal solutions by enumeration is impractical . this paper details a procedure to compute a lower bound on l max that will be used in follow up work to effectively evaluate the absolute performance of heuristic solutions . given an allocation of workers to machine groups , a lower bound on l max is first computed for each machine group using a network flow formulation . the lower bound on l max for the job shop is the largest of the lower bounds for the machine groups . a search algorithm then finds a worker allocation yielding the smallest such lower bound on l max for the job shop and the latter quantity is our proposed lower bound on l max . given a worker allocation , we use the virtual factory ( a heuristic scheduler developed by hodgson et al. in <digit> ) to generate a schedule . experiments with a wide variety of job shops indicated that the proposed lower bound on l max could often be achieved by a virtual factory schedule based on the worker allocation yielding this lower bound . however , there were problem instances for which other worker allocations enabled the virtual factory to generate better schedules . follow up work provides optimality criteria , and heuristics to find improved allocations if these criteria are not satisfied . ( c ) <digit> elsevier ltd. all rights reserved .
a local tree alignment approach to relation extraction of multiple arguments . <eos> in this paper , we address the problem of relation extraction of multiple arguments where the relation of entities is framed by multiple attributes . such complex relations are successfully extracted using a syntactic tree based pattern matching method . while induced subtree patterns are typically used to model the relations of multiple entities , we argue that hard pattern matching between a pattern database and instance trees can not allow us to examine similar tree structures . thus , we explore a tree alignment based soft pattern matching approach to improve the coverage of induced patterns . our pattern learning algorithm iteratively searches the most influential dependency tree patterns as well as a control parameter for each pattern . the resulting method outperforms two baselines , a pairwise approach with the tree kernel support vector machine and a hard pattern matching method , on two standard datasets for a complex relation extraction task .
edm in the danish public sector the fesd project . <eos> purpose to confirm that the purpose of the fesd project has been to provide a framework accepted <digit> february <digit> contract for the whole public sector covering the purchase of an edm system , technical and organisational consulting for implementation and organisational change . design methodology approach the project took the approach of working closely together with <digit> partnering organisations on developing the functional requirements for the system and participating in the tender negotiations with the bidding consortia . this has proved valuable , since the project has gained a profound legitimacy for its demands and a strong basis for the roll out in the rest of the public sector . findings the results of the project are manifold for the first time in the danish public sector a mutual framework contract has made it possible to put the same requirements forward to the bidding vendors . it has made it possible to develop mutual technical standards and to develop standardised work processes supported by the systems . furthermore , a number of long term findings will become evident over the next two years when the implementation projects begin to show results . practical implications originally it was one of the major tasks of the fesd project to show efficiency gains and return of investment within the project 's life span . this has not been possible due to the fact that the implementation projects in the partnering organisations are far from finished . also , efficiency gains are not always part of the success criteria and it may turn out that efficiency gains weigh more in the minds of planners than in the real implementation projects . originality value the article is a report from a country highly esteemed for its efforts in pushing public digital administration in order to create better service and higher efficiency .
locally conservative discontinuous petrovgalerkin finite elements for fluid problems . <eos> we develop a locally conservative formulation of the discontinuous petrovgalerkin finite element method ( dpg ) for convectiondiffusion type problems using lagrange multipliers to exactly enforce conservation over each element . we provide a proof of convergence as well as extensive numerical experiments showing that the method is indeed locally conservative . we also show that standard dpg , while not guaranteed to be conservative , is nearly conservative for many of the benchmarks considered . the new method preserves many of the attractive features of dpg , but turns the normally symmetric positive definite dpg system into a saddle point problem .
reassortment networks for investigating the evolution of segmented viruses . <eos> many viruses of interest , such as influenza a , have distinct segments in their genome . the evolution of these viruses involves mutation and reassortment , where segments are interchanged between viruses that coinfect a host . phylogenetic trees can be constructed to investigate the mutation driven evolution of individual viral segments . however , reassortment events among viral genomes are not well depicted in such bifurcating trees . we propose the concept of reassortment networks to analyze the evolution of segmented viruses . these are layered graphs in which the layers represent evolutionary stages such as a temporal series of seasons in which influenza viruses are isolated . nodes represent viral isolates and reassortment events between pairs of isolates . edges represent evolutionary steps , while weights on edges represent edit costs of reassortment and mutation events . paths represent possible transformation series among viruses . the length of each path is the sum edit cost of the events required to transform one virus into another . in order to analyze tau stages of evolution of n viruses with segments of maximum length m , we first compute the pairwise distances between all corresponding segments of all viruses in o ( m ( <digit> ) n ( <digit> ) ) time using dynamic programming . the reassortment network , with o ( tau n ( <digit> ) ) nodes , is then constructed using these distances . the ancestors and descendents of a specific virus can be traced via shortest paths in this network , which can be found in o ( tau n ( <digit> ) ) time .
scaffolding problem solving in technology enhanced learning environments ( teles ) bridging research and theory with practice . <eos> with the expanding availability and capability of varied technologies , classroom based problem solving has become an increasingly attainable , yet still elusive , goal . evidence of technology enhanced problem solving teaching and learning in schools has been scarce , understanding how to support students ' problem solving in classroom based , technology enhanced learning environments has been limited , and coherent frameworks to guide implementation have been slow to emerge . whereas researchers have examined the use and impact of scaffolds in mathematics , science , and reading , comparatively little research has focused on scaffolding learning in real world , everyday classroom settings . web based systems have been developed to support problem solving , but implementations suggest variable enactment and inconsistent impact . the purpose of this article is to identify critical issues in scaffolding students ' technology enhanced problem solving in everyday classrooms . first , we examine two key constructs ( problem solving and scaffolding ) and propose a framework that includes essential dimensions to be considered when teachers scaffold student problem solving in technology rich classes . we then investigate issues related to peer , teacher , and technology enhanced scaffolds , and conclude by examining implications for research . ( c ) <digit> elsevier ltd. all rights reserved .
a new approach to objective quality measures based on attribute matching . <eos> in this paper the results of a study of objective quality measures for a broad range of coding systems are presented . these objective measures take the linear and the nonlinear distortions of the coder into account . a correlation analysis was performed , in order to find out those measures which are most effective in predicting perceivable parametric attributes of speech quality . the results of this experiment , the so called attribute matching , yield a good composite measure for predicting the total quality for a wide range of coding systems and can be computed in pseudo realtime . furthermore , we describe the test signal we have used in our study , which was not natural speech but a speech model process .
building additive utilities for multi group hierarchical discrimination the mhdis method . <eos> the discrimination problem is of major interest in fields such as environmental management , human resources management , production management , finance , marketing , medicine , etc. for decades this problem has been studied from a multivariate statistical point of view . recently the possibilities of new approaches have been explored , based mainly on mathematical programming . this paper follows the methodological framework of multicriteria decision aid ( mcda ) , to propose a new method for multigroup discrimination based on a hierarchical procedure ( multi group hierarchical discrimination m.h.dis ) . the performance of the m.h.dis method is evaluated along with eight real world case studies from the fields of finance and marketing . a comparison is also performed with other mcda methods .
the effects of static versus dynamic 3d representations on 10th grade students atomic orbital mental model construction evidence from eye movement behaviors . <eos> dynamic 3d representations enhanced students performance . dynamic 3d representations fostered students to allocate greater attention . eye movements could predict students 3d mental models of an atomic orbital . low spatial ability students with dynamic 3d representations spent more attention .
a novel switching local evolutionary pso for quantitative analysis of lateral flow immunoassay . <eos> this paper presents a novel particle swarm optimization ( pso ) based on a non homogeneous markov chain and differential evolution ( de ) for quantification analysis of the lateral flow immunoassay ( lfia ) , which represents the first attempt to estimate the concentration of target analyte based on the well established state space model . a new switching local evolutionary pso ( slepso ) is developed and analyzed . the velocity updating equation jumps from one mode to another based on the non homogeneous markov chain , where the probability transition matrix is updated by calculating the diversity and current optimal solution . furthermore , de mutation and crossover operations are implemented to improve local best particles searching in pso . compared with some well known pso algorithms , the experiments results show the superiority of proposed slepso . finally , the new slepso is successfully exploited to quantification analysis of the lfia system , which is essentially nonlinear and dynamic . therefore , this can provide a new method for the area of quantitative interpretation of lfia system . ( c ) <digit> elsevier ltd. all rights reserved .
the frobenius anatomy of word meanings i subject and object relative pronouns . <eos> this article develops a compositional vector based semantics of subject and object relative pronouns within a categorical framework . frobenius algebras are used to formalize the operations required to model the semantics of relative pronouns , including passing information between the relative clause and the modified noun phrase , as well as copying , combining , and discarding parts of the relative clause . we develop two instantiations of the abstract semantics , one based on a truth theoretic approach and one based on corpus statistics .
blow up of compressible navierstokeskorteweg equations . <eos> based on the results of xin ( commun . pure appl . math . <digit> ( <digit> ) <digit> , <digit> ) , zhang and tan ( acta math . sin . engl . ser . <digit> ( <digit> ) <digit> , <digit> ) , we show the blow up phenomena of smooth solutions to the non isothermal compressible navierstokeskorteweg equations in arbitrary dimensions , under the assumption that the initial density has compact support . here the coefficients are generalized to a more general case which depends on density and temperature . our work extends the previous corresponding results .
a coordinated planning model for the design of a distributed database system . <eos> motivated by a problem faced by a multimedia entertainment retailer , we explore the problem of planning the design of a distributed database system . the problem consists of planning the design expansion of the distributed database system by introducing new database servers and retiring possibly some existing ones in order to reduce telecommunication costs for processing user queries and server acquisition , operations and maintenance cost in a multiperiod environment where user processing demand varies over time . we develop a mathematical programming model and an effective solution approach to determine the best decisions regarding acquisition and retirement of database servers and assignment of user processing demand to the servers over time . through a computational study , we investigate the impact of important parameters such as length of the planning horizon and demand growth on the solution quality and utilization of server capacity and examine the effectiveness of the solution approach in comparison with the commercial package lindo . we also discuss some extensions to the problem as directions for future research .
computationally feasible estimation of the covariance structure in generalized linear mixed models . <eos> in this paper , we discuss how a regression model , with a non continuous response variable , which allows for dependency between observations , should be estimated when observations are clustered and measurements on the subjects are repeated . the cluster sizes are assumed to be large . we find that the conventional estimation technique suggested by the literature on generalized linear mixed models ( glmm ) is slow and sometimes fails due to non convergence and lack of memory on standard pcs . we suggest to estimate the random effects as fixed effects by generalized linear model and to derive the covariance matrix from these estimates . a simulation study shows that our proposal is feasible in terms of mean square error and computation time . we recommend that our proposal be implemented in the software of glmm techniques so that the estimation procedure can switch between the conventional technique and our proposal , depending on the size of the clusters .
are there time and cost savings by using telemanagement for patients on intensified insulin therapy a randomised , controlled trial . <eos> background patients with insulin dependent diabetes require frequent advice if their metabolic control is not optimal . this study focuses on the fiscal and administrative aspects of telemanagement , which was used to establish a supervised autonomy of patients on intensified insulin therapy . methods a prospective , randomised trial with <digit> patients on intensified insulin therapy was conducted . travelling distance to the diabetes centre was <digit> min one way all patients had undergone a diabetes education course with lessons in dose adaptation . patients were randomly assigned to telecare ( n <digit> ) or conventional care ( n <digit> ) . they used bg meters with a storage capacity of <digit> values ( precision qid ( tm ) abbott medisense ) and transmitted their data over a combined modem interface via telephone line to the diabetes centre . data were displayed and stored by a customised software ( precision link plus ( tm ) , abbott medisense ) . advice for proper dose adjustment was given by telephone . results average time needed for instruction in the telemedical system was <digit> min . data were transmitted every <digit> <digit> weeks and a teleconsultation was performed by phone every <digit> <digit> weeks , depending on the extent of specific problems . on average , personal visits in the control group were performed once a month . physician 's time expenditure for telemanagement , compared to conventional advice was moderately higher ( <digit> vs. <digit> min per month ) . a substantial amount of time on the patients side could be saved through replacing personal communications by telephone contacts and data transmission reduction ( <digit> vs. <digit> min month including data transmission time ) . setting up an optimal telemanagement scenario , a cost analysis was carried out yielding savings of <digit> euro per year per patient . hba ( 1c ) dropped significantly from 8.2 to 7.0 % after <digit> months of observation , but there was no significant difference between the intervention and control groups . major technical problems with the telematic system did not occur during the study . conclusions telemanagement of insulin requiring diabetic patients is a cost and time saving procedure for the patients and results in metabolic control comparable to conventional outpatient management . ( c ) <digit> elsevier science ireland ltd. all rights reserved .
clinical and personality correlates of mmo gaming anxiety and absorption in problematic internet use . <eos> massively multiplayer online games ( mmos ) are increasingly popular worldwide . mmo gaming can result in problematic internet use ( piu or internet addiction ) , which is characterized by dysfunction in areas such as work or relationships . because piu in online gaming is increasingly seen in clinical populations , we explored piu in the context of mmo gaming . using a cross sectional design , we sought to identify clinical and personality factors , as well as motivations for gaming , that differentiated between people who scored high or low on a measure of problematic internet use . subjects completed all study procedures via an online survey . participants were <digit> mmo users recruited from the community , from gaming websites , and from online forums . subjects completed a series of demographic , mood , anxiety , and personality questionnaires . the study found that individuals in the high piu group ( n <digit> ) were more likely to have higher levels of social phobia ( p .000 ) , state ( p .000 ) and trait ( p .000 ) anxiety , introversion ( p .000 ) , neuroticism ( p .000 ) and absorption ( p .019 ) than individuals in the low piu group ( n <digit> ) . different reasons for gaming also characterized the group with more problematic internet use . our findings provide support for the idea that high anxiety and absorption may be risk factors for problematic internet use within the mmo gaming environment and suggest that gamers who endorse problematic internet use identify different motivations for online gaming than gamers who do not .
convergence of liustorey conjugate gradient method . <eos> the conjugate gradient method is a useful and powerful approach for solving large scale minimization problems . liu and storey developed a conjugate gradient method , which has good numerical performance but no global convergence result under traditional line searches such as armijo , wolfe and goldstein line searches . in this paper a convergent version of liustorey conjugate gradient method ( ls in short ) is proposed for minimizing functions that have lipschitz continuous partial derivatives . by estimating the lipschitz constant of the derivative of objective functions , we can find an adequate step size at each iteration so as to guarantee the global convergence and improve the efficiency of ls method in practical computation .
a space and power efficient multi match packet classification technique combining tcams and srams . <eos> packet classification is implemented in modern network routers for providing differentiated services based on packet header information . traditional packet classification only reports a single matched rule with the highest priority for an incoming packet and takes an action accordingly . with the emergence of new internet applications such as network intrusion detection system , all matched rules need to be reported . this multi match problem is more challenging and is attracting attentions in recent years . because of the stringent time budget on classification , architectural solutions using ternary content addressable memory ( tcam ) are the preferred choice for backbone network routers . however , despite its advantage on search speed , tcam is much more expensive than sram , and is notorious for its extraordinarily high power consumption . these problems limit the application and scalability of tcam based solutions . this paper presents a tree based multi match packet classification technique combining the benefits of both tcams and srams . the experiments show that the proposed solution achieves significantly more savings on both memory space and power consumption on packet matching compared to existing solutions .
oscillation and comparison theorems for half linear second order difference equations . <eos> the authors consider second order difference equations of the type delta ( ( deltay ( n ) ) ( alpha ) ) q ( n ) y ( sigma ( n ) ) ( alpha ) <digit> , ( e ) where alpha > <digit> is the ratio of odd positive integers , q ( n ) is a positive sequence , and sigma ( n ) is a positive increasing sequence of integers with sigma ( n ) > infinity as n > infinity . they give some oscillation and comparison results for equation ( e ) . ( c ) <digit> elsevier science ltd. all rights reserved .
automated 3d surface scanning based on cad model . <eos> this paper presents a method to automate the process of surface scanning using optical range sensors and based on a priori known information from a cad model . a volumetric model implemented through a 3d voxel map is generated from the object cad model and used to define a sensing plan composed of a set of viewpoints and the respective scanning trajectories . surface coverage with high data quality and scanning costs are the main aspects in sensing plan definition . a surface following scheme is used to define collision free and efficient scanning path trajectories . results of experimental tests performed on a typical industrial scanning system with <digit> dof are shown . ( c ) <digit> elsevier ltd. all rights reserved .
a new mechanism for tracking a mobile target using grid sensor networks . <eos> tracking moving targets is one of the important problems of wireless sensor networks . we have considered a sensor network where numerous sensor nodes are spread in a grid like manner . these sensor nodes are capable of storing data and thus act as a separate datasets . the entire network of these sensors act as a set of distributed datasets . each of these datasets has its local temporal dataset along with spatial data . and the geographical coordinates of a , given object or target . in this paper an algorithm is introduced that mines global temporal patterns from these datasets and results in the discovery of linear or nonlinear trajectories of moving objects tinder supervision . the main objective here is to perforin in network aggregation between the data contained in the various datasets to discover global spatio temporal patterns the main constraint is that there should be minimal communication among the participating nodes . we present the algorithm and analyze it in terms of the communication costs .
supply chain simulator a scenario based educational tool to enhance student learning . <eos> simulation based educational products are excellent set of illustrative tools that proffer features like visualization of the dynamic behavior of a real system , etc. such products have great efficacy in education and are known to be one of the first rate student centered learning methodologies . these products allow students to practice skills such as critical thinking and decision making . in this paper , a case is presented where a scenario based e learning product namely ' supply chain simulator ' is developed at kfupm for an introductory technology course . the product simulates a supply chain a network of facilities and distribution systems that carries out the task of procurement and transformation of materials from manufacturer to customer . the product was put to test during four semesters and results of the survey conducted by the instructors and the students are presented . the results clearly suggest the benefits of using such a tool in enhancing student learning . ( c ) <digit> elsevier ltd. all rights reserved .
obtaining traceability codes from chinese reminder theorem codes . <eos> traceability codes are used in schemes that prevent illegal redistribution of digital content . in this letter , we use chinese reminder theorem codes to construct traceability codes . both the code parameters and the traitor identification process take into account the non uniformity of the alphabet of chinese reminder theorem codes . moreover it is shown that the identification process can be done in polynomial time using list decoding techniques .
learning and control model of the arm for loading . <eos> we propose a learning and control model of the arm for a loading task in which an object is loaded onto one hand with the other hand , in the sagittal plane . postural control during object interactions provides important points to motor control theories in terms of how humans handle dynamics changes and use the information of prediction and sensory feedback . for the learning and control model , we coupled a feedback error learning scheme with an actor critic method used as a feedback controller . to overcome sensory delays , a feedforward dynamics model ( fdm ) was used in the sensory feedback path . we tested the proposed model in simulation using a two joint arm with six muscles , each with time delays in muscle force generation . by applying the proposed model to the loading task , we showed that motor commands started increasing , before an object was loaded on , to stabilize arm posture . we also found that the fdm contributes to the stabilization by predicting how the hand changes based on contexts of the object and efferent signals . for comparison with other computational models , we present the simulation results of a minimum variance model .
a finite capacity queue with exhaustive vacation close down setup times and markovian arrival processes . <eos> we consider a finite capacity single server vacation model with close down setup times and markovian arrival processes ( map ) . the queueing model has potential applications in classical ip over atm or ip switching systems , where the close down time corresponds to an inactive timer and the setup time to the time delay to set up a switched virtual connection ( svc ) by the signaling protocol . the vacation time may be considered as the time period required to release an svc or as the time during which the server goes to set up other svcs . by using the supplementary variable technique , we obtain the queue length distribution at an arbitrary instant , the loss probability , the setup rate , as well as the laplace stieltjes transforms of both the virtual and actual waiting time distributions .
adaptive sequential monte carlo by means of mixture of experts . <eos> appropriately designing the proposal kernel of particle filters is an issue of significant importance , since a bad choice may lead to deterioration of the particle sample and , consequently , waste of computational power . in this paper we introduce a novel algorithm adaptively approximating the so called optimal proposal kernel by a mixture of integrated curved exponential distributions with logistic weights . this family of distributions , referred to as mixtures of experts , is broad enough to be used in the presence of multi modality or strongly skewed distributions . the mixtures are fitted , via online em methods , to the optimal kernel through minimisation of the kullback leibler divergence between the auxiliary target and instrumental distributions of the particle filter . at each iteration of the particle filter , the algorithm is required to solve only a single optimisation problem for the whole particle sample , yielding an algorithm with only linear complexity . in addition , we illustrate in a simulation study how the method can be successfully applied to optimal filtering in nonlinear state space models .
personalized recommendation over a customer network for ubiquitous shopping . <eos> personalization services in a ubiquitous computing environment ubiquitous personalization services computing are expected to emerge in diverse environments . ubiquitous personalization must address limited computational power of personal devices and potential privacy issues . such characteristics require managing and maintaining a client side recommendation model for ubiquitous personalization . to implement the client side recommendation model , this paper proposes buying net , a customer network in ubiquitous shopping spaces . buying net is operated in a community , called the buying net space , of devices , customers , and services that cooperate together to achieve common goals . the customers connect to the buying net space using their own devices that contain software performing tasks of learning the customers ' preferences , searching for similar customers for network formation , and generating recommendation lists of items . buying net attempts to improve recommendation accuracy with less computational time by focusing on local relationship of customers and newly obtained information . we experimented with such customer networks in the area of multimedia content recommendation and validated that buying net outperformed a typical collaborative filtering based recommender system on accuracy as well as computational time . this shows that buying net has good potential to be a system for ubiquitous shopping .
modelling and analysis of a competitive model with stage structure . <eos> a two species lotka volterra type competition model with stage structures for both species is proposed and investigated . in our model , the individuals of each species are classified as belonging either the immature or the mature . first , we consider the stage structured model with constant coefficients . by constructing suitable lyapunov functions , sufficient conditions are derived for the global stability of nonnegative equilibria of the proposed model . it is shown that three typical dynamical behaviors ( coexistence , bistability , dominance ) are possible in stage structured competition model . next , we consider the stage structured competitive model in which the coefficients are assumed to be positively continuous periodic functions . by using gaines and mawhin 's continuation theorem of coincidence degree theory , a set of easily verifiable sufficient conditions are obtained for the existence of positive periodic solutions to the model . numerical simulations are also presented to illustrate the feasibility of our main results . ( c ) <digit> elsevier ltd. all rights reserved .
continuous k nearest neighbor query for moving objects with uncertain velocity . <eos> one of the most important queries in spatio temporal databases that aim at managing moving objects efficiently is the continuous k nearest neighbor ( cknn ) query . a cknn query is to retrieve the k nearest neighbors ( knns ) of a moving user at each time instant within a user given time interval t s , t e . in this paper , we investigate how to process a cknn query efficiently . different from the previous related works , our work relieves the past assumption , that an object moves with a fixed velocity , by allowing that the velocity of the object can vary within a known range . due to the introduction of this uncertainty on the velocity of each object , processing a cknn query becomes much more complicated . we will discuss the complications incurred by this uncertainty and propose a cost effective p2 knn algorithm to find the objects that could be the knns at each time instant within the given query time interval . besides , a probability based model is designed to quantify the possibility of each object being one of the knns . comprehensive experiments demonstrate the efficiency and the effectiveness of the proposed approach .
detecting topical events in digital video . <eos> the detection of events is essential to high level semantic querying of video databases . it is also a very challenging problem requiring the detection and integration of evidence for an event available in multiple information modalities , such as audio , video and language . this paper focuses on the detection of specific types of events , namely , topic of discussion events that occur in classroom lecture environments . specifically , we present a query driven approach to the detection of topic of discussion events with foils used in a lecture as a way to convey a topic . in particular , we use the image content of foils to detect visual events in which the foil is displayed and captured in the video stream . the recognition of a foil in video frames exploits the color and spatial layout of regions on foils using a technique called region hashing . next , we use the textual phrases listed on a foil as an indication of a topic , and detect topical audio events as places in the audio track where the best evidence for the topical phrases was heard . finally , we use a probabilistic model of event likelihood to combine the results of visual and audio avent detection that exploits their time cooccurrence . the resulting identification of topical events is evaluated in the domain of classroom lectures and talks .
on the guessing number of shift graphs . <eos> in this paper we investigate guessing number , a relatively new concept linked to network coding and certain long standing open questions in circuit complexity . here we study the bounds and a variety of properties concerning this parameter . as an application , we obtain the lower and upper bounds for shift graphs , a subclass of directed circulant graphs .
intelligent understanding of handwritten geometry theorem proving . <eos> computer based geometry systems have been widely used for teaching and learning , but largely based on mouse and keyboard interaction , these systems usually require users to draw figures by following strict task structures defined by menus , buttons , and mouse and keyboard actions . pen based designs offer a more natural way to develop geometry theorem proofs with hand drawn figures and scripts . this paper describes a pen based geometry theorem proving system that can effectively recognize hand drawn figures and hand written proof scripts , and accurately establish the correspondence between geometric components and proof steps . our system provides dynamic and intelligent visual assistance to help users understand the process of proving and allows users to manipulate geometric components and proof scripts based on structures rather than strokes . the results from evaluation study show that our system is well perceived and users have high satisfaction with the accuracy of sketch recognition , the effectiveness of visual hints , and the efficiency of structure based manipulation .
a semi infinite programming approach to preoperative planning of robotic cardiac surgery under geometric uncertainty . <eos> in this paper , a computational framework for patient specific preoperative planning of robotics assisted minimally invasive cardiac surgery ( ramics ) is presented . it is expected that the preoperative planning of ramics will improve the success rate by considering robot kinematics , patient specific thoracic anatomy , and procedure specific intraoperative conditions . given the significant anatomical features localized in the preoperative computed tomography images of a patient 's thorax , port locations , and robot orientations ( with respect to the patient 's body coordinate frame ) are determined to optimize qualities such as dexterity , reachability , tool approach angles , and maneuverability . to address intraoperative geometric uncertainty , the problem is formulated as a generalized semi infinite program ( gsip ) with a convex lower level problem to seek a plan that is less sensitive to geometric uncertainty in the neighborhood of surgical targets . it is demonstrated that with a proper formulation of the problem , the gsip can be replaced by a tractable constrained nonlinear program that uses a multicriteria objective function to balance between the nominal task performance and robustness to collisions and joint limit violations . finally , performance of the proposed formulation is demonstrated by a comparison between the plans generated by the algorithm and those recommended by an experienced surgeon for several case studies .
an accelerated ieee 802.11 handoff process based on the dynamic cluster chain method . <eos> the latency of the ieee 802.11 handoff process in wireless local area network ( wlan ) is much higher than <digit> ms. since the bearable maximum delay is <digit> ms. since the bearable maximum delay <digit> ms in multimedia applications , e.g. , voice over ip ( voip ) , such large handoff gap may bring up excessive jitter . therefore , many researches make much effort about how to fast handoff . in this paper , we propose an accelerated handoff mechanism in which three methods are involved ( <digit> ) dynamic cluster chain , ( <digit> ) pmska caching , and ( <digit> ) fast reassociation with the pairwise transient key security association ( ptksa ) establishment . access points ( aps ) are arranged as a cluster for each client station ( sta ) . aps that are cluster members can cache pmksa of sta in advance to reduce the extensible authentication protocol transport laver security ( eap tls ) authentication delay . the dynamic cluster chain which is arranged by a dynamic cluster selection and transition method , is proposed to assure that sta stays within a cluster . furthermore , the fast reassociation with the ptsa establishment process incorporates four way handshake into the ieee 802.11 reassociation process to further accelerate handoff process . ( c ) <digit> elsevier b.v. all rights reserved .
algorithmic aspects of acyclic edge colorings . <eos> a proper coloring of the edges of a graph g is called acyclic if there is no two colored cycle in g. the acyclic edge chromatic number of g , denoted by a ' ( g ) , is the least number of colors in an acyclic edge coloring of g. for certain graphs g , a ' ( g ) greater than or equal to delta ( g ) <digit> where delta ( g ) is the maximum degree in g. it is known that a ' ( g ) less than or equal to delta <digit> for almost all delta regular graphs , including all delta regular graphs whose girth is at least cdelta log delta . we prove that determining the acyclic edge chromatic number of an arbitrary graph is an np complete problem , ror graphs g with sufficiently large girth in terms of delta ( g ) , we present deterministic polynomial time algorithms that color the edges of g acyclically using at most delta ( g ) <digit> colors .
an improved meshless method with almost interpolation property for isotropic heat conduction problems . <eos> in the paper an improved element free galerkin method is presented for heat conduction problems with heat generation and spatially varying conductivity . in order to improve computational efficiency of meshless method based on galerkin weak form , the nodal influence domain of meshless method is extended to have arbitrary polygon shape . when the dimensionless size of the nodal influence domain approaches <digit> , the gauss quadrature point only contributes to those nodes in whose background cell the gauss quadrature point is located . thus , the bandwidth of global stiff matrix decreases obviously and the node search procedure is also avoided . moreover , the shape functions almost possess the kronecker delta function property , and essential boundary conditions can be implemented without any difficulties . numerical results show that arbitrary polygon shape nodal influence domain not only has high computational accuracy , but also enhances computational efficiency of meshless method greatly .
function defined shape metamorphoses in visual cyberworlds . <eos> animated shape transformations should be an intrinsic part of visual cyberworlds . however , quite often only limited animation of the polygon based shapes can be found there , specifically when using the virtual reality modeling language ( vrml ) and its successor extensible 3d ( x3d ) . this greatly limits the expressive power of visual cyberworlds and has motivated our research in this direction . in this paper , we present function based extensions of vrml and x3d , which allow for time dependent shape modeling on the web . our shape modeling approach is based on the concurrent use of implicit , explicit and parametric functions defining geometry , appearance and their transformations through time . the functions are typed straight in vrml x3d code as individual formulas and as function scripts . we have also developed a web enabled interactive software tool for modeling function based vrml x3d objects .
crosstalk analysis for a cmos gate driven coupled interconnects . <eos> this paper deals in crosstalk analysis of a cmos gate driven capacitively and inductively coupled interconnect . alpha power law model of a mos transistor is used to represent a cmos driver . this is combined with a transmission line based coupled interconnect model to develop a composite driver interconnect load model for analytical purposes . on this basis , a transient analysis of crosstalk noise is carried out . comparison of the analytical results with spice extracted results shows that the average error involved in estimating noise peak and their time of occurrence is less than <digit> % .
blocking reduction strategies in hierarchical text classification . <eos> one common approach in hierarchical text classification involves associating classifiers with nodes in the category tree and classifying text documents in a top down manner . classification methods using this top down approach can scale well and cope with changes to the category trees . however , all these methods suffer from blocking which refers to documents wrongly rejected by the classifiers at higher levels and can not be passed to the classifiers at lower levels . in this paper , we propose a classifier centric performance measure known as blocking factor to determine the extent of the blocking . three methods are proposed to address the blocking problem , namely , threshold reduction , restricted voting , and extended multiplicative . our experiments using support vector machine ( svm ) classifiers on the reuters collection have shown that they all could reduce blocking and improve the classification accuracy . our experiments have also shown that the restricted voting method delivered the best performance .
kernel ellipsoidal trimming . <eos> ellipsoid estimation is important in many practical areas such as control , system identification , visual audio tracking , experimental design , data mining , robust statistics and statistical outlier or novelty detection . a new method , called kernel minimum volume covering ellipsoid ( kmvce ) estimation , that finds an ellipsoid in a kernel defined feature space is presented . although the method is very general and can be applied to many of the aforementioned problems , the main focus is on the problem of statistical novelty outlier detection . a simple iterative algorithm based on mahalanobis type distances in the kernel defined feature space is proposed for practical implementation . the probability that a non outlier is misidentified by our algorithms is analyzed using bounds based on rademacher complexity . the kmvce method performs very well on a set of real life and simulated datasets , when compared with standard kernel based novelty detection methods .
supervised fuzzy logic modeling for building earthquake hazard assessment . <eos> building hazard assessment prior to earthquake occurrence exposes interesting problems especially in earthquake prone areas . such an assessment provides an early warning system for building owners as well as the local and central administrators about the possible hazards that may occur in the next scenario earthquake event , and hence pre and post earthquake preparedness can be arranged according to a systematic program . for such an achievement , it is necessary to have efficient models for the prediction of hazard scale of each building within the study area . although there are subjective intensity index methods for such evaluations , the objective of this paper is to propose a useful tool through fuzzy logic ( fl ) to classify the buildings that would be vulnerable to earthquake hazard . the fl is a soft computing intelligent reasoning methodology , which is rapid , simple and easily applicable with logical and rational association between the building hazard categories and the most effective factors . in this paper , among the most important factors are the story number ( building height ) , story height ratio , cantilever extension ratio , moment of inertia ( stiffness ) , number of frames , column and shear wall area percentages . their relationships with the five hazard categories are presented through a supervised hazard center classification method . these five categories are none , slight , moderate , extensive , and complete hazard classes . a new supervised fl classification methodology is proposed similar to the classical fuzzy c means procedure for the allocation of hazard categories to individual buildings . the application of the methodology is presented for zeytinburnu quarter of istanbul city , turkey . it is observed that out of <digit> inventoried buildings 7.6 % , 50.0 % , 14.6 % , 20.1 % , and 7.7 % are subject to expected earthquake with none , slight , moderate , extensive , and complete hazard classes , respectively .
color texture segmentation based on image pixel classification . <eos> image segmentation partitions an image into nonoverlapping regions , which ideally should be meaningful for a certain purpose . thus , image segmentation plays an important role in many multimedia applications . in recent years , many image segmentation algorithms have been developed , but they are often very complex and some undesired results occur frequently . by combination of fuzzy support vector machine ( fsvm ) and fuzzy c means ( fcm ) , a color texture segmentation based on image pixel classification is proposed in this paper . specifically , we first extract the pixel level color feature and texture feature of the image via the local spatial similarity measure model and localized fourier transform , which is used as input of fsvm model ( classifier ) . we then train the fsvm model ( classifier ) by using fcm with the extracted pixel level features . color image segmentation can be then performed through the trained fsvm model ( classifier ) . compared with three other segmentation algorithms , the results show that the proposed algorithm is more effective in color image segmentation .
museqor multi path failure tolerant security aware qos routing in ad hoc wireless networks . <eos> in this paper , we present museqor a new multi path routing , protocol that tackles the twin issues of reliability ( protection against failures of multiple paths ) and security , while ensuring minimum data redundancy . unlike ill all the previous studies . reliability is addressed in the context of both erasure and corruption channels . we also quantify the security of the protocol in terms of the number of eavesdropping nodes . the reliability and security requirements of a session are specified by a user and are related to the parameters of the protocol adaptively . this relationship is of central importance and shows how the protocol attempts to simultaneously achieve reliability and security . in addition . by using optimal coding schemes and by dispersing the original data . we minimize the redundancy . finally , extensive simulations were performed to assess the performance of the protocol under varying network conditions . the simulation studies clearly indicate the gains in using such a protocol and also highlight the enormous flexibility of the protocol . ( c ) <digit> elsevier b.v. all rights reserved .
robust location estimation under dependence . <eos> we discuss the approximation of the mean of autocorrelated data under contaminations of different types . many robust location estimators have been investigated carefully for independent data , but their properties have not been studied in detail under dependencies . we pay attention to estimators based on subranges like minimum volume ellipsoid and minimum covariance determinant estimators , mid ranges and trimmed means , of which the sample mean and median are special cases , and also include the hodges lehmann and the bickel hodges estimators . our interest is in small to moderate sample sizes .
determination of the rank of an integration lattice . <eos> the continuing and widespread use of lattice rules for high dimensional numerical quadrature is driving the development of a rich and detailed theory . part of this theory is devoted to computer searches for rules , appropriate to particular situations . in some applications , one is interested in obtaining the ( lattice ) rank of a lattice rule q ( lambda ) directly from the elements of a generator matrix b ( possibly in upper triangular lattice form ) of the corresponding dual lattice lambda ( perpendicular to ) . we treat this problem in detail , demonstrating the connections between this ( lattice ) rank and the conventional matrix rank deficiency of modulo p versions of b.
using cell phone data to measure quality of service and passenger flows of paris transit system . <eos> cell phone data are used to measure passenger flows in the underground part of paris transit system . travel times , trains level of occupancy and origindestination flows are measured . the measures are consistent with field observations , and with estimates from automated fare collection data . having an independent real time measure of train occupancy can be beneficial to the quality of the system .
a genetic algorithm based heuristic to the multi period fixed charge distribution problem . <eos> this paper proposes a genetic algorithm ( ga ) based heuristic to the multi period fixed charge distribution problem associated with backorder and inventories . the objective is to determine the size of the shipments , backorder and inventories at each period , so that , the total cost incurred during the entire period towards transportation , backorder and inventories is minimum . the model is formulated as pure integer nonlinear programming and <digit> mixed integer linear programming problems , and proposes a ga based heuristic to provide solution to the above problem . the proposed ga based heuristic is evaluated by comparing their solutions with lower bound , lingo solver and approximate solutions . the comparisons reveal that the ga generates better solutions than the approximate solutions , and is capable of providing solutions equal to lingo solutions and closer to the lower bound value of the problems .
an explorational exhibit of a pig 's heart . <eos> coronary heart diseases ( chd ) are one of the main causes of deaths in the united states . although it is well known that chd mainly occurs due to blocked arteries , many of the specifics of this disease are still subject to current research . it is commonly accepted that certain factors , such as a cholesterol high diet , increase the risk of coronary heart disease . as a consequence , people should be educated to adhere a diet low in low density lipoprotein ( ldl or bad cholesterol ) . in order for children to become familiar with these facts , educational , explorative computer systems can be employed to raise some awareness . this poster describes an educational computer system for children that serves this purpose . while practicing their navigation skills , the children can learn about the various types of blood cells and particles within the blood stream . a geometric model of the arterial vascular system of the heart has been developed , which considers vessels of different sizes . an interactive fly through using a standard game controller facilitates the exploration of the interior structure of the vasculature . a blood flow simulation including several different particles within the blood stream allows the young explorer to understand their functionality . this system has been deployed as an interactive museum exhibit for children . the primary age group addressed by the science museum where it is currently being displayed is <digit> <digit> years . with proper guidance by the museum personnel and the instructional material provided at the exhibit the game is also suitable for slightly younger and much older children.the implemented system simulates a submarine style navigation through the blood stream inside an arterial vascular tree of a heart . the vasculature is based on a computed tomography ( ct ) scan of a pig 's heart . the user has full control over the navigation by using a logitech wingman cordless rumblepad as input device . this controller provides two analog joysticks that can be used to achieve six degrees of freedom input . in this application , the user controls forward and backward movement ( acceleration and deceleration ) with the left joystick while changing the orientation ( left , right , up and down ) by using the right joystick . collision detection with the vessel walls ensures that the vasculature can not be left . on collision with the vessel wall as well as with any of the particles within the blood stream force feedback is provided by using the rumble feature of the input device . in addition , audio feedback with different types of sounds allows the player to distinguish between the different types of collisions . consequently , the user has complete manual control over the navigation while visual , audio , and force feedback provided by the system results in an easy to understand assessment of what is happening . this is especially important since the targeted audience are children of a relatively young age.the software is scalable in terms of the physical size of the blood vessel systems and the amount of geometry data that is used to represent it . it can be ported to various virtual environments ( ves ) . at this point , it has been tested on a regular desktop computer and a large projection screen at the museum site . especially the projection screen , which was used for the interactive exhibit , allows a user to fully immerse herself himself into the scene . overall , this computer system gives hands on experience of the functions of the circulatory system of the heart and exposes the user to the various particles present in the blood stream . as a museum exhibit , it was very well received by the targeted audience , i.e. , by children between the age of four and nine , and beyond . the learning experience in the virtual environment was validated in a conversation during a complementing stage performance , which included a scientist dissecting a real pig 's heart , where the children were asked to identify anatomical parts and discuss the importance of the circulatory system .
education reform and its needs for technical standards . <eos> this paper discusses some of the leading concepts in education reform and their need for technical standards . it also discusses the efforts by several organizations to develop such standards and specifications , and how new stakeholders can get involved or monitor this work . there are a variety of reform efforts being advocated and pursued by researchers , educators , learning institutions , corporate trainers , and government leaders . concepts such as student centered learning , computer based training , on line learning , distance learning , just in time learning , and self learning are widely accepted as having the potential to substantially improve the efficiency and effectiveness of learning . all of these concepts have the need for one or more underlying technical standards . this paper describes a number of these standards and the work that is being done to develop them . it is important to note that these technical standards are independent from the content material ( known as content standards ) that students would be required to learn , as well as from the amount of that content ( known as performance standards ) a student would be expected to master .
rationing mechanisms and inventory control policy parameters for a divergent supply chain operating with lost sales and costs of review . <eos> we consider a static divergent two stage supply chain with one distributor and many retailers . the unsatisfied demands at the retailers end are treated as lost sales , whereas the unsatisfied demand is assumed to be backlogged at the distributor . the distributor uses an inventory rationing mechanism to distribute the available on hand inventory among the retailers , when the sum of demands from the retailers is greater than the on hand inventory at the distributor . the present study aims at determining the best installation inventory control policy or order policy parameters such as the base stock levels and review periods , and inventory rationing quantities , with the objective of minimizing the total supply chain costs ( tscc ) consisting of holding costs , shortage costs and review costs in the supply chain over a finite planning horizon . an exact solution procedure involving a mathematical programming model is developed to determine the optimum tscc , base stock levels , review periods and inventory rationing quantities ( in the class of periodic review , order up to s policy ) for the supply chain model under study . on account of the computational complexity involved in optimally solving problems over a large finite time horizon , a genetic algorithm ( ga ) based heuristic methodology is presented .
a globally conforming method for solving flow in discrete fracture networks using the virtual element method . <eos> the virtual element method allows for meshes made up by arbitrary polygonal elements . guaranteed local and global conformity with no alteration of the geometry of the dfn . unconstrained fracture independent meshing . application of domain decomposition preconditioners .
multi objective optimization of facility planning for energy intensive companies . <eos> because of the energy shortage and energy price rise , energy efficiency becomes a worldwide hot spot problem . it is not only a problem about cost reduction , but also a great contribute to the environmental protection . however , the energy efficiency was always ignored in the past decades . in order to gain more benefit and become more competitive in the market , energy efficiency should be considered as an essential factor in early planning phase . to overcome these problems , a new approach , which introduces energy efficiency as a key criterion into the planning process , is presented in this article . an energy recovery network is built according to the analysis of process and product demands . afterwards the energy loss of the whole system , transport performance and space demand are simultaneously taken into account with the purpose of finding good facility planning from both energy and economic aspects . finally , a practical expanding case is used to validate the correctness and effectiveness of the proposed approach .
sparse margin based discriminant analysis for feature extraction . <eos> the existing margin based discriminant analysis methods such as nonparametric discriminant analysis use k nearest neighbor ( k nn ) technique to characterize the margin . the manifold learning based methods use k nn technique to characterize the local structure . these methods encounter a common problem , that is , the nearest neighbor parameter k should be chosen in advance . how to choose an optimal k is a theoretically difficult problem . in this paper , we present a new margin characterization method named sparse margin based discriminant analysis ( smda ) using the sparse representation . smda can successfully avoid the difficulty of parameter selection . sparse representation can be considered as a generalization of k nn technique . for a test sample , it can adaptively select the training samples that give the most compact representation . we characterize the margin by sparse representation . the proposed method is evaluated by using ar , extended yale b database , and the cenparmi handwritten numeral database . experimental results show the effectiveness of the proposed method its performance is better than some other state of the art feature extraction methods .
market application of the percolation model relative price distribution . <eos> we study a variant of the cont bouchaud model , which utilizes the percolation approach of multi agent simulations of the stock market fluctuations . here , instead of considering the relative price change as the difference of the total demand and total supply , we consider the relative price change to be proportional to the relative difference of demand and supply ( the ratio of the difference in total demand and total supply to the sum of the total demand and total supply ) . we then study the probability distribution of the price changes .
feedback control strategies for spatial navigation revealed by dynamic modelling of learning in the morris water maze . <eos> the morris water maze is an experimental procedure in which animals learn to escape swimming in a pool using environmental cues . despite its success in neuroscience and psychology for studying spatial learning and memory , the exact mnemonic and navigational demands of the task are not well understood . here , we provide a mathematical model of rat swimming dynamics on a behavioural level . the model consists of a random walk , a heading change and a feedback control component in which learning is reflected in parameter changes of the feedback mechanism . the simplicity of the model renders it accessible and useful for analysis of experiments in which swimming paths are recorded . here , we used the model to analyse an experiment in which rats were trained to find the platform with either three or one extramaze cue . results indicate that the <digit> cues group employs stronger feedback relying only on the actual visual input , whereas the <digit> cue group employs weaker feedback relying to some extent on memory . because the model parameters are linked to neurological processes , identifying different parameter values suggests the activation of different neuronal pathways .
a new regime for highly robust gamma oscillation with co exist of accurate and weak synchronization in excitatoryinhibitory networks . <eos> a great number of biological experiments show that gamma oscillation occurs in many brain areas after the presentation of stimulus . the neural systems in these brain areas are highly heterogeneous . specifically , the neurons and synapses in these neural systems are diversified the external inputs and parameters of these neurons and synapses are heterogeneous . how the gamma oscillation generated in such highly heterogeneous networks remains a challenging problem . aiming at this problem , a highly heterogeneous complex network model that takes account of many aspects of real neural circuits was constructed . the network model consists of excitatory neurons and fast spiking interneurons , has three types of synapses ( gabaa , ampa , and nmda ) , and has highly heterogeneous external drive currents . we found a new regime for robust gamma oscillation , i.e. the oscillation in inhibitory neurons is rather accurate but the oscillation in excitatory neurons is weak , in such highly heterogeneous neural networks . we also found that the mechanism of the oscillation is a mixture of interneuron gamma ( ing ) and pyramidal interneuron gamma ( ping ) . we explained the mixture ing and ping mechanism in a consistent way by a compound post synaptic current , which has a slowly rising excitatory stage and a sharp decreasing inhibitory stage .
oxygen incorporation into cds cdte thin film solar cells . <eos> cds cdte thin films with 2.1 ( upmu hbox m ) thickness were grown using r.f. magnetron sputtering in two different mixtures of ar and ( hbox o _ <digit> ) . the substrate was a commercially available pilkington glass with tco deposited . the concentration of ( hbox o _ <digit> ) was selected to be <digit> , <digit> and <digit> % . the crystallographic , morphological , optical and electrical properties of the as deposited samples were compared with the ones treated with ( hbox cdcl _ <digit> ) and subsequently annealed at high temperature . the films morphology and crystallinity were studied by x ray diffraction and scanning electron microscopy . x ray diffraction shows a transition of zinc blend cubic phase to hexagonal as the oxygen content increases from <digit> to <digit> % . the measurements show the larger band gap and grain sizes for the films with higher oxygen content . the band gap and transmission rate of the o ( _ <digit> ) free and oxygenated devices is different and the grains size is greatly affected by the oxygen content .
abstracting runtime heaps for program understanding . <eos> modern programming environments provide extensive support for inspecting , analyzing , and testing programs based on the algorithmic structure of a program . unfortunately , support for inspecting and understanding runtime data structures during execution is typically much more limited . this paper provides a general purpose technique for abstracting and summarizing entire runtime heaps . we describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging , comparing , and computing changes between abstract models . the abstract model is designed to emphasize high level concepts about heap based data structures , such as shape and size , as well as relationships between heap structures , such as sharing and connectivity . we demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler . we use this tool to identify , pinpoint , and correct sources of memory bloat for programs from dacapo .
on total domination vertex critical graphs of high connectivity . <eos> a graph is called critical if the removal of any vertex from the graph decreases the domination number , while a graph with no isolated vertex is t t critical if the removal of any vertex that is not adjacent to a vertex of degree <digit> decreases the total domination number . a t t critical graph that has total domination numberk k , is called k k t t critical . in this paper , we introduce a class of k k t t critical graphs of high connectivity for each integer k <digit> k <digit> . in particular , we provide a partial answer to the question which graphs are critical and t t critical or one but not the other posed in a recent work w. goddard , t.w. haynes , m.a. henning , l.c. van der merwe , the diameter of total domination vertex critical graphs , discrete math . <digit> ( <digit> ) <digit> .
idd based model validation of biochemical networks . <eos> this paper presents efficient techniques for the qualitative and quantitative analysis of biochemical networks , which are modeled by means of qualitative and stochastic petri nets , respectively . the analysis includes standard petri net properties as well as model checking of the computation tree logic and the continuous stochastic logic . efficiency is achieved by using interval decision diagrams to alleviate the well known problem of state space explosion , and by applying operations exploiting the petri structure and the principle of locality . all presented techniques are implemented in our tool idd mc which is available on our website . ( c ) <digit> elsevier b.v. all rights reserved .
the form of the solution and dynamics of a rational recursive sequence . <eos> we discuss in this paper the form of the solutions of the following recursive sequences x ( n <digit> ) x ( n <digit> ) x ( n <digit> ) x ( n ) ( <digit> x ( n <digit> ) x ( n <digit> ) ) , n <digit> , <digit> , ... , where the initial conditions are arbitrary real numbers . moreover , we study the dynamics and behavior of the solutions .
real time self maintenable data warehouse . <eos> data warehousing is an approach to data integration wherein integrated information is stored in a data warehouse for direct querying and analysis . to provide fast access , a data warehouse stores materialized views of the sources of its data . as a result , a data warehouse needs to be maintained to keep its contents consistent with the contents of its data sources . incremental maintenance is generally regarded as a more efficient way to maintain materialized views in a data warehouse . in this paper a strategy for the maintenance of data warehouse is presented . it has the following characteristics it is self maintainable ( weak ) , incremental , non blocking ( the analysts transactions and the maintenance transaction are executed concurrently ) and is performed in real time . the proposed algorithm is implemented for view definition spj ( select project join ) queries and it calculates the aggregate functions sum , avg , count , min and max . aggregate functions are calculated like algebraic functions ( the new result of the function can be computed using some small , constant size storage that accompanies the existing value of the aggregate ) . we have named this improved algorithm vnltr ( unlimited v ( versions ) , nl ( non blocking ) , tr ( in real time ) ) .
constructing error correcting pooling designs with symplectic space . <eos> we construct a family of error correcting pooling designs with the incidence matrix of two types of subspaces of symplectic spaces over finite fields . we show that the new construction gives better ratio of efficiency compared with previously known three constructions associated with subsets of a set , its analogue over a vector space , and the dual spaces of a symplectic space .
improving the memory behavior of vertical filtering in the discrete wavelet transform . <eos> the discrete wavelet transform ( dwt ) is used in several image and video compression standards , in particular jpeg2000 . a 2d dwt consists of horizontal filtering along the rows followed by vertical filtering along the columns . it is well known that a straightforward implementation of vertical filtering ( assuming a row major layout ) induces many cache misses , due to lack of spatial locality . this can be avoided by interchanging the loops . this paper shows , however , that the resulting implementation suffers significantly from 64k aliasing , which occurs in the pentium <digit> when two data blocks are accessed that are a multiple of 64k apart , and we propose two techniques to avoid it . in addition , if the filter length is longer than four , the number of ways of the l1 data cache of the pentium <digit> is insufficient to avoid cache conflict misses . consequently , we propose two methods for reducing conflict misses . although experimental results have been collected on the pentium <digit> , the techniques are general and can be applied to other processors with different cache organizations as well . the proposed techniques improve the performance of vertical filtering compared to already optimized baseline implementations by a factor of 3.11 for the ( 5,3 ) lifting scheme , 3.11 for daubechies ' transform of four coefficients , and by a factor of 1.99 for the cohen , daubechies , and feauveau <digit> <digit> transform .
analysis of multibackground memory testing techniques . <eos> march tests are widely used in the process of ram testing . this family of tests is very efficient in the case of simple faults such as stuck at or transition faults . in the case of a complex fault model such as pattern sensitive faults their efficiency is not sufficient . therefore we have to use other techniques to increase fault coverage for complex faults . multibackground memory testing is one of such techniques . in this case a selected march test is run many times . each time it is run with new initial conditions . one of the conditions which we can change is the initial memory background . in this paper we compare the efficiency of multibackground tests based on four different algorithms of background generation .
detecting image spam using visual features and near duplicate detection . <eos> email spam is a much studied topic , but even though current email spam detecting software has been gaining a competitive edge against text based email spam , new advances in spam generation have posed a new challenge image based spam . image based spam is email which includes embedded images containing the spam messages , but in binary format . in this paper , we study the characteristics of image spam to propose two solutions for detecting image based spam , while drawing a comparison with the existing techniques . the first solution , which uses the visual features for classification , offers an accuracy of about <digit> % , i.e. an improvement of at least <digit> % compared to existing solutions . svms ( support vector machines ) are used to train classifiers using judiciously decided color , texture and shape features . the second solution offers a novel approach for near duplication detection in images . it involves clustering of image gmms ( gaussian mixture models ) based on the agglomerative information bottleneck ( aib ) principle , using jensen shannon divergence ( js ) as the distance measure .
exploiting local intensity information in chanvese model for noisy image segmentation . <eos> this manuscript presents an improved region based active contour model for noisy image segmentation . we define a local energy according to intensity information within the neighborhood of each point in image domain . by introducing a kernel function , our method employs intensity information in local region to guide the motion of active contour . experiments on synthetic and real world images show that our model is robust to image noise while preserving the segmentation efficacy .
a probabilistic spectral framework for grouping and segmentation . <eos> this paper presents an iterative spectral framework for pairwise clustering and perceptual grouping . our model is expressed in terms of two sets of parameters . firstly , there are cluster memberships which represent the affinity of objects to clusters . secondly , there is a matrix of link weights for pairs of tokens . we adopt a model in which these two sets of variables are governed by a bernoulli model . we show how the likelihood function resulting from this model may be maximised with respect to both the elements of link weight matrix and the cluster membership variables . we establish the link between the maximisation of the log likelihood function and the eigenvectors of the link weight matrix . this leads us to an algorithm in which we iteratively update the link weight matrix by repeatedly refining its modal structure . each iteration of the algorithm is a three step process . first , we compute a link weight matrix for each cluster by taking the outer product of the vectors of current cluster membership indicators for that cluster . second , we extract the leading eigenvector from each modal link weight matrix . third , we compute a revised link weight matrix by taking the sum of the outer products of the leading eigenvectors of the modal link weight matrices .
composition of least privilege analysis results in software architectures ( position paper ) . <eos> security principles are often neglected by software architects , due to the lack of precise definitions . this results in potentially high risk threats to systems . our own previous work tackled this by introducing formal foundations for the least privilege ( lp ) principle in software architectures and providing a technique to identify violations to this principle . this work shows that this technique can scale by composing the results obtained from the analysis of the sub parts of a larger system . the technique decomposes the system into independently described subsystems and a description listing the interactions between these subsystems . these descriptions are thence analyzed to obtain lp violations and subsequently composed to obtain the violations of the overall system .
a software based eye tracking system for the study of air traffic displays . <eos> this paper describes a software based system for offline tracking of eye and head movements using stored video images , designed for use in the study of air traffic displays . these displays are typically dense with information to address the research questions , we wish to be able to localize gaze within a single word within a line of text ( a few minutes of arc ) , while at the same time allowing some freedom of movement to the subject . accurate gaze tracking in the presence of head movements requires high precision head tracking , and this was accomplished by registration of images from a forward looking scene camera with a narrow field of view .
a new lagrangian relaxation algorithm for scheduling dissimilar parallel machines with release dates . <eos> in this article we investigate the parallel machine scheduling problem with job release dates , focusing on the case that machines are dissimilar with each other . the goal of scheduling is to find an assignment and sequence for a set of jobs so that the total weighted completion time is minimised . this type of production environment is frequently encountered in process industry , such as chemical and steel industries , where the scheduling of jobs with different purposes is an important goal . this article formulates the problem as an integer linear programming model . because of the dissimilarity of machines , the ordinary job based decomposition method is no longer applicable , a novel machine based lagrangian relaxation algorithm is therefore proposed . penalty terms associated with violations of coupling constraints are introduced to the objective function by lagrangian multipliers , which are updated using subgradient optimisation method . for each machine level subproblem after decomposition , a forward dynamic programming algorithm is designed together with the weighted shortest processing time rule to provide an optimal solution . a heuristics is developed to obtain a feasible schedule from the solution of subproblems to provide an upper bound . numerical results show that the new approach is computationally effective to handle the addressed problem and provide high quality schedules .
fast chromatography of complex biocide mixtures using diode array detection and multivariate curve resolution . <eos> use of multivariate curve resolution alternating least squares ( mcr als ) is evaluated in the analysis of complex biocide environmental sample mixtures by liquid chromatography with diode array detection ( lc dad ) . chromatographic coelution problems caused either because of the presence of unknown matrix interferences or because of using short chromatographic columns to reduce analysis times are investigated . under such circumstances , lack of chromatographic resolution and lack of spectral selectivity of uv vis diode array detection is compensated by chemometric resolution using multivariate curve resolution . resolution of complex environmental mixtures and quantitative calibration curves for two types of chromatographic columns ( <digit> and 7.5 cm ) with different resolution and analysis times are shown . the limits of the proposed approach are investigated in the analysis of complex environmental samples with short lc columns and uv vis diode array detection . ( c ) <digit> elsevier b.v. all rights reserved .
<digit> channel micro magnetic flux sensor array for igbt current distribution measurement . <eos> current crowding of igbt and power diode in a chip or among chips is a barrier to the realization of highly reliable power module . the author developed and demonstrated <digit> channel flat sensitivity sensor array for igbt current distribution measurement . the sensor array consists of tiny scale film sensors with analog amps and shield case against noise .
state space reduction in modeling checking parameterized cache coherence protocol by two dimensional abstraction . <eos> scalability of cache coherence protocol is a key component in future shared memory multi core or multi processor systems . the state space explosion is the first hurdle while applying model checking to scalable protocols . in order to validate parameterized cache coherence protocols effectively , we present a new method of reducing the state space of parameterized systems , two dimensional abstraction ( tda ) . drawing inspiration from the design principle of parameterized systems , an abstract model of an unbounded system is constructed out of finite states . the mathematical principles underlying tda is presented . theoretical reasoning demonstrates that tda is correct and sound . an example of parameterized cache coherence protocol based on mesi illustrates how to produce a much smaller abstract model by tda . we also demonstrate the power of our method by applying it to various well known classes of protocols . during the development of th 1a supercomputer system , tda was used to verify the coherence protocol in ft <digit> cpu and showed the potential advantages in reducing the verification complexity .
a new antialiased line drawing algorithm . <eos> consider a line . conventional line drawing algorithms sample ( x , f ( x ) ) on the line , where x must be an integer , and then map ( x , f ( x ) ) to the frame buffer according to the defined filter and f ( x ) . in this paper , we propose to simulate a sampled point ( x , f ( x ) ) by the four pixels around it where x and f ( x ) are not necessary to be integers . based on the proposed low pass filtering , we show that the effect of sampling at infinite number of points along a line segment can be achieved since the closed form of the intensities assigned to pixels exists . furthermore , we show the coherence properties that can reduce the cost for computing these intensities .
high performance short sequence alignment with gpu acceleration . <eos> sequence alignment is a fundamental task for computational genomics research . we develop g aligner , which adopts the gpu as a hardware accelerator to speed up the sequence alignment process . a leading cpu based alignment tool is based on the bi bwt index however , a direct implementation of this algorithm on the gpu can not fully utilize the hardware power due to its irregular algorithmic structure . to better utilize the gpu hardware resource , we propose a filtering verification algorithm employing both the bi bwt search and direct matching . we further improve this algorithm on the gpu through various optimizations , e.g. , the split of a large kernel , the warp based implementation to avoid user level synchronization . as a result , g aligner outperforms another state of the art gpu accelerated alignment tools soap3 by 1.83.5 times for in memory sequence alignment .
to feel or not to feel the role of affect in human computer interaction . <eos> the past decade has witnessed an unprecedented growth in user interface and human computer interaction ( hci ) technologies and methods . the synergy of technological and methodological progress on the one hand , and changing user expectations on the other , are contributing to a redefinition of the requirements for effective and desirable human computer interaction . a key component of these emerging requirements , and of effective hci in general , is the ability of these emerging systems to address user affect . the objective of this special issue is to provide an introduction to the emerging research area of affective hci , some of the available methods and techniques , and representative systems and applications . ( c ) <digit> elsevier science ltd. all rights reserved .
exponential stability of a class of generalized neural networks with time varying delays . <eos> the dynamics of a class of generalized neural networks with time varying delays are analyzed . without constructing a lyapunov function , general sufficient conditions for the existence , uniqueness and exponential stability of an equilibrium of the neural networks are obtained by the nonlinear lipschitz measure approach . the new criteria are mild , independent of the delays and do not require the boundedness , differentiability or monotonicity assumption of the activation functions . moreover , the proposed results extend and improve existing ones .
sexism in online video games the role of conformity to masculine norms and social dominance orientation . <eos> the video game sexism scale was created to assess attitudes toward female gamers . conformity to some masculine norms predicted video game sexism . social dominance orientation predicted video game sexism .
pseudorandom generators for combinatorial checkerboards . <eos> we define a combinatorial checkerboard to be a function f <digit> , . . . , m ( d ) > <digit> , <digit> of the form for some functions f ( i ) <digit> , . . . , m > <digit> , <digit> . this is a variant of combinatorial rectangles , which can be defined in the same way but using <digit> , <digit> instead of <digit> , <digit> . we consider the problem of constructing explicit pseudorandom generators for combinatorial checkerboards . this is a generalization of small bias generators , which correspond to the case m <digit> . we construct a pseudorandom generator that fools all combinatorial checkerboards with seed length . previous work by impagliazzo , nisan , and wigderson implies a pseudorandom generator with seed length . our seed length is better except when <digit> epsilon > d ( omega ( log d ) ) .
the calendar is crucial coordination and awareness through the family calendar . <eos> everyday family life involves a myriad of mundane activities that need to be planned and coordinated . we describe findings from studies of <digit> different families ' calendaring routines to understand how to best design technology to support them . we outline how a typology of calendars containing family activities is used by three different types of families monocentric , pericentric , and polycentric which vary in the level of family involvement in the calendaring process . we describe these family types , the content of family calendars , the ways in which they are extended through annotations and augmentations , and the implications from these findings for design .
the myth and reality of reversal of aging by hormesis . <eos> hormesis is an adaptive response to low doses of otherwise harmful agents by triggering a cascade of stress specific resistance pathways . evidence from protozoa , nematodes , flies , rodents , and primates indicate that stress induced tolerance modulates survival and longevity . realit is that hormesis can prolong the healthy life span . genetic background provides the potential for longevity duration induced by stress . senesence , or aging , is generally thought to be due to a different impact of selection for alleles positive for reproduction during early life but harmful in later life , a process called antagonistic pleiotropy ( multiple phenotypic changes by a single gene ) . after reproduction , life span is invisible to selection . i propose the revision that mutations selected for survival until reproduction in early life may also extend later life ( protagonistic pleiotropy ) . the protagonist candidate genes for extended life span are hormetic response genes , which activate the protective effect in both early and later life . my revision of the earlier evolutionary theory implies that natural selection of genes critical for early survival ( life span until reproduction ) can also be beneficial for extended longevity in old age , tipping the evolutionary balance in favor of a latent inducible life span extension unless excess stressor challenge exceeds the protection capacity . mimetic triggers of the stress response promise the option of tricking the induction of metabolic pathways that confer resistance to environmental challenges , increased healthy life span , rejuvenation , and disease intervention without the danger of overwhelmiong damage by the stressor . public policy should anticipate an increase in healthy life span .
bimodal hci related affect recognition . <eos> perhaps the most fundamental application of affective computing will be human computer interaction ( hci ) in which the computer should have the ability to detect and track the user 's affective states , and make corresponding feedback . the human multi sensor affect system defines the expectation of multimodal affect analyzer . in this paper , we present our efforts toward audio visual hci related affect recognition . with hci applications in mind , we take into account some special affective states which indicate users ' cognitive motivational states . facing the fact that a facial expression is influenced by both an affective state and speech content , we apply a smoothing method to extract the information of the affective state from facial features . in our fusion stage , a voting method is applied to combine audio and visual modalities so that the final affect recognition accuracy is greatly improved . we test our bimodal affect recognition approach on <digit> subjects with <digit> hci related affect states . the extensive experimental results show that the average person dependent affect recognition accuracy is almost <digit> % for our bimodal fusion .
high order spacetime adaptive ader weno finite volume schemes for non conservative hyperbolic systems . <eos> better than second order accurate spacetime adaptive mesh refinement ( amr ) . time accurate local time stepping ( lts ) . high order ader weno finite volume scheme for non conservative hyperbolic systems . applications to the baernunziato model of compressible multiphase flows in 2d and 3d . very sharp resolution of material interfaces .
evaluating covariance in prognostic and system health management applications . <eos> simulated noisy data sets are used to compare the accuracy of four existing covariance estimation methodologies among the discussed methodologies the nnve algorithm provides the most accurate estimates of covariance . to further improve the accuracy of the covariance estimation , a new methodology based on a modification of the nnve methodology is proposed . the proposed methodology is shown to exhibit improved performance in classification as well as anomaly detection applications .
a comprehensive investigation of wireless lan for iec <digit> based smart distribution substation applications . <eos> today 's power grid is facing many challenges due to increasing load growth , aging of existing power infrastructures , high penetration of renewable , and lack of fast monitoring and control . utilizing recent developments in information and communication technologies ( ict ) at the power distribution level , various smart grid applications can be realized to achieve reliable , efficient , and green power . interoperable exchange of information is already standardized in the globally accepted smart grid standard , iec <digit> , over the local area networks ( lans ) . due to low installation cost , sufficient data rates , and ease of deployment , the industrial wireless lan technologies are gaining interest among power utilities , especially for less critical smart distribution network applications . extensive work is carried out to examine the wireless lan ( wlan ) technology within a power distribution substation . the first phase of the work is initiated with the radio noise interference measurements at 27.6 and 13.8 kv distribution substations , including circuit breaker switching operations . for a detailed investigation , the hardware prototypes of wlan enabled iec <digit> devices are developed using industrial embedded systems , and the performance of smart distribution substation monitoring , control , and protection applications is analyzed for various scenarios using a round trip time of iec <digit> application messages . finally , to examine the real world field performance , the developed prototype devices are installed in the switchyard and control room of 27.6 power distribution substation , and testing results of various applications are discussed .
an assessment of the effect of mass customization on suppliers inventory levels in a jit supply chain . <eos> in some industries , mass customization requires a supplier to provide an original equipment manufacturer ( oem ) with a wide range of variants of a given part . we consider an oem parts suppliers system for an automotive supply chain where parts are delivered to the assembly line several times a day in a just in time environment . simulating varying assembly schedule and parts delivery schemes , we assess the effect of mass customization on the level of inventory the supplier needs for each variant in order to prevent stockouts . we find , among other things , that as the level of mass customization increases , there tends to be an increase in the level of inventory the supplier needs to maintain for each part variant in order to prevent stockouts . theoretical support is provided for the phenomenon . the presented framework is also useful for evaluating the levels of mass customization that will enable the manufacturer meet customers requirements in a cost effective manner . furthermore , the study confirms the superiority , in terms of inventory levels , of the minmax over the minsum optimization framework .
on the interaction between knowledge and social commitments in multi agent systems . <eos> both knowledge and social commitments have received considerable attention in multi agent systems ( mass ) , specially for multi agent communication . plenty of work has been carried out to define their semantics . however , the relationship between social commitments and knowledge has not been investigated yet . in this paper , we aim to explore such a relationship from the semantics and model checking perspectives with respect to ctlk logic ( an extension of ctl logic with modality for reasoning about knowledge ) and ctlc logic ( an extension of ctl with modalities for reasoning about commitments and their fulfillments ) . to analyze this logical relationship , we simply combine the two logics in one new logic named ctlkc . the purpose of such a combination is not to advocate a new logic , but only to express and figure out some reasoning postulates merging both knowledge and commitments as they are currently defined in the literature . by so doing , we identify some paradoxes in the new logic showing that simply combining current versions of commitment and knowledge logics results in a logical language that violates some fundamental intuitions . consequently , we propose ctlkc , a new logic that fixes the identified paradoxes and allows us to reason about social commitments and knowledge simultaneously in a consistent manner . furthermore , we address the problem of model checking ctlkc by reducing it to the problem of model checking gctl , a generalized version of ctl with action formulae . by doing so , we directly benefit from cwb nc , the model checker of gctl . using this reduction , we also prove that the computational complexity of model checking ctlkc is still pspace complete for concurrent programs as the complexity of model checking ctlk and ctlc separately .
geometric point interpolation method in r3 r <digit> space with tangent directional constraint . <eos> this paper discusses a cubic b b spline interpolation problem with tangent directional constraint in r3 r <digit> space . given m m points and their tangent directional vectors as well , the interpolation problem is to find a cubic b b spline curve which interpolates both the positions of the points and their tangent directional vectors . given the knot vector of the resulting b b spline curve and parameter values to all of the data points , the corresponding control points can often be obtained by solving a system of linear equations . this paper presents a piecewise geometric interpolation method combining a unclamping technique with a knot extension technique , with which there is no need to solve a system of linear equations . it firstly uses geometric methods to construct a seed curve segment , which interpolates several data point pairs , i.e. , positions and tangent directional vectors of the points . the seed segment is then extended to interpolate the remaining data point pairs one by one in a piecewise fashion . we show that a b b spline curve segment can always be extended to interpolate a new data point pair by adding two more control points . methods for a curve segment extending to interpolate one more data point pair by adding one more control point are also provided , which are utilized to construct an interpolation b b spline curve with as small a number of control points as possible . numerical examples show the effectiveness and the efficiency of the new method .
a software package for interactive motor unit potential classification using fuzzy k nn classifier . <eos> we present an interactive software package for implementing the supervised classification task during electromyographic ( emg ) signal decomposition process using a fuzzy k nn classifier and utilizing the matlab high level programming language and its interactive environment . the method employs an assertion based classification that takes into account a combination of motor unit potential ( mup ) shapes and two modes of use of motor unit firing pattern information the passive and the active modes . the developed package consists of several graphical user interfaces used to detect individual mup waveforms from a raw emg signal , extract relevant features , and classify the mups into motor unit potential trains ( mupts ) using assertion based classifiers .
project selection for oil fields development by using the ahp and fuzzy topsis methods . <eos> the evaluation and selection of projects before investment decision is customarily done using , technical and information . in this paper , proposed a new methodology to provide a simple approach to assess alternative projects and help the decision maker to select the best one for national iranian oil company by using six criteria of comparing investment alternatives as criteria in an ahp and fuzzy topsis techniques . the ahp is used to analyze the structure of the project selection problem and to determine weights of the criteria , and fuzzy topsis method is used to obtain final ranking . this application is conducted to illustrate the utilization of the model for the project selection problems . additionally , in the application , it is shown that calculation of the criteria weights is important in fuzzy topsis method and they could change the ranking . the decision maker can use these different weight combinations in the decision making process according to priority .
on 2k variable symmetric boolean functions with maximum algebraic immunity k . <eos> given a positive even integer n , it is found that the weight distribution of any n variable symmetric boolean function with maximum algebraic immunity ( ai ) n <digit> is determined by the binary expansion of n. based on the foregoing , all n variable symmetric boolean functions with maximum ai are constructed . the amount is ( 2wt ( n ) <digit> ) <digit> ( log2n ) .
a theoretical study of transition metal complexes of c60 and c70 and their ring opened alternatives . <eos> ring opened structures of c60 and c70 are shown to be stabilized by complexation with transition metal fragments of the form cnhnm , where n <digit> to <digit> and m cr , mn , fe , co , and rh . the ring opening of c60 and c70 is compared with the reverse process of the well known catalytic conversion of acetylene into benzene . calculations at the semi empirical pm3 ( tm ) level show that the <digit> membered ring in c60 and c70 can be opened up in different ways through complexation with transition metal fragment . the mode of ring opening depends on the number of external <digit> and <digit> membered rings around the <digit> membered ring being cleaved . the structures and energetics of the various ring opened structures are discussed . <digit> by elsevier science inc .
an optimal galerkin scheme to solve the kinematic dynamo eigenvalue problem in a full sphere . <eos> the kinematic dynamo approximation describes the generation of magnetic field in a prescribed flow of electrically conducting liquid . one of its main uses is as a proof of concept tool to test hypotheses about self exciting dynamo action . indeed , it provided the very first quantitative evidence for the possibility of the geodynamo . despite its utility , due to the requirement of resolving fine structures , historically , numerical work has proven difficult and reported solutions were often plagued by poor convergence . in this paper , we demonstrate the numerical superiority of a galerkin scheme in solving the kinematic dynamo eigenvalue problem in a full sphere . after adopting a poloidaltoroidal decomposition and expanding in spherical harmonics , we express the radial dependence in terms of a basis of exponentially convergent orthogonal polynomials . each basis function is constructed from a terse sum of one sided jacobi polynomials that not only satisfies the boundary conditions of matching to an electrically insulating exterior , but is everywhere infinitely differentiable , including at the origin . this galerkin method exhibits more rapid convergence , for a given problem size , than any other scheme hitherto reported , as demonstrated by a benchmark of the magnetic diffusion problem and by comparison to numerous kinematic dynamos from the literature . in the axisymmetric flows we consider in this paper , at a magnetic reynolds number of o ( <digit> ) , a convergence of <digit> significant figures in the most unstable eigenvalue requires only <digit> radial basis functions alternatively , <digit> significant figures requires <digit> radial functions . the terse radial discretization becomes particularly advantageous when considering flows whose associated numerical solution requires a large number of coupled spherical harmonics . we exploit this new method to confirm the tentatively proposed positive growth rate of the planar flow of bachtiar et al. <digit> , thereby verifying a counter example to the zeldovich anti dynamo theorem in a spherical geometry .
logistic regression , adaboost and bregman distances . <eos> we give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of bregman distances . the striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously , and to easily adapt algorithms designed for one problem to the other . for both problems , we give new algorithms and explain their potential advantages over existing methods . these algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially ( one at a time ) or in parallel ( all at once ) . we also describe a parameterized family of algorithms that includes both a sequential and a parallel update algorithm as special cases , thus showing how the sequential and parallel approaches can themselves be unified . for all of the algorithms , we give convergence proofs using a general formalization of the auxiliary function proof technique . as one of our sequential update algorithms is equivalent to adaboost , this provides the first general proof of convergence for adaboost . we show that all of our algorithms generalize easily to the multiclass case , and we contrast the new algorithms with the iterative scaling algorithm . we conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings .
principles of scatter search . <eos> scatter search is an evolutionary method that has been successfully applied to hard optimization problems . the fundamental concepts and principles of the method were first proposed in the 1970s , based on formulations dating back to the 1960s for combining decision rules and problem constraints . in contrast to other evolutionary methods like genetic algorithms , scatter search is founded on the premise that systematic designs and methods for creating new solutions afford significant benefits beyond those derived from recourse to randomization . it uses strategies for search diversification and intensification that have proved effective in a variety of optimization problems . this paper provides the main principles and ideas of scatter search and its generalized form path relinking . we first describe a basic design to give the reader the tools to create relatively simple implementations . more advanced designs derive from the fact that scatter search and path relinking are also intimately related to the tabu search ( ts ) metaheuristic , and gain additional advantage by making use of ts adaptive memory and associated memory exploiting mechanisms capable of being tailored to particular contexts . these and other advanced processes described in the paper facilitate the creation of sophisticated implementations for hard problems that often arise in practical settings . due to their flexibility and proven effectiveness , scatter search and path relinking can be successfully adapted to tackle optimization problems spanning a wide range of applications and a diverse collection of structures , as shown in the papers of this volume .
weakly quasi hamiltonian set connected multipartite tournaments . <eos> a multipartite or c c partite tournament is an orientation of a complete c c partite graph . lu and guo ( submitted for publication ) <digit> recently introduced strong quasi hamiltonian connectivity of a multipartite tournament d d as follows for any two distinct vertices x x and y y of d d , there is a path with at least one vertex from each partite set of d d from x x to y y and from y y to x x . we obtain the definition for weak quasi hamiltonian connectivity , where only one of those paths , and weak quasi hamiltonian set connectivity , where only one such path between every two distinct partite sets has to exist , in a natural way . in this paper , we characterize weakly quasi hamiltonian set connected multipartite tournaments which extends a result of thomassen ( <digit> ) <digit> .
analysing knowledge requirements a case study . <eos> this paper presents the findings of a knowledge audit conducted to determine the knowledge requirements of a large service based enterprise in south africa . the objective of the knowledge audit was to identify and describe the current and future knowledge requirements of the enterprise . the results indicated that employees have some basic knowledge and information needs that must be satisfied before any further investigations take place . once the fundamental building blocks of knowledge content are established , it is recommended that more sophisticated solutions can be developed . broad recommendations for establishing a knowledge management strategy that will be a source of sustainable competitive advantage are proposed .
negative resistance active resistor with improved linearity and frequency response . <eos> an original active resistor circuit will be presented . the main advantages of the new proposed implementations are the improved linearity , small area consumption and improved frequency response . an original technique for linearizing the i ( v ) characteristic of the active resistor will be proposed , based on the utilization of a new linear differential amplifier , and on a current pass circuit . the linearization of the original differential structure is achieved by compensating the quadratic characteristic of the mos transistor operating in the saturation region by an original square root circuit . the errors introduced by the second order effects will be strongly reduced , while the circuit frequency response of the circuit is very good as a result of operating all mos transistors in the saturation region . in order to design a circuit having a negative equivalent resistance , an original method specific to the proposed implementation of the active resistor circuit will be presented . the circuit is implemented in 0.35 mu m cmos technology , the spice simulation con . firming the theoretical estimated results and showing a linearity error under a percent for an extended input range ( 500mv ) and a small value of the supply voltage ( 3v ) .
quantum computation , quantum theory and ai . <eos> the main purpose of this paper is to examine some ( potential ) applications of quantum computation in ai and to review the interplay between quantum theory and ai . for the readers who are not familiar with quantum computation , a brief introduction to it is provided , and a famous but simple quantum algorithm is introduced so that they can appreciate the power of quantum computation . also , a ( quite personal ) survey of quantum computation is presented in order to give the readers a ( unbalanced ) panorama of the field . the author hopes that this paper will be a useful map for ai researchers who are going to explore further and deeper connections between ai and quantum computation as well as quantum theory although some parts of the map are very rough and other parts are empty , and waiting for the readers to fill in . ( c ) <digit> elsevier b.v. all rights reserved .
automatic recognition of lower facial action units . <eos> the face is an important source of information in multimodal communication . facial expressions are generated by contractions of facial muscles , which lead to subtle changes in the area of the eyelids , eye brows , nose , lips and skin texture , often revealed by wrinkles and bulges . to measure these subtle changes , ekman et al. <digit> developed the facial action coding system ( facs ) . facs is a human observer based system designed to detect subtle changes in facial features , and describes facial expressions by action units ( aus ) . we present a technique to automatically recognize lower facial action units , independently from one another . even though we do not explicitly take into account au combinations , thereby making the classification process harder , an average f <digit> score of 94.83 % is achieved .
development , implementation , and analysis of direct integration offline programming method . <eos> wire bond programming ( wbp ) consists of information required to drive a wire bond machines movement during the wire bonding process . wire bond programs consist of three key components material handling , bonding parameter , and bonding path instructions . of the three components , the bonding path component requires effort and time the most to prepare , as the preparation of bond path is currently being carried out manually . the manual process is tedious and error prone . in comparison to a manual process , offline programming ( olp ) of bonding path creation provides a much more reliable and a less tedious method as it is error proof . olp can be categorized into two versions , mainly vendor specific olp and direct integration offline programming ( di olp ) , which is presented in this paper . vendor specific olp utilizes bonding diagrams created by a computer aided design program to generate wire bonding paths . di olp on the other hand utilizes the numeric coordinate data extracted from the bonding diagram creation software to generate the bonding path component of the wire bond program . di olp is a more flexible method as it has the potential to be adapted to different machine platforms . this paper explains the challenges in the implementation of di olp . the effectiveness and efficiency of the program created by di olp are evaluated as compared to a manual programming method . final results indicate that the offline programming is more efficient as it greatly reduces the time required to create the bonding paths for wire bond programs as compared to the manual methodology .
